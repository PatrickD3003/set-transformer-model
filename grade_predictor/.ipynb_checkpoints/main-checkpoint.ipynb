{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfe7aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import torch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d7d8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "import csv\n",
    "from modules_modified import ISAB, SAB, PMA\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31d760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models\n",
    "from model import (\n",
    "    SetTransformerClassifierXY,\n",
    "    SetTransformerClassifierXYAdditive,\n",
    "    SetTransformerClassifier,\n",
    "    DeepSetClassifierXYAdditive,\n",
    "    DeepSetClassifierXY,\n",
    "    DeepSetClassifier,\n",
    "    SetTransformerOrdinalXY,\n",
    "    SetTransformerOrdinalXYAdditive,\n",
    "    SetTransformerOrdinal,\n",
    "    DeepSetOrdinalXYAdditive,\n",
    "    DeepSetOrdinalXY,\n",
    "    DeepSetOrdinal,\n",
    "    SoftVotingEnsemble,\n",
    "    GeometricMeanEnsemble,\n",
    "    MedianEnsemble,\n",
    "    TrimmedMeanEnsemble,\n",
    "    StackingEnsemble,\n",
    ")\n",
    "from utils_ordinal import ordinal_logistic_loss, cumulative_to_labels, threshold_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0e8bb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A1': 0, 'B1': 1, 'C1': 2, 'D1': 3, 'E1': 4, 'F1': 5, 'G1': 6, 'H1': 7, 'I1': 8, 'J1': 9, 'K1': 10, 'A2': 11, 'B2': 12, 'C2': 13, 'D2': 14, 'E2': 15, 'F2': 16, 'G2': 17, 'H2': 18, 'I2': 19, 'J2': 20, 'K2': 21, 'A3': 22, 'B3': 23, 'C3': 24, 'D3': 25, 'E3': 26, 'F3': 27, 'G3': 28, 'H3': 29, 'I3': 30, 'J3': 31, 'K3': 32, 'A4': 33, 'B4': 34, 'C4': 35, 'D4': 36, 'E4': 37, 'F4': 38, 'G4': 39, 'H4': 40, 'I4': 41, 'J4': 42, 'K4': 43, 'A5': 44, 'B5': 45, 'C5': 46, 'D5': 47, 'E5': 48, 'F5': 49, 'G5': 50, 'H5': 51, 'I5': 52, 'J5': 53, 'K5': 54, 'A6': 55, 'B6': 56, 'C6': 57, 'D6': 58, 'E6': 59, 'F6': 60, 'G6': 61, 'H6': 62, 'I6': 63, 'J6': 64, 'K6': 65, 'A7': 66, 'B7': 67, 'C7': 68, 'D7': 69, 'E7': 70, 'F7': 71, 'G7': 72, 'H7': 73, 'I7': 74, 'J7': 75, 'K7': 76, 'A8': 77, 'B8': 78, 'C8': 79, 'D8': 80, 'E8': 81, 'F8': 82, 'G8': 83, 'H8': 84, 'I8': 85, 'J8': 86, 'K8': 87, 'A9': 88, 'B9': 89, 'C9': 90, 'D9': 91, 'E9': 92, 'F9': 93, 'G9': 94, 'H9': 95, 'I9': 96, 'J9': 97, 'K9': 98, 'A10': 99, 'B10': 100, 'C10': 101, 'D10': 102, 'E10': 103, 'F10': 104, 'G10': 105, 'H10': 106, 'I10': 107, 'J10': 108, 'K10': 109, 'A11': 110, 'B11': 111, 'C11': 112, 'D11': 113, 'E11': 114, 'F11': 115, 'G11': 116, 'H11': 117, 'I11': 118, 'J11': 119, 'K11': 120, 'A12': 121, 'B12': 122, 'C12': 123, 'D12': 124, 'E12': 125, 'F12': 126, 'G12': 127, 'H12': 128, 'I12': 129, 'J12': 130, 'K12': 131, 'A13': 132, 'B13': 133, 'C13': 134, 'D13': 135, 'E13': 136, 'F13': 137, 'G13': 138, 'H13': 139, 'I13': 140, 'J13': 141, 'K13': 142, 'A14': 143, 'B14': 144, 'C14': 145, 'D14': 146, 'E14': 147, 'F14': 148, 'G14': 149, 'H14': 150, 'I14': 151, 'J14': 152, 'K14': 153, 'A15': 154, 'B15': 155, 'C15': 156, 'D15': 157, 'E15': 158, 'F15': 159, 'G15': 160, 'H15': 161, 'I15': 162, 'J15': 163, 'K15': 164, 'A16': 165, 'B16': 166, 'C16': 167, 'D16': 168, 'E16': 169, 'F16': 170, 'G16': 171, 'H16': 172, 'I16': 173, 'J16': 174, 'K16': 175, 'A17': 176, 'B17': 177, 'C17': 178, 'D17': 179, 'E17': 180, 'F17': 181, 'G17': 182, 'H17': 183, 'I17': 184, 'J17': 185, 'K17': 186, 'A18': 187, 'B18': 188, 'C18': 189, 'D18': 190, 'E18': 191, 'F18': 192, 'G18': 193, 'H18': 194, 'I18': 195, 'J18': 196, 'K18': 197}\n"
     ]
    }
   ],
   "source": [
    "# Mappings --------------------------------------------------------\n",
    "# Map each hold like \"A1\"…\"K18\" to an integer 0…(11*18−1)=197\n",
    "cols = [chr(c) for c in range(ord('A'), ord('K')+1)]\n",
    "rows = list(range(1, 19))\n",
    "hold_to_idx = {f\"{c}{r}\": i for i, (c, r) in enumerate((c, r) for r in rows for c in cols)}\n",
    "\n",
    "\n",
    "# Map grades \"V4\"…\"V11\" \n",
    "grade_to_label = {f\"V{i}\": i - 4 for i in range(4, 12)}  \n",
    "label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "print(hold_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50a6dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully parsed hold difficulty file\n",
      "successfully prepare type vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Holds difficulty data --------------------------------------------------------\n",
    "hold_difficulty = {}\n",
    "with open(\"data/hold_difficulty.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if \":\" not in line:\n",
    "            continue  # skip malformed line\n",
    "        hold, rest = line.strip().split(\":\", 1)\n",
    "        parts = rest.strip().split(\",\")\n",
    "        difficulty = int(parts[0].strip())\n",
    "        types = [t.strip() for t in parts[1:]]\n",
    "        hold_difficulty[hold.strip()] = (difficulty, types)\n",
    "    print(\"successfully parsed hold difficulty file\")\n",
    "\n",
    "# prepare type vocabulary\n",
    "unique_types = set()\n",
    "for _, (_, types) in hold_difficulty.items():\n",
    "    unique_types.update(types)\n",
    "\n",
    "type_to_idx = {t: i for i, t in enumerate(sorted(unique_types))}\n",
    "print(f\"successfully prepare type vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e5b93f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created (x,y) position to each hold:\n",
      "{'A1': (0, 0), 'A2': (0, 1), 'A3': (0, 2), 'A4': (0, 3), 'A5': (0, 4), 'A6': (0, 5), 'A7': (0, 6), 'A8': (0, 7), 'A9': (0, 8), 'A10': (0, 9), 'A11': (0, 10), 'A12': (0, 11), 'A13': (0, 12), 'A14': (0, 13), 'A15': (0, 14), 'A16': (0, 15), 'A17': (0, 16), 'A18': (0, 17), 'B1': (1, 0), 'B2': (1, 1), 'B3': (1, 2), 'B4': (1, 3), 'B5': (1, 4), 'B6': (1, 5), 'B7': (1, 6), 'B8': (1, 7), 'B9': (1, 8), 'B10': (1, 9), 'B11': (1, 10), 'B12': (1, 11), 'B13': (1, 12), 'B14': (1, 13), 'B15': (1, 14), 'B16': (1, 15), 'B17': (1, 16), 'B18': (1, 17), 'C1': (2, 0), 'C2': (2, 1), 'C3': (2, 2), 'C4': (2, 3), 'C5': (2, 4), 'C6': (2, 5), 'C7': (2, 6), 'C8': (2, 7), 'C9': (2, 8), 'C10': (2, 9), 'C11': (2, 10), 'C12': (2, 11), 'C13': (2, 12), 'C14': (2, 13), 'C15': (2, 14), 'C16': (2, 15), 'C17': (2, 16), 'C18': (2, 17), 'D1': (3, 0), 'D2': (3, 1), 'D3': (3, 2), 'D4': (3, 3), 'D5': (3, 4), 'D6': (3, 5), 'D7': (3, 6), 'D8': (3, 7), 'D9': (3, 8), 'D10': (3, 9), 'D11': (3, 10), 'D12': (3, 11), 'D13': (3, 12), 'D14': (3, 13), 'D15': (3, 14), 'D16': (3, 15), 'D17': (3, 16), 'D18': (3, 17), 'E1': (4, 0), 'E2': (4, 1), 'E3': (4, 2), 'E4': (4, 3), 'E5': (4, 4), 'E6': (4, 5), 'E7': (4, 6), 'E8': (4, 7), 'E9': (4, 8), 'E10': (4, 9), 'E11': (4, 10), 'E12': (4, 11), 'E13': (4, 12), 'E14': (4, 13), 'E15': (4, 14), 'E16': (4, 15), 'E17': (4, 16), 'E18': (4, 17), 'F1': (5, 0), 'F2': (5, 1), 'F3': (5, 2), 'F4': (5, 3), 'F5': (5, 4), 'F6': (5, 5), 'F7': (5, 6), 'F8': (5, 7), 'F9': (5, 8), 'F10': (5, 9), 'F11': (5, 10), 'F12': (5, 11), 'F13': (5, 12), 'F14': (5, 13), 'F15': (5, 14), 'F16': (5, 15), 'F17': (5, 16), 'F18': (5, 17), 'G1': (6, 0), 'G2': (6, 1), 'G3': (6, 2), 'G4': (6, 3), 'G5': (6, 4), 'G6': (6, 5), 'G7': (6, 6), 'G8': (6, 7), 'G9': (6, 8), 'G10': (6, 9), 'G11': (6, 10), 'G12': (6, 11), 'G13': (6, 12), 'G14': (6, 13), 'G15': (6, 14), 'G16': (6, 15), 'G17': (6, 16), 'G18': (6, 17), 'H1': (7, 0), 'H2': (7, 1), 'H3': (7, 2), 'H4': (7, 3), 'H5': (7, 4), 'H6': (7, 5), 'H7': (7, 6), 'H8': (7, 7), 'H9': (7, 8), 'H10': (7, 9), 'H11': (7, 10), 'H12': (7, 11), 'H13': (7, 12), 'H14': (7, 13), 'H15': (7, 14), 'H16': (7, 15), 'H17': (7, 16), 'H18': (7, 17), 'I1': (8, 0), 'I2': (8, 1), 'I3': (8, 2), 'I4': (8, 3), 'I5': (8, 4), 'I6': (8, 5), 'I7': (8, 6), 'I8': (8, 7), 'I9': (8, 8), 'I10': (8, 9), 'I11': (8, 10), 'I12': (8, 11), 'I13': (8, 12), 'I14': (8, 13), 'I15': (8, 14), 'I16': (8, 15), 'I17': (8, 16), 'I18': (8, 17), 'J1': (9, 0), 'J2': (9, 1), 'J3': (9, 2), 'J4': (9, 3), 'J5': (9, 4), 'J6': (9, 5), 'J7': (9, 6), 'J8': (9, 7), 'J9': (9, 8), 'J10': (9, 9), 'J11': (9, 10), 'J12': (9, 11), 'J13': (9, 12), 'J14': (9, 13), 'J15': (9, 14), 'J16': (9, 15), 'J17': (9, 16), 'J18': (9, 17), 'K1': (10, 0), 'K2': (10, 1), 'K3': (10, 2), 'K4': (10, 3), 'K5': (10, 4), 'K6': (10, 5), 'K7': (10, 6), 'K8': (10, 7), 'K9': (10, 8), 'K10': (10, 9), 'K11': (10, 10), 'K12': (10, 11), 'K13': (10, 12), 'K14': (10, 13), 'K15': (10, 14), 'K16': (10, 15), 'K17': (10, 16), 'K18': (10, 17)}\n"
     ]
    }
   ],
   "source": [
    "# assign x,y position to each holds -------------------------------\n",
    "import string\n",
    "\n",
    "# Board columns A–K → indices 0–10\n",
    "cols = list(string.ascii_uppercase[:11])  # A–K\n",
    "# Rows 1–18 → indices 0–17\n",
    "rows = list(range(1, 19))  # 1–18\n",
    "\n",
    "# Generate hold_to_coord dictionary\n",
    "hold_to_coord = {}\n",
    "\n",
    "for x, col in enumerate(cols):\n",
    "    for y, row in enumerate(rows):\n",
    "        hold_name = f\"{col}{row}\"\n",
    "        hold_to_coord[hold_name] = (x, y)\n",
    "\n",
    "print(\"successfully created (x,y) position to each hold:\")\n",
    "print(hold_to_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcbe64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonBoardDataset(Dataset):\n",
    "    def __init__(self, json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord, max_difficulty=10):\n",
    "        self.hold_to_idx = hold_to_idx\n",
    "        self.grade_to_label = grade_to_label\n",
    "        self.hold_difficulty = hold_difficulty\n",
    "        self.type_to_idx = type_to_idx\n",
    "        self.hold_to_coord = hold_to_coord\n",
    "        self.max_difficulty = max_difficulty\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.raw = [json.loads(line) for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.raw[idx]\n",
    "        holds = item['holds']\n",
    "\n",
    "        hold_idxs = []\n",
    "        diff_values = []\n",
    "        type_vecs = []\n",
    "        xy_coords = []\n",
    "\n",
    "        for h in holds:\n",
    "            hold_idxs.append(self.hold_to_idx[h])\n",
    "\n",
    "            difficulty, types = self.hold_difficulty[h]\n",
    "            diff_values.append(difficulty / self.max_difficulty)\n",
    "\n",
    "            # multi-hot vector\n",
    "            type_vec = torch.zeros(len(self.type_to_idx), dtype=torch.float)\n",
    "            for t in types:\n",
    "                if t in self.type_to_idx:\n",
    "                    type_vec[self.type_to_idx[t]] = 1.0\n",
    "            type_vecs.append(type_vec)\n",
    "\n",
    "            # normalized (x, y)\n",
    "            x, y = self.hold_to_coord[h]\n",
    "            xy_coords.append(torch.tensor([x / 10.0, y / 17.0], dtype=torch.float))\n",
    "\n",
    "        return {\n",
    "            \"indices\": torch.tensor(hold_idxs, dtype=torch.long),\n",
    "            \"difficulty\": torch.tensor(diff_values, dtype=torch.float),\n",
    "            \"type\": torch.stack(type_vecs),       # (N, T)\n",
    "            \"xy\": torch.stack(xy_coords)          # (N, 2)\n",
    "        }, torch.tensor(self.grade_to_label[item['grade']], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66965b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop ------------------------------------------------\n",
    "\n",
    "# --- Set Hyperparameters ---\n",
    "json_path = './data/cleaned_moonboard2024_grouped.json'\n",
    "embed_dim = 64\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "XY_MODELS = {\n",
    "    'set_transformer_xy',\n",
    "    'set_transformer_additive',\n",
    "    'deepset_xy',\n",
    "    'deepset_xy_additive',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "}\n",
    "\n",
    "ORDINAL_MODELS = {\n",
    "    'set_transformer_ordinal',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "}\n",
    "\n",
    "# --- Collate Function Factory ---\n",
    "def make_collate_fn(model_type):\n",
    "    def collate_fn(batch):\n",
    "        X_indices = [x['indices'] for x, _ in batch]\n",
    "        X_difficulty = [x['difficulty'] for x, _ in batch]\n",
    "        X_type = [x['type'] for x, _ in batch]\n",
    "        y_batch = [y for _, y in batch]\n",
    "\n",
    "        X_indices = pad_sequence(X_indices, batch_first=True)\n",
    "        X_difficulty = pad_sequence(X_difficulty, batch_first=True)\n",
    "        X_type = pad_sequence(X_type, batch_first=True)\n",
    "        y_tensor = torch.stack(y_batch)\n",
    "\n",
    "        if model_type in XY_MODELS:\n",
    "            X_xy = [x['xy'] for x, _ in batch]\n",
    "            X_xy = pad_sequence(X_xy, batch_first=True)\n",
    "            return (X_indices, X_difficulty, X_type, X_xy), y_tensor\n",
    "        else:\n",
    "            return (X_indices,), y_tensor\n",
    "    return collate_fn\n",
    "\n",
    "# --- Dataset Loader ---\n",
    "def load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord):\n",
    "    return MoonBoardDataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "\n",
    "# --- DataLoader Preparation ---\n",
    "def prepare_dataloaders(dataset, grade_to_label, batch_size, collate_fn):\n",
    "    targets = [grade_to_label[item['grade']] for item in dataset.raw]\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(targets), y=targets)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(dataset))), test_size=0.2, stratify=targets, random_state=42\n",
    "    )\n",
    "\n",
    "    train_data = Subset(dataset, train_idx)\n",
    "    val_data = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, class_weights, train_idx, val_idx\n",
    "\n",
    "# --- Training ---\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, is_ordinal=False):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "            payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            outputs = model(payload)\n",
    "            if is_ordinal:\n",
    "                probs, logits = outputs\n",
    "                loss = criterion(logits, y)\n",
    "            else:\n",
    "                logits = outputs\n",
    "                loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch:02d} — loss: {total_loss / len(train_loader):.4f}\")\n",
    "    return model\n",
    "\n",
    "# --- Main Per Model ---\n",
    "def main(model_type):\n",
    "    dataset = load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "    targets = [grade_to_label[item['grade']] for item in dataset.raw]\n",
    "    num_classes = len(np.unique(targets))\n",
    "    vocab_size = len(hold_to_idx)\n",
    "    type_vec_dim = len(type_to_idx)\n",
    "    is_ordinal = model_type in ORDINAL_MODELS\n",
    "\n",
    "    if model_type == 'set_transformer':\n",
    "        ModelClass = SetTransformerClassifier\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'set_transformer_xy':\n",
    "        ModelClass = SetTransformerClassifierXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_additive':\n",
    "        ModelClass = SetTransformerClassifierXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset':\n",
    "        ModelClass = DeepSetClassifier\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'deepset_xy':\n",
    "        ModelClass = DeepSetClassifierXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_xy_additive':\n",
    "        ModelClass = DeepSetClassifierXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_ordinal':\n",
    "        ModelClass = SetTransformerOrdinal\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'set_transformer_ordinal_xy':\n",
    "        ModelClass = SetTransformerOrdinalXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_ordinal_xy_additive':\n",
    "        ModelClass = SetTransformerOrdinalXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_ordinal':\n",
    "        ModelClass = DeepSetOrdinal\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'deepset_ordinal_xy':\n",
    "        ModelClass = DeepSetOrdinalXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_ordinal_xy_additive':\n",
    "        ModelClass = DeepSetOrdinalXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "    collate_fn = make_collate_fn(model_type)\n",
    "    train_loader, val_loader, class_weights, train_idx, val_idx = prepare_dataloaders(dataset, grade_to_label, batch_size, collate_fn)\n",
    "\n",
    "    model = ModelClass(**kwargs).to(device)\n",
    "    model.is_ordinal = is_ordinal\n",
    "    model.num_classes = num_classes\n",
    "\n",
    "    if is_ordinal:\n",
    "        def criterion_fn(logits, targets):\n",
    "            return ordinal_logistic_loss(logits, targets)\n",
    "    else:\n",
    "        criterion_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model = train_model(model, train_loader, val_loader, criterion_fn, optimizer, epochs, is_ordinal=is_ordinal)\n",
    "    return train_loader, val_loader, model, dataset, train_idx, val_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5db06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training set_transformer =====\n",
      "Epoch 01 — loss: 1.7873\n",
      "Epoch 02 — loss: 1.6348\n",
      "Epoch 03 — loss: 1.5919\n",
      "Epoch 04 — loss: 1.5436\n",
      "Epoch 05 — loss: 1.4955\n",
      "Epoch 06 — loss: 1.4599\n",
      "Epoch 07 — loss: 1.4310\n",
      "Epoch 08 — loss: 1.3894\n",
      "Epoch 09 — loss: 1.3664\n",
      "Epoch 10 — loss: 1.3268\n",
      "Epoch 11 — loss: 1.2877\n",
      "Epoch 12 — loss: 1.2654\n",
      "Epoch 13 — loss: 1.2227\n",
      "Epoch 14 — loss: 1.1781\n",
      "Epoch 15 — loss: 1.1487\n",
      "Epoch 16 — loss: 1.1030\n",
      "Epoch 17 — loss: 1.0729\n",
      "Epoch 18 — loss: 1.0287\n",
      "Epoch 19 — loss: 0.9953\n",
      "Epoch 20 — loss: 0.9536\n",
      "Confusion matrix for set_transformer saved and inserted into result/model_comparison_results.xlsx (sheet: set_transformer)\n",
      "Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "===== Training deepset =====\n",
      "Epoch 01 — loss: 1.8724\n",
      "Epoch 02 — loss: 1.5957\n",
      "Epoch 03 — loss: 1.5398\n",
      "Epoch 04 — loss: 1.5248\n",
      "Epoch 05 — loss: 1.5066\n",
      "Epoch 06 — loss: 1.4956\n",
      "Epoch 07 — loss: 1.4878\n",
      "Epoch 08 — loss: 1.4742\n",
      "Epoch 09 — loss: 1.4709\n",
      "Epoch 10 — loss: 1.4671\n",
      "Epoch 11 — loss: 1.4538\n",
      "Epoch 12 — loss: 1.4518\n",
      "Epoch 13 — loss: 1.4573\n",
      "Epoch 14 — loss: 1.4466\n",
      "Epoch 15 — loss: 1.4516\n",
      "Epoch 16 — loss: 1.4468\n",
      "Epoch 17 — loss: 1.4475\n",
      "Epoch 18 — loss: 1.4474\n",
      "Epoch 19 — loss: 1.4453\n",
      "Epoch 20 — loss: 1.4391\n",
      "Confusion matrix for deepset saved and inserted into result/model_comparison_results.xlsx (sheet: deepset)\n",
      "Predictions for deepset_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "===== Training set_transformer_xy =====\n",
      "Epoch 01 — loss: 1.7402\n",
      "Epoch 02 — loss: 1.5894\n",
      "Epoch 03 — loss: 1.5396\n",
      "Epoch 04 — loss: 1.5019\n",
      "Epoch 05 — loss: 1.4672\n",
      "Epoch 06 — loss: 1.4465\n",
      "Epoch 07 — loss: 1.4121\n",
      "Epoch 08 — loss: 1.3808\n",
      "Epoch 09 — loss: 1.3534\n",
      "Epoch 10 — loss: 1.3245\n",
      "Epoch 11 — loss: 1.2893\n",
      "Epoch 12 — loss: 1.2573\n",
      "Epoch 13 — loss: 1.2275\n",
      "Epoch 14 — loss: 1.1913\n",
      "Epoch 15 — loss: 1.1677\n",
      "Epoch 16 — loss: 1.1353\n",
      "Epoch 17 — loss: 1.0819\n",
      "Epoch 18 — loss: 1.0576\n",
      "Epoch 19 — loss: 1.0196\n",
      "Epoch 20 — loss: 0.9831\n",
      "Confusion matrix for set_transformer_xy saved and inserted into result/model_comparison_results.xlsx (sheet: set_transformer_xy)\n",
      "Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "===== Training deepset_xy =====\n",
      "Epoch 01 — loss: 1.8579\n",
      "Epoch 02 — loss: 1.5873\n",
      "Epoch 03 — loss: 1.5376\n",
      "Epoch 04 — loss: 1.5110\n",
      "Epoch 05 — loss: 1.5010\n",
      "Epoch 06 — loss: 1.4888\n",
      "Epoch 07 — loss: 1.4810\n",
      "Epoch 08 — loss: 1.4722\n",
      "Epoch 09 — loss: 1.4619\n",
      "Epoch 10 — loss: 1.4606\n",
      "Epoch 11 — loss: 1.4571\n",
      "Epoch 12 — loss: 1.4517\n",
      "Epoch 13 — loss: 1.4528\n",
      "Epoch 14 — loss: 1.4546\n",
      "Epoch 15 — loss: 1.4424\n",
      "Epoch 16 — loss: 1.4452\n",
      "Epoch 17 — loss: 1.4392\n",
      "Epoch 18 — loss: 1.4382\n",
      "Epoch 19 — loss: 1.4418\n",
      "Epoch 20 — loss: 1.4366\n",
      "Confusion matrix for deepset_xy saved and inserted into result/model_comparison_results.xlsx (sheet: deepset_xy)\n",
      "Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "===== Training set_transformer_additive =====\n",
      "Epoch 01 — loss: 1.7057\n",
      "Epoch 02 — loss: 1.6298\n",
      "Epoch 03 — loss: 1.5883\n",
      "Epoch 04 — loss: 1.5560\n",
      "Epoch 05 — loss: 1.5278\n",
      "Epoch 06 — loss: 1.5172\n",
      "Epoch 07 — loss: 1.4822\n",
      "Epoch 08 — loss: 1.4720\n",
      "Epoch 09 — loss: 1.4454\n",
      "Epoch 10 — loss: 1.4286\n",
      "Epoch 11 — loss: 1.4093\n",
      "Epoch 12 — loss: 1.3869\n",
      "Epoch 13 — loss: 1.3590\n",
      "Epoch 14 — loss: 1.3430\n",
      "Epoch 15 — loss: 1.3190\n",
      "Epoch 16 — loss: 1.3000\n",
      "Epoch 17 — loss: 1.2770\n",
      "Epoch 18 — loss: 1.2487\n",
      "Epoch 19 — loss: 1.2181\n",
      "Epoch 20 — loss: 1.1934\n",
      "Confusion matrix for set_transformer_additive saved and inserted into result/model_comparison_results.xlsx (sheet: set_transformer_additive)\n",
      "Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "===== Training deepset_xy_additive =====\n",
      "Epoch 01 — loss: 1.8219\n",
      "Epoch 02 — loss: 1.6219\n",
      "Epoch 03 — loss: 1.5740\n",
      "Epoch 04 — loss: 1.5409\n",
      "Epoch 05 — loss: 1.5134\n",
      "Epoch 06 — loss: 1.5066\n",
      "Epoch 07 — loss: 1.4900\n",
      "Epoch 08 — loss: 1.4868\n",
      "Epoch 09 — loss: 1.4799\n",
      "Epoch 10 — loss: 1.4669\n",
      "Epoch 11 — loss: 1.4738\n",
      "Epoch 12 — loss: 1.4681\n",
      "Epoch 13 — loss: 1.4681\n",
      "Epoch 14 — loss: 1.4573\n",
      "Epoch 15 — loss: 1.4528\n",
      "Epoch 16 — loss: 1.4551\n",
      "Epoch 17 — loss: 1.4544\n",
      "Epoch 18 — loss: 1.4514\n",
      "Epoch 19 — loss: 1.4467\n",
      "Epoch 20 — loss: 1.4518\n",
      "Confusion matrix for deepset_xy_additive saved and inserted into result/model_comparison_results.xlsx (sheet: deepset_xy_additive)\n",
      "Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "===== Evaluating ensembles =====\n",
      "--- Evaluating all ensembles (6 models) ---\n",
      "Stacking meta epoch 1: loss=1.1225\n",
      "Stacking meta epoch 2: loss=0.8850\n",
      "Stacking meta epoch 3: loss=0.8141\n",
      "Stacking meta epoch 4: loss=0.7763\n",
      "Stacking meta epoch 5: loss=0.7536\n",
      "Confusion matrix for soft_voting_ensemble_all saved and inserted into result/model_comparison_results.xlsx (sheet: soft_voting_ensemble_all)\n",
      "Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "Confusion matrix for geometric_mean_ensemble_all saved and inserted into result/model_comparison_results.xlsx (sheet: geometric_mean_ensemble_all)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for geometric_mean_ensemble_all_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for median_ensemble_all saved and inserted into result/model_comparison_results.xlsx (sheet: median_ensemble_all)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for median_ensemble_all_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for trimmed_mean_ensemble_all saved and inserted into result/model_comparison_results.xlsx (sheet: trimmed_mean_ensemble_all)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for trimmed_mean_ensemble_all_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for stacking_ensemble_all saved and inserted into result/model_comparison_results.xlsx (sheet: stacking_ensemble_all)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "--- Evaluating set_transformer ensembles (3 models) ---\n",
      "Stacking meta epoch 1: loss=1.0713\n",
      "Stacking meta epoch 2: loss=0.8654\n",
      "Stacking meta epoch 3: loss=0.8121\n",
      "Stacking meta epoch 4: loss=0.7829\n",
      "Stacking meta epoch 5: loss=0.7647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for soft_voting_ensemble_set_transformer saved and inserted into result/model_comparison_results.xlsx (sheet: soft_voting_ensemble_set_transformer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for geometric_mean_ensemble_set_transformer saved and inserted into result/model_comparison_results.xlsx (sheet: geometric_mean_ensemble_set_transformer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for geometric_mean_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for median_ensemble_set_transformer saved and inserted into result/model_comparison_results.xlsx (sheet: median_ensemble_set_transformer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for median_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for trimmed_mean_ensemble_set_transformer saved and inserted into result/model_comparison_results.xlsx (sheet: trimmed_mean_ensemble_set_transformer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for trimmed_mean_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for stacking_ensemble_set_transformer saved and inserted into result/model_comparison_results.xlsx (sheet: stacking_ensemble_set_transformer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "--- Evaluating deepset ensembles (3 models) ---\n",
      "Stacking meta epoch 1: loss=1.2830\n",
      "Stacking meta epoch 2: loss=1.2298\n",
      "Stacking meta epoch 3: loss=1.2280\n",
      "Stacking meta epoch 4: loss=1.2267\n",
      "Stacking meta epoch 5: loss=1.2257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for soft_voting_ensemble_deepset saved and inserted into result/model_comparison_results.xlsx (sheet: soft_voting_ensemble_deepset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for geometric_mean_ensemble_deepset saved and inserted into result/model_comparison_results.xlsx (sheet: geometric_mean_ensemble_deepset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for geometric_mean_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for median_ensemble_deepset saved and inserted into result/model_comparison_results.xlsx (sheet: median_ensemble_deepset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for median_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for trimmed_mean_ensemble_deepset saved and inserted into result/model_comparison_results.xlsx (sheet: trimmed_mean_ensemble_deepset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for trimmed_mean_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for stacking_ensemble_deepset saved and inserted into result/model_comparison_results.xlsx (sheet: stacking_ensemble_deepset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Comparison Summary ===\n",
      "                                 Model Type  Train Strict Accuracy (%)  \\\n",
      "0                           set_transformer                      61.72   \n",
      "1                                   deepset                      45.08   \n",
      "2                        set_transformer_xy                      64.91   \n",
      "3                                deepset_xy                      44.44   \n",
      "4                  set_transformer_additive                      54.74   \n",
      "5                       deepset_xy_additive                      46.33   \n",
      "6                  soft_voting_ensemble_all                      64.19   \n",
      "7               geometric_mean_ensemble_all                      63.03   \n",
      "8                       median_ensemble_all                      54.92   \n",
      "9                 trimmed_mean_ensemble_all                      61.02   \n",
      "10                    stacking_ensemble_all                      70.11   \n",
      "11     soft_voting_ensemble_set_transformer                      68.48   \n",
      "12  geometric_mean_ensemble_set_transformer                      68.44   \n",
      "13          median_ensemble_set_transformer                      66.79   \n",
      "14    trimmed_mean_ensemble_set_transformer                      68.48   \n",
      "15        stacking_ensemble_set_transformer                      69.95   \n",
      "16             soft_voting_ensemble_deepset                      45.99   \n",
      "17          geometric_mean_ensemble_deepset                      45.97   \n",
      "18                  median_ensemble_deepset                      46.06   \n",
      "19            trimmed_mean_ensemble_deepset                      45.99   \n",
      "20                stacking_ensemble_deepset                      49.22   \n",
      "\n",
      "    Train ±1 Grade Accuracy (%)  Val Strict Accuracy (%)  \\\n",
      "0                         87.65                    43.79   \n",
      "1                         84.08                    42.49   \n",
      "2                         91.30                    46.63   \n",
      "3                         81.62                    42.44   \n",
      "4                         83.85                    43.17   \n",
      "5                         84.06                    45.48   \n",
      "6                         89.80                    47.59   \n",
      "7                         90.11                    47.98   \n",
      "8                         87.15                    46.34   \n",
      "9                         88.84                    47.21   \n",
      "10                        93.89                    49.52   \n",
      "11                        91.53                    48.17   \n",
      "12                        91.85                    48.51   \n",
      "13                        91.10                    47.93   \n",
      "14                        91.53                    48.17   \n",
      "15                        93.38                    50.00   \n",
      "16                        84.01                    43.94   \n",
      "17                        84.19                    43.98   \n",
      "18                        84.34                    44.56   \n",
      "19                        84.01                    43.94   \n",
      "20                        83.18                    47.55   \n",
      "\n",
      "    Val ±1 Grade Accuracy (%)  \n",
      "0                       77.91  \n",
      "1                       81.38  \n",
      "2                       82.44  \n",
      "3                       79.98  \n",
      "4                       77.24  \n",
      "5                       81.81  \n",
      "6                       82.39  \n",
      "7                       83.40  \n",
      "8                       82.92  \n",
      "9                       82.82  \n",
      "10                      83.35  \n",
      "11                      82.10  \n",
      "12                      83.35  \n",
      "13                      82.00  \n",
      "14                      82.10  \n",
      "15                      83.49  \n",
      "16                      81.95  \n",
      "17                      82.05  \n",
      "18                      82.10  \n",
      "19                      81.95  \n",
      "20                      81.38  \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image as XLImage\n",
    "import csv\n",
    "\n",
    "MODEL_TYPES = [\n",
    "    \"set_transformer\",\n",
    "    \"deepset\",\n",
    "    \"set_transformer_xy\",\n",
    "    \"deepset_xy\",\n",
    "    \"set_transformer_additive\",\n",
    "    \"deepset_xy_additive\"\n",
    "]\n",
    "ENSEMBLE_TYPES = [\n",
    "    \"soft_voting_ensemble\",\n",
    "    \"geometric_mean_ensemble\",\n",
    "    \"median_ensemble\",\n",
    "    \"trimmed_mean_ensemble\",\n",
    "    \"stacking_ensemble\",\n",
    "]\n",
    "MODEL_COUNT_COLUMNS = {name: f\"{name}_count\" for name in MODEL_TYPES + ENSEMBLE_TYPES}\n",
    "\n",
    "# --- plot confusion matrix and save to excel---\n",
    "def save_confusion_matrix_to_excel(y_true, y_pred, class_labels, model_name, excel_path):\n",
    "    # Plot confusion matrix and save as image\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(class_labels)), normalize='true')\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "    plt.xlabel(\"Predicted Grade\")\n",
    "    plt.ylabel(\"Actual Grade\")\n",
    "    plt.tight_layout()\n",
    "    img_path = f\"result/confusion_{model_name}.png\"\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Insert image into Excel (new sheet per model)\n",
    "    wb = load_workbook(excel_path)\n",
    "    if model_name in wb.sheetnames:\n",
    "        ws = wb[model_name]\n",
    "    else:\n",
    "        ws = wb.create_sheet(title=model_name)\n",
    "    img = XLImage(img_path)\n",
    "    ws.add_image(img, \"A1\")\n",
    "    wb.save(excel_path)\n",
    "    print(f\"Confusion matrix for {model_name} saved and inserted into {excel_path} (sheet: {model_name})\")\n",
    "\n",
    "# --- export the predictions to excel ---\n",
    "def _update_outlier_excel(df_all_preds, model_name, outlier_filename=\"result/outlier.xlsx\", sheet_name=\"outliers\", threshold=3):\n",
    "    \"\"\"\n",
    "    From a DataFrame with columns [problem_name, y_true, y_pred, diff],\n",
    "    keep rows where abs(diff) > threshold and aggregate per problem_name:\n",
    "        - count = number of times flagged\n",
    "        - per-model counts = number of times flagged per model across runs\n",
    "        - y_true = mode (most frequent true label)\n",
    "        - y_pred_avg = average predicted label across occurrences\n",
    "    Save to outlier.xlsx.\n",
    "    \"\"\"\n",
    "    current_model_col = MODEL_COUNT_COLUMNS.get(model_name, f\"{model_name}_count\")\n",
    "    model_count_columns = dict(MODEL_COUNT_COLUMNS)\n",
    "    if model_name not in MODEL_COUNT_COLUMNS:\n",
    "        model_count_columns[model_name] = current_model_col\n",
    "\n",
    "    # Filter outliers\n",
    "    outliers = df_all_preds.loc[df_all_preds[\"diff\"].abs() > threshold,\n",
    "                                [\"problem_name\", \"y_true\", \"y_pred\"]]\n",
    "    if outliers.empty:\n",
    "        print(f\"No outliers (abs(diff) > {threshold}). Skipped creating outlier.xlsx.\")\n",
    "        return\n",
    "\n",
    "    # Group & aggregate\n",
    "    grouped = (outliers\n",
    "               .groupby(\"problem_name\")\n",
    "               .agg(\n",
    "                   count=(\"problem_name\", \"size\"),\n",
    "                   y_true=(\"y_true\", lambda x: x.mode().iat[0] if not x.mode().empty else x.iloc[0]),\n",
    "                   y_pred_avg=(\"y_pred\", lambda x: round(pd.to_numeric(x, errors=\"coerce\").mean(), 2))\n",
    "               )\n",
    "               .reset_index())\n",
    "\n",
    "    for col in model_count_columns.values():\n",
    "        if col not in grouped.columns:\n",
    "            grouped[col] = 0\n",
    "    grouped[current_model_col] = grouped[\"count\"]\n",
    "\n",
    "    # If a previous file exists, merge and accumulate counts\n",
    "    if os.path.exists(outlier_filename):\n",
    "        try:\n",
    "            existing = pd.read_excel(outlier_filename, sheet_name=sheet_name)\n",
    "            for col in model_count_columns.values():\n",
    "                if col not in existing.columns:\n",
    "                    existing[col] = 0\n",
    "            if set(existing.columns) >= {\"problem_name\", \"count\", \"y_true\", \"y_pred_avg\"}:\n",
    "                merged = pd.concat([existing, grouped], ignore_index=True, sort=False)\n",
    "                for col in model_count_columns.values():\n",
    "                    if col not in merged.columns:\n",
    "                        merged[col] = 0\n",
    "                agg_map = {\n",
    "                    \"count\": (\"count\", \"sum\"),\n",
    "                    \"y_true\": (\"y_true\", lambda x: x.mode().iat[0] if not x.mode().empty else x.iloc[0]),\n",
    "                    \"y_pred_avg\": (\"y_pred_avg\", \"mean\")\n",
    "                }\n",
    "                agg_map.update({col: (col, \"sum\") for col in model_count_columns.values()})\n",
    "                grouped = (merged\n",
    "                           .groupby(\"problem_name\")\n",
    "                           .agg(**agg_map)\n",
    "                           .reset_index())\n",
    "            # else keep grouped as new\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    grouped[\"y_pred_avg\"] = pd.to_numeric(grouped[\"y_pred_avg\"], errors=\"coerce\").round(2)\n",
    "    grouped[\"count\"] = grouped[\"count\"].fillna(0).astype(int)\n",
    "    for col in model_count_columns.values():\n",
    "        if col in grouped.columns:\n",
    "            grouped[col] = grouped[col].fillna(0).astype(int)\n",
    "\n",
    "    ordered_cols = [\"problem_name\", \"count\"]\n",
    "    ordered_cols.extend([model_count_columns[name] for name in MODEL_TYPES + ENSEMBLE_TYPES if model_count_columns[name] in grouped.columns])\n",
    "    if current_model_col in grouped.columns and current_model_col not in ordered_cols:\n",
    "        ordered_cols.append(current_model_col)\n",
    "    ordered_cols.extend([\"y_true\", \"y_pred_avg\"])\n",
    "    ordered_cols.extend([col for col in grouped.columns if col not in ordered_cols])\n",
    "    grouped = grouped[ordered_cols]\n",
    "\n",
    "    # Save\n",
    "    with pd.ExcelWriter(outlier_filename, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "        grouped.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Outliers saved to: {os.path.abspath(outlier_filename)}\")\n",
    "\n",
    "\n",
    "def export_predictions_to_excel(model, dataloader, device, grade_to_label, excel_path, sheet_name, model_name=None):\n",
    "    results = []\n",
    "    raw_dataset = dataloader.dataset.dataset  # MoonBoardDataset\n",
    "    indices = dataloader.dataset.indices      # Subset indices\n",
    "    label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "    current_index = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if isinstance(X, tuple):\n",
    "                inputs = tuple(x.to(device) for x in X)\n",
    "                payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            else:\n",
    "                payload = X.to(device)\n",
    "            outputs = model(payload)\n",
    "            if isinstance(outputs, tuple):\n",
    "                if getattr(model, \"is_ordinal\", False):\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = cumulative_to_labels(probs)\n",
    "                else:\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = probs.argmax(dim=1)\n",
    "            else:\n",
    "                preds_tensor = outputs.argmax(dim=1)\n",
    "            y = y.to(device)\n",
    "            preds_cpu = preds_tensor.detach().cpu()\n",
    "            y_cpu = y.detach().cpu()\n",
    "            for i in range(y_cpu.size(0)):\n",
    "                real_label = int(y_cpu[i].item())\n",
    "                pred_label = int(preds_cpu[i].item())\n",
    "                dataset_index = indices[current_index]\n",
    "                current_index += 1\n",
    "                raw_item = raw_dataset.raw[dataset_index]\n",
    "                problem_name = raw_item.get('problem_name', f\"problem_{dataset_index}\")\n",
    "                results.append({\n",
    "                    \"problem_name\": problem_name,\n",
    "                    \"y_true\": real_label,  # keep numeric for averaging/aggregation\n",
    "                    \"y_pred\": pred_label,\n",
    "                    \"diff\": real_label - pred_label\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if model_name is None:\n",
    "        model_name = sheet_name\n",
    "    df[\"model_name\"] = model_name\n",
    "\n",
    "    # 1) Save all predictions into your main Excel file\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        # Convert numeric labels back to grade strings for readability\n",
    "        df_out = df.copy()\n",
    "        df_out[\"y_true\"] = df_out[\"y_true\"].map(lambda x: label_to_grade.get(x, f\"Unknown({x})\"))\n",
    "        df_out[\"y_pred\"] = df_out[\"y_pred\"].map(lambda x: label_to_grade.get(x, f\"Unknown({x})\"))\n",
    "        df_out.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Predictions for {sheet_name} exported to: {excel_path}\")\n",
    "\n",
    "    # 2) Create/update outlier.xlsx (problem_name, count, per-model counts, y_true, y_pred_avg)\n",
    "    _update_outlier_excel(df, model_name=model_name, outlier_filename=\"result/outlier.xlsx\", sheet_name=\"outliers\", threshold=3)\n",
    "\n",
    "\n",
    "# --- compute training and validation accuracy ---\n",
    "def compute_accuracy(model, dataloader, device):\n",
    "    strict_correct, loose_correct, total = 0, 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "\n",
    "            payload = X[0] if len(X) == 1 else X\n",
    "            outputs = model(payload)\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "                if getattr(model, \"is_ordinal\", False):\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = cumulative_to_labels(probs)\n",
    "                else:\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = probs.argmax(dim=1)\n",
    "            else:\n",
    "                preds_tensor = outputs.argmax(dim=1)\n",
    "\n",
    "            if isinstance(preds_tensor, torch.Tensor):\n",
    "                preds_tensor = preds_tensor.to(y.device)\n",
    "            preds = preds_tensor\n",
    "            total += y.size(0)\n",
    "            strict_correct += (preds == y).sum().item()\n",
    "            loose_correct += ((preds - y).abs() <= 1).sum().item()\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.detach().cpu().numpy())\n",
    "    strict_acc = 100.0 * strict_correct / total\n",
    "    loose_acc = 100.0 * loose_correct / total\n",
    "    return strict_acc, loose_acc, y_true, y_pred\n",
    "\n",
    "def log_accuracy_to_csv(model_type, train_strict_acc, train_loose_acc, val_strict_acc, val_loose_acc, csv_path=\"result/accuracy.csv\"):\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "    with open(csv_path, mode='a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\n",
    "                \"Model Type\",\n",
    "                \"Train Strict Accuracy (%)\",\n",
    "                \"Train ±1 Grade Accuracy (%)\",\n",
    "                \"Val Strict Accuracy (%)\",\n",
    "                \"Val ±1 Grade Accuracy (%)\"\n",
    "            ])\n",
    "        writer.writerow([\n",
    "            model_type,\n",
    "            round(train_strict_acc, 2),\n",
    "            round(train_loose_acc, 2),\n",
    "            round(val_strict_acc, 2),\n",
    "            round(val_loose_acc, 2)\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "def train_stacking_meta_model(stacking_model, dataloader, device, epochs=5, lr=1e-3):\n",
    "    \"\"\"Train stacking meta-learner on frozen base model outputs.\"\"\"\n",
    "    if epochs <= 0:\n",
    "        return\n",
    "    stacking_model.meta_model.train()\n",
    "    for member in stacking_model.models.values():\n",
    "        member.eval()\n",
    "    optimizer = torch.optim.Adam(stacking_model.meta_model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_samples = 0\n",
    "        for X, y in dataloader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            targets = y.to(device)\n",
    "            member_feats = stacking_model._member_features(inputs)\n",
    "            M, B, C = member_feats.shape\n",
    "            if stacking_model.combine == \"mean\":\n",
    "                feat = member_feats.mean(dim=0)\n",
    "            else:\n",
    "                feat = member_feats.permute(1, 0, 2).reshape(B, M * C)\n",
    "\n",
    "            logits = stacking_model.meta_model(feat)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size = targets.size(0)\n",
    "            epoch_loss += loss.item() * batch_size\n",
    "            num_samples += batch_size\n",
    "\n",
    "        denom = num_samples if num_samples > 0 else 1\n",
    "        avg_loss = epoch_loss / denom\n",
    "        print(f\"Stacking meta epoch {epoch + 1}: loss={avg_loss:.4f}\")\n",
    "\n",
    "    stacking_model.meta_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "def build_ensemble_models(\n",
    "    ensemble_names,\n",
    "    base_model_items,\n",
    "    ensemble_weights,\n",
    "    num_classes,\n",
    "    device,\n",
    "    train_loader,\n",
    "    stacking_meta_epochs=5,\n",
    "    stacking_meta_lr=1e-3,\n",
    "    label_suffix=\"\",\n",
    "):\n",
    "    \"\"\"Create configured ensemble models from trained base models.\"\"\"\n",
    "    ensembles = {}\n",
    "    base_items = list(base_model_items)\n",
    "\n",
    "    if not base_items:\n",
    "        return ensembles\n",
    "\n",
    "    def _resolve_group_weights(items):\n",
    "        if ensemble_weights is None:\n",
    "            return None\n",
    "        if isinstance(ensemble_weights, dict):\n",
    "            filtered = {name: ensemble_weights[name] for name, _ in items if name in ensemble_weights}\n",
    "            if len(filtered) != len(items):\n",
    "                missing = [name for name, _ in items if name not in filtered]\n",
    "                if missing:\n",
    "                    print(f\"Warning: missing weights for {missing}; using uniform weights.\")\n",
    "                return None\n",
    "            return filtered\n",
    "        weight_list = list(ensemble_weights)\n",
    "        if len(weight_list) != len(items):\n",
    "            print(\"Warning: weight list length mismatch; using uniform weights.\")\n",
    "            return None\n",
    "        return weight_list\n",
    "\n",
    "    resolved_weights = _resolve_group_weights(base_items)\n",
    "\n",
    "    for base_name in ensemble_names:\n",
    "        cloned_items = [(model_name, copy.deepcopy(model)) for model_name, model in base_items]\n",
    "        if not cloned_items:\n",
    "            continue\n",
    "\n",
    "        weights = resolved_weights\n",
    "        if isinstance(weights, list):\n",
    "            weights = list(weights)\n",
    "\n",
    "        ensemble_key = f\"{base_name}{label_suffix}\" if label_suffix else base_name\n",
    "\n",
    "        if base_name == \"soft_voting_ensemble\":\n",
    "            ensemble_model = SoftVotingEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == \"geometric_mean_ensemble\":\n",
    "            ensemble_model = GeometricMeanEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == \"median_ensemble\":\n",
    "            ensemble_model = MedianEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == \"trimmed_mean_ensemble\":\n",
    "            ensemble_model = TrimmedMeanEnsemble(cloned_items, weights=weights, freeze_members=True, trim_frac=0.2).to(device)\n",
    "        elif base_name == \"stacking_ensemble\":\n",
    "            feature_dim = len(cloned_items) * num_classes\n",
    "            meta_model = nn.Linear(feature_dim, num_classes).to(device)\n",
    "            ensemble_model = StackingEnsemble(\n",
    "                cloned_items,\n",
    "                weights=weights,\n",
    "                freeze_members=True,\n",
    "                meta_model=meta_model,\n",
    "                feature_source=\"logits\",\n",
    "                combine=\"concat\",\n",
    "            ).to(device)\n",
    "            train_stacking_meta_model(\n",
    "                ensemble_model,\n",
    "                train_loader,\n",
    "                device,\n",
    "                epochs=stacking_meta_epochs,\n",
    "                lr=stacking_meta_lr,\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Unknown ensemble type '{base_name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        ensemble_model.eval()\n",
    "        ensembles[ensemble_key] = ensemble_model\n",
    "\n",
    "    return ensembles\n",
    "\n",
    "\n",
    "def compare_models(\n",
    "    model_types=None,\n",
    "    include_ensemble=True,\n",
    "    ensemble_weights=None,\n",
    "    ensemble_types=None,\n",
    "    stacking_meta_epochs=5,\n",
    "    stacking_meta_lr=1e-3,\n",
    "):\n",
    "    model_types = model_types or MODEL_TYPES\n",
    "    results = []\n",
    "    excel_path = \"result/model_comparison_results.xlsx\"\n",
    "    class_labels = [f\"V{i}\" for i in range(4, 12)]\n",
    "\n",
    "    trained_models = {}\n",
    "    base_dataset = None\n",
    "    base_train_idx = None\n",
    "    base_val_idx = None\n",
    "    num_classes = None\n",
    "\n",
    "    for idx, mtype in enumerate(model_types):\n",
    "        print(f\"===== Training {mtype} =====\")\n",
    "        train_loader, val_loader, model, dataset, train_idx, val_idx = main(mtype)\n",
    "\n",
    "        model.eval()\n",
    "        trained_models[mtype] = model\n",
    "        if base_dataset is None:\n",
    "            base_dataset = dataset\n",
    "            base_train_idx = train_idx\n",
    "            base_val_idx = val_idx\n",
    "        if num_classes is None:\n",
    "            num_classes = getattr(model, \"num_classes\", None)\n",
    "\n",
    "        train_strict_acc, train_loose_acc, _, _ = compute_accuracy(model, train_loader, device)\n",
    "        val_strict_acc, val_loose_acc, y_true, y_pred = compute_accuracy(model, val_loader, device)\n",
    "\n",
    "        log_accuracy_to_csv(mtype, train_strict_acc, train_loose_acc, val_strict_acc, val_loose_acc)\n",
    "\n",
    "        results.append({\n",
    "            \"Model Type\": mtype,\n",
    "            \"Train Strict Accuracy (%)\": round(train_strict_acc, 2),\n",
    "            \"Train ±1 Grade Accuracy (%)\": round(train_loose_acc, 2),\n",
    "            \"Val Strict Accuracy (%)\": round(val_strict_acc, 2),\n",
    "            \"Val ±1 Grade Accuracy (%)\": round(val_loose_acc, 2),\n",
    "        })\n",
    "\n",
    "        if idx == 0:\n",
    "            df_results = pd.DataFrame(results)\n",
    "            df_results.to_excel(excel_path, index=False)\n",
    "\n",
    "        save_confusion_matrix_to_excel(y_true, y_pred, class_labels, mtype, excel_path)\n",
    "        export_predictions_to_excel(\n",
    "            model,\n",
    "            val_loader,\n",
    "            device,\n",
    "            grade_to_label,\n",
    "            excel_path,\n",
    "            sheet_name=f\"{mtype}_preds\",\n",
    "            model_name=mtype,\n",
    "        )\n",
    "\n",
    "    if include_ensemble and trained_models:\n",
    "        effective_ensemble_types = ensemble_types or ENSEMBLE_TYPES\n",
    "        if not effective_ensemble_types:\n",
    "            print(\"No ensemble types specified; skipping ensemble evaluation.\")\n",
    "        else:\n",
    "            if base_dataset is None or base_train_idx is None or base_val_idx is None:\n",
    "                raise RuntimeError(\"Dataset indices are unavailable for ensemble evaluation.\")\n",
    "            if num_classes is None:\n",
    "                raise RuntimeError(\"Unable to determine number of classes for ensembles.\")\n",
    "\n",
    "            print(\"===== Evaluating ensembles =====\")\n",
    "            collate_fn = make_collate_fn(\"set_transformer_xy\")\n",
    "            train_subset = Subset(base_dataset, base_train_idx)\n",
    "            val_subset = Subset(base_dataset, base_val_idx)\n",
    "            ensemble_train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "            ensemble_val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "            base_items = list(trained_models.items())\n",
    "            ensemble_groups = [\n",
    "                (\"all\", base_items),\n",
    "                (\"set_transformer\", [(name, model) for name, model in base_items if \"set_transformer\" in name]),\n",
    "                (\"deepset\", [(name, model) for name, model in base_items if \"deepset\" in name]),\n",
    "            ]\n",
    "\n",
    "            for group_name, group_items in ensemble_groups:\n",
    "                if not group_items:\n",
    "                    print(f\"Skipping {group_name} ensemble group (no models).\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"--- Evaluating {group_name} ensembles ({len(group_items)} models) ---\")\n",
    "                ensembles = build_ensemble_models(\n",
    "                    effective_ensemble_types,\n",
    "                    group_items,\n",
    "                    ensemble_weights,\n",
    "                    num_classes,\n",
    "                    device,\n",
    "                    ensemble_train_loader,\n",
    "                    stacking_meta_epochs=stacking_meta_epochs,\n",
    "                    stacking_meta_lr=stacking_meta_lr,\n",
    "                    label_suffix=f\"_{group_name}\",\n",
    "                )\n",
    "\n",
    "                if not ensembles:\n",
    "                    print(f\"No ensembles constructed for group {group_name}.\")\n",
    "                    continue\n",
    "\n",
    "                for name, ensemble_model in ensembles.items():\n",
    "                    MODEL_COUNT_COLUMNS.setdefault(name, f\"{name}_count\")\n",
    "\n",
    "                    train_strict_acc, train_loose_acc, _, _ = compute_accuracy(ensemble_model, ensemble_train_loader, device)\n",
    "                    val_strict_acc, val_loose_acc, y_true, y_pred = compute_accuracy(ensemble_model, ensemble_val_loader, device)\n",
    "\n",
    "                    log_accuracy_to_csv(name, train_strict_acc, train_loose_acc, val_strict_acc, val_loose_acc)\n",
    "\n",
    "                    results.append({\n",
    "                        \"Model Type\": name,\n",
    "                        \"Train Strict Accuracy (%)\": round(train_strict_acc, 2),\n",
    "                        \"Train ±1 Grade Accuracy (%)\": round(train_loose_acc, 2),\n",
    "                        \"Val Strict Accuracy (%)\": round(val_strict_acc, 2),\n",
    "                        \"Val ±1 Grade Accuracy (%)\": round(val_loose_acc, 2),\n",
    "                    })\n",
    "\n",
    "                    save_confusion_matrix_to_excel(y_true, y_pred, class_labels, name, excel_path)\n",
    "                    export_predictions_to_excel(\n",
    "                        ensemble_model,\n",
    "                        ensemble_val_loader,\n",
    "                        device,\n",
    "                        grade_to_label,\n",
    "                        excel_path,\n",
    "                        sheet_name=f\"{name}_preds\",\n",
    "                        model_name=name,\n",
    "                    )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        df_results.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    print(\"=== Model Comparison Summary ===\")\n",
    "    print(df_results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# usage\n",
    "for i in range(25):\n",
    "    compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c11d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal evaluation helpers\n",
    "def evaluate_ordinal_thresholds(model, loader, grade_to_label, device, decision_threshold=0.5, model_name=None, output_dir='./result'):\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "    targets_list = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "            payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            outputs = model(payload)\n",
    "            if not isinstance(outputs, tuple):\n",
    "                raise ValueError('Model is not configured for ordinal outputs.')\n",
    "            probs, logits = outputs\n",
    "            probs_list.append(probs.cpu())\n",
    "            targets_list.append(y.cpu())\n",
    "    if not probs_list:\n",
    "        raise ValueError('No samples available for ordinal evaluation.')\n",
    "    probs = torch.cat(probs_list, dim=0)\n",
    "    targets = torch.cat(targets_list, dim=0)\n",
    "    acc_per_threshold = threshold_accuracy(probs, targets, threshold=decision_threshold).cpu()\n",
    "    grade_by_label = {v: k for k, v in grade_to_label.items()}\n",
    "    threshold_labels = []\n",
    "    for idx in range(acc_per_threshold.size(0)):\n",
    "        grade = grade_by_label.get(idx, f'label_{idx}')\n",
    "        threshold_labels.append(f\"P(>{grade})\")\n",
    "    df = pd.DataFrame({\n",
    "        'threshold': threshold_labels,\n",
    "        'accuracy': (acc_per_threshold.numpy() * 100).round(2)\n",
    "    })\n",
    "    overall_pred = cumulative_to_labels(probs, threshold=decision_threshold)\n",
    "    overall_acc = (overall_pred == targets).float().mean().item() * 100\n",
    "    if model_name:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, f'ordinal_metrics_{model_name}.csv')\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f'Saved threshold table to {output_path}')\n",
    "    print(df)\n",
    "    print(f'Overall accuracy: {overall_acc:.2f}%')\n",
    "    return df, overall_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "261a5130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ordinal model: set_transformer_ordinal ===\n",
      "Epoch 01 — loss: 0.3832\n",
      "Epoch 02 — loss: 0.3188\n",
      "Epoch 03 — loss: 0.3060\n",
      "Epoch 04 — loss: 0.2940\n",
      "Epoch 05 — loss: 0.2862\n",
      "Epoch 06 — loss: 0.2746\n",
      "Epoch 07 — loss: 0.2687\n",
      "Epoch 08 — loss: 0.2599\n",
      "Epoch 09 — loss: 0.2545\n",
      "Epoch 10 — loss: 0.2471\n",
      "Epoch 11 — loss: 0.2401\n",
      "Epoch 12 — loss: 0.2329\n",
      "Epoch 13 — loss: 0.2280\n",
      "Epoch 14 — loss: 0.2227\n",
      "Epoch 15 — loss: 0.2168\n",
      "Epoch 16 — loss: 0.2100\n",
      "Epoch 17 — loss: 0.2051\n",
      "Epoch 18 — loss: 0.2019\n",
      "Epoch 19 — loss: 0.1933\n",
      "Epoch 20 — loss: 0.1902\n",
      "Saved threshold table to ./result/ordinal_metrics_set_transformer_ordinal.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  82.820000\n",
      "1    P(>V5)  81.180000\n",
      "2    P(>V6)  84.839996\n",
      "3    P(>V7)  87.540001\n",
      "4    P(>V8)  91.820000\n",
      "5    P(>V9)  96.489998\n",
      "Overall accuracy: 46.97%\n",
      "=== Training ordinal model: set_transformer_ordinal_xy ===\n",
      "Epoch 01 — loss: 0.3665\n",
      "Epoch 02 — loss: 0.3083\n",
      "Epoch 03 — loss: 0.2954\n",
      "Epoch 04 — loss: 0.2852\n",
      "Epoch 05 — loss: 0.2747\n",
      "Epoch 06 — loss: 0.2659\n",
      "Epoch 07 — loss: 0.2576\n",
      "Epoch 08 — loss: 0.2512\n",
      "Epoch 09 — loss: 0.2422\n",
      "Epoch 10 — loss: 0.2378\n",
      "Epoch 11 — loss: 0.2312\n",
      "Epoch 12 — loss: 0.2259\n",
      "Epoch 13 — loss: 0.2181\n",
      "Epoch 14 — loss: 0.2130\n",
      "Epoch 15 — loss: 0.2086\n",
      "Epoch 16 — loss: 0.2060\n",
      "Epoch 17 — loss: 0.1982\n",
      "Epoch 18 — loss: 0.1931\n",
      "Epoch 19 — loss: 0.1892\n",
      "Epoch 20 — loss: 0.1815\n",
      "Saved threshold table to ./result/ordinal_metrics_set_transformer_ordinal_xy.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  84.019997\n",
      "1    P(>V5)  81.809998\n",
      "2    P(>V6)  84.989998\n",
      "3    P(>V7)  88.070000\n",
      "4    P(>V8)  92.250000\n",
      "5    P(>V9)  96.730003\n",
      "Overall accuracy: 48.46%\n",
      "=== Training ordinal model: set_transformer_ordinal_xy_additive ===\n",
      "Epoch 01 — loss: 0.3685\n",
      "Epoch 02 — loss: 0.3112\n",
      "Epoch 03 — loss: 0.2995\n",
      "Epoch 04 — loss: 0.2930\n",
      "Epoch 05 — loss: 0.2845\n",
      "Epoch 06 — loss: 0.2796\n",
      "Epoch 07 — loss: 0.2731\n",
      "Epoch 08 — loss: 0.2609\n",
      "Epoch 09 — loss: 0.2575\n",
      "Epoch 10 — loss: 0.2522\n",
      "Epoch 11 — loss: 0.2467\n",
      "Epoch 12 — loss: 0.2438\n",
      "Epoch 13 — loss: 0.2367\n",
      "Epoch 14 — loss: 0.2311\n",
      "Epoch 15 — loss: 0.2268\n",
      "Epoch 16 — loss: 0.2208\n",
      "Epoch 17 — loss: 0.2197\n",
      "Epoch 18 — loss: 0.2136\n",
      "Epoch 19 — loss: 0.2101\n",
      "Epoch 20 — loss: 0.2064\n",
      "Saved threshold table to ./result/ordinal_metrics_set_transformer_ordinal_xy_additive.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  84.410004\n",
      "1    P(>V5)  82.150002\n",
      "2    P(>V6)  85.320000\n",
      "3    P(>V7)  87.870003\n",
      "4    P(>V8)  92.779999\n",
      "5    P(>V9)  97.019997\n",
      "Overall accuracy: 48.75%\n",
      "=== Training ordinal model: deepset_ordinal ===\n",
      "Epoch 01 — loss: 0.4696\n",
      "Epoch 02 — loss: 0.3393\n",
      "Epoch 03 — loss: 0.2982\n",
      "Epoch 04 — loss: 0.2898\n",
      "Epoch 05 — loss: 0.2853\n",
      "Epoch 06 — loss: 0.2806\n",
      "Epoch 07 — loss: 0.2783\n",
      "Epoch 08 — loss: 0.2750\n",
      "Epoch 09 — loss: 0.2746\n",
      "Epoch 10 — loss: 0.2735\n",
      "Epoch 11 — loss: 0.2727\n",
      "Epoch 12 — loss: 0.2716\n",
      "Epoch 13 — loss: 0.2717\n",
      "Epoch 14 — loss: 0.2692\n",
      "Epoch 15 — loss: 0.2714\n",
      "Epoch 16 — loss: 0.2702\n",
      "Epoch 17 — loss: 0.2717\n",
      "Epoch 18 — loss: 0.2701\n",
      "Epoch 19 — loss: 0.2704\n",
      "Epoch 20 — loss: 0.2675\n",
      "Saved threshold table to ./result/ordinal_metrics_deepset_ordinal.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  81.419998\n",
      "1    P(>V5)  81.040001\n",
      "2    P(>V6)  84.260002\n",
      "3    P(>V7)  87.250000\n",
      "4    P(>V8)  92.930000\n",
      "5    P(>V9)  97.019997\n",
      "Overall accuracy: 46.05%\n",
      "=== Training ordinal model: deepset_ordinal_xy ===\n",
      "Epoch 01 — loss: 0.4800\n",
      "Epoch 02 — loss: 0.3374\n",
      "Epoch 03 — loss: 0.2958\n",
      "Epoch 04 — loss: 0.2877\n",
      "Epoch 05 — loss: 0.2838\n",
      "Epoch 06 — loss: 0.2819\n",
      "Epoch 07 — loss: 0.2779\n",
      "Epoch 08 — loss: 0.2758\n",
      "Epoch 09 — loss: 0.2753\n",
      "Epoch 10 — loss: 0.2726\n",
      "Epoch 11 — loss: 0.2721\n",
      "Epoch 12 — loss: 0.2730\n",
      "Epoch 13 — loss: 0.2704\n",
      "Epoch 14 — loss: 0.2703\n",
      "Epoch 15 — loss: 0.2704\n",
      "Epoch 16 — loss: 0.2698\n",
      "Epoch 17 — loss: 0.2696\n",
      "Epoch 18 — loss: 0.2688\n",
      "Epoch 19 — loss: 0.2682\n",
      "Epoch 20 — loss: 0.2690\n",
      "Saved threshold table to ./result/ordinal_metrics_deepset_ordinal_xy.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  80.650002\n",
      "1    P(>V5)  79.930000\n",
      "2    P(>V6)  83.779999\n",
      "3    P(>V7)  87.540001\n",
      "4    P(>V8)  92.639999\n",
      "5    P(>V9)  97.019997\n",
      "Overall accuracy: 45.72%\n",
      "=== Training ordinal model: deepset_ordinal_xy_additive ===\n",
      "Epoch 01 — loss: 0.4394\n",
      "Epoch 02 — loss: 0.3289\n",
      "Epoch 03 — loss: 0.3100\n",
      "Epoch 04 — loss: 0.2997\n",
      "Epoch 05 — loss: 0.2946\n",
      "Epoch 06 — loss: 0.2900\n",
      "Epoch 07 — loss: 0.2840\n",
      "Epoch 08 — loss: 0.2821\n",
      "Epoch 09 — loss: 0.2795\n",
      "Epoch 10 — loss: 0.2793\n",
      "Epoch 11 — loss: 0.2786\n",
      "Epoch 12 — loss: 0.2766\n",
      "Epoch 13 — loss: 0.2756\n",
      "Epoch 14 — loss: 0.2759\n",
      "Epoch 15 — loss: 0.2743\n",
      "Epoch 16 — loss: 0.2739\n",
      "Epoch 17 — loss: 0.2719\n",
      "Epoch 18 — loss: 0.2729\n",
      "Epoch 19 — loss: 0.2713\n",
      "Epoch 20 — loss: 0.2723\n",
      "Saved threshold table to ./result/ordinal_metrics_deepset_ordinal_xy_additive.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  80.940002\n",
      "1    P(>V5)  80.080002\n",
      "2    P(>V6)  84.220001\n",
      "3    P(>V7)  88.110001\n",
      "4    P(>V8)  92.059998\n",
      "5    P(>V9)  96.970001\n",
      "Overall accuracy: 44.47%\n",
      "Saved combined ordinal results to ./result/ordinal_result.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --- Ordinal variants sweep ---\n",
    "ordinal_model_types = [\n",
    "    'set_transformer_ordinal',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "]\n",
    "\n",
    "ordinal_tables = []\n",
    "ordinal_summary = []\n",
    "\n",
    "for model_key in ordinal_model_types:\n",
    "    print(f\"=== Training ordinal model: {model_key} ===\")\n",
    "    train_loader, val_loader, model, dataset, train_idx, val_idx = main(model_key)\n",
    "    table, overall_acc = evaluate_ordinal_thresholds(\n",
    "        model,\n",
    "        val_loader,\n",
    "        grade_to_label=grade_to_label,\n",
    "        device=device,\n",
    "        decision_threshold=0.5,\n",
    "        model_name=model_key\n",
    "    )\n",
    "    table = table.copy()\n",
    "    table['model'] = model_key\n",
    "    table['overall_accuracy'] = overall_acc\n",
    "    ordinal_tables.append(table)\n",
    "    ordinal_summary.append({'model': model_key, 'overall_accuracy': overall_acc})\n",
    "\n",
    "if ordinal_tables:\n",
    "    combined = pd.concat(ordinal_tables, ignore_index=True)\n",
    "    summary_df = pd.DataFrame(ordinal_summary)\n",
    "\n",
    "    threshold_order = [f\"P(>{grade})\" for grade in sorted(grade_to_label.keys(), key=lambda g: grade_to_label[g])]\n",
    "    pivot_df = (combined\n",
    "                .pivot_table(index='model', columns='threshold', values='accuracy')\n",
    "                .reindex(columns=[c for c in threshold_order if c in combined['threshold'].unique()]))\n",
    "    pivot_df = pivot_df.sort_index()\n",
    "    pivot_df['Overall Accuracy'] = summary_df.set_index('model')['overall_accuracy']\n",
    "\n",
    "    combined = combined.sort_values(['model', 'threshold']).reset_index(drop=True)\n",
    "    summary_df = summary_df.sort_values('model').reset_index(drop=True)\n",
    "\n",
    "    output_path = './result/ordinal_result.xlsx'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with pd.ExcelWriter(output_path) as writer:\n",
    "        pivot_df.to_excel(writer, sheet_name='threshold_matrix')\n",
    "        combined.to_excel(writer, sheet_name='threshold_long', index=False)\n",
    "        summary_df.to_excel(writer, sheet_name='overall', index=False)\n",
    "    print(f\"Saved combined ordinal results to {output_path}\")\n",
    "else:\n",
    "    print('No ordinal results generated.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "092f8281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 — loss: 1.6954\n",
      "Epoch 02 — loss: 1.6113\n",
      "Epoch 03 — loss: 1.5759\n",
      "Epoch 04 — loss: 1.5471\n",
      "Epoch 05 — loss: 1.5294\n",
      "Epoch 06 — loss: 1.5121\n",
      "Epoch 07 — loss: 1.4916\n",
      "Epoch 08 — loss: 1.4670\n",
      "Epoch 09 — loss: 1.4462\n",
      "Epoch 10 — loss: 1.4240\n",
      "Epoch 11 — loss: 1.4007\n",
      "Epoch 12 — loss: 1.3870\n",
      "Epoch 13 — loss: 1.3634\n",
      "Epoch 14 — loss: 1.3400\n",
      "Epoch 15 — loss: 1.3153\n",
      "Epoch 16 — loss: 1.2935\n",
      "Epoch 17 — loss: 1.2701\n",
      "Epoch 18 — loss: 1.2379\n",
      "Epoch 19 — loss: 1.2152\n",
      "Epoch 20 — loss: 1.2015\n",
      "=== MoonBoard Problem Evaluation ===\n",
      "🔹 Problem Name   : PHYSICAL Q\n",
      "   Holds Used     : {'I18': 3, 'J12': 8, 'F13': 5, 'D10': 4, 'E6': 3, 'J2': 5}\n",
      "   Setter Grade   : V9\n",
      "   Predicted Grade: V8\n",
      "   Dataset Split  : Validation\n",
      "🔹 Problem Name   : TRIANGULATION Q\n",
      "   Holds Used     : {'A18': 5, 'J13': 4, 'D16': 4, 'E9': 3, 'I4': 4}\n",
      "   Setter Grade   : V7\n",
      "   Predicted Grade: V5\n",
      "   Dataset Split  : Train\n",
      "🔹 Problem Name   : WARMUP CRIMPS Q\n",
      "   Holds Used     : {'I18': 3, 'I7': 6, 'I9': 7, 'I15': 8, 'G11': 7, 'J14': 6, 'J12': 8, 'H4': 4, 'K6': 6}\n",
      "   Setter Grade   : V4\n",
      "   Predicted Grade: V4\n",
      "   Dataset Split  : Train\n",
      "🔹 Problem Name   : RONANI QD\n",
      "   Holds Used     : {'F18': 4, 'I15': 8, 'I10': 8, 'K9': 6, 'K6': 6, 'G14': 6, 'D16': 4, 'E9': 3, 'E4': 4, 'H5': 3}\n",
      "   Setter Grade   : V5\n",
      "   Predicted Grade: V4\n",
      "   Dataset Split  : Train\n",
      "🔹 Problem Name   : Don't Fart Alan\n",
      "   Holds Used     : {'K18': 5, 'J15': 3, 'F14': 7, 'F13': 5, 'D10': 4, 'E6': 3, 'I7': 6, 'I5': 7, 'F1': 3}\n",
      "   Setter Grade   : Unknown\n",
      "   Predicted Grade: V7\n",
      "   Dataset Split  : Not Found\n",
      "🔹 Problem Name   : FINALE MAXI 2025 POCKET 2 V9\n",
      "   Holds Used     : {'G3': 5, 'F3': 7, 'F4': 8, 'A6': 3, 'A11': 4, 'B17': 10, 'C9': 6, 'D17': 9, 'H18': 4}\n",
      "   Setter Grade   : Unknown\n",
      "   Predicted Grade: V9\n",
      "   Dataset Split  : Not Found\n",
      "🔹 Problem Name   : Khai's V7\n",
      "   Holds Used     : {'D18': 9, 'A15': 4, 'A12': 3, 'C9': 6, 'E7': 9, 'H8': 8, 'I6': 4, 'E1': 3}\n",
      "   Setter Grade   : Unknown\n",
      "   Predicted Grade: V8\n",
      "   Dataset Split  : Not Found\n",
      "🔹 Problem Name   : YUMS IN MY TUMS 9\n",
      "   Holds Used     : {'F18': 4, 'G12': 3, 'E1': 3, 'D13': 9, 'I9': 7, 'F8': 7, 'I2': 4, 'F16': 4, 'E4': 4, 'E6': 3}\n",
      "   Setter Grade   : V5\n",
      "   Predicted Grade: V5\n",
      "   Dataset Split  : Train\n"
     ]
    }
   ],
   "source": [
    "# evaluate problems\n",
    "def evaluate_problems(\n",
    "    model, problem_dict, hold_to_idx, hold_difficulty, type_to_idx, device,\n",
    "    grade_to_label, hold_to_coord, dataset, train_idx, val_idx, model_type\n",
    "):\n",
    "    label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "    print(\"=== MoonBoard Problem Evaluation ===\")\n",
    "\n",
    "    for fallback_name, holds in problem_dict.items():\n",
    "        try:\n",
    "            hold_idxs = []\n",
    "            diff_values = []\n",
    "            type_vecs = []\n",
    "            xy_coords = []\n",
    "\n",
    "            for h in holds:\n",
    "                if h not in hold_difficulty or h not in hold_to_idx or h not in hold_to_coord:\n",
    "                    raise ValueError(f\"[ERROR] Hold '{h}' missing from required dictionaries.\")\n",
    "\n",
    "                hold_idxs.append(hold_to_idx[h])\n",
    "                difficulty, types = hold_difficulty[h]\n",
    "                diff_values.append(difficulty / 10.0)\n",
    "\n",
    "                # Multi-hot vector\n",
    "                type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "                for t in types:\n",
    "                    if t in type_to_idx:\n",
    "                        type_vec[type_to_idx[t]] = 1.0\n",
    "                type_vecs.append(type_vec)\n",
    "\n",
    "                xy_coords.append(torch.tensor([hold_to_coord[h][0] / 10.0, hold_to_coord[h][1] / 17.0], dtype=torch.float))\n",
    "\n",
    "            # Convert to tensors\n",
    "            hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)\n",
    "            difficulty_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)\n",
    "            type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)\n",
    "            xy_tensor = torch.stack(xy_coords).unsqueeze(0).to(device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # --- Select input format based on model_type ---\n",
    "                if model_type in XY_MODELS:\n",
    "                    input_data = (hold_tensor, difficulty_tensor, type_tensor, xy_tensor)\n",
    "                elif model_type in {'set_transformer', 'deepset', 'set_transformer_ordinal', 'deepset_ordinal'}:\n",
    "                    input_data = (hold_tensor,)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "                payload = input_data[0] if isinstance(input_data, tuple) and len(input_data) == 1 else input_data\n",
    "                outputs = model(payload)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    probs, logits = outputs\n",
    "                    pred_label = (probs > 0.5).sum(dim=1).item()\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                    pred_label = logits.argmax(dim=1).item()\n",
    "                pred_grade = label_to_grade.get(pred_label, f\"Unknown({pred_label})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{fallback_name}] Skipping due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Search in dataset for match\n",
    "        found_idx = None\n",
    "        split = \"Not Found\"\n",
    "        setter_grade = \"Unknown\"\n",
    "        problem_name = fallback_name\n",
    "\n",
    "        for idx_item, item in enumerate(dataset.raw):\n",
    "            if set(item['holds']) == set(holds):\n",
    "                found_idx = idx_item\n",
    "                setter_grade = item.get('grade', 'Unknown')\n",
    "                problem_name = item.get('problem_name', fallback_name)\n",
    "                if found_idx in train_idx:\n",
    "                    split = \"Train\"\n",
    "                elif found_idx in val_idx:\n",
    "                    split = \"Validation\"\n",
    "                else:\n",
    "                    split = \"Found (Unknown Split)\"\n",
    "                break\n",
    "\n",
    "        holds_with_difficulty = {h: hold_difficulty[h][0] if h in hold_difficulty else \"N/A\" for h in holds}\n",
    "        print(f\"🔹 Problem Name   : {problem_name}\")\n",
    "        print(f\"   Holds Used     : {holds_with_difficulty}\")\n",
    "        print(f\"   Setter Grade   : {setter_grade}\")\n",
    "        print(f\"   Predicted Grade: {pred_grade}\")\n",
    "        print(f\"   Dataset Split  : {split}\")\n",
    "\n",
    "named_problems = {\n",
    "    \"Physical V9 Benchmark\": [\"I18\", \"J12\", \"F13\", \"D10\", \"E6\", \"J2\"],\n",
    "    \"Triangulation V7\": [\"A18\", \"J13\", \"D16\", \"E9\", \"E9\", \"I4\"],\n",
    "    \"warmup crimps\": [\"I18\", \"I7\", \"I9\", \"I15\", \"G11\", \"J14\", \"J12\", \"I15\", \"J14\", \"H4\", \"K6\"],\n",
    "    \"Ronani V5\": [\"F18\", \"I15\", \"I10\", \"K9\", \"K6\", \"G14\", \"D16\", \"E9\", \"K6\", \"I15\", \"E4\", \"H5\"],\n",
    "    \"Don't Fart Alan\": [\"K18\", \"J15\", \"F14\", \"F13\", \"D10\", \"E6\", \"I7\", \"I5\", \"F1\"],\n",
    "    \"FINALE MAXI 2025 POCKET 2 V9\": [\"G3\", \"F3\", \"F4\", \"A6\", \"A11\", \"B17\", \"C9\", \"D17\", \"H18\"],\n",
    "    \"Khai's V7\": [\"D18\", \"A15\", \"A12\", \"C9\", \"E7\", \"H8\", \"I6\", \"E1\"],\n",
    "    \"Yums In My Tums V5\": [\"F18\", \"G12\", \"E1\", \"D13\", \"I9\", \"F8\", \"I2\", \"F16\", \"E4\", \"E6\"],\n",
    "}\n",
    "\n",
    "team_problems = {\n",
    "    \"simma mot strommen\": [\"A18\", \"C12\", \"A9\", \"B14\", \"B16\", \"D1\", \"F5\", \"F5\"],\n",
    "    \"MAXIMUS!\": [\"K18\", \"E3\", \"K14\", \"I13\", \"K7\", \"I2\", \"H16\", \"K11\", \"G7\", \"H4\"],\n",
    "    \"interstate\": [\"K18\", \"H17\", \"J11\", \"I9\", \"G13\", \"H15\", \"I5\", \"I6\"],\n",
    "    \"krakatoa pusher\": [\"H18\", \"H11\", \"J8\", \"F7\", \"K15\", \"F4\", \"J3\"],\n",
    "    \"doublement\": [\"A18\", \"E16\", \"F8\", \"B14\", \"G8\", \"E12\", \"F4\", \"F3\", \"F3\"],\n",
    "    \"animal instinct\": [\"F18\", \"J11\", \"F9\", \"H15\", \"E13\", \"J11\", \"I6\", \"F4\"],\n",
    "    \"blue bin day\": [\"B18\", \"C18\", \"A8\", \"C12\", \"B15\", \"A5\", \"C3\"]\n",
    "}\n",
    "\n",
    "# --- Classification baseline run ---\n",
    "clf_train_loader, clf_val_loader, clf_model, clf_dataset, clf_train_idx, clf_val_idx = main('set_transformer_additive')\n",
    "\n",
    "evaluate_problems(\n",
    "    model=clf_model,\n",
    "    problem_dict=named_problems,\n",
    "    hold_to_idx=hold_to_idx,\n",
    "    hold_difficulty=hold_difficulty,\n",
    "    type_to_idx=type_to_idx,\n",
    "    device=device,\n",
    "    grade_to_label=grade_to_label,\n",
    "    hold_to_coord=hold_to_coord,\n",
    "    dataset=clf_dataset,\n",
    "    train_idx=clf_train_idx,\n",
    "    val_idx=clf_val_idx,\n",
    "    model_type='set_transformer_additive'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "228393ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "def visualize_attention_for_problem(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device):\n",
    "    model.eval()\n",
    "\n",
    "    hold_idxs = []\n",
    "    diff_values = []\n",
    "    type_vecs = []\n",
    "    xy_coords = []\n",
    "\n",
    "    for h in holds:\n",
    "        hold_idxs.append(hold_to_idx[h])\n",
    "        difficulty, types = hold_difficulty[h]\n",
    "        diff_values.append(difficulty / 10.0)\n",
    "\n",
    "        type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "        for t in types:\n",
    "            if t in type_to_idx:\n",
    "                type_vec[type_to_idx[t]] = 1.0\n",
    "        type_vecs.append(type_vec)\n",
    "\n",
    "        x, y = hold_to_coord[h]\n",
    "        xy_coords.append([x / 10.0, y / 17.0])\n",
    "\n",
    "    hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    diff_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)\n",
    "    type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)\n",
    "    xy_tensor = torch.tensor(xy_coords, dtype=torch.float).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model((hold_tensor, diff_tensor, type_tensor, xy_tensor))\n",
    "\n",
    "    attn_isab1 = model.encoder[0].mab0.attn_weights.cpu().numpy()\n",
    "    attn_isab2 = model.encoder[1].mab0.attn_weights.cpu().numpy()\n",
    "\n",
    "    num_heads = attn_isab1.shape[0]\n",
    "    fig, axes = plt.subplots(2, num_heads, figsize=(4 * num_heads, 8))\n",
    "    if num_heads == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "\n",
    "    for h in range(num_heads):\n",
    "        sns.heatmap(attn_isab1[h], ax=axes[0, h], cmap=\"viridis\", xticklabels=holds)\n",
    "        axes[0, h].set_title(f\"ISAB1 – Head {h}\")\n",
    "        axes[0, h].set_xlabel(\"Key (Hold)\")\n",
    "        axes[0, h].set_ylabel(\"Seed\")\n",
    "\n",
    "        sns.heatmap(attn_isab2[h], ax=axes[1, h], cmap=\"viridis\", xticklabels=holds)\n",
    "        axes[1, h].set_title(f\"ISAB2 – Head {h}\")\n",
    "        axes[1, h].set_xlabel(\"Key (Hold)\")\n",
    "        axes[1, h].set_ylabel(\"Seed\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66931c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_attention_per_hold(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device):\n",
    "    model.eval()\n",
    "\n",
    "    hold_idxs = []\n",
    "    diff_values = []\n",
    "    type_vecs = []\n",
    "    xy_coords = []\n",
    "\n",
    "    for h in holds:\n",
    "        hold_idxs.append(hold_to_idx[h])\n",
    "        difficulty, types = hold_difficulty[h]\n",
    "        diff_values.append(difficulty / 10.0)\n",
    "\n",
    "        # Multi-hot type vector\n",
    "        type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "        for t in types:\n",
    "            if t in type_to_idx:\n",
    "                type_vec[type_to_idx[t]] = 1.0\n",
    "        type_vecs.append(type_vec)\n",
    "\n",
    "        # XY coordinate\n",
    "        if h not in hold_to_coord:\n",
    "            raise ValueError(f\"[ERROR] Hold '{h}' has no coordinate in hold_to_coord.\")\n",
    "        x, y = hold_to_coord[h]\n",
    "        xy_coords.append([x / 10.0, y / 17.0])\n",
    "\n",
    "    # Build model input tensors\n",
    "    hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)       # (1, N)\n",
    "    diff_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)    # (1, N)\n",
    "    type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)                          # (1, N, T)\n",
    "    xy_tensor = torch.tensor(xy_coords, dtype=torch.float).unsqueeze(0).to(device)        # (1, N, 2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model((hold_tensor, diff_tensor, type_tensor, xy_tensor))\n",
    "\n",
    "    attn_weights = model.encoder[0].mab0.attn_weights  # shape: (heads, seeds, holds)\n",
    "    avg_attn = attn_weights.mean(dim=(0, 1)).cpu().numpy()  # average across heads & seeds → (num_holds,)\n",
    "\n",
    "    return list(zip(holds, avg_attn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fce93aac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The provided model does not support attention visualization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m holds \u001b[38;5;241m=\u001b[39m named_problems[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarmup crimps\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmab0\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmab0, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_weights\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provided model does not support attention visualization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m visualize_attention_for_problem(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device)\n\u001b[1;32m      9\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m get_avg_attention_per_hold(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device)\n",
      "\u001b[0;31mValueError\u001b[0m: The provided model does not support attention visualization."
     ]
    }
   ],
   "source": [
    "# Visualize attention and scores (with XY support)\n",
    "\n",
    "holds = named_problems[\"warmup crimps\"]\n",
    "\n",
    "if not hasattr(model.encoder[0], 'mab0') or not hasattr(model.encoder[0].mab0, 'attn_weights'):\n",
    "    raise ValueError(\"The provided model does not support attention visualization.\")\n",
    "\n",
    "visualize_attention_for_problem(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device)\n",
    "attention_scores = get_avg_attention_per_hold(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device)\n",
    "\n",
    "# Print sorted scores\n",
    "attention_scores_sorted = sorted(attention_scores, key=lambda x: x[1], reverse=True)\n",
    "print(\"Average Attention Per Hold (sorted):\")\n",
    "for h, score in attention_scores_sorted:\n",
    "    difficulty = hold_difficulty[h][0] if h in hold_difficulty else \"N/A\"\n",
    "    print(f\"{h}: {score:.4f} (difficulty: {difficulty})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb97c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read accuracy.csv file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./result/accuracy.csv')\n",
    "# print(df)\n",
    "\n",
    "filtered_df = df[df['model'] == 'set_transformer_xy']\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa75f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_prp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
