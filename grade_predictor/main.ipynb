{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d877fdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /Users/patrickdharma/anaconda3/envs/sousei/lib/python3.9/site-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe7aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import torch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d7d8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "import csv\n",
    "from modules_modified import ISAB, SAB, PMA\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31d760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models\n",
    "from model import (\n",
    "    SetTransformerClassifierXY,\n",
    "    SetTransformerClassifierXYAdditive,\n",
    "    SetTransformerClassifier,\n",
    "    DeepSetClassifierXYAdditive,\n",
    "    DeepSetClassifierXY,\n",
    "    DeepSetClassifier,\n",
    "    SetTransformerOrdinalXY,\n",
    "    SetTransformerOrdinalXYAdditive,\n",
    "    SetTransformerOrdinal,\n",
    "    DeepSetOrdinalXYAdditive,\n",
    "    DeepSetOrdinalXY,\n",
    "    DeepSetOrdinal,\n",
    ")\n",
    "from utils_ordinal import ordinal_logistic_loss, cumulative_to_labels, threshold_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0e8bb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A1': 0, 'B1': 1, 'C1': 2, 'D1': 3, 'E1': 4, 'F1': 5, 'G1': 6, 'H1': 7, 'I1': 8, 'J1': 9, 'K1': 10, 'A2': 11, 'B2': 12, 'C2': 13, 'D2': 14, 'E2': 15, 'F2': 16, 'G2': 17, 'H2': 18, 'I2': 19, 'J2': 20, 'K2': 21, 'A3': 22, 'B3': 23, 'C3': 24, 'D3': 25, 'E3': 26, 'F3': 27, 'G3': 28, 'H3': 29, 'I3': 30, 'J3': 31, 'K3': 32, 'A4': 33, 'B4': 34, 'C4': 35, 'D4': 36, 'E4': 37, 'F4': 38, 'G4': 39, 'H4': 40, 'I4': 41, 'J4': 42, 'K4': 43, 'A5': 44, 'B5': 45, 'C5': 46, 'D5': 47, 'E5': 48, 'F5': 49, 'G5': 50, 'H5': 51, 'I5': 52, 'J5': 53, 'K5': 54, 'A6': 55, 'B6': 56, 'C6': 57, 'D6': 58, 'E6': 59, 'F6': 60, 'G6': 61, 'H6': 62, 'I6': 63, 'J6': 64, 'K6': 65, 'A7': 66, 'B7': 67, 'C7': 68, 'D7': 69, 'E7': 70, 'F7': 71, 'G7': 72, 'H7': 73, 'I7': 74, 'J7': 75, 'K7': 76, 'A8': 77, 'B8': 78, 'C8': 79, 'D8': 80, 'E8': 81, 'F8': 82, 'G8': 83, 'H8': 84, 'I8': 85, 'J8': 86, 'K8': 87, 'A9': 88, 'B9': 89, 'C9': 90, 'D9': 91, 'E9': 92, 'F9': 93, 'G9': 94, 'H9': 95, 'I9': 96, 'J9': 97, 'K9': 98, 'A10': 99, 'B10': 100, 'C10': 101, 'D10': 102, 'E10': 103, 'F10': 104, 'G10': 105, 'H10': 106, 'I10': 107, 'J10': 108, 'K10': 109, 'A11': 110, 'B11': 111, 'C11': 112, 'D11': 113, 'E11': 114, 'F11': 115, 'G11': 116, 'H11': 117, 'I11': 118, 'J11': 119, 'K11': 120, 'A12': 121, 'B12': 122, 'C12': 123, 'D12': 124, 'E12': 125, 'F12': 126, 'G12': 127, 'H12': 128, 'I12': 129, 'J12': 130, 'K12': 131, 'A13': 132, 'B13': 133, 'C13': 134, 'D13': 135, 'E13': 136, 'F13': 137, 'G13': 138, 'H13': 139, 'I13': 140, 'J13': 141, 'K13': 142, 'A14': 143, 'B14': 144, 'C14': 145, 'D14': 146, 'E14': 147, 'F14': 148, 'G14': 149, 'H14': 150, 'I14': 151, 'J14': 152, 'K14': 153, 'A15': 154, 'B15': 155, 'C15': 156, 'D15': 157, 'E15': 158, 'F15': 159, 'G15': 160, 'H15': 161, 'I15': 162, 'J15': 163, 'K15': 164, 'A16': 165, 'B16': 166, 'C16': 167, 'D16': 168, 'E16': 169, 'F16': 170, 'G16': 171, 'H16': 172, 'I16': 173, 'J16': 174, 'K16': 175, 'A17': 176, 'B17': 177, 'C17': 178, 'D17': 179, 'E17': 180, 'F17': 181, 'G17': 182, 'H17': 183, 'I17': 184, 'J17': 185, 'K17': 186, 'A18': 187, 'B18': 188, 'C18': 189, 'D18': 190, 'E18': 191, 'F18': 192, 'G18': 193, 'H18': 194, 'I18': 195, 'J18': 196, 'K18': 197}\n"
     ]
    }
   ],
   "source": [
    "# Mappings --------------------------------------------------------\n",
    "# Map each hold like \"A1\"…\"K18\" to an integer 0…(11*18−1)=197\n",
    "cols = [chr(c) for c in range(ord('A'), ord('K')+1)]\n",
    "rows = list(range(1, 19))\n",
    "hold_to_idx = {f\"{c}{r}\": i for i, (c, r) in enumerate((c, r) for r in rows for c in cols)}\n",
    "\n",
    "\n",
    "# Map grades \"V4\"…\"V11\" \n",
    "grade_to_label = {f\"V{i}\": i - 4 for i in range(4, 12)}  \n",
    "label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "print(hold_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50a6dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully parsed hold difficulty file\n",
      "successfully prepare type vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Holds difficulty data --------------------------------------------------------\n",
    "hold_difficulty = {}\n",
    "with open(\"data/hold_difficulty.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if \":\" not in line:\n",
    "            continue  # skip malformed line\n",
    "        hold, rest = line.strip().split(\":\", 1)\n",
    "        parts = rest.strip().split(\",\")\n",
    "        difficulty = int(parts[0].strip())\n",
    "        types = [t.strip() for t in parts[1:]]\n",
    "        hold_difficulty[hold.strip()] = (difficulty, types)\n",
    "    print(\"successfully parsed hold difficulty file\")\n",
    "\n",
    "# prepare type vocabulary\n",
    "unique_types = set()\n",
    "for _, (_, types) in hold_difficulty.items():\n",
    "    unique_types.update(types)\n",
    "\n",
    "type_to_idx = {t: i for i, t in enumerate(sorted(unique_types))}\n",
    "print(f\"successfully prepare type vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e5b93f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created (x,y) position to each hold:\n",
      "{'A1': (0, 0), 'A2': (0, 1), 'A3': (0, 2), 'A4': (0, 3), 'A5': (0, 4), 'A6': (0, 5), 'A7': (0, 6), 'A8': (0, 7), 'A9': (0, 8), 'A10': (0, 9), 'A11': (0, 10), 'A12': (0, 11), 'A13': (0, 12), 'A14': (0, 13), 'A15': (0, 14), 'A16': (0, 15), 'A17': (0, 16), 'A18': (0, 17), 'B1': (1, 0), 'B2': (1, 1), 'B3': (1, 2), 'B4': (1, 3), 'B5': (1, 4), 'B6': (1, 5), 'B7': (1, 6), 'B8': (1, 7), 'B9': (1, 8), 'B10': (1, 9), 'B11': (1, 10), 'B12': (1, 11), 'B13': (1, 12), 'B14': (1, 13), 'B15': (1, 14), 'B16': (1, 15), 'B17': (1, 16), 'B18': (1, 17), 'C1': (2, 0), 'C2': (2, 1), 'C3': (2, 2), 'C4': (2, 3), 'C5': (2, 4), 'C6': (2, 5), 'C7': (2, 6), 'C8': (2, 7), 'C9': (2, 8), 'C10': (2, 9), 'C11': (2, 10), 'C12': (2, 11), 'C13': (2, 12), 'C14': (2, 13), 'C15': (2, 14), 'C16': (2, 15), 'C17': (2, 16), 'C18': (2, 17), 'D1': (3, 0), 'D2': (3, 1), 'D3': (3, 2), 'D4': (3, 3), 'D5': (3, 4), 'D6': (3, 5), 'D7': (3, 6), 'D8': (3, 7), 'D9': (3, 8), 'D10': (3, 9), 'D11': (3, 10), 'D12': (3, 11), 'D13': (3, 12), 'D14': (3, 13), 'D15': (3, 14), 'D16': (3, 15), 'D17': (3, 16), 'D18': (3, 17), 'E1': (4, 0), 'E2': (4, 1), 'E3': (4, 2), 'E4': (4, 3), 'E5': (4, 4), 'E6': (4, 5), 'E7': (4, 6), 'E8': (4, 7), 'E9': (4, 8), 'E10': (4, 9), 'E11': (4, 10), 'E12': (4, 11), 'E13': (4, 12), 'E14': (4, 13), 'E15': (4, 14), 'E16': (4, 15), 'E17': (4, 16), 'E18': (4, 17), 'F1': (5, 0), 'F2': (5, 1), 'F3': (5, 2), 'F4': (5, 3), 'F5': (5, 4), 'F6': (5, 5), 'F7': (5, 6), 'F8': (5, 7), 'F9': (5, 8), 'F10': (5, 9), 'F11': (5, 10), 'F12': (5, 11), 'F13': (5, 12), 'F14': (5, 13), 'F15': (5, 14), 'F16': (5, 15), 'F17': (5, 16), 'F18': (5, 17), 'G1': (6, 0), 'G2': (6, 1), 'G3': (6, 2), 'G4': (6, 3), 'G5': (6, 4), 'G6': (6, 5), 'G7': (6, 6), 'G8': (6, 7), 'G9': (6, 8), 'G10': (6, 9), 'G11': (6, 10), 'G12': (6, 11), 'G13': (6, 12), 'G14': (6, 13), 'G15': (6, 14), 'G16': (6, 15), 'G17': (6, 16), 'G18': (6, 17), 'H1': (7, 0), 'H2': (7, 1), 'H3': (7, 2), 'H4': (7, 3), 'H5': (7, 4), 'H6': (7, 5), 'H7': (7, 6), 'H8': (7, 7), 'H9': (7, 8), 'H10': (7, 9), 'H11': (7, 10), 'H12': (7, 11), 'H13': (7, 12), 'H14': (7, 13), 'H15': (7, 14), 'H16': (7, 15), 'H17': (7, 16), 'H18': (7, 17), 'I1': (8, 0), 'I2': (8, 1), 'I3': (8, 2), 'I4': (8, 3), 'I5': (8, 4), 'I6': (8, 5), 'I7': (8, 6), 'I8': (8, 7), 'I9': (8, 8), 'I10': (8, 9), 'I11': (8, 10), 'I12': (8, 11), 'I13': (8, 12), 'I14': (8, 13), 'I15': (8, 14), 'I16': (8, 15), 'I17': (8, 16), 'I18': (8, 17), 'J1': (9, 0), 'J2': (9, 1), 'J3': (9, 2), 'J4': (9, 3), 'J5': (9, 4), 'J6': (9, 5), 'J7': (9, 6), 'J8': (9, 7), 'J9': (9, 8), 'J10': (9, 9), 'J11': (9, 10), 'J12': (9, 11), 'J13': (9, 12), 'J14': (9, 13), 'J15': (9, 14), 'J16': (9, 15), 'J17': (9, 16), 'J18': (9, 17), 'K1': (10, 0), 'K2': (10, 1), 'K3': (10, 2), 'K4': (10, 3), 'K5': (10, 4), 'K6': (10, 5), 'K7': (10, 6), 'K8': (10, 7), 'K9': (10, 8), 'K10': (10, 9), 'K11': (10, 10), 'K12': (10, 11), 'K13': (10, 12), 'K14': (10, 13), 'K15': (10, 14), 'K16': (10, 15), 'K17': (10, 16), 'K18': (10, 17)}\n"
     ]
    }
   ],
   "source": [
    "# assign x,y position to each holds -------------------------------\n",
    "import string\n",
    "\n",
    "# Board columns A–K → indices 0–10\n",
    "cols = list(string.ascii_uppercase[:11])  # A–K\n",
    "# Rows 1–18 → indices 0–17\n",
    "rows = list(range(1, 19))  # 1–18\n",
    "\n",
    "# Generate hold_to_coord dictionary\n",
    "hold_to_coord = {}\n",
    "\n",
    "for x, col in enumerate(cols):\n",
    "    for y, row in enumerate(rows):\n",
    "        hold_name = f\"{col}{row}\"\n",
    "        hold_to_coord[hold_name] = (x, y)\n",
    "\n",
    "print(\"successfully created (x,y) position to each hold:\")\n",
    "print(hold_to_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcbe64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonBoardDataset(Dataset):\n",
    "    def __init__(self, json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord, max_difficulty=10):\n",
    "        self.hold_to_idx = hold_to_idx\n",
    "        self.grade_to_label = grade_to_label\n",
    "        self.hold_difficulty = hold_difficulty\n",
    "        self.type_to_idx = type_to_idx\n",
    "        self.hold_to_coord = hold_to_coord\n",
    "        self.max_difficulty = max_difficulty\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.raw = [json.loads(line) for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.raw[idx]\n",
    "        holds = item['holds']\n",
    "\n",
    "        hold_idxs = []\n",
    "        diff_values = []\n",
    "        type_vecs = []\n",
    "        xy_coords = []\n",
    "\n",
    "        for h in holds:\n",
    "            hold_idxs.append(self.hold_to_idx[h])\n",
    "\n",
    "            difficulty, types = self.hold_difficulty[h]\n",
    "            diff_values.append(difficulty / self.max_difficulty)\n",
    "\n",
    "            # multi-hot vector\n",
    "            type_vec = torch.zeros(len(self.type_to_idx), dtype=torch.float)\n",
    "            for t in types:\n",
    "                if t in self.type_to_idx:\n",
    "                    type_vec[self.type_to_idx[t]] = 1.0\n",
    "            type_vecs.append(type_vec)\n",
    "\n",
    "            # normalized (x, y)\n",
    "            x, y = self.hold_to_coord[h]\n",
    "            xy_coords.append(torch.tensor([x / 10.0, y / 17.0], dtype=torch.float))\n",
    "\n",
    "        return {\n",
    "            \"indices\": torch.tensor(hold_idxs, dtype=torch.long),\n",
    "            \"difficulty\": torch.tensor(diff_values, dtype=torch.float),\n",
    "            \"type\": torch.stack(type_vecs),       # (N, T)\n",
    "            \"xy\": torch.stack(xy_coords)          # (N, 2)\n",
    "        }, torch.tensor(self.grade_to_label[item['grade']], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66965b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop ------------------------------------------------\n",
    "\n",
    "# --- Set Hyperparameters ---\n",
    "json_path = './data/cleaned_moonboard2024_grouped.json'\n",
    "embed_dim = 64\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "XY_MODELS = {\n",
    "    'set_transformer_xy',\n",
    "    'set_transformer_additive',\n",
    "    'deepset_xy',\n",
    "    'deepset_xy_additive',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "}\n",
    "\n",
    "ORDINAL_MODELS = {\n",
    "    'set_transformer_ordinal',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "}\n",
    "\n",
    "# --- Collate Function Factory ---\n",
    "def make_collate_fn(model_type):\n",
    "    def collate_fn(batch):\n",
    "        X_indices = [x['indices'] for x, _ in batch]\n",
    "        X_difficulty = [x['difficulty'] for x, _ in batch]\n",
    "        X_type = [x['type'] for x, _ in batch]\n",
    "        y_batch = [y for _, y in batch]\n",
    "\n",
    "        X_indices = pad_sequence(X_indices, batch_first=True)\n",
    "        X_difficulty = pad_sequence(X_difficulty, batch_first=True)\n",
    "        X_type = pad_sequence(X_type, batch_first=True)\n",
    "        y_tensor = torch.stack(y_batch)\n",
    "\n",
    "        if model_type in XY_MODELS:\n",
    "            X_xy = [x['xy'] for x, _ in batch]\n",
    "            X_xy = pad_sequence(X_xy, batch_first=True)\n",
    "            return (X_indices, X_difficulty, X_type, X_xy), y_tensor\n",
    "        else:\n",
    "            return (X_indices,), y_tensor\n",
    "    return collate_fn\n",
    "\n",
    "# --- Dataset Loader ---\n",
    "def load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord):\n",
    "    return MoonBoardDataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "\n",
    "# --- DataLoader Preparation ---\n",
    "def prepare_dataloaders(dataset, grade_to_label, batch_size, collate_fn):\n",
    "    targets = [grade_to_label[item['grade']] for item in dataset.raw]\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(targets), y=targets)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(dataset))), test_size=0.2, stratify=targets, random_state=42\n",
    "    )\n",
    "\n",
    "    train_data = Subset(dataset, train_idx)\n",
    "    val_data = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, class_weights, train_idx, val_idx\n",
    "\n",
    "# --- Training ---\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, is_ordinal=False):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "            payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            outputs = model(payload)\n",
    "            if is_ordinal:\n",
    "                probs, logits = outputs\n",
    "                loss = criterion(logits, y)\n",
    "            else:\n",
    "                logits = outputs\n",
    "                loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch:02d} — loss: {total_loss / len(train_loader):.4f}\")\n",
    "    return model\n",
    "\n",
    "# --- Main Per Model ---\n",
    "def main(model_type):\n",
    "    dataset = load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "    targets = [grade_to_label[item['grade']] for item in dataset.raw]\n",
    "    num_classes = len(np.unique(targets))\n",
    "    vocab_size = len(hold_to_idx)\n",
    "    type_vec_dim = len(type_to_idx)\n",
    "    is_ordinal = model_type in ORDINAL_MODELS\n",
    "\n",
    "    if model_type == 'set_transformer':\n",
    "        ModelClass = SetTransformerClassifier\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'set_transformer_xy':\n",
    "        ModelClass = SetTransformerClassifierXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_additive':\n",
    "        ModelClass = SetTransformerClassifierXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset':\n",
    "        ModelClass = DeepSetClassifier\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'deepset_xy':\n",
    "        ModelClass = DeepSetClassifierXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_xy_additive':\n",
    "        ModelClass = DeepSetClassifierXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_ordinal':\n",
    "        ModelClass = SetTransformerOrdinal\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'set_transformer_ordinal_xy':\n",
    "        ModelClass = SetTransformerOrdinalXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_ordinal_xy_additive':\n",
    "        ModelClass = SetTransformerOrdinalXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_ordinal':\n",
    "        ModelClass = DeepSetOrdinal\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'deepset_ordinal_xy':\n",
    "        ModelClass = DeepSetOrdinalXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_ordinal_xy_additive':\n",
    "        ModelClass = DeepSetOrdinalXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "    collate_fn = make_collate_fn(model_type)\n",
    "    train_loader, val_loader, class_weights, train_idx, val_idx = prepare_dataloaders(dataset, grade_to_label, batch_size, collate_fn)\n",
    "\n",
    "    model = ModelClass(**kwargs).to(device)\n",
    "    model.is_ordinal = is_ordinal\n",
    "    model.num_classes = num_classes\n",
    "\n",
    "    if is_ordinal:\n",
    "        def criterion_fn(logits, targets):\n",
    "            return ordinal_logistic_loss(logits, targets)\n",
    "    else:\n",
    "        criterion_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model = train_model(model, train_loader, val_loader, criterion_fn, optimizer, epochs, is_ordinal=is_ordinal)\n",
    "    return train_loader, val_loader, model, dataset, train_idx, val_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f5db06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training set_transformer =====\n",
      "Epoch 01 — loss: 1.7587\n",
      "Epoch 02 — loss: 1.6138\n",
      "Epoch 03 — loss: 1.5777\n",
      "Epoch 04 — loss: 1.5432\n",
      "Epoch 05 — loss: 1.5168\n",
      "Epoch 06 — loss: 1.4786\n",
      "Epoch 07 — loss: 1.4544\n",
      "Epoch 08 — loss: 1.4270\n",
      "Epoch 09 — loss: 1.3925\n",
      "Epoch 10 — loss: 1.3617\n",
      "Epoch 11 — loss: 1.3321\n",
      "Epoch 12 — loss: 1.3078\n",
      "Epoch 13 — loss: 1.2692\n",
      "Epoch 14 — loss: 1.2354\n",
      "Epoch 15 — loss: 1.2097\n",
      "Epoch 16 — loss: 1.1642\n",
      "Epoch 17 — loss: 1.1484\n",
      "Epoch 18 — loss: 1.1060\n",
      "Epoch 19 — loss: 1.0737\n",
      "Epoch 20 — loss: 1.0235\n",
      "Confusion matrix for set_transformer saved and inserted into result/model_comparison_results.xlsx (sheet: set_transformer)\n",
      "Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "\n",
      "===== Training deepset =====\n",
      "Epoch 01 — loss: 1.8994\n",
      "Epoch 02 — loss: 1.6313\n",
      "Epoch 03 — loss: 1.5484\n",
      "Epoch 04 — loss: 1.5198\n",
      "Epoch 05 — loss: 1.5111\n",
      "Epoch 06 — loss: 1.4882\n",
      "Epoch 07 — loss: 1.4822\n",
      "Epoch 08 — loss: 1.4734\n",
      "Epoch 09 — loss: 1.4801\n",
      "Epoch 10 — loss: 1.4688\n",
      "Epoch 11 — loss: 1.4702\n",
      "Epoch 12 — loss: 1.4575\n",
      "Epoch 13 — loss: 1.4618\n",
      "Epoch 14 — loss: 1.4616\n",
      "Epoch 15 — loss: 1.4578\n",
      "Epoch 16 — loss: 1.4565\n",
      "Epoch 17 — loss: 1.4462\n",
      "Epoch 18 — loss: 1.4422\n",
      "Epoch 19 — loss: 1.4486\n",
      "Epoch 20 — loss: 1.4491\n",
      "Confusion matrix for deepset saved and inserted into result/model_comparison_results.xlsx (sheet: deepset)\n",
      "Predictions for deepset_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "\n",
      "===== Training set_transformer_xy =====\n",
      "Epoch 01 — loss: 1.7268\n",
      "Epoch 02 — loss: 1.6100\n",
      "Epoch 03 — loss: 1.5720\n",
      "Epoch 04 — loss: 1.5382\n",
      "Epoch 05 — loss: 1.5123\n",
      "Epoch 06 — loss: 1.4795\n",
      "Epoch 07 — loss: 1.4528\n",
      "Epoch 08 — loss: 1.4243\n",
      "Epoch 09 — loss: 1.3967\n",
      "Epoch 10 — loss: 1.3672\n",
      "Epoch 11 — loss: 1.3254\n",
      "Epoch 12 — loss: 1.2993\n",
      "Epoch 13 — loss: 1.2735\n",
      "Epoch 14 — loss: 1.2228\n",
      "Epoch 15 — loss: 1.1964\n",
      "Epoch 16 — loss: 1.1517\n",
      "Epoch 17 — loss: 1.1162\n",
      "Epoch 18 — loss: 1.0797\n",
      "Epoch 19 — loss: 1.0462\n",
      "Epoch 20 — loss: 1.0006\n",
      "Confusion matrix for set_transformer_xy saved and inserted into result/model_comparison_results.xlsx (sheet: set_transformer_xy)\n",
      "Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "\n",
      "===== Training deepset_xy =====\n",
      "Epoch 01 — loss: 1.8589\n",
      "Epoch 02 — loss: 1.5974\n",
      "Epoch 03 — loss: 1.5429\n",
      "Epoch 04 — loss: 1.5204\n",
      "Epoch 05 — loss: 1.5008\n",
      "Epoch 06 — loss: 1.4939\n",
      "Epoch 07 — loss: 1.4858\n",
      "Epoch 08 — loss: 1.4826\n",
      "Epoch 09 — loss: 1.4798\n",
      "Epoch 10 — loss: 1.4728\n",
      "Epoch 11 — loss: 1.4637\n",
      "Epoch 12 — loss: 1.4615\n",
      "Epoch 13 — loss: 1.4630\n",
      "Epoch 14 — loss: 1.4569\n",
      "Epoch 15 — loss: 1.4680\n",
      "Epoch 16 — loss: 1.4611\n",
      "Epoch 17 — loss: 1.4515\n",
      "Epoch 18 — loss: 1.4556\n",
      "Epoch 19 — loss: 1.4471\n",
      "Epoch 20 — loss: 1.4487\n",
      "Confusion matrix for deepset_xy saved and inserted into result/model_comparison_results.xlsx (sheet: deepset_xy)\n",
      "Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "\n",
      "===== Training set_transformer_additive =====\n",
      "Epoch 01 — loss: 1.7152\n",
      "Epoch 02 — loss: 1.6248\n",
      "Epoch 03 — loss: 1.5941\n",
      "Epoch 04 — loss: 1.5582\n",
      "Epoch 05 — loss: 1.5367\n",
      "Epoch 06 — loss: 1.5109\n",
      "Epoch 07 — loss: 1.4940\n",
      "Epoch 08 — loss: 1.4739\n",
      "Epoch 09 — loss: 1.4480\n",
      "Epoch 10 — loss: 1.4281\n",
      "Epoch 11 — loss: 1.4087\n",
      "Epoch 12 — loss: 1.3817\n",
      "Epoch 13 — loss: 1.3524\n",
      "Epoch 14 — loss: 1.3246\n",
      "Epoch 15 — loss: 1.3058\n",
      "Epoch 16 — loss: 1.2906\n",
      "Epoch 17 — loss: 1.2533\n",
      "Epoch 18 — loss: 1.2252\n",
      "Epoch 19 — loss: 1.2015\n",
      "Epoch 20 — loss: 1.1686\n",
      "Confusion matrix for set_transformer_additive saved and inserted into result/model_comparison_results.xlsx (sheet: set_transformer_additive)\n",
      "Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "\n",
      "===== Training deepset_xy_additive =====\n",
      "Epoch 01 — loss: 1.7923\n",
      "Epoch 02 — loss: 1.6330\n",
      "Epoch 03 — loss: 1.5875\n",
      "Epoch 04 — loss: 1.5536\n",
      "Epoch 05 — loss: 1.5270\n",
      "Epoch 06 — loss: 1.5232\n",
      "Epoch 07 — loss: 1.5082\n",
      "Epoch 08 — loss: 1.4947\n",
      "Epoch 09 — loss: 1.4896\n",
      "Epoch 10 — loss: 1.4893\n",
      "Epoch 11 — loss: 1.4843\n",
      "Epoch 12 — loss: 1.4769\n",
      "Epoch 13 — loss: 1.4754\n",
      "Epoch 14 — loss: 1.4733\n",
      "Epoch 15 — loss: 1.4672\n",
      "Epoch 16 — loss: 1.4639\n",
      "Epoch 17 — loss: 1.4657\n",
      "Epoch 18 — loss: 1.4620\n",
      "Epoch 19 — loss: 1.4655\n",
      "Epoch 20 — loss: 1.4525\n",
      "Confusion matrix for deepset_xy_additive saved and inserted into result/model_comparison_results.xlsx (sheet: deepset_xy_additive)\n",
      "Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx\n",
      "Outliers saved to: /Users/patrickdharma/Desktop/university/卒業課題/my_models/grade_predictor/result/outlier.xlsx\n",
      "\n",
      "=== Model Comparison Summary ===\n",
      "                 Model Type  Train Strict Accuracy (%)  \\\n",
      "0           set_transformer                      61.39   \n",
      "1                   deepset                      42.93   \n",
      "2        set_transformer_xy                      59.48   \n",
      "3                deepset_xy                      46.59   \n",
      "4  set_transformer_additive                      55.63   \n",
      "5       deepset_xy_additive                      45.60   \n",
      "\n",
      "   Train ±1 Grade Accuracy (%)  Val Strict Accuracy (%)  \\\n",
      "0                        87.29                    43.50   \n",
      "1                        79.46                    43.02   \n",
      "2                        85.95                    42.64   \n",
      "3                        84.05                    45.57   \n",
      "4                        85.33                    43.98   \n",
      "5                        82.90                    44.71   \n",
      "\n",
      "   Val ±1 Grade Accuracy (%)  \n",
      "0                      79.02  \n",
      "1                      78.49  \n",
      "2                      78.68  \n",
      "3                      81.47  \n",
      "4                      79.45  \n",
      "5                      81.57  \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image as XLImage\n",
    "import csv\n",
    "\n",
    "# --- plot confusion matrix and save to excel---\n",
    "def save_confusion_matrix_to_excel(y_true, y_pred, class_labels, model_name, excel_path):\n",
    "    # Plot confusion matrix and save as image\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(class_labels)), normalize='true')\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "    plt.xlabel(\"Predicted Grade\")\n",
    "    plt.ylabel(\"Actual Grade\")\n",
    "    plt.tight_layout()\n",
    "    img_path = f\"result/confusion_{model_name}.png\"\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Insert image into Excel (new sheet per model)\n",
    "    wb = load_workbook(excel_path)\n",
    "    if model_name in wb.sheetnames:\n",
    "        ws = wb[model_name]\n",
    "    else:\n",
    "        ws = wb.create_sheet(title=model_name)\n",
    "    img = XLImage(img_path)\n",
    "    ws.add_image(img, \"A1\")\n",
    "    wb.save(excel_path)\n",
    "    print(f\"Confusion matrix for {model_name} saved and inserted into {excel_path} (sheet: {model_name})\")\n",
    "\n",
    "# --- export the predictions to excel ---\n",
    "def _update_outlier_excel(df_all_preds, outlier_filename=\"result/outlier.xlsx\", sheet_name=\"outliers\", threshold=3):\n",
    "    \"\"\"\n",
    "    From a DataFrame with columns [problem_name, y_true, y_pred, diff],\n",
    "    keep rows where abs(diff) > threshold and aggregate per problem_name:\n",
    "        - count = number of times flagged\n",
    "        - y_true = mode (most frequent true label)\n",
    "        - y_pred_avg = average predicted label across occurrences\n",
    "    Save to outlier.xlsx.\n",
    "    \"\"\"\n",
    "    # Filter outliers\n",
    "    outliers = df_all_preds.loc[df_all_preds[\"diff\"].abs() > threshold,\n",
    "                                [\"problem_name\", \"y_true\", \"y_pred\"]]\n",
    "    if outliers.empty:\n",
    "        print(f\"No outliers (abs(diff) > {threshold}). Skipped creating outlier.xlsx.\")\n",
    "        return\n",
    "\n",
    "    # Group & aggregate\n",
    "    grouped = (outliers\n",
    "               .groupby(\"problem_name\")\n",
    "               .agg(\n",
    "                   count=(\"problem_name\", \"size\"),\n",
    "                   y_true=(\"y_true\", lambda x: x.mode().iat[0] if not x.mode().empty else x.iloc[0]),\n",
    "                   y_pred_avg=(\"y_pred\", lambda x: round(pd.to_numeric(x, errors=\"coerce\").mean(), 2))\n",
    "               )\n",
    "               .reset_index())\n",
    "\n",
    "    # If a previous file exists, merge and accumulate counts\n",
    "    if os.path.exists(outlier_filename):\n",
    "        try:\n",
    "            existing = pd.read_excel(outlier_filename, sheet_name=sheet_name)\n",
    "            if set(existing.columns) >= {\"problem_name\", \"count\", \"y_true\", \"y_pred_avg\"}:\n",
    "                merged = pd.concat([existing, grouped], ignore_index=True)\n",
    "                # Re-aggregate: sum counts, keep most common y_true, recompute y_pred_avg\n",
    "                grouped = (merged\n",
    "                           .groupby(\"problem_name\")\n",
    "                           .agg(\n",
    "                               count=(\"count\", \"sum\"),\n",
    "                               y_true=(\"y_true\", lambda x: x.mode().iat[0] if not x.mode().empty else x.iloc[0]),\n",
    "                               y_pred_avg=(\"y_pred_avg\", \"mean\")\n",
    "                           )\n",
    "                           .reset_index())\n",
    "            # else keep grouped as new\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Save\n",
    "    with pd.ExcelWriter(outlier_filename, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "        grouped.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Outliers saved to: {os.path.abspath(outlier_filename)}\")\n",
    "\n",
    "\n",
    "def export_predictions_to_excel(model, dataloader, device, grade_to_label, excel_path, sheet_name):\n",
    "    results = []\n",
    "    raw_dataset = dataloader.dataset.dataset  # MoonBoardDataset\n",
    "    indices = dataloader.dataset.indices      # Subset indices\n",
    "    label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "    current_index = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if isinstance(X, tuple):\n",
    "                inputs = tuple(x.to(device) for x in X)\n",
    "                payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            else:\n",
    "                payload = X.to(device)\n",
    "            outputs = model(payload)\n",
    "            if isinstance(outputs, tuple):\n",
    "                probs, logits = outputs\n",
    "                preds_tensor = cumulative_to_labels(probs)\n",
    "            else:\n",
    "                preds_tensor = outputs.argmax(dim=1)\n",
    "            y = y.to(device)\n",
    "            preds_cpu = preds_tensor.detach().cpu()\n",
    "            y_cpu = y.detach().cpu()\n",
    "            for i in range(y_cpu.size(0)):\n",
    "                real_label = int(y_cpu[i].item())\n",
    "                pred_label = int(preds_cpu[i].item())\n",
    "                dataset_index = indices[current_index]\n",
    "                current_index += 1\n",
    "                raw_item = raw_dataset.raw[dataset_index]\n",
    "                problem_name = raw_item.get('problem_name', f\"problem_{dataset_index}\")\n",
    "                results.append({\n",
    "                    \"problem_name\": problem_name,\n",
    "                    \"y_true\": real_label,  # keep numeric for averaging/aggregation\n",
    "                    \"y_pred\": pred_label,\n",
    "                    \"diff\": real_label - pred_label\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # 1) Save all predictions into your main Excel file\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        # Convert numeric labels back to grade strings for readability\n",
    "        df_out = df.copy()\n",
    "        df_out[\"y_true\"] = df_out[\"y_true\"].map(lambda x: label_to_grade.get(x, f\"Unknown({x})\"))\n",
    "        df_out[\"y_pred\"] = df_out[\"y_pred\"].map(lambda x: label_to_grade.get(x, f\"Unknown({x})\"))\n",
    "        df_out.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Predictions for {sheet_name} exported to: {excel_path}\")\n",
    "\n",
    "    # 2) Create/update outlier.xlsx (problem_name, count, y_true, y_pred_avg)\n",
    "    _update_outlier_excel(df, outlier_filename=\"result/outlier.xlsx\", sheet_name=\"outliers\", threshold=3)\n",
    "\n",
    "\n",
    "# --- compute training and validation accuracy ---\n",
    "def compute_accuracy(model, dataloader, device):\n",
    "    strict_correct, loose_correct, total = 0, 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "            if len(X) == 1:\n",
    "                preds = model(X[0]).argmax(dim=1)\n",
    "            else:\n",
    "                preds = model(X).argmax(dim=1)\n",
    "            total += y.size(0)\n",
    "            strict_correct += (preds == y).sum().item()\n",
    "            loose_correct += ((preds - y).abs() <= 1).sum().item()\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    strict_acc = 100.0 * strict_correct / total\n",
    "    loose_acc = 100.0 * loose_correct / total\n",
    "    return strict_acc, loose_acc, y_true, y_pred\n",
    "\n",
    "def log_accuracy_to_csv(model_type, train_strict_acc, train_loose_acc, val_strict_acc, val_loose_acc, csv_path=\"result/accuracy.csv\"):\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "    with open(csv_path, mode='a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\n",
    "                \"Model Type\",\n",
    "                \"Train Strict Accuracy (%)\",\n",
    "                \"Train ±1 Grade Accuracy (%)\",\n",
    "                \"Val Strict Accuracy (%)\",\n",
    "                \"Val ±1 Grade Accuracy (%)\"\n",
    "            ])\n",
    "        writer.writerow([\n",
    "            model_type,\n",
    "            round(train_strict_acc, 2),\n",
    "            round(train_loose_acc, 2),\n",
    "            round(val_strict_acc, 2),\n",
    "            round(val_loose_acc, 2)\n",
    "        ])\n",
    "\n",
    "\n",
    "def compare_models():\n",
    "    model_types = [\n",
    "        \"set_transformer\",\n",
    "        \"deepset\",\n",
    "        \"set_transformer_xy\",\n",
    "        \"deepset_xy\",\n",
    "        \"set_transformer_additive\",\n",
    "        \"deepset_xy_additive\"\n",
    "    ]\n",
    "    results = []\n",
    "    excel_path = \"result/model_comparison_results.xlsx\"\n",
    "    class_labels = [f\"V{i}\" for i in range(4, 12)]\n",
    "\n",
    "    for idx, mtype in enumerate(model_types):\n",
    "        print(f\"\\n===== Training {mtype} =====\")\n",
    "        train_loader, val_loader, model, dataset, train_idx, val_idx = main(mtype)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        train_strict_acc, train_loose_acc, _, _ = compute_accuracy(model, train_loader, device)\n",
    "        # Compute validation accuracy\n",
    "        val_strict_acc, val_loose_acc, y_true, y_pred = compute_accuracy(model, val_loader, device)\n",
    "\n",
    "        # log to CSV\n",
    "        log_accuracy_to_csv(mtype, train_strict_acc, train_loose_acc, val_strict_acc, val_loose_acc)\n",
    "        \n",
    "        results.append({\n",
    "            \"Model Type\": mtype,\n",
    "            \"Train Strict Accuracy (%)\": round(train_strict_acc, 2),\n",
    "            \"Train ±1 Grade Accuracy (%)\": round(train_loose_acc, 2),\n",
    "            \"Val Strict Accuracy (%)\": round(val_strict_acc, 2),\n",
    "            \"Val ±1 Grade Accuracy (%)\": round(val_loose_acc, 2)\n",
    "        })\n",
    "\n",
    "        # Save summary table on first iteration (so Excel file exists)\n",
    "        if idx == 0:\n",
    "            df_results = pd.DataFrame(results)\n",
    "            df_results.to_excel(excel_path, index=False)\n",
    "\n",
    "        # Save confusion matrix to Excel\n",
    "        save_confusion_matrix_to_excel(y_true, y_pred, class_labels, mtype, excel_path)\n",
    "\n",
    "        # Export predictions to Excel (new sheet per model)\n",
    "        export_predictions_to_excel(model, val_loader, device, grade_to_label, excel_path, sheet_name=f\"{mtype}_preds\")\n",
    "\n",
    "    # Save summary table again at the end (with all models)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        df_results.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    print(\"\\n=== Model Comparison Summary ===\")\n",
    "    print(df_results)\n",
    "\n",
    "# usage\n",
    "for i in range(1):\n",
    "    compare_models()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c11d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal evaluation helpers\n",
    "def evaluate_ordinal_thresholds(model, loader, grade_to_label, device, decision_threshold=0.5, model_name=None, output_dir='./result'):\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "    targets_list = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "            payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            outputs = model(payload)\n",
    "            if not isinstance(outputs, tuple):\n",
    "                raise ValueError('Model is not configured for ordinal outputs.')\n",
    "            probs, logits = outputs\n",
    "            probs_list.append(probs.cpu())\n",
    "            targets_list.append(y.cpu())\n",
    "    if not probs_list:\n",
    "        raise ValueError('No samples available for ordinal evaluation.')\n",
    "    probs = torch.cat(probs_list, dim=0)\n",
    "    targets = torch.cat(targets_list, dim=0)\n",
    "    acc_per_threshold = threshold_accuracy(probs, targets, threshold=decision_threshold).cpu()\n",
    "    grade_by_label = {v: k for k, v in grade_to_label.items()}\n",
    "    threshold_labels = []\n",
    "    for idx in range(acc_per_threshold.size(0)):\n",
    "        grade = grade_by_label.get(idx, f'label_{idx}')\n",
    "        threshold_labels.append(f\"P(>{grade})\")\n",
    "    df = pd.DataFrame({\n",
    "        'threshold': threshold_labels,\n",
    "        'accuracy': (acc_per_threshold.numpy() * 100).round(2)\n",
    "    })\n",
    "    overall_pred = cumulative_to_labels(probs, threshold=decision_threshold)\n",
    "    overall_acc = (overall_pred == targets).float().mean().item() * 100\n",
    "    if model_name:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, f'ordinal_metrics_{model_name}.csv')\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f'Saved threshold table to {output_path}')\n",
    "    print(df)\n",
    "    print(f'Overall accuracy: {overall_acc:.2f}%')\n",
    "    return df, overall_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "261a5130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ordinal model: set_transformer_ordinal ===\n",
      "Epoch 01 — loss: 0.3742\n",
      "Epoch 02 — loss: 0.3150\n",
      "Epoch 03 — loss: 0.3059\n",
      "Epoch 04 — loss: 0.2946\n",
      "Epoch 05 — loss: 0.2848\n",
      "Epoch 06 — loss: 0.2764\n",
      "Epoch 07 — loss: 0.2659\n",
      "Epoch 08 — loss: 0.2589\n",
      "Epoch 09 — loss: 0.2522\n",
      "Epoch 10 — loss: 0.2448\n",
      "Epoch 11 — loss: 0.2381\n",
      "Epoch 12 — loss: 0.2345\n",
      "Epoch 13 — loss: 0.2240\n",
      "Epoch 14 — loss: 0.2191\n",
      "Epoch 15 — loss: 0.2134\n",
      "Epoch 16 — loss: 0.2070\n",
      "Epoch 17 — loss: 0.2008\n",
      "Epoch 18 — loss: 0.1966\n",
      "Epoch 19 — loss: 0.1921\n",
      "Epoch 20 — loss: 0.1849\n",
      "Saved threshold table to ./result/ordinal_metrics_set_transformer_ordinal.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  83.059998\n",
      "1    P(>V5)  81.949997\n",
      "2    P(>V6)  85.029999\n",
      "3    P(>V7)  88.550003\n",
      "4    P(>V8)  92.589996\n",
      "5    P(>V9)  96.540001\n",
      "Overall accuracy: 49.18%\n",
      "=== Training ordinal model: set_transformer_ordinal_xy ===\n",
      "Epoch 01 — loss: 0.3709\n",
      "Epoch 02 — loss: 0.3108\n",
      "Epoch 03 — loss: 0.3011\n",
      "Epoch 04 — loss: 0.2937\n",
      "Epoch 05 — loss: 0.2841\n",
      "Epoch 06 — loss: 0.2767\n",
      "Epoch 07 — loss: 0.2712\n",
      "Epoch 08 — loss: 0.2620\n",
      "Epoch 09 — loss: 0.2562\n",
      "Epoch 10 — loss: 0.2467\n",
      "Epoch 11 — loss: 0.2435\n",
      "Epoch 12 — loss: 0.2345\n",
      "Epoch 13 — loss: 0.2312\n",
      "Epoch 14 — loss: 0.2233\n",
      "Epoch 15 — loss: 0.2201\n",
      "Epoch 16 — loss: 0.2155\n",
      "Epoch 17 — loss: 0.2073\n",
      "Epoch 18 — loss: 0.2007\n",
      "Epoch 19 — loss: 0.1963\n",
      "Epoch 20 — loss: 0.1911\n",
      "Saved threshold table to ./result/ordinal_metrics_set_transformer_ordinal_xy.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  83.300003\n",
      "1    P(>V5)  83.010002\n",
      "2    P(>V6)  84.940002\n",
      "3    P(>V7)  86.809998\n",
      "4    P(>V8)  92.300003\n",
      "5    P(>V9)  96.099998\n",
      "Overall accuracy: 48.60%\n",
      "=== Training ordinal model: set_transformer_ordinal_xy_additive ===\n",
      "Epoch 01 — loss: 0.3575\n",
      "Epoch 02 — loss: 0.3133\n",
      "Epoch 03 — loss: 0.2985\n",
      "Epoch 04 — loss: 0.2906\n",
      "Epoch 05 — loss: 0.2894\n",
      "Epoch 06 — loss: 0.2801\n",
      "Epoch 07 — loss: 0.2740\n",
      "Epoch 08 — loss: 0.2681\n",
      "Epoch 09 — loss: 0.2640\n",
      "Epoch 10 — loss: 0.2599\n",
      "Epoch 11 — loss: 0.2538\n",
      "Epoch 12 — loss: 0.2480\n",
      "Epoch 13 — loss: 0.2454\n",
      "Epoch 14 — loss: 0.2383\n",
      "Epoch 15 — loss: 0.2334\n",
      "Epoch 16 — loss: 0.2299\n",
      "Epoch 17 — loss: 0.2262\n",
      "Epoch 18 — loss: 0.2209\n",
      "Epoch 19 — loss: 0.2148\n",
      "Epoch 20 — loss: 0.2108\n",
      "Saved threshold table to ./result/ordinal_metrics_set_transformer_ordinal_xy_additive.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  83.540001\n",
      "1    P(>V5)  82.290001\n",
      "2    P(>V6)  84.550003\n",
      "3    P(>V7)  86.430000\n",
      "4    P(>V8)  92.589996\n",
      "5    P(>V9)  96.580002\n",
      "Overall accuracy: 46.78%\n",
      "=== Training ordinal model: deepset_ordinal ===\n",
      "Epoch 01 — loss: 0.4626\n",
      "Epoch 02 — loss: 0.3407\n",
      "Epoch 03 — loss: 0.3059\n",
      "Epoch 04 — loss: 0.2950\n",
      "Epoch 05 — loss: 0.2884\n",
      "Epoch 06 — loss: 0.2857\n",
      "Epoch 07 — loss: 0.2822\n",
      "Epoch 08 — loss: 0.2791\n",
      "Epoch 09 — loss: 0.2781\n",
      "Epoch 10 — loss: 0.2776\n",
      "Epoch 11 — loss: 0.2736\n",
      "Epoch 12 — loss: 0.2754\n",
      "Epoch 13 — loss: 0.2741\n",
      "Epoch 14 — loss: 0.2708\n",
      "Epoch 15 — loss: 0.2734\n",
      "Epoch 16 — loss: 0.2728\n",
      "Epoch 17 — loss: 0.2721\n",
      "Epoch 18 — loss: 0.2711\n",
      "Epoch 19 — loss: 0.2719\n",
      "Epoch 20 — loss: 0.2692\n",
      "Saved threshold table to ./result/ordinal_metrics_deepset_ordinal.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  81.709999\n",
      "1    P(>V5)  80.510002\n",
      "2    P(>V6)  84.260002\n",
      "3    P(>V7)  87.629997\n",
      "4    P(>V8)  92.830002\n",
      "5    P(>V9)  97.019997\n",
      "Overall accuracy: 45.81%\n",
      "=== Training ordinal model: deepset_ordinal_xy ===\n",
      "Epoch 01 — loss: 0.4595\n",
      "Epoch 02 — loss: 0.3252\n",
      "Epoch 03 — loss: 0.3002\n",
      "Epoch 04 — loss: 0.2904\n",
      "Epoch 05 — loss: 0.2875\n",
      "Epoch 06 — loss: 0.2848\n",
      "Epoch 07 — loss: 0.2833\n",
      "Epoch 08 — loss: 0.2786\n",
      "Epoch 09 — loss: 0.2787\n",
      "Epoch 10 — loss: 0.2792\n",
      "Epoch 11 — loss: 0.2756\n",
      "Epoch 12 — loss: 0.2753\n",
      "Epoch 13 — loss: 0.2745\n",
      "Epoch 14 — loss: 0.2729\n",
      "Epoch 15 — loss: 0.2737\n",
      "Epoch 16 — loss: 0.2731\n",
      "Epoch 17 — loss: 0.2725\n",
      "Epoch 18 — loss: 0.2717\n",
      "Epoch 19 — loss: 0.2709\n",
      "Epoch 20 — loss: 0.2704\n",
      "Saved threshold table to ./result/ordinal_metrics_deepset_ordinal_xy.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  81.230003\n",
      "1    P(>V5)  80.650002\n",
      "2    P(>V6)  83.639999\n",
      "3    P(>V7)  87.050003\n",
      "4    P(>V8)  93.209999\n",
      "5    P(>V9)  97.019997\n",
      "Overall accuracy: 46.10%\n",
      "=== Training ordinal model: deepset_ordinal_xy_additive ===\n",
      "Epoch 01 — loss: 0.4264\n",
      "Epoch 02 — loss: 0.3279\n",
      "Epoch 03 — loss: 0.3079\n",
      "Epoch 04 — loss: 0.2921\n",
      "Epoch 05 — loss: 0.2855\n",
      "Epoch 06 — loss: 0.2834\n",
      "Epoch 07 — loss: 0.2826\n",
      "Epoch 08 — loss: 0.2797\n",
      "Epoch 09 — loss: 0.2756\n",
      "Epoch 10 — loss: 0.2787\n",
      "Epoch 11 — loss: 0.2760\n",
      "Epoch 12 — loss: 0.2762\n",
      "Epoch 13 — loss: 0.2748\n",
      "Epoch 14 — loss: 0.2752\n",
      "Epoch 15 — loss: 0.2734\n",
      "Epoch 16 — loss: 0.2746\n",
      "Epoch 17 — loss: 0.2736\n",
      "Epoch 18 — loss: 0.2724\n",
      "Epoch 19 — loss: 0.2731\n",
      "Epoch 20 — loss: 0.2730\n",
      "Saved threshold table to ./result/ordinal_metrics_deepset_ordinal_xy_additive.csv\n",
      "  threshold   accuracy\n",
      "0    P(>V4)  80.800003\n",
      "1    P(>V5)  80.169998\n",
      "2    P(>V6)  83.540001\n",
      "3    P(>V7)  87.870003\n",
      "4    P(>V8)  93.260002\n",
      "5    P(>V9)  97.019997\n",
      "Overall accuracy: 46.01%\n",
      "Saved combined ordinal results to ./result/ordinal_result.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --- Ordinal variants sweep ---\n",
    "ordinal_model_types = [\n",
    "    'set_transformer_ordinal',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "]\n",
    "\n",
    "ordinal_tables = []\n",
    "ordinal_summary = []\n",
    "\n",
    "for model_key in ordinal_model_types:\n",
    "    print(f\"=== Training ordinal model: {model_key} ===\")\n",
    "    train_loader, val_loader, model, dataset, train_idx, val_idx = main(model_key)\n",
    "    table, overall_acc = evaluate_ordinal_thresholds(\n",
    "        model,\n",
    "        val_loader,\n",
    "        grade_to_label=grade_to_label,\n",
    "        device=device,\n",
    "        decision_threshold=0.5,\n",
    "        model_name=model_key\n",
    "    )\n",
    "    table = table.copy()\n",
    "    table['model'] = model_key\n",
    "    table['overall_accuracy'] = overall_acc\n",
    "    ordinal_tables.append(table)\n",
    "    ordinal_summary.append({'model': model_key, 'overall_accuracy': overall_acc})\n",
    "\n",
    "if ordinal_tables:\n",
    "    combined = pd.concat(ordinal_tables, ignore_index=True)\n",
    "    summary_df = pd.DataFrame(ordinal_summary)\n",
    "\n",
    "    threshold_order = [f\"P(>{grade})\" for grade in sorted(grade_to_label.keys(), key=lambda g: grade_to_label[g])]\n",
    "    pivot_df = (combined\n",
    "                .pivot_table(index='model', columns='threshold', values='accuracy')\n",
    "                .reindex(columns=[c for c in threshold_order if c in combined['threshold'].unique()]))\n",
    "    pivot_df = pivot_df.sort_index()\n",
    "    pivot_df['Overall Accuracy'] = summary_df.set_index('model')['overall_accuracy']\n",
    "\n",
    "    combined = combined.sort_values(['model', 'threshold']).reset_index(drop=True)\n",
    "    summary_df = summary_df.sort_values('model').reset_index(drop=True)\n",
    "\n",
    "    output_path = './result/ordinal_result.xlsx'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with pd.ExcelWriter(output_path) as writer:\n",
    "        pivot_df.to_excel(writer, sheet_name='threshold_matrix')\n",
    "        combined.to_excel(writer, sheet_name='threshold_long', index=False)\n",
    "        summary_df.to_excel(writer, sheet_name='overall', index=False)\n",
    "    print(f\"Saved combined ordinal results to {output_path}\")\n",
    "else:\n",
    "    print('No ordinal results generated.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f8281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 — loss: 1.7030\n",
      "Epoch 02 — loss: 1.6260\n",
      "Epoch 02 — loss: 1.6260\n",
      "Epoch 03 — loss: 1.6025\n",
      "Epoch 03 — loss: 1.6025\n",
      "Epoch 04 — loss: 1.5573\n",
      "Epoch 04 — loss: 1.5573\n",
      "Epoch 05 — loss: 1.5306\n",
      "Epoch 05 — loss: 1.5306\n",
      "Epoch 06 — loss: 1.5167\n",
      "Epoch 06 — loss: 1.5167\n",
      "Epoch 07 — loss: 1.4908\n",
      "Epoch 07 — loss: 1.4908\n",
      "Epoch 08 — loss: 1.4637\n",
      "Epoch 08 — loss: 1.4637\n",
      "Epoch 09 — loss: 1.4356\n",
      "Epoch 09 — loss: 1.4356\n",
      "Epoch 10 — loss: 1.4186\n",
      "Epoch 10 — loss: 1.4186\n",
      "Epoch 11 — loss: 1.4024\n",
      "Epoch 11 — loss: 1.4024\n",
      "Epoch 12 — loss: 1.3722\n",
      "Epoch 12 — loss: 1.3722\n",
      "Epoch 13 — loss: 1.3418\n",
      "Epoch 13 — loss: 1.3418\n",
      "Epoch 14 — loss: 1.3292\n",
      "Epoch 14 — loss: 1.3292\n",
      "Epoch 15 — loss: 1.3051\n",
      "Epoch 15 — loss: 1.3051\n",
      "Epoch 16 — loss: 1.2856\n",
      "Epoch 16 — loss: 1.2856\n",
      "Epoch 17 — loss: 1.2500\n",
      "Epoch 17 — loss: 1.2500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 106\u001b[0m\n\u001b[1;32m     85\u001b[0m named_problems \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhysical V9 Benchmark\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI18\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ12\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF13\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD10\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE6\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ2\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTriangulation V7\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA18\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ13\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE9\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE9\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI4\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYums In My Tums V5\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF18\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG12\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD13\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI9\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE6\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     94\u001b[0m }\n\u001b[1;32m     96\u001b[0m group1 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimma mot strommen\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA18\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC12\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA9\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB14\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF5\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAXIMUS!\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK18\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK14\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI13\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK11\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH4\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue bin day\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB18\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC18\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC12\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC3\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    104\u001b[0m }\n\u001b[0;32m--> 106\u001b[0m train_loader, val_loader, model, dataset, train_idx, val_idx \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mset_transformer_additive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m evaluate_problems(\n\u001b[1;32m    109\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    110\u001b[0m     problem_dict\u001b[38;5;241m=\u001b[39mnamed_problems,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_transformer_xy\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# or your current model type\u001b[39;00m\n\u001b[1;32m    121\u001b[0m )\n",
      "Cell \u001b[0;32mIn[9], line 105\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_type)\u001b[0m\n\u001b[1;32m    102\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mclass_weights)\n\u001b[1;32m    103\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m--> 105\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loader, val_loader, model, dataset, train_idx, val_idx\n",
      "Cell \u001b[0;32mIn[9], line 67\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[1;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 67\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     69\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/sousei/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sousei/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# evaluate problems\n",
    "def evaluate_problems(\n",
    "    model, problem_dict, hold_to_idx, hold_difficulty, type_to_idx, device,\n",
    "    grade_to_label, hold_to_coord, dataset, train_idx, val_idx, model_type\n",
    "):\n",
    "    label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "    print(\"=== MoonBoard Problem Evaluation ===\")\n",
    "\n",
    "    for fallback_name, holds in problem_dict.items():\n",
    "        try:\n",
    "            hold_idxs = []\n",
    "            diff_values = []\n",
    "            type_vecs = []\n",
    "            xy_coords = []\n",
    "\n",
    "            for h in holds:\n",
    "                if h not in hold_difficulty or h not in hold_to_idx or h not in hold_to_coord:\n",
    "                    raise ValueError(f\"[ERROR] Hold '{h}' missing from required dictionaries.\")\n",
    "\n",
    "                hold_idxs.append(hold_to_idx[h])\n",
    "                difficulty, types = hold_difficulty[h]\n",
    "                diff_values.append(difficulty / 10.0)\n",
    "\n",
    "                # Multi-hot vector\n",
    "                type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "                for t in types:\n",
    "                    if t in type_to_idx:\n",
    "                        type_vec[type_to_idx[t]] = 1.0\n",
    "                type_vecs.append(type_vec)\n",
    "\n",
    "                xy_coords.append(torch.tensor([hold_to_coord[h][0] / 10.0, hold_to_coord[h][1] / 17.0], dtype=torch.float))\n",
    "\n",
    "            # Convert to tensors\n",
    "            hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)\n",
    "            difficulty_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)\n",
    "            type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)\n",
    "            xy_tensor = torch.stack(xy_coords).unsqueeze(0).to(device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # --- Select input format based on model_type ---\n",
    "                if model_type in XY_MODELS:\n",
    "                    input_data = (hold_tensor, difficulty_tensor, type_tensor, xy_tensor)\n",
    "                elif model_type in {'set_transformer', 'deepset', 'set_transformer_ordinal', 'deepset_ordinal'}:\n",
    "                    input_data = (hold_tensor,)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "                payload = input_data[0] if isinstance(input_data, tuple) and len(input_data) == 1 else input_data\n",
    "                outputs = model(payload)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    probs, logits = outputs\n",
    "                    pred_label = (probs > 0.5).sum(dim=1).item()\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                    pred_label = logits.argmax(dim=1).item()\n",
    "                pred_grade = label_to_grade.get(pred_label, f\"Unknown({pred_label})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{fallback_name}] Skipping due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Search in dataset for match\n",
    "        found_idx = None\n",
    "        split = \"Not Found\"\n",
    "        setter_grade = \"Unknown\"\n",
    "        problem_name = fallback_name\n",
    "\n",
    "        for idx_item, item in enumerate(dataset.raw):\n",
    "            if set(item['holds']) == set(holds):\n",
    "                found_idx = idx_item\n",
    "                setter_grade = item.get('grade', 'Unknown')\n",
    "                problem_name = item.get('problem_name', fallback_name)\n",
    "                if found_idx in train_idx:\n",
    "                    split = \"Train\"\n",
    "                elif found_idx in val_idx:\n",
    "                    split = \"Validation\"\n",
    "                else:\n",
    "                    split = \"Found (Unknown Split)\"\n",
    "                break\n",
    "\n",
    "        holds_with_difficulty = {h: hold_difficulty[h][0] if h in hold_difficulty else \"N/A\" for h in holds}\n",
    "        print(f\"🔹 Problem Name   : {problem_name}\")\n",
    "        print(f\"   Holds Used     : {holds_with_difficulty}\")\n",
    "        print(f\"   Setter Grade   : {setter_grade}\")\n",
    "        print(f\"   Predicted Grade: {pred_grade}\")\n",
    "        print(f\"   Dataset Split  : {split}\")\n",
    "\n",
    "named_problems = {\n",
    "    \"Physical V9 Benchmark\": [\"I18\", \"J12\", \"F13\", \"D10\", \"E6\", \"J2\"],\n",
    "    \"Triangulation V7\": [\"A18\", \"J13\", \"D16\", \"E9\", \"E9\", \"I4\"],\n",
    "    \"warmup crimps\": [\"I18\", \"I7\", \"I9\", \"I15\", \"G11\", \"J14\", \"J12\", \"I15\", \"J14\", \"H4\", \"K6\"],\n",
    "    \"Ronani V5\": [\"F18\", \"I15\", \"I10\", \"K9\", \"K6\", \"G14\", \"D16\", \"E9\", \"K6\", \"I15\", \"E4\", \"H5\"],\n",
    "    \"Don't Fart Alan\": [\"K18\", \"J15\", \"F14\", \"F13\", \"D10\", \"E6\", \"I7\", \"I5\", \"F1\"],\n",
    "    \"FINALE MAXI 2025 POCKET 2 V9\": [\"G3\", \"F3\", \"F4\", \"A6\", \"A11\", \"B17\", \"C9\", \"D17\", \"H18\"],\n",
    "    \"Khai's V7\": [\"D18\", \"A15\", \"A12\", \"C9\", \"E7\", \"H8\", \"I6\", \"E1\"],\n",
    "    \"Yums In My Tums V5\": [\"F18\", \"G12\", \"E1\", \"D13\", \"I9\", \"F8\", \"I2\", \"F16\", \"E4\", \"E6\"],\n",
    "}\n",
    "\n",
    "team_problems = {\n",
    "    \"simma mot strommen\": [\"A18\", \"C12\", \"A9\", \"B14\", \"B16\", \"D1\", \"F5\", \"F5\"],\n",
    "    \"MAXIMUS!\": [\"K18\", \"E3\", \"K14\", \"I13\", \"K7\", \"I2\", \"H16\", \"K11\", \"G7\", \"H4\"],\n",
    "    \"interstate\": [\"K18\", \"H17\", \"J11\", \"I9\", \"G13\", \"H15\", \"I5\", \"I6\"],\n",
    "    \"krakatoa pusher\": [\"H18\", \"H11\", \"J8\", \"F7\", \"K15\", \"F4\", \"J3\"],\n",
    "    \"doublement\": [\"A18\", \"E16\", \"F8\", \"B14\", \"G8\", \"E12\", \"F4\", \"F3\", \"F3\"],\n",
    "    \"animal instinct\": [\"F18\", \"J11\", \"F9\", \"H15\", \"E13\", \"J11\", \"I6\", \"F4\"],\n",
    "    \"blue bin day\": [\"B18\", \"C18\", \"A8\", \"C12\", \"B15\", \"A5\", \"C3\"]\n",
    "}\n",
    "\n",
    "# --- Classification baseline run ---\n",
    "clf_train_loader, clf_val_loader, clf_model, clf_dataset, clf_train_idx, clf_val_idx = main('set_transformer_additive')\n",
    "\n",
    "evaluate_problems(\n",
    "    model=clf_model,\n",
    "    problem_dict=named_problems,\n",
    "    hold_to_idx=hold_to_idx,\n",
    "    hold_difficulty=hold_difficulty,\n",
    "    type_to_idx=type_to_idx,\n",
    "    device=device,\n",
    "    grade_to_label=grade_to_label,\n",
    "    hold_to_coord=hold_to_coord,\n",
    "    dataset=clf_dataset,\n",
    "    train_idx=clf_train_idx,\n",
    "    val_idx=clf_val_idx,\n",
    "    model_type='set_transformer_additive'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228393ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "def visualize_attention_for_problem(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device):\n",
    "    model.eval()\n",
    "\n",
    "    hold_idxs = []\n",
    "    diff_values = []\n",
    "    type_vecs = []\n",
    "    xy_coords = []\n",
    "\n",
    "    for h in holds:\n",
    "        hold_idxs.append(hold_to_idx[h])\n",
    "        difficulty, types = hold_difficulty[h]\n",
    "        diff_values.append(difficulty / 10.0)\n",
    "\n",
    "        type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "        for t in types:\n",
    "            if t in type_to_idx:\n",
    "                type_vec[type_to_idx[t]] = 1.0\n",
    "        type_vecs.append(type_vec)\n",
    "\n",
    "        x, y = hold_to_coord[h]\n",
    "        xy_coords.append([x / 10.0, y / 17.0])\n",
    "\n",
    "    hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    diff_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)\n",
    "    type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)\n",
    "    xy_tensor = torch.tensor(xy_coords, dtype=torch.float).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model((hold_tensor, diff_tensor, type_tensor, xy_tensor))\n",
    "\n",
    "    attn_isab1 = model.encoder[0].mab0.attn_weights.cpu().numpy()\n",
    "    attn_isab2 = model.encoder[1].mab0.attn_weights.cpu().numpy()\n",
    "\n",
    "    num_heads = attn_isab1.shape[0]\n",
    "    fig, axes = plt.subplots(2, num_heads, figsize=(4 * num_heads, 8))\n",
    "    if num_heads == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "\n",
    "    for h in range(num_heads):\n",
    "        sns.heatmap(attn_isab1[h], ax=axes[0, h], cmap=\"viridis\", xticklabels=holds)\n",
    "        axes[0, h].set_title(f\"ISAB1 – Head {h}\")\n",
    "        axes[0, h].set_xlabel(\"Key (Hold)\")\n",
    "        axes[0, h].set_ylabel(\"Seed\")\n",
    "\n",
    "        sns.heatmap(attn_isab2[h], ax=axes[1, h], cmap=\"viridis\", xticklabels=holds)\n",
    "        axes[1, h].set_title(f\"ISAB2 – Head {h}\")\n",
    "        axes[1, h].set_xlabel(\"Key (Hold)\")\n",
    "        axes[1, h].set_ylabel(\"Seed\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66931c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_attention_per_hold(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device):\n",
    "    model.eval()\n",
    "\n",
    "    hold_idxs = []\n",
    "    diff_values = []\n",
    "    type_vecs = []\n",
    "    xy_coords = []\n",
    "\n",
    "    for h in holds:\n",
    "        hold_idxs.append(hold_to_idx[h])\n",
    "        difficulty, types = hold_difficulty[h]\n",
    "        diff_values.append(difficulty / 10.0)\n",
    "\n",
    "        # Multi-hot type vector\n",
    "        type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "        for t in types:\n",
    "            if t in type_to_idx:\n",
    "                type_vec[type_to_idx[t]] = 1.0\n",
    "        type_vecs.append(type_vec)\n",
    "\n",
    "        # XY coordinate\n",
    "        if h not in hold_to_coord:\n",
    "            raise ValueError(f\"[ERROR] Hold '{h}' has no coordinate in hold_to_coord.\")\n",
    "        x, y = hold_to_coord[h]\n",
    "        xy_coords.append([x / 10.0, y / 17.0])\n",
    "\n",
    "    # Build model input tensors\n",
    "    hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)       # (1, N)\n",
    "    diff_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)    # (1, N)\n",
    "    type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)                          # (1, N, T)\n",
    "    xy_tensor = torch.tensor(xy_coords, dtype=torch.float).unsqueeze(0).to(device)        # (1, N, 2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model((hold_tensor, diff_tensor, type_tensor, xy_tensor))\n",
    "\n",
    "    attn_weights = model.encoder[0].mab0.attn_weights  # shape: (heads, seeds, holds)\n",
    "    avg_attn = attn_weights.mean(dim=(0, 1)).cpu().numpy()  # average across heads & seeds → (num_holds,)\n",
    "\n",
    "    return list(zip(holds, avg_attn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce93aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention and scores (with XY support)\n",
    "\n",
    "holds = named_problems[\"warmup crimps\"]\n",
    "\n",
    "if not hasattr(model.encoder[0], 'mab0') or not hasattr(model.encoder[0].mab0, 'attn_weights'):\n",
    "    raise ValueError(\"The provided model does not support attention visualization.\")\n",
    "\n",
    "visualize_attention_for_problem(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device)\n",
    "attention_scores = get_avg_attention_per_hold(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device)\n",
    "\n",
    "# Print sorted scores\n",
    "attention_scores_sorted = sorted(attention_scores, key=lambda x: x[1], reverse=True)\n",
    "print(\"Average Attention Per Hold (sorted):\")\n",
    "for h, score in attention_scores_sorted:\n",
    "    difficulty = hold_difficulty[h][0] if h in hold_difficulty else \"N/A\"\n",
    "    print(f\"{h}: {score:.4f} (difficulty: {difficulty})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb97c2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 model  strict_train  ±1_train  strict_test  ±1_test\n",
      "2   set_transformer_xy         61.56     87.46        43.98    78.97\n",
      "7   set_transformer_xy         65.11     91.18        45.43    81.86\n",
      "12  set_transformer_xy         59.61     86.25        44.37    79.50\n",
      "17  set_transformer_xy         64.78     91.20        44.37    81.23\n",
      "22  set_transformer_xy         60.65     87.16        42.93    78.87\n",
      "27  set_transformer_xy         64.07     90.74        45.81    81.28\n",
      "32  set_transformer_xy         58.98     87.99        42.35    80.17\n",
      "37  set_transformer_xy         61.40     88.75        44.32    80.51\n",
      "42  set_transformer_xy         63.49     90.59        46.01    81.09\n",
      "47  set_transformer_xy         61.77     88.91        43.31    80.65\n"
     ]
    }
   ],
   "source": [
    "# read accuracy.csv file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./result/accuracy.csv')\n",
    "# print(df)\n",
    "\n",
    "filtered_df = df[df['model'] == 'set_transformer_xy']\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa75f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sousei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
