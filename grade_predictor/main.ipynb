{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import torch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "import csv\n",
    "from grade_predictor.models.modules_modified import ISAB, SAB, PMA\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models\n",
    "from models.ensemble import (\n",
    "    SoftVotingEnsemble,\n",
    "    GeometricMeanEnsemble,\n",
    "    MedianEnsemble,\n",
    "    TrimmedMeanEnsemble,\n",
    "    StackingEnsemble,\n",
    "    AdaBoostEnsemble,\n",
    "    GBMEnsemble,\n",
    "    XGBoostEnsemble,\n",
    "    LightGBMEnsemble,\n",
    ")\n",
    "\n",
    "from models.classifier import (\n",
    "    SetTransformerClassifierXY,\n",
    "    SetTransformerClassifierXYAdditive,\n",
    "    SetTransformerClassifier,\n",
    "    DeepSetClassifierXYAdditive,\n",
    "    DeepSetClassifierXY,\n",
    "    DeepSetClassifier,\n",
    ")\n",
    "\n",
    "from models.ordinal import (\n",
    "    SetTransformerOrdinalXY,\n",
    "    SetTransformerOrdinalXYAdditive,\n",
    "    SetTransformerOrdinal,\n",
    "    DeepSetOrdinalXYAdditive,\n",
    "    DeepSetOrdinalXY,\n",
    "    DeepSetOrdinal,\n",
    "    OrdinalSoftVotingEnsemble,\n",
    "    OrdinalGeometricMeanEnsemble,\n",
    "    OrdinalMedianEnsemble,\n",
    "    OrdinalTrimmedMeanEnsemble,\n",
    "    OrdinalStackingEnsemble,\n",
    "    OrdinalGBMEnsemble,\n",
    "    OrdinalXGBoostEnsemble,\n",
    "    OrdinalLightGBMEnsemble,\n",
    "    OrdinalAdaBoostEnsemble,\n",
    ")\n",
    "\n",
    "from utils_ordinal import ordinal_logistic_loss, cumulative_to_labels, threshold_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappings --------------------------------------------------------\n",
    "# Map each hold like \"A1\"…\"K18\" to an integer 0…(11*18−1)=197\n",
    "cols = [chr(c) for c in range(ord('A'), ord('K')+1)]\n",
    "rows = list(range(1, 19))\n",
    "hold_to_idx = {f\"{c}{r}\": i for i, (c, r) in enumerate((c, r) for r in rows for c in cols)}\n",
    "\n",
    "\n",
    "# Map grades \"V4\"…\"V11\" \n",
    "grade_to_label = {f\"V{i}\": i - 4 for i in range(4, 12)}  \n",
    "label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "print(hold_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holds difficulty data --------------------------------------------------------\n",
    "hold_difficulty = {}\n",
    "with open(\"data/hold_difficulty.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if \":\" not in line:\n",
    "            continue  # skip malformed line\n",
    "        hold, rest = line.strip().split(\":\", 1)\n",
    "        parts = rest.strip().split(\",\")\n",
    "        difficulty = int(parts[0].strip())\n",
    "        types = [t.strip() for t in parts[1:]]\n",
    "        hold_difficulty[hold.strip()] = (difficulty, types)\n",
    "    print(\"successfully parsed hold difficulty file\")\n",
    "\n",
    "# prepare type vocabulary\n",
    "unique_types = set()\n",
    "for _, (_, types) in hold_difficulty.items():\n",
    "    unique_types.update(types)\n",
    "\n",
    "type_to_idx = {t: i for i, t in enumerate(sorted(unique_types))}\n",
    "print(f\"successfully prepare type vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign x,y position to each holds -------------------------------\n",
    "import string\n",
    "\n",
    "# Board columns A–K → indices 0–10\n",
    "cols = list(string.ascii_uppercase[:11])  # A–K\n",
    "# Rows 1–18 → indices 0–17\n",
    "rows = list(range(1, 19))  # 1–18\n",
    "\n",
    "# Generate hold_to_coord dictionary\n",
    "hold_to_coord = {}\n",
    "\n",
    "for x, col in enumerate(cols):\n",
    "    for y, row in enumerate(rows):\n",
    "        hold_name = f\"{col}{row}\"\n",
    "        hold_to_coord[hold_name] = (x, y)\n",
    "\n",
    "print(\"successfully created (x,y) position to each hold:\")\n",
    "print(hold_to_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonBoardDataset(Dataset):\n",
    "    def __init__(self, json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord, max_difficulty=10):\n",
    "        self.hold_to_idx = hold_to_idx\n",
    "        self.grade_to_label = grade_to_label\n",
    "        self.hold_difficulty = hold_difficulty\n",
    "        self.type_to_idx = type_to_idx\n",
    "        self.hold_to_coord = hold_to_coord\n",
    "        self.max_difficulty = max_difficulty\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.raw = [json.loads(line) for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.raw[idx]\n",
    "        holds = item['holds']\n",
    "\n",
    "        hold_idxs = []\n",
    "        diff_values = []\n",
    "        type_vecs = []\n",
    "        xy_coords = []\n",
    "\n",
    "        for h in holds:\n",
    "            hold_idxs.append(self.hold_to_idx[h])\n",
    "\n",
    "            difficulty, types = self.hold_difficulty[h]\n",
    "            diff_values.append(difficulty / self.max_difficulty)\n",
    "\n",
    "            # multi-hot vector\n",
    "            type_vec = torch.zeros(len(self.type_to_idx), dtype=torch.float)\n",
    "            for t in types:\n",
    "                if t in self.type_to_idx:\n",
    "                    type_vec[self.type_to_idx[t]] = 1.0\n",
    "            type_vecs.append(type_vec)\n",
    "\n",
    "            # normalized (x, y)\n",
    "            x, y = self.hold_to_coord[h]\n",
    "            xy_coords.append(torch.tensor([x / 10.0, y / 17.0], dtype=torch.float))\n",
    "\n",
    "        return {\n",
    "            \"indices\": torch.tensor(hold_idxs, dtype=torch.long),\n",
    "            \"difficulty\": torch.tensor(diff_values, dtype=torch.float),\n",
    "            \"type\": torch.stack(type_vecs),       # (N, T)\n",
    "            \"xy\": torch.stack(xy_coords)          # (N, 2)\n",
    "        }, torch.tensor(self.grade_to_label[item['grade']], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66965b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# --- Set Hyperparameters ---\n",
    "json_path = './data/cleaned_moonboard2024_grouped.json'\n",
    "embed_dim = 64\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "XY_MODELS = {\n",
    "    'set_transformer_xy',\n",
    "    'set_transformer_additive',\n",
    "    'deepset_xy',\n",
    "    'deepset_xy_additive',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "}\n",
    "\n",
    "ORDINAL_MODELS = {\n",
    "    'set_transformer_ordinal',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "}\n",
    "\n",
    "# --- Collate Function Factory ---\n",
    "def make_collate_fn(model_type):\n",
    "    def collate_fn(batch):\n",
    "        X_indices = [x['indices'] for x, _ in batch]\n",
    "        X_difficulty = [x['difficulty'] for x, _ in batch]\n",
    "        X_type = [x['type'] for x, _ in batch]\n",
    "        y_batch = [y for _, y in batch]\n",
    "\n",
    "        X_indices = pad_sequence(X_indices, batch_first=True)\n",
    "        X_difficulty = pad_sequence(X_difficulty, batch_first=True)\n",
    "        X_type = pad_sequence(X_type, batch_first=True)\n",
    "        y_tensor = torch.stack(y_batch)\n",
    "\n",
    "        if model_type in XY_MODELS:\n",
    "            X_xy = [x['xy'] for x, _ in batch]\n",
    "            X_xy = pad_sequence(X_xy, batch_first=True)\n",
    "            return (X_indices, X_difficulty, X_type, X_xy), y_tensor\n",
    "        else:\n",
    "            return (X_indices,), y_tensor\n",
    "    return collate_fn\n",
    "\n",
    "# --- Dataset Loader ---\n",
    "def load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord):\n",
    "    return MoonBoardDataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "\n",
    "# --- DataLoader Preparation ---\n",
    "def prepare_dataloaders(dataset, grade_to_label, batch_size, collate_fn):\n",
    "    targets = [grade_to_label[item['grade']] for item in dataset.raw]\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(targets), y=targets)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(dataset))), test_size=0.2, stratify=targets, random_state=42\n",
    "    )\n",
    "\n",
    "    train_data = Subset(dataset, train_idx)\n",
    "    val_data = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, class_weights, train_idx, val_idx\n",
    "\n",
    "# --- Training ---\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, is_ordinal=False):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "            payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            outputs = model(payload)\n",
    "            if is_ordinal:\n",
    "                probs, logits = outputs\n",
    "                loss = criterion(logits, y)\n",
    "            else:\n",
    "                logits = outputs\n",
    "                loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch:02d} — loss: {total_loss / len(train_loader):.4f}\")\n",
    "    return model\n",
    "\n",
    "# --- Main Per Model ---\n",
    "def main(model_type):\n",
    "    dataset = load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "    targets = [grade_to_label[item['grade']] for item in dataset.raw]\n",
    "    num_classes = len(np.unique(targets))\n",
    "    vocab_size = len(hold_to_idx)\n",
    "    type_vec_dim = len(type_to_idx)\n",
    "    is_ordinal = model_type in ORDINAL_MODELS\n",
    "\n",
    "    if model_type == 'set_transformer':\n",
    "        ModelClass = SetTransformerClassifier\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'set_transformer_xy':\n",
    "        ModelClass = SetTransformerClassifierXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_additive':\n",
    "        ModelClass = SetTransformerClassifierXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset':\n",
    "        ModelClass = DeepSetClassifier\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'deepset_xy':\n",
    "        ModelClass = DeepSetClassifierXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_xy_additive':\n",
    "        ModelClass = DeepSetClassifierXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_ordinal':\n",
    "        ModelClass = SetTransformerOrdinal\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'set_transformer_ordinal_xy':\n",
    "        ModelClass = SetTransformerOrdinalXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_ordinal_xy_additive':\n",
    "        ModelClass = SetTransformerOrdinalXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_ordinal':\n",
    "        ModelClass = DeepSetOrdinal\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'deepset_ordinal_xy':\n",
    "        ModelClass = DeepSetOrdinalXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_ordinal_xy_additive':\n",
    "        ModelClass = DeepSetOrdinalXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "    collate_fn = make_collate_fn(model_type)\n",
    "    train_loader, val_loader, class_weights, train_idx, val_idx = prepare_dataloaders(dataset, grade_to_label, batch_size, collate_fn)\n",
    "\n",
    "    model = ModelClass(**kwargs).to(device)\n",
    "    model.is_ordinal = is_ordinal\n",
    "    model.num_classes = num_classes\n",
    "\n",
    "    if is_ordinal:\n",
    "        def criterion_fn(logits, targets):\n",
    "            return ordinal_logistic_loss(logits, targets)\n",
    "    else:\n",
    "        criterion_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model = train_model(model, train_loader, val_loader, criterion_fn, optimizer, epochs, is_ordinal=is_ordinal)\n",
    "    return train_loader, val_loader, model, dataset, train_idx, val_idx\n",
    "\n",
    "\n",
    "def train_boosting_main(model_type, num_stages=5, weak_epochs=3):\n",
    "    \"\"\"\n",
    "    Replaces the `main(mtype)` call for boosting models.\n",
    "    Trains a sequential AdaBoost-style ensemble.\n",
    "    \n",
    "    Returns the same tuple as `main()`:\n",
    "    (train_loader, val_loader, final_model, dataset, train_idx, val_idx)\n",
    "    \"\"\"\n",
    "    print(f\"===== Training Boosting Ensemble ({model_type}, {num_stages} stages) =====\")\n",
    "    \n",
    "    # --- 1. Standard Dataset Setup (copied from `main`) ---\n",
    "    dataset = load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "    targets = [grade_to_label[item['grade']] for item in dataset.raw]\n",
    "    num_classes = len(np.unique(targets))\n",
    "    vocab_size = len(hold_to_idx)\n",
    "    type_vec_dim = len(type_to_idx)\n",
    "    is_ordinal = False # Boosting classifiers, not ordinal models\n",
    "\n",
    "    collate_fn = make_collate_fn(model_type) # Use collate fn for the base model\n",
    "    \n",
    "    # Get train/val split (we need the indices)\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(dataset))), test_size=0.2, stratify=targets, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_data_subset = Subset(dataset, train_idx)\n",
    "    val_data_subset = Subset(dataset, val_idx)\n",
    "    \n",
    "    # This is the standard val_loader, used for final eval\n",
    "    val_loader = DataLoader(val_data_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # This loader is for *evaluating* the weak learner on train data (no shuffle)\n",
    "    train_eval_loader = DataLoader(train_data_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # --- 2. Boosting-Specific Setup ---\n",
    "    num_train_samples = len(train_idx)\n",
    "    # Initialize uniform sample weights\n",
    "    sample_weights = torch.full((num_train_samples,), 1.0 / num_train_samples, device=device)\n",
    "    \n",
    "    trained_models_list = [] # (name, model)\n",
    "    model_alphas = []        # [alpha]\n",
    "    \n",
    "    # --- 3. The Sequential Training Loop ---\n",
    "    for m in range(num_stages):\n",
    "        print(f\"--- Boosting Stage {m+1}/{num_stages} ---\")\n",
    "        \n",
    "        # a. Create a new dataloader for this stage\n",
    "        #    It samples from train_data_subset based on the *current* sample_weights\n",
    "        sampler = WeightedRandomSampler(sample_weights.cpu(), num_train_samples, replacement=True)\n",
    "        train_loader_stage = DataLoader(train_data_subset, batch_size=batch_size, sampler=sampler, collate_fn=collate_fn)\n",
    "\n",
    "        # b. Create and train the weak learner\n",
    "        #    We re-use the logic from your `main()` function to get the model\n",
    "        if model_type == 'set_transformer':\n",
    "            ModelClass = SetTransformerClassifier\n",
    "            kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "        elif model_type == 'set_transformer_xy':\n",
    "            ModelClass = SetTransformerClassifierXY\n",
    "            kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "        elif model_type == 'set_transformer_additive':\n",
    "            ModelClass = SetTransformerClassifierXYAdditive\n",
    "            kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "        elif model_type == 'deepset':\n",
    "            ModelClass = DeepSetClassifier\n",
    "            kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "        elif model_type == 'deepset_xy':\n",
    "            ModelClass = DeepSetClassifierXY\n",
    "            kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "        elif model_type == 'deepset_xy_additive':\n",
    "            ModelClass = DeepSetClassifierXYAdditive\n",
    "            kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "        else:\n",
    "            # This is the correct error message\n",
    "            raise ValueError(f\"Unsupported weak learner type for boosting: {model_type}\")\n",
    "\n",
    "        model_m = ModelClass(**kwargs).to(device)\n",
    "        \n",
    "        # We can use standard CrossEntropyLoss because the *sampler* already weighted the data\n",
    "        criterion_fn = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model_m.parameters(), lr=lr)\n",
    "        \n",
    "        # c. Train this weak learner (RE-USING YOUR EXISTING `train_model` FUNCTION!)\n",
    "        print(f\"Training weak learner {m+1} for {weak_epochs} epochs...\")\n",
    "        model_m = train_model(model_m, train_loader_stage, val_loader, criterion_fn, optimizer, weak_epochs, is_ordinal=is_ordinal)\n",
    "        \n",
    "        # d. Evaluate on *all* training data (unshuffled)\n",
    "        model_m.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for X, y in train_eval_loader: # Use NON-shuffled loader\n",
    "                inputs = tuple(x.to(device) for x in X)\n",
    "                payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "                logits = model_m(payload)\n",
    "                all_preds.append(logits.argmax(dim=1))\n",
    "                all_targets.append(y.to(device))\n",
    "        \n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "        \n",
    "        # e. Compute weighted error\n",
    "        is_incorrect = (all_preds != all_targets).float() # [num_train_samples]\n",
    "        err_m = (is_incorrect * sample_weights).sum() # Sum of weights of incorrect samples\n",
    "        \n",
    "        if err_m <= 0 or err_m >= (1.0 - 1.0 / num_classes):\n",
    "            print(f\"Stage {m+1} model is perfect or too weak (err={err_m:.4f}). Stopping.\")\n",
    "            if err_m <= 0: # Add perfect model and break\n",
    "                model_alphas.append(1.0) # Use a reasonable weight\n",
    "                trained_models_list.append((f\"boost_model_{m}\", model_m))\n",
    "            break\n",
    "        \n",
    "        # f. Compute model weight (alpha)\n",
    "        alpha_m = torch.log((1.0 - err_m) / err_m) + torch.log(torch.tensor(num_classes - 1.0, device=device))\n",
    "        \n",
    "        # g. Update sample weights\n",
    "        sample_weights *= torch.exp(alpha_m * is_incorrect)\n",
    "        sample_weights /= sample_weights.sum() # Normalize\n",
    "        \n",
    "        # h. Save\n",
    "        trained_models_list.append((f\"boost_model_{m}\", model_m))\n",
    "        model_alphas.append(alpha_m.item())\n",
    "        print(f\"Stage {m+1}: Error={err_m:.4f}, Alpha={alpha_m:.4f}\")\n",
    "    \n",
    "    # --- 4. Build the Final Ensemble Model ---\n",
    "    if not trained_models_list:\n",
    "        raise RuntimeError(\"Boosting training failed, no models were trained.\")\n",
    "        \n",
    "    print(f\"Building final ensemble with {len(trained_models_list)} models.\")\n",
    "    \n",
    "    # *** USE THE NEW NAME HERE ***\n",
    "    final_ensemble = AdaBoostEnsemble(trained_models_list, weights=model_alphas, freeze_members=True).to(device)\n",
    "    \n",
    "    final_ensemble.is_ordinal = is_ordinal\n",
    "    final_ensemble.num_classes = num_classes\n",
    "    \n",
    "    # Return the same \"package\" as main()\n",
    "    # `train_eval_loader` is the unshuffled train loader, which `compare_models`\n",
    "    # can use for ensemble training (like stacking) if needed.\n",
    "    return train_eval_loader, val_loader, final_ensemble, dataset, train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5db06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image as XLImage\n",
    "\n",
    "BASE_MODEL_TYPES = [\n",
    "    \"set_transformer\",\n",
    "    \"deepset\",\n",
    "    \"set_transformer_xy\",\n",
    "    \"deepset_xy\",\n",
    "    \"set_transformer_additive\",\n",
    "    \"deepset_xy_additive\"\n",
    "]\n",
    "\n",
    "BOOSTING_TYPES = {\n",
    "    \"adaboost_deepset\": \"deepset\",\n",
    "    \"adaboost_deepset_xy_additive\": \"deepset_xy_additive\",\n",
    "    \"adaboost_deepset_xy\": \"deepset_xy\",\n",
    "    \"adaboost_set_transformer\": \"set_transformer\",\n",
    "    \"adaboost_set_transformer_additive\": \"set_transformer_additive\",\n",
    "    \"adaboost_set_transformer_xy\": \"set_transformer_xy\",\n",
    "}\n",
    "\n",
    "MODEL_TYPES = BASE_MODEL_TYPES + list(BOOSTING_TYPES.keys())\n",
    "\n",
    "ENSEMBLE_TYPES = [\n",
    "    \"soft_voting_ensemble\",\n",
    "    \"geometric_mean_ensemble\",\n",
    "    \"median_ensemble\",\n",
    "    \"trimmed_mean_ensemble\",\n",
    "    \"stacking_ensemble\",\n",
    "    \"gbm_ensemble\",\n",
    "    \"xgboost_ensemble\",\n",
    "    \"lightgbm_ensemble\"\n",
    "]\n",
    "\n",
    "MODEL_COUNT_COLUMNS = {name: f\"{name}_count\" for name in MODEL_TYPES + ENSEMBLE_TYPES}\n",
    "\n",
    "\n",
    "# --- plot confusion matrix and save to excel---\n",
    "def save_confusion_matrix_to_excel(y_true, y_pred, class_labels, model_name, excel_path):\n",
    "    # Plot confusion matrix and save as image\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(class_labels)), normalize='true')\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "    plt.xlabel(\"Predicted Grade\")\n",
    "    plt.ylabel(\"Actual Grade\")\n",
    "    plt.tight_layout()\n",
    "    img_path = f\"result/confusion_{model_name}.png\"\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Insert image into Excel (new sheet per model)\n",
    "    wb = load_workbook(excel_path)\n",
    "    if model_name in wb.sheetnames:\n",
    "        ws = wb[model_name]\n",
    "    else:\n",
    "        ws = wb.create_sheet(title=model_name)\n",
    "    img = XLImage(img_path)\n",
    "    ws.add_image(img, \"A1\")\n",
    "    wb.save(excel_path)\n",
    "    print(f\"Confusion matrix for {model_name} saved and inserted into {excel_path} (sheet: {model_name})\")\n",
    "\n",
    "\n",
    "# --- export the predictions to excel ---\n",
    "def _update_outlier_excel(df_all_preds, model_name, outlier_filename=\"result/outlier.xlsx\", sheet_name=\"outliers\", threshold=3):\n",
    "    \"\"\"\n",
    "    From a DataFrame with columns [problem_name, y_true, y_pred, diff],\n",
    "    keep rows where abs(diff) > threshold and aggregate per problem_name:\n",
    "        - count = number of times flagged\n",
    "        - per-model counts = number of times flagged per model across runs\n",
    "        - y_true = mode (most frequent true label)\n",
    "        - y_pred_avg = average predicted label across occurrences\n",
    "    Save to outlier.xlsx.\n",
    "    \"\"\"\n",
    "    current_model_col = MODEL_COUNT_COLUMNS.get(model_name, f\"{model_name}_count\")\n",
    "    model_count_columns = dict(MODEL_COUNT_COLUMNS)\n",
    "    if model_name not in MODEL_COUNT_COLUMNS:\n",
    "        model_count_columns[model_name] = current_model_col\n",
    "\n",
    "    # Filter outliers\n",
    "    outliers = df_all_preds.loc[df_all_preds[\"diff\"].abs() > threshold,\n",
    "                                [\"problem_name\", \"y_true\", \"y_pred\"]]\n",
    "    if outliers.empty:\n",
    "        print(f\"No outliers (abs(diff) > {threshold}). Skipped creating outlier.xlsx.\")\n",
    "        return\n",
    "\n",
    "    # Group & aggregate\n",
    "    grouped = (outliers\n",
    "               .groupby(\"problem_name\")\n",
    "               .agg(\n",
    "                   count=(\"problem_name\", \"size\"),\n",
    "                   y_true=(\"y_true\", lambda x: x.mode().iat[0] if not x.mode().empty else x.iloc[0]),\n",
    "                   y_pred_avg=(\"y_pred\", lambda x: round(pd.to_numeric(x, errors=\"coerce\").mean(), 2))\n",
    "               )\n",
    "               .reset_index())\n",
    "\n",
    "    for col in model_count_columns.values():\n",
    "        if col not in grouped.columns:\n",
    "            grouped[col] = 0\n",
    "    grouped[current_model_col] = grouped[\"count\"]\n",
    "\n",
    "    # If a previous file exists, merge and accumulate counts\n",
    "    if os.path.exists(outlier_filename):\n",
    "        try:\n",
    "            existing = pd.read_excel(outlier_filename, sheet_name=sheet_name)\n",
    "            for col in model_count_columns.values():\n",
    "                if col not in existing.columns:\n",
    "                    existing[col] = 0\n",
    "            if set(existing.columns) >= {\"problem_name\", \"count\", \"y_true\", \"y_pred_avg\"}:\n",
    "                merged = pd.concat([existing, grouped], ignore_index=True, sort=False)\n",
    "                for col in model_count_columns.values():\n",
    "                    if col not in merged.columns:\n",
    "                        merged[col] = 0\n",
    "                agg_map = {\n",
    "                    \"count\": (\"count\", \"sum\"),\n",
    "                    \"y_true\": (\"y_true\", lambda x: x.mode().iat[0] if not x.mode().empty else x.iloc[0]),\n",
    "                    \"y_pred_avg\": (\"y_pred_avg\", \"mean\")\n",
    "                }\n",
    "                agg_map.update({col: (col, \"sum\") for col in model_count_columns.values()})\n",
    "                grouped = (merged\n",
    "                           .groupby(\"problem_name\")\n",
    "                           .agg(**agg_map)\n",
    "                           .reset_index())\n",
    "            # else keep grouped as new\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    grouped[\"y_pred_avg\"] = pd.to_numeric(grouped[\"y_pred_avg\"], errors=\"coerce\").round(2)\n",
    "    grouped[\"count\"] = grouped[\"count\"].fillna(0).astype(int)\n",
    "    for col in model_count_columns.values():\n",
    "        if col in grouped.columns:\n",
    "            grouped[col] = grouped[col].fillna(0).astype(int)\n",
    "\n",
    "    ordered_cols = [\"problem_name\", \"count\"]\n",
    "    ordered_cols.extend([model_count_columns[name] for name in MODEL_TYPES + ENSEMBLE_TYPES if model_count_columns[name] in grouped.columns])\n",
    "    if current_model_col in grouped.columns and current_model_col not in ordered_cols:\n",
    "        ordered_cols.append(current_model_col)\n",
    "    ordered_cols.extend([\"y_true\", \"y_pred_avg\"])\n",
    "    ordered_cols.extend([col for col in grouped.columns if col not in ordered_cols])\n",
    "    grouped = grouped[ordered_cols]\n",
    "\n",
    "    # Save\n",
    "    with pd.ExcelWriter(outlier_filename, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "        grouped.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Outliers saved to: {os.path.abspath(outlier_filename)}\")\n",
    "\n",
    "\n",
    "def export_predictions_to_excel(model, dataloader, device, grade_to_label, excel_path, sheet_name, model_name=None):\n",
    "    results = []\n",
    "    raw_dataset = dataloader.dataset.dataset  # MoonBoardDataset\n",
    "    indices = dataloader.dataset.indices      # Subset indices\n",
    "    label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "    current_index = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if isinstance(X, tuple):\n",
    "                inputs = tuple(x.to(device) for x in X)\n",
    "                payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            else:\n",
    "                payload = X.to(device)\n",
    "            outputs = model(payload)\n",
    "            if isinstance(outputs, tuple):\n",
    "                if getattr(model, \"is_ordinal\", False):\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = cumulative_to_labels(probs)\n",
    "                else:\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = probs.argmax(dim=1)\n",
    "            else:\n",
    "                preds_tensor = outputs.argmax(dim=1)\n",
    "            y = y.to(device)\n",
    "            preds_cpu = preds_tensor.detach().cpu()\n",
    "            y_cpu = y.detach().cpu()\n",
    "            for i in range(y_cpu.size(0)):\n",
    "                real_label = int(y_cpu[i].item())\n",
    "                pred_label = int(preds_cpu[i].item())\n",
    "                dataset_index = indices[current_index]\n",
    "                current_index += 1\n",
    "                raw_item = raw_dataset.raw[dataset_index]\n",
    "                problem_name = raw_item.get('problem_name', f\"problem_{dataset_index}\")\n",
    "                results.append({\n",
    "                    \"problem_name\": problem_name,\n",
    "                    \"y_true\": real_label,  # keep numeric for averaging/aggregation\n",
    "                    \"y_pred\": pred_label,\n",
    "                    \"diff\": real_label - pred_label\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if model_name is None:\n",
    "        model_name = sheet_name\n",
    "    df[\"model_name\"] = model_name\n",
    "    df[\"model\"] = model_name\n",
    "\n",
    "    # 1) Save all predictions into your main Excel file\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        # Convert numeric labels back to grade strings for readability\n",
    "        df_out = df.copy()\n",
    "        df_out[\"y_true\"] = df_out[\"y_true\"].map(lambda x: label_to_grade.get(x, f\"Unknown({x})\"))\n",
    "        df_out[\"y_pred\"] = df_out[\"y_pred\"].map(lambda x: label_to_grade.get(x, f\"Unknown({x})\"))\n",
    "        df_out.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Predictions for {sheet_name} exported to: {excel_path}\")\n",
    "\n",
    "    # 2) Create/update outlier.xlsx (problem_name, count, per-model counts, y_true, y_pred_avg)\n",
    "    _update_outlier_excel(df, model_name=model_name, outlier_filename=\"result/outlier.xlsx\", sheet_name=\"outliers\", threshold=3)\n",
    "\n",
    "\n",
    "# --- compute training and validation accuracy ---\n",
    "def compute_accuracy(model, dataloader, device):\n",
    "    strict_correct, loose_correct, total = 0, 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "\n",
    "            payload = X[0] if len(X) == 1 else X\n",
    "            outputs = model(payload)\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "                if getattr(model, \"is_ordinal\", False):\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = cumulative_to_labels(probs)\n",
    "                else:\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = probs.argmax(dim=1)\n",
    "            else:\n",
    "                preds_tensor = outputs.argmax(dim=1)\n",
    "\n",
    "            if isinstance(preds_tensor, torch.Tensor):\n",
    "                preds_tensor = preds_tensor.to(y.device)\n",
    "            preds = preds_tensor\n",
    "            total += y.size(0)\n",
    "            strict_correct += (preds == y).sum().item()\n",
    "            loose_correct += ((preds - y).abs() <= 1).sum().item()\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.detach().cpu().numpy())\n",
    "    strict_acc = 100.0 * strict_correct / total\n",
    "    loose_acc = 100.0 * loose_correct / total\n",
    "    return strict_acc, loose_acc, y_true, y_pred\n",
    "\n",
    "\n",
    "def log_accuracy_to_csv(model_type, train_strict_acc, train_loose_acc, val_strict_acc, val_loose_acc, csv_path=\"result/accuracy.csv\"):\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "    with open(csv_path, mode='a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\n",
    "                \"model\",\n",
    "                \"Train Strict Accuracy (%)\",\n",
    "                \"Train ±1 Grade Accuracy (%)\",\n",
    "                \"Val Strict Accuracy (%)\",\n",
    "                \"Val ±1 Grade Accuracy (%)\"\n",
    "            ])\n",
    "        writer.writerow([\n",
    "            model_type,\n",
    "            round(train_strict_acc, 2),\n",
    "            round(train_loose_acc, 2),\n",
    "            round(val_strict_acc, 2),\n",
    "            round(val_loose_acc, 2)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae87271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stacking_meta_model(stacking_model, dataloader, device, epochs=5, lr=1e-3):\n",
    "    \"\"\"Train stacking meta-learner on frozen base model outputs.\"\"\"\n",
    "    if epochs <= 0:\n",
    "        return\n",
    "    stacking_model.meta_model.train()\n",
    "    for member in stacking_model.models.values():\n",
    "        member.eval()\n",
    "    optimizer = torch.optim.Adam(stacking_model.meta_model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_samples = 0\n",
    "        for X, y in dataloader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            targets = y.to(device)\n",
    "            member_feats = stacking_model._member_features(inputs)\n",
    "            M, B, F = member_feats.shape\n",
    "            if stacking_model.combine == \"mean\":\n",
    "                feat = member_feats.mean(dim=0)\n",
    "            else:\n",
    "                feat = member_feats.permute(1, 0, 2).reshape(B, M * F)\n",
    "\n",
    "            logits = stacking_model.meta_model(feat)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size = targets.size(0)\n",
    "            epoch_loss += loss.item() * batch_size\n",
    "            num_samples += batch_size\n",
    "\n",
    "        denom = num_samples if num_samples > 0 else 1\n",
    "        avg_loss = epoch_loss / denom\n",
    "        print(f\"Stacking meta epoch {epoch + 1}: loss={avg_loss:.4f}\")\n",
    "\n",
    "    stacking_model.meta_model.eval()\n",
    "\n",
    "\n",
    "def train_tree_meta_model(tree_ensemble, dataloader, device):\n",
    "    \"\"\"Fit a tree-based meta-learner (GBM/XGBoost/LightGBM) on frozen base outputs.\"\"\"\n",
    "    tree_ensemble.eval()\n",
    "    for member in tree_ensemble.models.values():\n",
    "        member.eval()\n",
    "    feature_blocks = []\n",
    "    target_blocks = []\n",
    "    for X, y in dataloader:\n",
    "        inputs = tuple(x.to(device) for x in X)\n",
    "        member_feats = tree_ensemble._member_features(inputs)\n",
    "        feat = tree_ensemble._build_feature_matrix(member_feats)\n",
    "        feature_blocks.append(feat.detach().cpu().numpy())\n",
    "        target_blocks.append(y.detach().cpu().numpy())\n",
    "    if not feature_blocks:\n",
    "        raise RuntimeError(\"No data available to fit tree-based meta learner.\")\n",
    "    features = np.concatenate(feature_blocks, axis=0)\n",
    "    targets = np.concatenate(target_blocks, axis=0)\n",
    "    tree_ensemble.fit_meta_model(features, targets)\n",
    "\n",
    "def infer_stacking_feature_dim(stacking_model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Determine the flattened feature dimension seen by the stacking meta-model.\n",
    "    \"\"\"\n",
    "    stacking_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in dataloader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            member_feats = stacking_model._member_features(inputs)\n",
    "            M, _, F = member_feats.shape\n",
    "            return F if stacking_model.combine == \"mean\" else M * F\n",
    "    raise RuntimeError(\"Unable to infer stacking feature dimension (empty dataloader?).\")\n",
    "\n",
    "\n",
    "def build_ensemble_models(\n",
    "    ensemble_names,\n",
    "    base_model_items,\n",
    "    ensemble_weights,\n",
    "    num_classes,\n",
    "    device,\n",
    "    train_loader,\n",
    "    stacking_meta_epochs=5,\n",
    "    stacking_meta_lr=1e-3,\n",
    "    label_suffix=\"\",\n",
    "):\n",
    "    \"\"\"Create configured ensemble models from trained base models.\"\"\"\n",
    "    ensembles = {}\n",
    "    base_items = list(base_model_items)\n",
    "\n",
    "    if not base_items:\n",
    "        return ensembles\n",
    "\n",
    "    def _resolve_group_weights(items):\n",
    "        if ensemble_weights is None:\n",
    "            return None\n",
    "        if isinstance(ensemble_weights, dict):\n",
    "            filtered = {name: ensemble_weights[name] for name, _ in items if name in ensemble_weights}\n",
    "            if len(filtered) != len(items):\n",
    "                missing = [name for name, _ in items if name not in filtered]\n",
    "                if missing:\n",
    "                    print(f\"Warning: missing weights for {missing}; using uniform weights.\")\n",
    "                return None\n",
    "            return filtered\n",
    "        weight_list = list(ensemble_weights)\n",
    "        if len(weight_list) != len(items):\n",
    "            print(\"Warning: weight list length mismatch; using uniform weights.\")\n",
    "            return None\n",
    "        return weight_list\n",
    "\n",
    "    resolved_weights = _resolve_group_weights(base_items)\n",
    "\n",
    "    for base_name in ensemble_names:\n",
    "        cloned_items = [(model_name, copy.deepcopy(model)) for model_name, model in base_items]\n",
    "        if not cloned_items:\n",
    "            continue\n",
    "\n",
    "        weights = resolved_weights\n",
    "        if isinstance(weights, list):\n",
    "            weights = list(weights)\n",
    "\n",
    "        ensemble_key = f\"{base_name}{label_suffix}\" if label_suffix else base_name\n",
    "\n",
    "        if base_name == \"soft_voting_ensemble\":\n",
    "            ensemble_model = SoftVotingEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == \"geometric_mean_ensemble\":\n",
    "            ensemble_model = GeometricMeanEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == \"median_ensemble\":\n",
    "            ensemble_model = MedianEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == \"trimmed_mean_ensemble\":\n",
    "            ensemble_model = TrimmedMeanEnsemble(cloned_items, weights=weights, freeze_members=True, trim_frac=0.2).to(device)\n",
    "        elif base_name == \"stacking_ensemble\":\n",
    "            placeholder_dim = len(cloned_items) * num_classes\n",
    "            meta_model = nn.Linear(placeholder_dim, num_classes).to(device)\n",
    "            ensemble_model = StackingEnsemble(\n",
    "                cloned_items,\n",
    "                weights=weights,\n",
    "                freeze_members=True,\n",
    "                meta_model=meta_model,\n",
    "                feature_source=\"logits+internal\",\n",
    "                combine=\"concat\",\n",
    "            ).to(device)\n",
    "            inferred_dim = infer_stacking_feature_dim(ensemble_model, train_loader, device)\n",
    "            if inferred_dim != placeholder_dim:\n",
    "                ensemble_model.meta_model = nn.Linear(inferred_dim, num_classes).to(device)\n",
    "            train_stacking_meta_model(\n",
    "                ensemble_model,\n",
    "                train_loader,\n",
    "                device,\n",
    "                epochs=stacking_meta_epochs,\n",
    "                lr=stacking_meta_lr,\n",
    "            )\n",
    "        elif base_name == \"gbm_ensemble\":\n",
    "            try:\n",
    "                ensemble_model = GBMEnsemble(\n",
    "                    cloned_items,\n",
    "                    weights=weights,\n",
    "                    freeze_members=True,\n",
    "                    num_classes=num_classes,\n",
    "                    feature_source=\"logits\",\n",
    "                    combine=\"concat\",\n",
    "                    meta_kwargs={\"random_state\": 42},\n",
    "                ).to(device)\n",
    "            except ImportError as exc:\n",
    "                print(f\"Skipping {base_name}: {exc}\")\n",
    "                continue\n",
    "            train_tree_meta_model(ensemble_model, train_loader, device)\n",
    "        elif base_name == \"xgboost_ensemble\":\n",
    "            try:\n",
    "                ensemble_model = XGBoostEnsemble(\n",
    "                    cloned_items,\n",
    "                    weights=weights,\n",
    "                    freeze_members=True,\n",
    "                    num_classes=num_classes,\n",
    "                    feature_source=\"logits\",\n",
    "                    combine=\"concat\",\n",
    "                    meta_kwargs={\"n_estimators\": 300, \"learning_rate\": 0.05, \"max_depth\": 4},\n",
    "                ).to(device)\n",
    "            except ImportError as exc:\n",
    "                print(f\"Skipping {base_name}: {exc}\")\n",
    "                continue\n",
    "            train_tree_meta_model(ensemble_model, train_loader, device)\n",
    "        elif base_name == \"lightgbm_ensemble\":\n",
    "            try:\n",
    "                ensemble_model = LightGBMEnsemble(\n",
    "                    cloned_items,\n",
    "                    weights=weights,\n",
    "                    freeze_members=True,\n",
    "                    num_classes=num_classes,\n",
    "                    feature_source=\"logits\",\n",
    "                    combine=\"concat\",\n",
    "                    meta_kwargs={\"n_estimators\": 300, \"learning_rate\": 0.05, \"max_depth\": -1},\n",
    "                ).to(device)\n",
    "            except ImportError as exc:\n",
    "                print(f\"Skipping {base_name}: {exc}\")\n",
    "                continue\n",
    "            train_tree_meta_model(ensemble_model, train_loader, device)\n",
    "        else:\n",
    "            print(f\"Unknown ensemble type '{base_name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        ensemble_model.eval()\n",
    "        ensembles[ensemble_key] = ensemble_model\n",
    "\n",
    "    return ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(\n",
    "    model_types=None,\n",
    "    include_ensemble=True,\n",
    "    ensemble_weights=None,\n",
    "    ensemble_types=None,\n",
    "    stacking_meta_epochs=5,\n",
    "    stacking_meta_lr=1e-3,\n",
    "    boosting_num_stages=5, \n",
    "    boosting_weak_epochs=3 \n",
    "):\n",
    "    model_types = model_types or MODEL_TYPES\n",
    "    results = []\n",
    "    excel_path = \"result/model_comparison_results.xlsx\"\n",
    "    class_labels = [f\"V{i}\" for i in range(4, 12)]\n",
    "\n",
    "    # BOOSTING_TYPES (defined globally) maps each boosting variant to its base learner.\n",
    "\n",
    "    trained_models = {}\n",
    "    base_dataset = None\n",
    "    base_train_idx = None\n",
    "    base_val_idx = None\n",
    "    num_classes = None\n",
    "\n",
    "    # --- THIS IS THE MODIFIED LOOP ---\n",
    "    for idx, mtype in enumerate(model_types):\n",
    "        print(f\"===== Processing {mtype} =====\")\n",
    "        \n",
    "        if mtype in BOOSTING_TYPES:\n",
    "            # --- This is a SEQUENTIAL (Boosting) Model ---\n",
    "            base_model_type = BOOSTING_TYPES[mtype]\n",
    "            # Call our new boosting trainer\n",
    "            train_loader, val_loader, model, dataset, train_idx, val_idx = train_boosting_main(\n",
    "                model_type=base_model_type,\n",
    "                num_stages=boosting_num_stages,\n",
    "                weak_epochs=boosting_weak_epochs\n",
    "            )\n",
    "        else:\n",
    "            # --- This is a PARALLEL (Standard) Model ---\n",
    "            print(f\"===== Training {mtype} =====\")\n",
    "            train_loader, val_loader, model, dataset, train_idx, val_idx = main(mtype)\n",
    "\n",
    "        # --- THE REST OF THE LOOP IS UNCHANGED ---\n",
    "        # Because both `main` and `train_boosting_main` return the same\n",
    "        # tuple, the evaluation code works perfectly for both.\n",
    "        model.eval()\n",
    "        trained_models[mtype] = model\n",
    "        if base_dataset is None:\n",
    "            base_dataset = dataset\n",
    "            base_train_idx = train_idx\n",
    "            base_val_idx = val_idx\n",
    "        if num_classes is None:\n",
    "            num_classes = getattr(model, \"num_classes\", None)\n",
    "\n",
    "        train_strict_acc, train_loose_acc, _, _ = compute_accuracy(model, train_loader, device)\n",
    "        val_strict_acc, val_loose_acc, y_true, y_pred = compute_accuracy(model, val_loader, device)\n",
    "\n",
    "        log_accuracy_to_csv(mtype, train_strict_acc, train_loose_acc, val_strict_acc, val_loose_acc)\n",
    "\n",
    "        results.append({\n",
    "            \"Model Type\": mtype,\n",
    "            \"Train Strict Accuracy (%)\": train_strict_acc,\n",
    "            \"Train ±1 Grade Accuracy (%)\": train_loose_acc,\n",
    "            \"Val Strict Accuracy (%)\": val_strict_acc,\n",
    "            \"Val ±1 Grade Accuracy (%)\": val_loose_acc,\n",
    "        })\n",
    "\n",
    "        if idx == 0:\n",
    "            df_results = pd.DataFrame(results)\n",
    "            df_results.to_excel(excel_path, index=False)\n",
    "\n",
    "        save_confusion_matrix_to_excel(y_true, y_pred, class_labels, mtype, excel_path)\n",
    "        export_predictions_to_excel(\n",
    "            model,\n",
    "            val_loader,\n",
    "            device,\n",
    "            grade_to_label,\n",
    "            excel_path,\n",
    "            sheet_name=f\"{mtype}_preds\",\n",
    "            model_name=mtype,\n",
    "        )\n",
    "\n",
    "    # --- ENSEMBLE EVALUATION SECTION ---\n",
    "    if include_ensemble and trained_models:\n",
    "        effective_ensemble_types = ensemble_types or ENSEMBLE_TYPES\n",
    "        if not effective_ensemble_types:\n",
    "            print(\"No ensemble types specified; skipping ensemble evaluation.\")\n",
    "        else:\n",
    "            if base_dataset is None or base_train_idx is None or base_val_idx is None:\n",
    "                raise RuntimeError(\"Dataset indices are unavailable for ensemble evaluation.\")\n",
    "            if num_classes is None:\n",
    "                raise RuntimeError(\"Unable to determine number of classes for ensembles.\")\n",
    "\n",
    "            print(\"===== Evaluating ensembles =====\")\n",
    "            collate_fn = make_collate_fn(\"set_transformer_xy\")\n",
    "            train_subset = Subset(base_dataset, base_train_idx)\n",
    "            val_subset = Subset(base_dataset, base_val_idx)\n",
    "            ensemble_train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "            ensemble_val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "            # Filter trained_models to EXCLUDE boosting models from being ensembled\n",
    "            base_items = [(name, model) for name, model in trained_models.items() if name not in BOOSTING_TYPES]\n",
    "\n",
    "            # Now ensemble_groups will only contain the original base models\n",
    "            ensemble_groups = [\n",
    "                (\"all\", base_items),\n",
    "                (\"set_transformer\", [(name, model) for name, model in base_items if \"set_transformer\" in name]),\n",
    "                (\"deepset\", [(name, model) for name, model in base_items if \"deepset\" in name]),\n",
    "            ]\n",
    "\n",
    "            for group_name, group_items in ensemble_groups:\n",
    "                if not group_items:\n",
    "                    print(f\"Skipping {group_name} ensemble group (no models).\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"--- Evaluating {group_name} ensembles ({len(group_items)} models) ---\")\n",
    "                ensembles = build_ensemble_models(\n",
    "                    effective_ensemble_types,\n",
    "                    group_items,\n",
    "                    ensemble_weights,\n",
    "                    num_classes,\n",
    "                    device,\n",
    "                    ensemble_train_loader,\n",
    "                    stacking_meta_epochs=stacking_meta_epochs,\n",
    "                    stacking_meta_lr=stacking_meta_lr,\n",
    "                    label_suffix=f\"_{group_name}\",\n",
    "                )\n",
    "\n",
    "                if not ensembles:\n",
    "                    print(f\"No ensembles constructed for group {group_name}.\")\n",
    "                    continue\n",
    "\n",
    "                for name, ensemble_model in ensembles.items():\n",
    "                    MODEL_COUNT_COLUMNS.setdefault(name, f\"{name}_count\")\n",
    "\n",
    "                    train_strict_acc, train_loose_acc, _, _ = compute_accuracy(ensemble_model, ensemble_train_loader, device)\n",
    "                    val_strict_acc, val_loose_acc, y_true, y_pred = compute_accuracy(ensemble_model, ensemble_val_loader, device)\n",
    "\n",
    "                    log_accuracy_to_csv(name, train_strict_acc, train_loose_acc, val_strict_acc, val_loose_acc)\n",
    "\n",
    "                    results.append({\n",
    "                        \"Model Type\": name,\n",
    "                        \"Train Strict Accuracy (%)\": train_strict_acc,\n",
    "                        \"Train ±1 Grade Accuracy (%)\": train_loose_acc,\n",
    "                        \"Val Strict Accuracy (%)\": val_strict_acc,\n",
    "                        \"Val ±1 Grade Accuracy (%)\": val_loose_acc,\n",
    "                    })\n",
    "\n",
    "                    save_confusion_matrix_to_excel(y_true, y_pred, class_labels, name, excel_path)\n",
    "                    export_predictions_to_excel(\n",
    "                        ensemble_model,\n",
    "                        ensemble_val_loader,\n",
    "                        device,\n",
    "                        grade_to_label,\n",
    "                        excel_path,\n",
    "                        sheet_name=f\"{name}_preds\",\n",
    "                        model_name=name,\n",
    "                    )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results_rounded = df_results.round(2)\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        df_results_rounded.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    print(\"=== Model Comparison Summary ===\")\n",
    "    print(df_results_rounded)\n",
    "    return results\n",
    "\n",
    "\n",
    "def summarize_accuracy_stability(\n",
    "    all_results,\n",
    "    excel_path=\"result/model_comparison_results.xlsx\",\n",
    "    sheet_name=\"Stability\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Aggregate per-model accuracy metrics across multiple runs and compute\n",
    "    mean/std statistics to highlight training instability.\n",
    "    \"\"\"\n",
    "    if not all_results:\n",
    "        print(\"No accuracy records collected; skipping stability summary.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    metric_keys = [\n",
    "        (\"Train Strict Accuracy (%)\", \"Train Strict\"),\n",
    "        (\"Train ±1 Grade Accuracy (%)\", \"Train ±1 Grade\"),\n",
    "        (\"Val Strict Accuracy (%)\", \"Val Strict\"),\n",
    "        (\"Val ±1 Grade Accuracy (%)\", \"Val ±1 Grade\"),\n",
    "    ]\n",
    "    aggregated = defaultdict(lambda: {key: [] for key, _ in metric_keys})\n",
    "    for record in all_results:\n",
    "        model_name = record.get(\"Model Type\")\n",
    "        if not model_name:\n",
    "            continue\n",
    "        target = aggregated[model_name]\n",
    "        for key, _ in metric_keys:\n",
    "            value = record.get(key)\n",
    "            if value is not None:\n",
    "                target[key].append(float(value))\n",
    "\n",
    "    summary_rows = []\n",
    "    for model_name, metric_lists in aggregated.items():\n",
    "        row = {\"Model Type\": model_name}\n",
    "        for key, label in metric_keys:\n",
    "            values = metric_lists.get(key, [])\n",
    "            if not values:\n",
    "                continue\n",
    "            row[f\"{label} Mean (%)\"] = float(np.mean(values))\n",
    "            if len(values) > 1:\n",
    "                row[f\"{label} Std (%)\"] = float(np.std(values, ddof=1))\n",
    "            else:\n",
    "                row[f\"{label} Std (%)\"] = 0.0\n",
    "        summary_rows.append(row)\n",
    "\n",
    "    if not summary_rows:\n",
    "        print(\"Accuracy records contained no numeric values; skipping stability summary.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values(\"Model Type\")\n",
    "    summary_df_rounded = summary_df.round(3)\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        summary_df_rounded.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(\"=== Accuracy Stability Summary ===\")\n",
    "    print(summary_df_rounded)\n",
    "    return summary_df_rounded\n",
    "\n",
    "\n",
    "def run_multiple_iterations(num_iterations=25, **compare_kwargs):\n",
    "    \"\"\"\n",
    "    Execute `compare_models` multiple times and report aggregated accuracy statistics.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for i in range(num_iterations):\n",
    "        print(f\"------------------------------------iteration no {i+1}------------------------------------\")\n",
    "        iteration_results = compare_models(**compare_kwargs)\n",
    "        if iteration_results:\n",
    "            all_results.extend(iteration_results)\n",
    "    summarize_accuracy_stability(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage\n",
    "if __name__ == \"__main__\":\n",
    "    run_multiple_iterations(num_iterations=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal evaluation helpers\n",
    "def evaluate_ordinal_thresholds(model, loader, grade_to_label, device, decision_threshold=0.5, model_name=None, output_dir='./result'):\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "    targets_list = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "            payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            outputs = model(payload)\n",
    "            if not isinstance(outputs, tuple):\n",
    "                raise ValueError('Model is not configured for ordinal outputs.')\n",
    "            probs, logits = outputs\n",
    "            probs_list.append(probs.cpu())\n",
    "            targets_list.append(y.cpu())\n",
    "    if not probs_list:\n",
    "        raise ValueError('No samples available for ordinal evaluation.')\n",
    "    probs = torch.cat(probs_list, dim=0)\n",
    "    targets = torch.cat(targets_list, dim=0)\n",
    "    acc_per_threshold = threshold_accuracy(probs, targets, threshold=decision_threshold).cpu()\n",
    "    grade_by_label = {v: k for k, v in grade_to_label.items()}\n",
    "    threshold_labels = []\n",
    "    for idx in range(acc_per_threshold.size(0)):\n",
    "        grade = grade_by_label.get(idx, f'label_{idx}')\n",
    "        threshold_labels.append(f\"P(>{grade})\")\n",
    "    df = pd.DataFrame({\n",
    "        'threshold': threshold_labels,\n",
    "        'accuracy': (acc_per_threshold.numpy() * 100).round(2)\n",
    "    })\n",
    "    overall_pred = cumulative_to_labels(probs, threshold=decision_threshold)\n",
    "    overall_acc = (overall_pred == targets).float().mean().item() * 100\n",
    "    if model_name:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, f'ordinal_metrics_{model_name}.csv')\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f'Saved threshold table to {output_path}')\n",
    "    print(df)\n",
    "    print(f'Overall accuracy: {overall_acc:.2f}%')\n",
    "    return df, overall_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ordinal ensembles and multi-run utilities ---\n",
    "ORDINAL_BASE_MODEL_TYPES = [\n",
    "    'set_transformer_ordinal',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "]\n",
    "\n",
    "ORDINAL_ENSEMBLE_TYPES = [\n",
    "    'ordinal_soft_voting_ensemble',\n",
    "    'ordinal_geometric_mean_ensemble',\n",
    "    'ordinal_median_ensemble',\n",
    "    'ordinal_trimmed_mean_ensemble',\n",
    "    'ordinal_stacking_ensemble',\n",
    "    'ordinal_gbm_ensemble',\n",
    "    'ordinal_xgboost_ensemble',\n",
    "    'ordinal_lightgbm_ensemble',\n",
    "    'ordinal_adaboost_ensemble',\n",
    "]\n",
    "\n",
    "\n",
    "def train_ordinal_stacking_meta_model(stacking_model, dataloader, device, epochs=5, lr=1e-3):\n",
    "    if epochs <= 0:\n",
    "        return\n",
    "    stacking_model.meta_model.train()\n",
    "    for member in stacking_model.models.values():\n",
    "        member.eval()\n",
    "    optimizer = torch.optim.Adam(stacking_model.meta_model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        for X, y in dataloader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            targets = y.to(device)\n",
    "            _, logits = stacking_model(inputs)\n",
    "            loss = ordinal_logistic_loss(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_size = targets.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "        if total_samples > 0:\n",
    "            avg_loss = total_loss / total_samples\n",
    "            print(f\"Ordinal stacking meta epoch {epoch + 1}: loss={avg_loss:.4f}\")\n",
    "    stacking_model.meta_model.eval()\n",
    "\n",
    "\n",
    "def infer_ordinal_stacking_feature_dim(stacking_model, dataloader, device):\n",
    "    stacking_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in dataloader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            member_feats = stacking_model._member_features(inputs)\n",
    "            M, _, F = member_feats.shape\n",
    "            return F if stacking_model.combine == 'mean' else M * F\n",
    "    raise RuntimeError('Unable to infer ordinal stacking feature dimension.')\n",
    "\n",
    "\n",
    "def train_ordinal_tree_meta_model(tree_ensemble, dataloader, device):\n",
    "    tree_ensemble.eval()\n",
    "    for member in tree_ensemble.models.values():\n",
    "        member.eval()\n",
    "    feature_blocks = []\n",
    "    target_blocks = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            member_feats = tree_ensemble._member_features(inputs)\n",
    "            feat = tree_ensemble._build_feature_matrix(member_feats)\n",
    "            feature_blocks.append(feat.detach().cpu().numpy())\n",
    "            target_blocks.append(y.detach().cpu().numpy())\n",
    "    if not feature_blocks:\n",
    "        raise RuntimeError('No data available to fit ordinal tree-based meta learner.')\n",
    "    features = np.concatenate(feature_blocks, axis=0)\n",
    "    targets = np.concatenate(target_blocks, axis=0)\n",
    "    tree_ensemble.fit_meta_model(features, targets)\n",
    "\n",
    "\n",
    "def build_ordinal_ensemble_models(\n",
    "    ensemble_names,\n",
    "    base_model_items,\n",
    "    num_classes,\n",
    "    device,\n",
    "    train_loader,\n",
    "    stacking_meta_epochs=5,\n",
    "    stacking_meta_lr=1e-3,\n",
    "    ensemble_weights=None,\n",
    "    label_suffix='',\n",
    "):\n",
    "    ensembles = {}\n",
    "    base_items = list(base_model_items)\n",
    "    if not base_items:\n",
    "        return ensembles\n",
    "\n",
    "    def _resolve_group_weights(items):\n",
    "        if ensemble_weights is None:\n",
    "            return None\n",
    "        if isinstance(ensemble_weights, dict):\n",
    "            filtered = {name: ensemble_weights[name] for name, _ in items if name in ensemble_weights}\n",
    "            if len(filtered) != len(items):\n",
    "                missing = [name for name, _ in items if name not in filtered]\n",
    "                if missing:\n",
    "                    print(f\"Warning: missing weights for {missing}; using uniform weights.\")\n",
    "                return None\n",
    "            return filtered\n",
    "        weight_list = list(ensemble_weights)\n",
    "        if len(weight_list) != len(items):\n",
    "            print('Warning: weight list length mismatch; using uniform weights.')\n",
    "            return None\n",
    "        return weight_list\n",
    "\n",
    "    resolved_weights = _resolve_group_weights(base_items)\n",
    "\n",
    "    for base_name in ensemble_names:\n",
    "        cloned_items = [(name, copy.deepcopy(model)) for name, model in base_items]\n",
    "        if not cloned_items:\n",
    "            continue\n",
    "\n",
    "        weights = resolved_weights\n",
    "        if isinstance(weights, list):\n",
    "            weights = list(weights)\n",
    "\n",
    "        ensemble_key = f\"{base_name}{label_suffix}\" if label_suffix else base_name\n",
    "\n",
    "        if base_name == 'ordinal_soft_voting_ensemble':\n",
    "            ensemble_model = OrdinalSoftVotingEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == 'ordinal_geometric_mean_ensemble':\n",
    "            ensemble_model = OrdinalGeometricMeanEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == 'ordinal_median_ensemble':\n",
    "            ensemble_model = OrdinalMedianEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == 'ordinal_trimmed_mean_ensemble':\n",
    "            ensemble_model = OrdinalTrimmedMeanEnsemble(cloned_items, weights=weights, freeze_members=True, trim_frac=0.2).to(device)\n",
    "        elif base_name == 'ordinal_stacking_ensemble':\n",
    "            placeholder_dim = max(1, len(cloned_items) * (num_classes - 1))\n",
    "            meta_model = nn.Linear(placeholder_dim, num_classes - 1).to(device)\n",
    "            ensemble_model = OrdinalStackingEnsemble(\n",
    "                cloned_items,\n",
    "                num_classes=num_classes,\n",
    "                weights=weights,\n",
    "                freeze_members=True,\n",
    "                meta_model=meta_model,\n",
    "                feature_source='logits',\n",
    "                combine='concat',\n",
    "            ).to(device)\n",
    "            inferred_dim = infer_ordinal_stacking_feature_dim(ensemble_model, train_loader, device)\n",
    "            if inferred_dim != placeholder_dim:\n",
    "                ensemble_model.meta_model = nn.Linear(inferred_dim, num_classes - 1).to(device)\n",
    "            train_ordinal_stacking_meta_model(\n",
    "                ensemble_model,\n",
    "                train_loader,\n",
    "                device,\n",
    "                epochs=stacking_meta_epochs,\n",
    "                lr=stacking_meta_lr,\n",
    "            )\n",
    "        elif base_name == 'ordinal_gbm_ensemble':\n",
    "            try:\n",
    "                ensemble_model = OrdinalGBMEnsemble(\n",
    "                    cloned_items,\n",
    "                    num_classes=num_classes,\n",
    "                    weights=weights,\n",
    "                    freeze_members=True,\n",
    "                    feature_source='logits',\n",
    "                    combine='concat',\n",
    "                ).to(device)\n",
    "            except ImportError as exc:\n",
    "                print(f\"Skipping {base_name}: {exc}\")\n",
    "                continue\n",
    "            train_ordinal_tree_meta_model(ensemble_model, train_loader, device)\n",
    "        elif base_name == 'ordinal_xgboost_ensemble':\n",
    "            try:\n",
    "                ensemble_model = OrdinalXGBoostEnsemble(\n",
    "                    cloned_items,\n",
    "                    num_classes=num_classes,\n",
    "                    weights=weights,\n",
    "                    freeze_members=True,\n",
    "                    feature_source='logits',\n",
    "                    combine='concat',\n",
    "                ).to(device)\n",
    "            except ImportError as exc:\n",
    "                print(f\"Skipping {base_name}: {exc}\")\n",
    "                continue\n",
    "            train_ordinal_tree_meta_model(ensemble_model, train_loader, device)\n",
    "        elif base_name == 'ordinal_lightgbm_ensemble':\n",
    "            try:\n",
    "                ensemble_model = OrdinalLightGBMEnsemble(\n",
    "                    cloned_items,\n",
    "                    num_classes=num_classes,\n",
    "                    weights=weights,\n",
    "                    freeze_members=True,\n",
    "                    feature_source='logits',\n",
    "                    combine='concat',\n",
    "                ).to(device)\n",
    "            except ImportError as exc:\n",
    "                print(f\"Skipping {base_name}: {exc}\")\n",
    "                continue\n",
    "            train_ordinal_tree_meta_model(ensemble_model, train_loader, device)\n",
    "        elif base_name == 'ordinal_adaboost_ensemble':\n",
    "            ensemble_model = OrdinalAdaBoostEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        else:\n",
    "            print(f\"Unknown ordinal ensemble type '{base_name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        ensemble_model.eval()\n",
    "        ensembles[ensemble_key] = ensemble_model\n",
    "\n",
    "    return ensembles\n",
    "\n",
    "\n",
    "def run_ordinal_iterations(\n",
    "    ordinal_model_types=None,\n",
    "    ordinal_ensemble_types=None,\n",
    "    num_iterations=25,\n",
    "    decision_threshold=0.5,\n",
    "    stacking_meta_epochs=5,\n",
    "    stacking_meta_lr=1e-3,\n",
    "    ensemble_weights=None,\n",
    "    output_excel='./result/ordinal_result.xlsx',\n",
    "):\n",
    "    ordinal_model_types = ordinal_model_types or ORDINAL_BASE_MODEL_TYPES\n",
    "    ordinal_ensemble_types = ordinal_ensemble_types or ORDINAL_ENSEMBLE_TYPES\n",
    "    threshold_records = []\n",
    "    summary_records = []\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"----------------- Ordinal iteration {iteration + 1}/{num_iterations} -----------------\")\n",
    "        trained_base_models = {}\n",
    "        base_dataset = None\n",
    "        base_train_idx = None\n",
    "        base_val_idx = None\n",
    "        num_classes = None\n",
    "\n",
    "        def record_metrics(model_name, table, overall_acc):\n",
    "            for _, row in table.iterrows():\n",
    "                threshold_records.append({\n",
    "                    'iteration': iteration + 1,\n",
    "                    'model': model_name,\n",
    "                    'threshold': row['threshold'],\n",
    "                    'accuracy': float(row['accuracy']),\n",
    "                })\n",
    "            summary_records.append({\n",
    "                'iteration': iteration + 1,\n",
    "                'model': model_name,\n",
    "                'overall_accuracy': float(overall_acc),\n",
    "            })\n",
    "\n",
    "        for model_key in ordinal_model_types:\n",
    "            print(f\"=== Training ordinal model: {model_key} ===\")\n",
    "            train_loader, val_loader, model, dataset, train_idx, val_idx = main(model_key)\n",
    "            trained_base_models[model_key] = model\n",
    "            if base_dataset is None:\n",
    "                base_dataset = dataset\n",
    "                base_train_idx = train_idx\n",
    "                base_val_idx = val_idx\n",
    "            if num_classes is None:\n",
    "                num_classes = getattr(model, 'num_classes', len(grade_to_label))\n",
    "\n",
    "            table, overall_acc = evaluate_ordinal_thresholds(\n",
    "                model,\n",
    "                val_loader,\n",
    "                grade_to_label=grade_to_label,\n",
    "                device=device,\n",
    "                decision_threshold=decision_threshold,\n",
    "                model_name=None,\n",
    "            )\n",
    "            record_metrics(model_key, table, overall_acc)\n",
    "\n",
    "        if ordinal_ensemble_types and trained_base_models:\n",
    "            if base_dataset is None or base_train_idx is None or base_val_idx is None:\n",
    "                raise RuntimeError('Dataset indices unavailable for ordinal ensembles.')\n",
    "            collate_fn = make_collate_fn('set_transformer_xy')\n",
    "            train_subset = Subset(base_dataset, base_train_idx)\n",
    "            val_subset = Subset(base_dataset, base_val_idx)\n",
    "            ensemble_train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "            ensemble_val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "            base_items = list(trained_base_models.items())\n",
    "            ensembles = build_ordinal_ensemble_models(\n",
    "                ordinal_ensemble_types,\n",
    "                base_items,\n",
    "                num_classes=num_classes or len(grade_to_label),\n",
    "                device=device,\n",
    "                train_loader=ensemble_train_loader,\n",
    "                stacking_meta_epochs=stacking_meta_epochs,\n",
    "                stacking_meta_lr=stacking_meta_lr,\n",
    "                ensemble_weights=ensemble_weights,\n",
    "            )\n",
    "\n",
    "            for name, ensemble_model in ensembles.items():\n",
    "                table, overall_acc = evaluate_ordinal_thresholds(\n",
    "                    ensemble_model,\n",
    "                    ensemble_val_loader,\n",
    "                    grade_to_label=grade_to_label,\n",
    "                    device=device,\n",
    "                    decision_threshold=decision_threshold,\n",
    "                    model_name=None,\n",
    "                )\n",
    "                record_metrics(name, table, overall_acc)\n",
    "\n",
    "    if not summary_records:\n",
    "        print('No ordinal evaluations were executed.')\n",
    "        return None\n",
    "\n",
    "    threshold_df = pd.DataFrame(threshold_records)\n",
    "    summary_df = pd.DataFrame(summary_records)\n",
    "\n",
    "    threshold_agg = (threshold_df.groupby(['model', 'threshold'])['accuracy']\n",
    "                     .agg(['mean', 'std'])\n",
    "                     .reset_index()\n",
    "                     .rename(columns={'mean': 'accuracy_mean', 'std': 'accuracy_std'}))\n",
    "    summary_agg = (summary_df.groupby('model')['overall_accuracy']\n",
    "                   .agg(['mean', 'std'])\n",
    "                   .reset_index()\n",
    "                   .rename(columns={'mean': 'overall_accuracy_mean', 'std': 'overall_accuracy_std'}))\n",
    "\n",
    "    output_dir = os.path.dirname(output_excel)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:\n",
    "        threshold_df.to_excel(writer, sheet_name='threshold_iterations', index=False)\n",
    "        summary_df.to_excel(writer, sheet_name='overall_iterations', index=False)\n",
    "        threshold_agg.to_excel(writer, sheet_name='threshold_avg', index=False)\n",
    "        summary_agg.to_excel(writer, sheet_name='overall_avg', index=False)\n",
    "    print(f\"Saved aggregated ordinal results to {output_excel}\")\n",
    "    print(summary_agg.sort_values('model'))\n",
    "    return {\n",
    "        'threshold_iterations': threshold_df,\n",
    "        'overall_iterations': summary_df,\n",
    "        'threshold_summary': threshold_agg,\n",
    "        'overall_summary': summary_agg,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ordinal_iterations(num_iterations=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sousei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
