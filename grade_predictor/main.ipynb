{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import torch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "import csv\n",
    "from modules_modified import ISAB, SAB, PMA\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models\n",
    "from model import (\n",
    "    SetTransformerClassifierXY,\n",
    "    SetTransformerClassifierXYAdditive,\n",
    "    SetTransformerClassifier,\n",
    "    DeepSetClassifierXYAdditive,\n",
    "    DeepSetClassifierXY,\n",
    "    DeepSetClassifier,\n",
    "    SetTransformerOrdinalXY,\n",
    "    SetTransformerOrdinalXYAdditive,\n",
    "    SetTransformerOrdinal,\n",
    "    DeepSetOrdinalXYAdditive,\n",
    "    DeepSetOrdinalXY,\n",
    "    DeepSetOrdinal,\n",
    "    SoftVotingEnsemble,\n",
    "    GeometricMeanEnsemble,\n",
    "    MedianEnsemble,\n",
    "    TrimmedMeanEnsemble,\n",
    "    StackingEnsemble,\n",
    "    AdaBoostEnsemble\n",
    ")\n",
    "from utils_ordinal import ordinal_logistic_loss, cumulative_to_labels, threshold_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappings --------------------------------------------------------\n",
    "# Map each hold like \"A1\"…\"K18\" to an integer 0…(11*18−1)=197\n",
    "cols = [chr(c) for c in range(ord('A'), ord('K')+1)]\n",
    "rows = list(range(1, 19))\n",
    "hold_to_idx = {f\"{c}{r}\": i for i, (c, r) in enumerate((c, r) for r in rows for c in cols)}\n",
    "\n",
    "\n",
    "# Map grades \"V4\"…\"V11\" \n",
    "grade_to_label = {f\"V{i}\": i - 4 for i in range(4, 12)}  \n",
    "label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "print(hold_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holds difficulty data --------------------------------------------------------\n",
    "hold_difficulty = {}\n",
    "with open(\"data/hold_difficulty.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if \":\" not in line:\n",
    "            continue  # skip malformed line\n",
    "        hold, rest = line.strip().split(\":\", 1)\n",
    "        parts = rest.strip().split(\",\")\n",
    "        difficulty = int(parts[0].strip())\n",
    "        types = [t.strip() for t in parts[1:]]\n",
    "        hold_difficulty[hold.strip()] = (difficulty, types)\n",
    "    print(\"successfully parsed hold difficulty file\")\n",
    "\n",
    "# prepare type vocabulary\n",
    "unique_types = set()\n",
    "for _, (_, types) in hold_difficulty.items():\n",
    "    unique_types.update(types)\n",
    "\n",
    "type_to_idx = {t: i for i, t in enumerate(sorted(unique_types))}\n",
    "print(f\"successfully prepare type vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign x,y position to each holds -------------------------------\n",
    "import string\n",
    "\n",
    "# Board columns A–K → indices 0–10\n",
    "cols = list(string.ascii_uppercase[:11])  # A–K\n",
    "# Rows 1–18 → indices 0–17\n",
    "rows = list(range(1, 19))  # 1–18\n",
    "\n",
    "# Generate hold_to_coord dictionary\n",
    "hold_to_coord = {}\n",
    "\n",
    "for x, col in enumerate(cols):\n",
    "    for y, row in enumerate(rows):\n",
    "        hold_name = f\"{col}{row}\"\n",
    "        hold_to_coord[hold_name] = (x, y)\n",
    "\n",
    "print(\"successfully created (x,y) position to each hold:\")\n",
    "print(hold_to_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonBoardDataset(Dataset):\n",
    "    def __init__(self, json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord, max_difficulty=10):\n",
    "        self.hold_to_idx = hold_to_idx\n",
    "        self.grade_to_label = grade_to_label\n",
    "        self.hold_difficulty = hold_difficulty\n",
    "        self.type_to_idx = type_to_idx\n",
    "        self.hold_to_coord = hold_to_coord\n",
    "        self.max_difficulty = max_difficulty\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.raw = [json.loads(line) for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.raw[idx]\n",
    "        holds = item['holds']\n",
    "\n",
    "        hold_idxs = []\n",
    "        diff_values = []\n",
    "        type_vecs = []\n",
    "        xy_coords = []\n",
    "\n",
    "        for h in holds:\n",
    "            hold_idxs.append(self.hold_to_idx[h])\n",
    "\n",
    "            difficulty, types = self.hold_difficulty[h]\n",
    "            diff_values.append(difficulty / self.max_difficulty)\n",
    "\n",
    "            # multi-hot vector\n",
    "            type_vec = torch.zeros(len(self.type_to_idx), dtype=torch.float)\n",
    "            for t in types:\n",
    "                if t in self.type_to_idx:\n",
    "                    type_vec[self.type_to_idx[t]] = 1.0\n",
    "            type_vecs.append(type_vec)\n",
    "\n",
    "            # normalized (x, y)\n",
    "            x, y = self.hold_to_coord[h]\n",
    "            xy_coords.append(torch.tensor([x / 10.0, y / 17.0], dtype=torch.float))\n",
    "\n",
    "        return {\n",
    "            \"indices\": torch.tensor(hold_idxs, dtype=torch.long),\n",
    "            \"difficulty\": torch.tensor(diff_values, dtype=torch.float),\n",
    "            \"type\": torch.stack(type_vecs),       # (N, T)\n",
    "            \"xy\": torch.stack(xy_coords)          # (N, 2)\n",
    "        }, torch.tensor(self.grade_to_label[item['grade']], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66965b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# --- Set Hyperparameters ---\n",
    "json_path = './data/cleaned_moonboard2024_grouped.json'\n",
    "embed_dim = 64\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "XY_MODELS = {\n",
    "    'set_transformer_xy',\n",
    "    'set_transformer_additive',\n",
    "    'deepset_xy',\n",
    "    'deepset_xy_additive',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "}\n",
    "\n",
    "ORDINAL_MODELS = {\n",
    "    'set_transformer_ordinal',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "}\n",
    "\n",
    "# --- Collate Function Factory ---\n",
    "def make_collate_fn(model_type):\n",
    "    def collate_fn(batch):\n",
    "        X_indices = [x['indices'] for x, _ in batch]\n",
    "        X_difficulty = [x['difficulty'] for x, _ in batch]\n",
    "        X_type = [x['type'] for x, _ in batch]\n",
    "        y_batch = [y for _, y in batch]\n",
    "\n",
    "        X_indices = pad_sequence(X_indices, batch_first=True)\n",
    "        X_difficulty = pad_sequence(X_difficulty, batch_first=True)\n",
    "        X_type = pad_sequence(X_type, batch_first=True)\n",
    "        y_tensor = torch.stack(y_batch)\n",
    "\n",
    "        if model_type in XY_MODELS:\n",
    "            X_xy = [x['xy'] for x, _ in batch]\n",
    "            X_xy = pad_sequence(X_xy, batch_first=True)\n",
    "            return (X_indices, X_difficulty, X_type, X_xy), y_tensor\n",
    "        else:\n",
    "            return (X_indices,), y_tensor\n",
    "    return collate_fn\n",
    "\n",
    "# --- Dataset Loader ---\n",
    "def load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord):\n",
    "    return MoonBoardDataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "\n",
    "# --- DataLoader Preparation ---\n",
    "def prepare_dataloaders(dataset, grade_to_label, batch_size, collate_fn):\n",
    "    targets = [grade_to_label[item['grade']] for item in dataset.raw]\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(targets), y=targets)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(dataset))), test_size=0.2, stratify=targets, random_state=42\n",
    "    )\n",
    "\n",
    "    train_data = Subset(dataset, train_idx)\n",
    "    val_data = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, class_weights, train_idx, val_idx\n",
    "\n",
    "# --- Training ---\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, is_ordinal=False):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "            payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            outputs = model(payload)\n",
    "            if is_ordinal:\n",
    "                probs, logits = outputs\n",
    "                loss = criterion(logits, y)\n",
    "            else:\n",
    "                logits = outputs\n",
    "                loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch:02d} — loss: {total_loss / len(train_loader):.4f}\")\n",
    "    return model\n",
    "\n",
    "# --- Main Per Model ---\n",
    "def main(model_type):\n",
    "    dataset = load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "    targets = [grade_to_label[item['grade']] for item in dataset.raw]\n",
    "    num_classes = len(np.unique(targets))\n",
    "    vocab_size = len(hold_to_idx)\n",
    "    type_vec_dim = len(type_to_idx)\n",
    "    is_ordinal = model_type in ORDINAL_MODELS\n",
    "\n",
    "    if model_type == 'set_transformer':\n",
    "        ModelClass = SetTransformerClassifier\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'set_transformer_xy':\n",
    "        ModelClass = SetTransformerClassifierXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_additive':\n",
    "        ModelClass = SetTransformerClassifierXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset':\n",
    "        ModelClass = DeepSetClassifier\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'deepset_xy':\n",
    "        ModelClass = DeepSetClassifierXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_xy_additive':\n",
    "        ModelClass = DeepSetClassifierXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_ordinal':\n",
    "        ModelClass = SetTransformerOrdinal\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'set_transformer_ordinal_xy':\n",
    "        ModelClass = SetTransformerOrdinalXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'set_transformer_ordinal_xy_additive':\n",
    "        ModelClass = SetTransformerOrdinalXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_ordinal':\n",
    "        ModelClass = DeepSetOrdinal\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == 'deepset_ordinal_xy':\n",
    "        ModelClass = DeepSetOrdinalXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == 'deepset_ordinal_xy_additive':\n",
    "        ModelClass = DeepSetOrdinalXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "    collate_fn = make_collate_fn(model_type)\n",
    "    train_loader, val_loader, class_weights, train_idx, val_idx = prepare_dataloaders(dataset, grade_to_label, batch_size, collate_fn)\n",
    "\n",
    "    model = ModelClass(**kwargs).to(device)\n",
    "    model.is_ordinal = is_ordinal\n",
    "    model.num_classes = num_classes\n",
    "\n",
    "    if is_ordinal:\n",
    "        def criterion_fn(logits, targets):\n",
    "            return ordinal_logistic_loss(logits, targets)\n",
    "    else:\n",
    "        criterion_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model = train_model(model, train_loader, val_loader, criterion_fn, optimizer, epochs, is_ordinal=is_ordinal)\n",
    "    return train_loader, val_loader, model, dataset, train_idx, val_idx\n",
    "\n",
    "\n",
    "def train_boosting_main(model_type, num_stages=5, weak_epochs=3):\n",
    "    \"\"\"\n",
    "    Replaces the `main(mtype)` call for boosting models.\n",
    "    Trains a sequential AdaBoost-style ensemble.\n",
    "    \n",
    "    Returns the same tuple as `main()`:\n",
    "    (train_loader, val_loader, final_model, dataset, train_idx, val_idx)\n",
    "    \"\"\"\n",
    "    print(f\"===== Training Boosting Ensemble ({model_type}, {num_stages} stages) =====\")\n",
    "    \n",
    "    # --- 1. Standard Dataset Setup (copied from `main`) ---\n",
    "    dataset = load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "    targets = [grade_to_label[item['grade']] for item in dataset.raw]\n",
    "    num_classes = len(np.unique(targets))\n",
    "    vocab_size = len(hold_to_idx)\n",
    "    type_vec_dim = len(type_to_idx)\n",
    "    is_ordinal = False # Boosting classifiers, not ordinal models\n",
    "\n",
    "    collate_fn = make_collate_fn(model_type) # Use collate fn for the base model\n",
    "    \n",
    "    # Get train/val split (we need the indices)\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(dataset))), test_size=0.2, stratify=targets, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_data_subset = Subset(dataset, train_idx)\n",
    "    val_data_subset = Subset(dataset, val_idx)\n",
    "    \n",
    "    # This is the standard val_loader, used for final eval\n",
    "    val_loader = DataLoader(val_data_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # This loader is for *evaluating* the weak learner on train data (no shuffle)\n",
    "    train_eval_loader = DataLoader(train_data_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # --- 2. Boosting-Specific Setup ---\n",
    "    num_train_samples = len(train_idx)\n",
    "    # Initialize uniform sample weights\n",
    "    sample_weights = torch.full((num_train_samples,), 1.0 / num_train_samples, device=device)\n",
    "    \n",
    "    trained_models_list = [] # (name, model)\n",
    "    model_alphas = []        # [alpha]\n",
    "    \n",
    "    # --- 3. The Sequential Training Loop ---\n",
    "    for m in range(num_stages):\n",
    "        print(f\"--- Boosting Stage {m+1}/{num_stages} ---\")\n",
    "        \n",
    "        # a. Create a new dataloader for this stage\n",
    "        #    It samples from train_data_subset based on the *current* sample_weights\n",
    "        sampler = WeightedRandomSampler(sample_weights.cpu(), num_train_samples, replacement=True)\n",
    "        train_loader_stage = DataLoader(train_data_subset, batch_size=batch_size, sampler=sampler, collate_fn=collate_fn)\n",
    "\n",
    "        # b. Create and train the weak learner\n",
    "        #    We re-use the logic from your `main()` function to get the model\n",
    "        if model_type == 'set_transformer':\n",
    "            ModelClass = SetTransformerClassifier\n",
    "            kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "        elif model_type == 'set_transformer_xy':\n",
    "            ModelClass = SetTransformerClassifierXY\n",
    "            kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "        elif model_type == 'set_transformer_additive':\n",
    "            ModelClass = SetTransformerClassifierXYAdditive\n",
    "            kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "        elif model_type == 'deepset':\n",
    "            ModelClass = DeepSetClassifier\n",
    "            kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "        elif model_type == 'deepset_xy':\n",
    "            ModelClass = DeepSetClassifierXY\n",
    "            kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "        elif model_type == 'deepset_xy_additive':\n",
    "            ModelClass = DeepSetClassifierXYAdditive\n",
    "            kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "        else:\n",
    "            # This is the correct error message\n",
    "            raise ValueError(f\"Unsupported weak learner type for boosting: {model_type}\")\n",
    "\n",
    "        model_m = ModelClass(**kwargs).to(device)\n",
    "        \n",
    "        # We can use standard CrossEntropyLoss because the *sampler* already weighted the data\n",
    "        criterion_fn = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model_m.parameters(), lr=lr)\n",
    "        \n",
    "        # c. Train this weak learner (RE-USING YOUR EXISTING `train_model` FUNCTION!)\n",
    "        print(f\"Training weak learner {m+1} for {weak_epochs} epochs...\")\n",
    "        model_m = train_model(model_m, train_loader_stage, val_loader, criterion_fn, optimizer, weak_epochs, is_ordinal=is_ordinal)\n",
    "        \n",
    "        # d. Evaluate on *all* training data (unshuffled)\n",
    "        model_m.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for X, y in train_eval_loader: # Use NON-shuffled loader\n",
    "                inputs = tuple(x.to(device) for x in X)\n",
    "                payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "                logits = model_m(payload)\n",
    "                all_preds.append(logits.argmax(dim=1))\n",
    "                all_targets.append(y.to(device))\n",
    "        \n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_targets = torch.cat(all_targets)\n",
    "        \n",
    "        # e. Compute weighted error\n",
    "        is_incorrect = (all_preds != all_targets).float() # [num_train_samples]\n",
    "        err_m = (is_incorrect * sample_weights).sum() # Sum of weights of incorrect samples\n",
    "        \n",
    "        if err_m <= 0 or err_m >= (1.0 - 1.0 / num_classes):\n",
    "            print(f\"Stage {m+1} model is perfect or too weak (err={err_m:.4f}). Stopping.\")\n",
    "            if err_m <= 0: # Add perfect model and break\n",
    "                model_alphas.append(1.0) # Use a reasonable weight\n",
    "                trained_models_list.append((f\"boost_model_{m}\", model_m))\n",
    "            break\n",
    "        \n",
    "        # f. Compute model weight (alpha)\n",
    "        alpha_m = torch.log((1.0 - err_m) / err_m) + torch.log(torch.tensor(num_classes - 1.0, device=device))\n",
    "        \n",
    "        # g. Update sample weights\n",
    "        sample_weights *= torch.exp(alpha_m * is_incorrect)\n",
    "        sample_weights /= sample_weights.sum() # Normalize\n",
    "        \n",
    "        # h. Save\n",
    "        trained_models_list.append((f\"boost_model_{m}\", model_m))\n",
    "        model_alphas.append(alpha_m.item())\n",
    "        print(f\"Stage {m+1}: Error={err_m:.4f}, Alpha={alpha_m:.4f}\")\n",
    "    \n",
    "    # --- 4. Build the Final Ensemble Model ---\n",
    "    if not trained_models_list:\n",
    "        raise RuntimeError(\"Boosting training failed, no models were trained.\")\n",
    "        \n",
    "    print(f\"Building final ensemble with {len(trained_models_list)} models.\")\n",
    "    \n",
    "    # *** USE THE NEW NAME HERE ***\n",
    "    final_ensemble = AdaBoostEnsemble(trained_models_list, weights=model_alphas, freeze_members=True).to(device)\n",
    "    \n",
    "    final_ensemble.is_ordinal = is_ordinal\n",
    "    final_ensemble.num_classes = num_classes\n",
    "    \n",
    "    # Return the same \"package\" as main()\n",
    "    # `train_eval_loader` is the unshuffled train loader, which `compare_models`\n",
    "    # can use for ensemble training (like stacking) if needed.\n",
    "    return train_eval_loader, val_loader, final_ensemble, dataset, train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5db06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image as XLImage\n",
    "\n",
    "BASE_MODEL_TYPES = [\n",
    "    \"set_transformer\",\n",
    "    \"deepset\",\n",
    "    \"set_transformer_xy\",\n",
    "    \"deepset_xy\",\n",
    "    \"set_transformer_additive\",\n",
    "    \"deepset_xy_additive\"\n",
    "]\n",
    "\n",
    "BOOSTING_TYPES = {\n",
    "    \"adaboost_deepset\": \"deepset\",\n",
    "    \"adaboost_deepset_xy_additive\": \"deepset_xy_additive\",\n",
    "    \"adaboost_deepset_xy\": \"deepset_xy\",\n",
    "    \"adaboost_set_transformer\": \"set_transformer\",\n",
    "    \"adaboost_set_transformer_additive\": \"set_transformer_additive\",\n",
    "    \"adaboost_set_transformer_xy\": \"set_transformer_xy\",\n",
    "}\n",
    "\n",
    "MODEL_TYPES = BASE_MODEL_TYPES + list(BOOSTING_TYPES.keys())\n",
    "ENSEMBLE_TYPES = [\n",
    "    \"soft_voting_ensemble\",\n",
    "    \"geometric_mean_ensemble\",\n",
    "    \"median_ensemble\",\n",
    "    \"trimmed_mean_ensemble\",\n",
    "    \"stacking_ensemble\"\n",
    "]\n",
    "\n",
    "MODEL_COUNT_COLUMNS = {name: f\"{name}_count\" for name in MODEL_TYPES + ENSEMBLE_TYPES}\n",
    "\n",
    "\n",
    "# --- plot confusion matrix and save to excel---\n",
    "def save_confusion_matrix_to_excel(y_true, y_pred, class_labels, model_name, excel_path):\n",
    "    # Plot confusion matrix and save as image\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(class_labels)), normalize='true')\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "    plt.xlabel(\"Predicted Grade\")\n",
    "    plt.ylabel(\"Actual Grade\")\n",
    "    plt.tight_layout()\n",
    "    img_path = f\"result/confusion_{model_name}.png\"\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Insert image into Excel (new sheet per model)\n",
    "    wb = load_workbook(excel_path)\n",
    "    if model_name in wb.sheetnames:\n",
    "        ws = wb[model_name]\n",
    "    else:\n",
    "        ws = wb.create_sheet(title=model_name)\n",
    "    img = XLImage(img_path)\n",
    "    ws.add_image(img, \"A1\")\n",
    "    wb.save(excel_path)\n",
    "    print(f\"Confusion matrix for {model_name} saved and inserted into {excel_path} (sheet: {model_name})\")\n",
    "\n",
    "\n",
    "# --- export the predictions to excel ---\n",
    "def _update_outlier_excel(df_all_preds, model_name, outlier_filename=\"result/outlier.xlsx\", sheet_name=\"outliers\", threshold=3):\n",
    "    \"\"\"\n",
    "    From a DataFrame with columns [problem_name, y_true, y_pred, diff],\n",
    "    keep rows where abs(diff) > threshold and aggregate per problem_name:\n",
    "        - count = number of times flagged\n",
    "        - per-model counts = number of times flagged per model across runs\n",
    "        - y_true = mode (most frequent true label)\n",
    "        - y_pred_avg = average predicted label across occurrences\n",
    "    Save to outlier.xlsx.\n",
    "    \"\"\"\n",
    "    current_model_col = MODEL_COUNT_COLUMNS.get(model_name, f\"{model_name}_count\")\n",
    "    model_count_columns = dict(MODEL_COUNT_COLUMNS)\n",
    "    if model_name not in MODEL_COUNT_COLUMNS:\n",
    "        model_count_columns[model_name] = current_model_col\n",
    "\n",
    "    # Filter outliers\n",
    "    outliers = df_all_preds.loc[df_all_preds[\"diff\"].abs() > threshold,\n",
    "                                [\"problem_name\", \"y_true\", \"y_pred\"]]\n",
    "    if outliers.empty:\n",
    "        print(f\"No outliers (abs(diff) > {threshold}). Skipped creating outlier.xlsx.\")\n",
    "        return\n",
    "\n",
    "    # Group & aggregate\n",
    "    grouped = (outliers\n",
    "               .groupby(\"problem_name\")\n",
    "               .agg(\n",
    "                   count=(\"problem_name\", \"size\"),\n",
    "                   y_true=(\"y_true\", lambda x: x.mode().iat[0] if not x.mode().empty else x.iloc[0]),\n",
    "                   y_pred_avg=(\"y_pred\", lambda x: round(pd.to_numeric(x, errors=\"coerce\").mean(), 2))\n",
    "               )\n",
    "               .reset_index())\n",
    "\n",
    "    for col in model_count_columns.values():\n",
    "        if col not in grouped.columns:\n",
    "            grouped[col] = 0\n",
    "    grouped[current_model_col] = grouped[\"count\"]\n",
    "\n",
    "    # If a previous file exists, merge and accumulate counts\n",
    "    if os.path.exists(outlier_filename):\n",
    "        try:\n",
    "            existing = pd.read_excel(outlier_filename, sheet_name=sheet_name)\n",
    "            for col in model_count_columns.values():\n",
    "                if col not in existing.columns:\n",
    "                    existing[col] = 0\n",
    "            if set(existing.columns) >= {\"problem_name\", \"count\", \"y_true\", \"y_pred_avg\"}:\n",
    "                merged = pd.concat([existing, grouped], ignore_index=True, sort=False)\n",
    "                for col in model_count_columns.values():\n",
    "                    if col not in merged.columns:\n",
    "                        merged[col] = 0\n",
    "                agg_map = {\n",
    "                    \"count\": (\"count\", \"sum\"),\n",
    "                    \"y_true\": (\"y_true\", lambda x: x.mode().iat[0] if not x.mode().empty else x.iloc[0]),\n",
    "                    \"y_pred_avg\": (\"y_pred_avg\", \"mean\")\n",
    "                }\n",
    "                agg_map.update({col: (col, \"sum\") for col in model_count_columns.values()})\n",
    "                grouped = (merged\n",
    "                           .groupby(\"problem_name\")\n",
    "                           .agg(**agg_map)\n",
    "                           .reset_index())\n",
    "            # else keep grouped as new\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    grouped[\"y_pred_avg\"] = pd.to_numeric(grouped[\"y_pred_avg\"], errors=\"coerce\").round(2)\n",
    "    grouped[\"count\"] = grouped[\"count\"].fillna(0).astype(int)\n",
    "    for col in model_count_columns.values():\n",
    "        if col in grouped.columns:\n",
    "            grouped[col] = grouped[col].fillna(0).astype(int)\n",
    "\n",
    "    ordered_cols = [\"problem_name\", \"count\"]\n",
    "    ordered_cols.extend([model_count_columns[name] for name in MODEL_TYPES + ENSEMBLE_TYPES if model_count_columns[name] in grouped.columns])\n",
    "    if current_model_col in grouped.columns and current_model_col not in ordered_cols:\n",
    "        ordered_cols.append(current_model_col)\n",
    "    ordered_cols.extend([\"y_true\", \"y_pred_avg\"])\n",
    "    ordered_cols.extend([col for col in grouped.columns if col not in ordered_cols])\n",
    "    grouped = grouped[ordered_cols]\n",
    "\n",
    "    # Save\n",
    "    with pd.ExcelWriter(outlier_filename, engine=\"openpyxl\", mode=\"w\") as writer:\n",
    "        grouped.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Outliers saved to: {os.path.abspath(outlier_filename)}\")\n",
    "\n",
    "\n",
    "def export_predictions_to_excel(model, dataloader, device, grade_to_label, excel_path, sheet_name, model_name=None):\n",
    "    results = []\n",
    "    raw_dataset = dataloader.dataset.dataset  # MoonBoardDataset\n",
    "    indices = dataloader.dataset.indices      # Subset indices\n",
    "    label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "    current_index = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if isinstance(X, tuple):\n",
    "                inputs = tuple(x.to(device) for x in X)\n",
    "                payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            else:\n",
    "                payload = X.to(device)\n",
    "            outputs = model(payload)\n",
    "            if isinstance(outputs, tuple):\n",
    "                if getattr(model, \"is_ordinal\", False):\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = cumulative_to_labels(probs)\n",
    "                else:\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = probs.argmax(dim=1)\n",
    "            else:\n",
    "                preds_tensor = outputs.argmax(dim=1)\n",
    "            y = y.to(device)\n",
    "            preds_cpu = preds_tensor.detach().cpu()\n",
    "            y_cpu = y.detach().cpu()\n",
    "            for i in range(y_cpu.size(0)):\n",
    "                real_label = int(y_cpu[i].item())\n",
    "                pred_label = int(preds_cpu[i].item())\n",
    "                dataset_index = indices[current_index]\n",
    "                current_index += 1\n",
    "                raw_item = raw_dataset.raw[dataset_index]\n",
    "                problem_name = raw_item.get('problem_name', f\"problem_{dataset_index}\")\n",
    "                results.append({\n",
    "                    \"problem_name\": problem_name,\n",
    "                    \"y_true\": real_label,  # keep numeric for averaging/aggregation\n",
    "                    \"y_pred\": pred_label,\n",
    "                    \"diff\": real_label - pred_label\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if model_name is None:\n",
    "        model_name = sheet_name\n",
    "    df[\"model_name\"] = model_name\n",
    "    df[\"model\"] = model_name\n",
    "\n",
    "    # 1) Save all predictions into your main Excel file\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        # Convert numeric labels back to grade strings for readability\n",
    "        df_out = df.copy()\n",
    "        df_out[\"y_true\"] = df_out[\"y_true\"].map(lambda x: label_to_grade.get(x, f\"Unknown({x})\"))\n",
    "        df_out[\"y_pred\"] = df_out[\"y_pred\"].map(lambda x: label_to_grade.get(x, f\"Unknown({x})\"))\n",
    "        df_out.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Predictions for {sheet_name} exported to: {excel_path}\")\n",
    "\n",
    "    # 2) Create/update outlier.xlsx (problem_name, count, per-model counts, y_true, y_pred_avg)\n",
    "    _update_outlier_excel(df, model_name=model_name, outlier_filename=\"result/outlier.xlsx\", sheet_name=\"outliers\", threshold=3)\n",
    "\n",
    "\n",
    "# --- compute training and validation accuracy ---\n",
    "def compute_accuracy(model, dataloader, device):\n",
    "    strict_correct, loose_correct, total = 0, 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "\n",
    "            payload = X[0] if len(X) == 1 else X\n",
    "            outputs = model(payload)\n",
    "\n",
    "            if isinstance(outputs, tuple):\n",
    "                if getattr(model, \"is_ordinal\", False):\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = cumulative_to_labels(probs)\n",
    "                else:\n",
    "                    probs = outputs[0]\n",
    "                    preds_tensor = probs.argmax(dim=1)\n",
    "            else:\n",
    "                preds_tensor = outputs.argmax(dim=1)\n",
    "\n",
    "            if isinstance(preds_tensor, torch.Tensor):\n",
    "                preds_tensor = preds_tensor.to(y.device)\n",
    "            preds = preds_tensor\n",
    "            total += y.size(0)\n",
    "            strict_correct += (preds == y).sum().item()\n",
    "            loose_correct += ((preds - y).abs() <= 1).sum().item()\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.detach().cpu().numpy())\n",
    "    strict_acc = 100.0 * strict_correct / total\n",
    "    loose_acc = 100.0 * loose_correct / total\n",
    "    return strict_acc, loose_acc, y_true, y_pred\n",
    "\n",
    "\n",
    "def log_accuracy_to_csv(model_type, train_strict_acc, train_loose_acc, val_strict_acc, val_loose_acc, csv_path=\"result/accuracy.csv\"):\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "    with open(csv_path, mode='a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\n",
    "                \"model\",\n",
    "                \"Train Strict Accuracy (%)\",\n",
    "                \"Train ±1 Grade Accuracy (%)\",\n",
    "                \"Val Strict Accuracy (%)\",\n",
    "                \"Val ±1 Grade Accuracy (%)\"\n",
    "            ])\n",
    "        writer.writerow([\n",
    "            model_type,\n",
    "            round(train_strict_acc, 2),\n",
    "            round(train_loose_acc, 2),\n",
    "            round(val_strict_acc, 2),\n",
    "            round(val_loose_acc, 2)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae87271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stacking_meta_model(stacking_model, dataloader, device, epochs=5, lr=1e-3):\n",
    "    \"\"\"Train stacking meta-learner on frozen base model outputs.\"\"\"\n",
    "    if epochs <= 0:\n",
    "        return\n",
    "    stacking_model.meta_model.train()\n",
    "    for member in stacking_model.models.values():\n",
    "        member.eval()\n",
    "    optimizer = torch.optim.Adam(stacking_model.meta_model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_samples = 0\n",
    "        for X, y in dataloader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            targets = y.to(device)\n",
    "            member_feats = stacking_model._member_features(inputs)\n",
    "            M, B, C = member_feats.shape\n",
    "            if stacking_model.combine == \"mean\":\n",
    "                feat = member_feats.mean(dim=0)\n",
    "            else:\n",
    "                feat = member_feats.permute(1, 0, 2).reshape(B, M * C)\n",
    "\n",
    "            logits = stacking_model.meta_model(feat)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size = targets.size(0)\n",
    "            epoch_loss += loss.item() * batch_size\n",
    "            num_samples += batch_size\n",
    "\n",
    "        denom = num_samples if num_samples > 0 else 1\n",
    "        avg_loss = epoch_loss / denom\n",
    "        print(f\"Stacking meta epoch {epoch + 1}: loss={avg_loss:.4f}\")\n",
    "\n",
    "    stacking_model.meta_model.eval()\n",
    "\n",
    "\n",
    "def build_ensemble_models(\n",
    "    ensemble_names,\n",
    "    base_model_items,\n",
    "    ensemble_weights,\n",
    "    num_classes,\n",
    "    device,\n",
    "    train_loader,\n",
    "    stacking_meta_epochs=5,\n",
    "    stacking_meta_lr=1e-3,\n",
    "    label_suffix=\"\",\n",
    "):\n",
    "    \"\"\"Create configured ensemble models from trained base models.\"\"\"\n",
    "    ensembles = {}\n",
    "    base_items = list(base_model_items)\n",
    "\n",
    "    if not base_items:\n",
    "        return ensembles\n",
    "\n",
    "    def _resolve_group_weights(items):\n",
    "        if ensemble_weights is None:\n",
    "            return None\n",
    "        if isinstance(ensemble_weights, dict):\n",
    "            filtered = {name: ensemble_weights[name] for name, _ in items if name in ensemble_weights}\n",
    "            if len(filtered) != len(items):\n",
    "                missing = [name for name, _ in items if name not in filtered]\n",
    "                if missing:\n",
    "                    print(f\"Warning: missing weights for {missing}; using uniform weights.\")\n",
    "                return None\n",
    "            return filtered\n",
    "        weight_list = list(ensemble_weights)\n",
    "        if len(weight_list) != len(items):\n",
    "            print(\"Warning: weight list length mismatch; using uniform weights.\")\n",
    "            return None\n",
    "        return weight_list\n",
    "\n",
    "    resolved_weights = _resolve_group_weights(base_items)\n",
    "\n",
    "    for base_name in ensemble_names:\n",
    "        cloned_items = [(model_name, copy.deepcopy(model)) for model_name, model in base_items]\n",
    "        if not cloned_items:\n",
    "            continue\n",
    "\n",
    "        weights = resolved_weights\n",
    "        if isinstance(weights, list):\n",
    "            weights = list(weights)\n",
    "\n",
    "        ensemble_key = f\"{base_name}{label_suffix}\" if label_suffix else base_name\n",
    "\n",
    "        if base_name == \"soft_voting_ensemble\":\n",
    "            ensemble_model = SoftVotingEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == \"geometric_mean_ensemble\":\n",
    "            ensemble_model = GeometricMeanEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == \"median_ensemble\":\n",
    "            ensemble_model = MedianEnsemble(cloned_items, weights=weights, freeze_members=True).to(device)\n",
    "        elif base_name == \"trimmed_mean_ensemble\":\n",
    "            ensemble_model = TrimmedMeanEnsemble(cloned_items, weights=weights, freeze_members=True, trim_frac=0.2).to(device)\n",
    "        elif base_name == \"stacking_ensemble\":\n",
    "            feature_dim = len(cloned_items) * num_classes\n",
    "            meta_model = nn.Linear(feature_dim, num_classes).to(device)\n",
    "            ensemble_model = StackingEnsemble(\n",
    "                cloned_items,\n",
    "                weights=weights,\n",
    "                freeze_members=True,\n",
    "                meta_model=meta_model,\n",
    "                feature_source=\"logits\",\n",
    "                combine=\"concat\",\n",
    "            ).to(device)\n",
    "            train_stacking_meta_model(\n",
    "                ensemble_model,\n",
    "                train_loader,\n",
    "                device,\n",
    "                epochs=stacking_meta_epochs,\n",
    "                lr=stacking_meta_lr,\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Unknown ensemble type '{base_name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        ensemble_model.eval()\n",
    "        ensembles[ensemble_key] = ensemble_model\n",
    "\n",
    "    return ensembles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(\n",
    "    model_types=None,\n",
    "    include_ensemble=True,\n",
    "    ensemble_weights=None,\n",
    "    ensemble_types=None,\n",
    "    stacking_meta_epochs=5,\n",
    "    stacking_meta_lr=1e-3,\n",
    "    boosting_num_stages=5, \n",
    "    boosting_weak_epochs=3 \n",
    "):\n",
    "    model_types = model_types or MODEL_TYPES\n",
    "    results = []\n",
    "    excel_path = \"result/model_comparison_results.xlsx\"\n",
    "    class_labels = [f\"V{i}\" for i in range(4, 12)]\n",
    "\n",
    "    # BOOSTING_TYPES (defined globally) maps each boosting variant to its base learner.\n",
    "\n",
    "    trained_models = {}\n",
    "    base_dataset = None\n",
    "    base_train_idx = None\n",
    "    base_val_idx = None\n",
    "    num_classes = None\n",
    "\n",
    "    # --- THIS IS THE MODIFIED LOOP ---\n",
    "    for idx, mtype in enumerate(model_types):\n",
    "        print(f\"===== Processing {mtype} =====\")\n",
    "        \n",
    "        if mtype in BOOSTING_TYPES:\n",
    "            # --- This is a SEQUENTIAL (Boosting) Model ---\n",
    "            base_model_type = BOOSTING_TYPES[mtype]\n",
    "            # Call our new boosting trainer\n",
    "            train_loader, val_loader, model, dataset, train_idx, val_idx = train_boosting_main(\n",
    "                model_type=base_model_type,\n",
    "                num_stages=boosting_num_stages,\n",
    "                weak_epochs=boosting_weak_epochs\n",
    "            )\n",
    "        else:\n",
    "            # --- This is a PARALLEL (Standard) Model ---\n",
    "            print(f\"===== Training {mtype} =====\")\n",
    "            train_loader, val_loader, model, dataset, train_idx, val_idx = main(mtype)\n",
    "\n",
    "        # --- THE REST OF THE LOOP IS UNCHANGED ---\n",
    "        # Because both `main` and `train_boosting_main` return the same\n",
    "        # tuple, the evaluation code works perfectly for both.\n",
    "        model.eval()\n",
    "        trained_models[mtype] = model\n",
    "        if base_dataset is None:\n",
    "            base_dataset = dataset\n",
    "            base_train_idx = train_idx\n",
    "            base_val_idx = val_idx\n",
    "        if num_classes is None:\n",
    "            num_classes = getattr(model, \"num_classes\", None)\n",
    "\n",
    "        train_strict_acc, train_loose_acc, _, _ = compute_accuracy(model, train_loader, device)\n",
    "        val_strict_acc, val_loose_acc, y_true, y_pred = compute_accuracy(model, val_loader, device)\n",
    "\n",
    "        log_accuracy_to_csv(mtype, train_strict_acc, train_loose_acc, val_strict_acc, val_loose_acc)\n",
    "\n",
    "        results.append({\n",
    "            \"Model Type\": mtype,\n",
    "            \"Train Strict Accuracy (%)\": round(train_strict_acc, 2),\n",
    "            \"Train ±1 Grade Accuracy (%)\": round(train_loose_acc, 2),\n",
    "            \"Val Strict Accuracy (%)\": round(val_strict_acc, 2),\n",
    "            \"Val ±1 Grade Accuracy (%)\": round(val_loose_acc, 2),\n",
    "        })\n",
    "\n",
    "        if idx == 0:\n",
    "            df_results = pd.DataFrame(results)\n",
    "            df_results.to_excel(excel_path, index=False)\n",
    "\n",
    "        save_confusion_matrix_to_excel(y_true, y_pred, class_labels, mtype, excel_path)\n",
    "        export_predictions_to_excel(\n",
    "            model,\n",
    "            val_loader,\n",
    "            device,\n",
    "            grade_to_label,\n",
    "            excel_path,\n",
    "            sheet_name=f\"{mtype}_preds\",\n",
    "            model_name=mtype,\n",
    "        )\n",
    "\n",
    "    # --- ENSEMBLE EVALUATION SECTION ---\n",
    "    if include_ensemble and trained_models:\n",
    "        effective_ensemble_types = ensemble_types or ENSEMBLE_TYPES\n",
    "        if not effective_ensemble_types:\n",
    "            print(\"No ensemble types specified; skipping ensemble evaluation.\")\n",
    "        else:\n",
    "            if base_dataset is None or base_train_idx is None or base_val_idx is None:\n",
    "                raise RuntimeError(\"Dataset indices are unavailable for ensemble evaluation.\")\n",
    "            if num_classes is None:\n",
    "                raise RuntimeError(\"Unable to determine number of classes for ensembles.\")\n",
    "\n",
    "            print(\"===== Evaluating ensembles =====\")\n",
    "            collate_fn = make_collate_fn(\"set_transformer_xy\")\n",
    "            train_subset = Subset(base_dataset, base_train_idx)\n",
    "            val_subset = Subset(base_dataset, base_val_idx)\n",
    "            ensemble_train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "            ensemble_val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "            # Filter trained_models to EXCLUDE boosting models from being ensembled\n",
    "            base_items = [(name, model) for name, model in trained_models.items() if name not in BOOSTING_TYPES]\n",
    "\n",
    "            # Now ensemble_groups will only contain the original base models\n",
    "            ensemble_groups = [\n",
    "                (\"all\", base_items),\n",
    "                (\"set_transformer\", [(name, model) for name, model in base_items if \"set_transformer\" in name]),\n",
    "                (\"deepset\", [(name, model) for name, model in base_items if \"deepset\" in name]),\n",
    "            ]\n",
    "\n",
    "            for group_name, group_items in ensemble_groups:\n",
    "                if not group_items:\n",
    "                    print(f\"Skipping {group_name} ensemble group (no models).\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"--- Evaluating {group_name} ensembles ({len(group_items)} models) ---\")\n",
    "                ensembles = build_ensemble_models(\n",
    "                    effective_ensemble_types,\n",
    "                    group_items,\n",
    "                    ensemble_weights,\n",
    "                    num_classes,\n",
    "                    device,\n",
    "                    ensemble_train_loader,\n",
    "                    stacking_meta_epochs=stacking_meta_epochs,\n",
    "                    stacking_meta_lr=stacking_meta_lr,\n",
    "                    label_suffix=f\"_{group_name}\",\n",
    "                )\n",
    "\n",
    "                if not ensembles:\n",
    "                    print(f\"No ensembles constructed for group {group_name}.\")\n",
    "                    continue\n",
    "\n",
    "                for name, ensemble_model in ensembles.items():\n",
    "                    MODEL_COUNT_COLUMNS.setdefault(name, f\"{name}_count\")\n",
    "\n",
    "                    train_strict_acc, train_loose_acc, _, _ = compute_accuracy(ensemble_model, ensemble_train_loader, device)\n",
    "                    val_strict_acc, val_loose_acc, y_true, y_pred = compute_accuracy(ensemble_model, ensemble_val_loader, device)\n",
    "\n",
    "                    log_accuracy_to_csv(name, train_strict_acc, train_loose_acc, val_strict_acc, val_loose_acc)\n",
    "\n",
    "                    results.append({\n",
    "                        \"Model Type\": name,\n",
    "                        \"Train Strict Accuracy (%)\": round(train_strict_acc, 2),\n",
    "                        \"Train ±1 Grade Accuracy (%)\": round(train_loose_acc, 2),\n",
    "                        \"Val Strict Accuracy (%)\": round(val_strict_acc, 2),\n",
    "                        \"Val ±1 Grade Accuracy (%)\": round(val_loose_acc, 2),\n",
    "                    })\n",
    "\n",
    "                    save_confusion_matrix_to_excel(y_true, y_pred, class_labels, name, excel_path)\n",
    "                    export_predictions_to_excel(\n",
    "                        ensemble_model,\n",
    "                        ensemble_val_loader,\n",
    "                        device,\n",
    "                        grade_to_label,\n",
    "                        excel_path,\n",
    "                        sheet_name=f\"{name}_preds\",\n",
    "                        model_name=name,\n",
    "                    )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        df_results.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    print(\"=== Model Comparison Summary ===\")\n",
    "    print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage\n",
    "for i in range(1):\n",
    "    compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal evaluation helpers\n",
    "def evaluate_ordinal_thresholds(model, loader, grade_to_label, device, decision_threshold=0.5, model_name=None, output_dir='./result'):\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "    targets_list = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            inputs = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "            payload = inputs[0] if len(inputs) == 1 else inputs\n",
    "            outputs = model(payload)\n",
    "            if not isinstance(outputs, tuple):\n",
    "                raise ValueError('Model is not configured for ordinal outputs.')\n",
    "            probs, logits = outputs\n",
    "            probs_list.append(probs.cpu())\n",
    "            targets_list.append(y.cpu())\n",
    "    if not probs_list:\n",
    "        raise ValueError('No samples available for ordinal evaluation.')\n",
    "    probs = torch.cat(probs_list, dim=0)\n",
    "    targets = torch.cat(targets_list, dim=0)\n",
    "    acc_per_threshold = threshold_accuracy(probs, targets, threshold=decision_threshold).cpu()\n",
    "    grade_by_label = {v: k for k, v in grade_to_label.items()}\n",
    "    threshold_labels = []\n",
    "    for idx in range(acc_per_threshold.size(0)):\n",
    "        grade = grade_by_label.get(idx, f'label_{idx}')\n",
    "        threshold_labels.append(f\"P(>{grade})\")\n",
    "    df = pd.DataFrame({\n",
    "        'threshold': threshold_labels,\n",
    "        'accuracy': (acc_per_threshold.numpy() * 100).round(2)\n",
    "    })\n",
    "    overall_pred = cumulative_to_labels(probs, threshold=decision_threshold)\n",
    "    overall_acc = (overall_pred == targets).float().mean().item() * 100\n",
    "    if model_name:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, f'ordinal_metrics_{model_name}.csv')\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f'Saved threshold table to {output_path}')\n",
    "    print(df)\n",
    "    print(f'Overall accuracy: {overall_acc:.2f}%')\n",
    "    return df, overall_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ordinal variants sweep ---\n",
    "ordinal_model_types = [\n",
    "    'set_transformer_ordinal',\n",
    "    'set_transformer_ordinal_xy',\n",
    "    'set_transformer_ordinal_xy_additive',\n",
    "    'deepset_ordinal',\n",
    "    'deepset_ordinal_xy',\n",
    "    'deepset_ordinal_xy_additive',\n",
    "]\n",
    "\n",
    "ordinal_tables = []\n",
    "ordinal_summary = []\n",
    "\n",
    "for model_key in ordinal_model_types:\n",
    "    print(f\"=== Training ordinal model: {model_key} ===\")\n",
    "    train_loader, val_loader, model, dataset, train_idx, val_idx = main(model_key)\n",
    "    table, overall_acc = evaluate_ordinal_thresholds(\n",
    "        model,\n",
    "        val_loader,\n",
    "        grade_to_label=grade_to_label,\n",
    "        device=device,\n",
    "        decision_threshold=0.5,\n",
    "        model_name=model_key\n",
    "    )\n",
    "    table = table.copy()\n",
    "    table['model'] = model_key\n",
    "    table['overall_accuracy'] = overall_acc\n",
    "    ordinal_tables.append(table)\n",
    "    ordinal_summary.append({'model': model_key, 'overall_accuracy': overall_acc})\n",
    "\n",
    "if ordinal_tables:\n",
    "    combined = pd.concat(ordinal_tables, ignore_index=True)\n",
    "    summary_df = pd.DataFrame(ordinal_summary)\n",
    "\n",
    "    threshold_order = [f\"P(>{grade})\" for grade in sorted(grade_to_label.keys(), key=lambda g: grade_to_label[g])]\n",
    "    pivot_df = (combined\n",
    "                .pivot_table(index='model', columns='threshold', values='accuracy')\n",
    "                .reindex(columns=[c for c in threshold_order if c in combined['threshold'].unique()]))\n",
    "    pivot_df = pivot_df.sort_index()\n",
    "    pivot_df['Overall Accuracy'] = summary_df.set_index('model')['overall_accuracy']\n",
    "\n",
    "    combined = combined.sort_values(['model', 'threshold']).reset_index(drop=True)\n",
    "    summary_df = summary_df.sort_values('model').reset_index(drop=True)\n",
    "\n",
    "    output_path = './result/ordinal_result.xlsx'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with pd.ExcelWriter(output_path) as writer:\n",
    "        pivot_df.to_excel(writer, sheet_name='threshold_matrix')\n",
    "        combined.to_excel(writer, sheet_name='threshold_long', index=False)\n",
    "        summary_df.to_excel(writer, sheet_name='overall', index=False)\n",
    "    print(f\"Saved combined ordinal results to {output_path}\")\n",
    "else:\n",
    "    print('No ordinal results generated.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f8281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate problems\n",
    "def evaluate_problems(\n",
    "    model, problem_dict, hold_to_idx, hold_difficulty, type_to_idx, device,\n",
    "    grade_to_label, hold_to_coord, dataset, train_idx, val_idx, model_type\n",
    "):\n",
    "    label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "    print(\"=== MoonBoard Problem Evaluation ===\")\n",
    "\n",
    "    for fallback_name, holds in problem_dict.items():\n",
    "        try:\n",
    "            hold_idxs = []\n",
    "            diff_values = []\n",
    "            type_vecs = []\n",
    "            xy_coords = []\n",
    "\n",
    "            for h in holds:\n",
    "                if h not in hold_difficulty or h not in hold_to_idx or h not in hold_to_coord:\n",
    "                    raise ValueError(f\"[ERROR] Hold '{h}' missing from required dictionaries.\")\n",
    "\n",
    "                hold_idxs.append(hold_to_idx[h])\n",
    "                difficulty, types = hold_difficulty[h]\n",
    "                diff_values.append(difficulty / 10.0)\n",
    "\n",
    "                # Multi-hot vector\n",
    "                type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "                for t in types:\n",
    "                    if t in type_to_idx:\n",
    "                        type_vec[type_to_idx[t]] = 1.0\n",
    "                type_vecs.append(type_vec)\n",
    "\n",
    "                xy_coords.append(torch.tensor([hold_to_coord[h][0] / 10.0, hold_to_coord[h][1] / 17.0], dtype=torch.float))\n",
    "\n",
    "            # Convert to tensors\n",
    "            hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)\n",
    "            difficulty_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)\n",
    "            type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)\n",
    "            xy_tensor = torch.stack(xy_coords).unsqueeze(0).to(device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # --- Select input format based on model_type ---\n",
    "                if model_type in XY_MODELS:\n",
    "                    input_data = (hold_tensor, difficulty_tensor, type_tensor, xy_tensor)\n",
    "                elif model_type in {'set_transformer', 'deepset', 'set_transformer_ordinal', 'deepset_ordinal'}:\n",
    "                    input_data = (hold_tensor,)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "                payload = input_data[0] if isinstance(input_data, tuple) and len(input_data) == 1 else input_data\n",
    "                outputs = model(payload)\n",
    "                if isinstance(outputs, tuple):\n",
    "                    probs, logits = outputs\n",
    "                    pred_label = (probs > 0.5).sum(dim=1).item()\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                    pred_label = logits.argmax(dim=1).item()\n",
    "                pred_grade = label_to_grade.get(pred_label, f\"Unknown({pred_label})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{fallback_name}] Skipping due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Search in dataset for match\n",
    "        found_idx = None\n",
    "        split = \"Not Found\"\n",
    "        setter_grade = \"Unknown\"\n",
    "        problem_name = fallback_name\n",
    "\n",
    "        for idx_item, item in enumerate(dataset.raw):\n",
    "            if set(item['holds']) == set(holds):\n",
    "                found_idx = idx_item\n",
    "                setter_grade = item.get('grade', 'Unknown')\n",
    "                problem_name = item.get('problem_name', fallback_name)\n",
    "                if found_idx in train_idx:\n",
    "                    split = \"Train\"\n",
    "                elif found_idx in val_idx:\n",
    "                    split = \"Validation\"\n",
    "                else:\n",
    "                    split = \"Found (Unknown Split)\"\n",
    "                break\n",
    "\n",
    "        holds_with_difficulty = {h: hold_difficulty[h][0] if h in hold_difficulty else \"N/A\" for h in holds}\n",
    "        print(f\"🔹 Problem Name   : {problem_name}\")\n",
    "        print(f\"   Holds Used     : {holds_with_difficulty}\")\n",
    "        print(f\"   Setter Grade   : {setter_grade}\")\n",
    "        print(f\"   Predicted Grade: {pred_grade}\")\n",
    "        print(f\"   Dataset Split  : {split}\")\n",
    "\n",
    "named_problems = {\n",
    "    \"Physical V9 Benchmark\": [\"I18\", \"J12\", \"F13\", \"D10\", \"E6\", \"J2\"],\n",
    "    \"Triangulation V7\": [\"A18\", \"J13\", \"D16\", \"E9\", \"E9\", \"I4\"],\n",
    "    \"warmup crimps\": [\"I18\", \"I7\", \"I9\", \"I15\", \"G11\", \"J14\", \"J12\", \"I15\", \"J14\", \"H4\", \"K6\"],\n",
    "    \"Ronani V5\": [\"F18\", \"I15\", \"I10\", \"K9\", \"K6\", \"G14\", \"D16\", \"E9\", \"K6\", \"I15\", \"E4\", \"H5\"],\n",
    "    \"Don't Fart Alan\": [\"K18\", \"J15\", \"F14\", \"F13\", \"D10\", \"E6\", \"I7\", \"I5\", \"F1\"],\n",
    "    \"FINALE MAXI 2025 POCKET 2 V9\": [\"G3\", \"F3\", \"F4\", \"A6\", \"A11\", \"B17\", \"C9\", \"D17\", \"H18\"],\n",
    "    \"Khai's V7\": [\"D18\", \"A15\", \"A12\", \"C9\", \"E7\", \"H8\", \"I6\", \"E1\"],\n",
    "    \"Yums In My Tums V5\": [\"F18\", \"G12\", \"E1\", \"D13\", \"I9\", \"F8\", \"I2\", \"F16\", \"E4\", \"E6\"],\n",
    "}\n",
    "\n",
    "team_problems = {\n",
    "    \"simma mot strommen\": [\"A18\", \"C12\", \"A9\", \"B14\", \"B16\", \"D1\", \"F5\", \"F5\"],\n",
    "    \"MAXIMUS!\": [\"K18\", \"E3\", \"K14\", \"I13\", \"K7\", \"I2\", \"H16\", \"K11\", \"G7\", \"H4\"],\n",
    "    \"interstate\": [\"K18\", \"H17\", \"J11\", \"I9\", \"G13\", \"H15\", \"I5\", \"I6\"],\n",
    "    \"krakatoa pusher\": [\"H18\", \"H11\", \"J8\", \"F7\", \"K15\", \"F4\", \"J3\"],\n",
    "    \"doublement\": [\"A18\", \"E16\", \"F8\", \"B14\", \"G8\", \"E12\", \"F4\", \"F3\", \"F3\"],\n",
    "    \"animal instinct\": [\"F18\", \"J11\", \"F9\", \"H15\", \"E13\", \"J11\", \"I6\", \"F4\"],\n",
    "    \"blue bin day\": [\"B18\", \"C18\", \"A8\", \"C12\", \"B15\", \"A5\", \"C3\"]\n",
    "}\n",
    "\n",
    "# --- Classification baseline run ---\n",
    "clf_train_loader, clf_val_loader, clf_model, clf_dataset, clf_train_idx, clf_val_idx = main('set_transformer_additive')\n",
    "\n",
    "evaluate_problems(\n",
    "    model=clf_model,\n",
    "    problem_dict=named_problems,\n",
    "    hold_to_idx=hold_to_idx,\n",
    "    hold_difficulty=hold_difficulty,\n",
    "    type_to_idx=type_to_idx,\n",
    "    device=device,\n",
    "    grade_to_label=grade_to_label,\n",
    "    hold_to_coord=hold_to_coord,\n",
    "    dataset=clf_dataset,\n",
    "    train_idx=clf_train_idx,\n",
    "    val_idx=clf_val_idx,\n",
    "    model_type='set_transformer_additive'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228393ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "def visualize_attention_for_problem(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device):\n",
    "    model.eval()\n",
    "\n",
    "    hold_idxs = []\n",
    "    diff_values = []\n",
    "    type_vecs = []\n",
    "    xy_coords = []\n",
    "\n",
    "    for h in holds:\n",
    "        hold_idxs.append(hold_to_idx[h])\n",
    "        difficulty, types = hold_difficulty[h]\n",
    "        diff_values.append(difficulty / 10.0)\n",
    "\n",
    "        type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "        for t in types:\n",
    "            if t in type_to_idx:\n",
    "                type_vec[type_to_idx[t]] = 1.0\n",
    "        type_vecs.append(type_vec)\n",
    "\n",
    "        x, y = hold_to_coord[h]\n",
    "        xy_coords.append([x / 10.0, y / 17.0])\n",
    "\n",
    "    hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    diff_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)\n",
    "    type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)\n",
    "    xy_tensor = torch.tensor(xy_coords, dtype=torch.float).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model((hold_tensor, diff_tensor, type_tensor, xy_tensor))\n",
    "\n",
    "    attn_isab1 = model.encoder[0].mab0.attn_weights.cpu().numpy()\n",
    "    attn_isab2 = model.encoder[1].mab0.attn_weights.cpu().numpy()\n",
    "\n",
    "    num_heads = attn_isab1.shape[0]\n",
    "    fig, axes = plt.subplots(2, num_heads, figsize=(4 * num_heads, 8))\n",
    "    if num_heads == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "\n",
    "    for h in range(num_heads):\n",
    "        sns.heatmap(attn_isab1[h], ax=axes[0, h], cmap=\"viridis\", xticklabels=holds)\n",
    "        axes[0, h].set_title(f\"ISAB1 – Head {h}\")\n",
    "        axes[0, h].set_xlabel(\"Key (Hold)\")\n",
    "        axes[0, h].set_ylabel(\"Seed\")\n",
    "\n",
    "        sns.heatmap(attn_isab2[h], ax=axes[1, h], cmap=\"viridis\", xticklabels=holds)\n",
    "        axes[1, h].set_title(f\"ISAB2 – Head {h}\")\n",
    "        axes[1, h].set_xlabel(\"Key (Hold)\")\n",
    "        axes[1, h].set_ylabel(\"Seed\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66931c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_attention_per_hold(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device):\n",
    "    model.eval()\n",
    "\n",
    "    hold_idxs = []\n",
    "    diff_values = []\n",
    "    type_vecs = []\n",
    "    xy_coords = []\n",
    "\n",
    "    for h in holds:\n",
    "        hold_idxs.append(hold_to_idx[h])\n",
    "        difficulty, types = hold_difficulty[h]\n",
    "        diff_values.append(difficulty / 10.0)\n",
    "\n",
    "        # Multi-hot type vector\n",
    "        type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "        for t in types:\n",
    "            if t in type_to_idx:\n",
    "                type_vec[type_to_idx[t]] = 1.0\n",
    "        type_vecs.append(type_vec)\n",
    "\n",
    "        # XY coordinate\n",
    "        if h not in hold_to_coord:\n",
    "            raise ValueError(f\"[ERROR] Hold '{h}' has no coordinate in hold_to_coord.\")\n",
    "        x, y = hold_to_coord[h]\n",
    "        xy_coords.append([x / 10.0, y / 17.0])\n",
    "\n",
    "    # Build model input tensors\n",
    "    hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)       # (1, N)\n",
    "    diff_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)    # (1, N)\n",
    "    type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)                          # (1, N, T)\n",
    "    xy_tensor = torch.tensor(xy_coords, dtype=torch.float).unsqueeze(0).to(device)        # (1, N, 2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model((hold_tensor, diff_tensor, type_tensor, xy_tensor))\n",
    "\n",
    "    attn_weights = model.encoder[0].mab0.attn_weights  # shape: (heads, seeds, holds)\n",
    "    avg_attn = attn_weights.mean(dim=(0, 1)).cpu().numpy()  # average across heads & seeds → (num_holds,)\n",
    "\n",
    "    return list(zip(holds, avg_attn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce93aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention and scores (with XY support)\n",
    "\n",
    "holds = named_problems[\"warmup crimps\"]\n",
    "\n",
    "if not hasattr(model.encoder[0], 'mab0') or not hasattr(model.encoder[0].mab0, 'attn_weights'):\n",
    "    raise ValueError(\"The provided model does not support attention visualization.\")\n",
    "\n",
    "visualize_attention_for_problem(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device)\n",
    "attention_scores = get_avg_attention_per_hold(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device)\n",
    "\n",
    "# Print sorted scores\n",
    "attention_scores_sorted = sorted(attention_scores, key=lambda x: x[1], reverse=True)\n",
    "print(\"Average Attention Per Hold (sorted):\")\n",
    "for h, score in attention_scores_sorted:\n",
    "    difficulty = hold_difficulty[h][0] if h in hold_difficulty else \"N/A\"\n",
    "    print(f\"{h}: {score:.4f} (difficulty: {difficulty})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb97c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read accuracy.csv file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./result/accuracy.csv')\n",
    "# print(df)\n",
    "\n",
    "filtered_df = df[df['model'] == 'set_transformer_xy']\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa75f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sousei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
