{'A1': 0, 'B1': 1, 'C1': 2, 'D1': 3, 'E1': 4, 'F1': 5, 'G1': 6, 'H1': 7, 'I1': 8, 'J1': 9, 'K1': 10, 'A2': 11, 'B2': 12, 'C2': 13, 'D2': 14, 'E2': 15, 'F2': 16, 'G2': 17, 'H2': 18, 'I2': 19, 'J2': 20, 'K2': 21, 'A3': 22, 'B3': 23, 'C3': 24, 'D3': 25, 'E3': 26, 'F3': 27, 'G3': 28, 'H3': 29, 'I3': 30, 'J3': 31, 'K3': 32, 'A4': 33, 'B4': 34, 'C4': 35, 'D4': 36, 'E4': 37, 'F4': 38, 'G4': 39, 'H4': 40, 'I4': 41, 'J4': 42, 'K4': 43, 'A5': 44, 'B5': 45, 'C5': 46, 'D5': 47, 'E5': 48, 'F5': 49, 'G5': 50, 'H5': 51, 'I5': 52, 'J5': 53, 'K5': 54, 'A6': 55, 'B6': 56, 'C6': 57, 'D6': 58, 'E6': 59, 'F6': 60, 'G6': 61, 'H6': 62, 'I6': 63, 'J6': 64, 'K6': 65, 'A7': 66, 'B7': 67, 'C7': 68, 'D7': 69, 'E7': 70, 'F7': 71, 'G7': 72, 'H7': 73, 'I7': 74, 'J7': 75, 'K7': 76, 'A8': 77, 'B8': 78, 'C8': 79, 'D8': 80, 'E8': 81, 'F8': 82, 'G8': 83, 'H8': 84, 'I8': 85, 'J8': 86, 'K8': 87, 'A9': 88, 'B9': 89, 'C9': 90, 'D9': 91, 'E9': 92, 'F9': 93, 'G9': 94, 'H9': 95, 'I9': 96, 'J9': 97, 'K9': 98, 'A10': 99, 'B10': 100, 'C10': 101, 'D10': 102, 'E10': 103, 'F10': 104, 'G10': 105, 'H10': 106, 'I10': 107, 'J10': 108, 'K10': 109, 'A11': 110, 'B11': 111, 'C11': 112, 'D11': 113, 'E11': 114, 'F11': 115, 'G11': 116, 'H11': 117, 'I11': 118, 'J11': 119, 'K11': 120, 'A12': 121, 'B12': 122, 'C12': 123, 'D12': 124, 'E12': 125, 'F12': 126, 'G12': 127, 'H12': 128, 'I12': 129, 'J12': 130, 'K12': 131, 'A13': 132, 'B13': 133, 'C13': 134, 'D13': 135, 'E13': 136, 'F13': 137, 'G13': 138, 'H13': 139, 'I13': 140, 'J13': 141, 'K13': 142, 'A14': 143, 'B14': 144, 'C14': 145, 'D14': 146, 'E14': 147, 'F14': 148, 'G14': 149, 'H14': 150, 'I14': 151, 'J14': 152, 'K14': 153, 'A15': 154, 'B15': 155, 'C15': 156, 'D15': 157, 'E15': 158, 'F15': 159, 'G15': 160, 'H15': 161, 'I15': 162, 'J15': 163, 'K15': 164, 'A16': 165, 'B16': 166, 'C16': 167, 'D16': 168, 'E16': 169, 'F16': 170, 'G16': 171, 'H16': 172, 'I16': 173, 'J16': 174, 'K16': 175, 'A17': 176, 'B17': 177, 'C17': 178, 'D17': 179, 'E17': 180, 'F17': 181, 'G17': 182, 'H17': 183, 'I17': 184, 'J17': 185, 'K17': 186, 'A18': 187, 'B18': 188, 'C18': 189, 'D18': 190, 'E18': 191, 'F18': 192, 'G18': 193, 'H18': 194, 'I18': 195, 'J18': 196, 'K18': 197}
successfully parsed hold difficulty file
successfully prepare type vocabulary
successfully created (x,y) position to each hold:
{'A1': (0, 0), 'A2': (0, 1), 'A3': (0, 2), 'A4': (0, 3), 'A5': (0, 4), 'A6': (0, 5), 'A7': (0, 6), 'A8': (0, 7), 'A9': (0, 8), 'A10': (0, 9), 'A11': (0, 10), 'A12': (0, 11), 'A13': (0, 12), 'A14': (0, 13), 'A15': (0, 14), 'A16': (0, 15), 'A17': (0, 16), 'A18': (0, 17), 'B1': (1, 0), 'B2': (1, 1), 'B3': (1, 2), 'B4': (1, 3), 'B5': (1, 4), 'B6': (1, 5), 'B7': (1, 6), 'B8': (1, 7), 'B9': (1, 8), 'B10': (1, 9), 'B11': (1, 10), 'B12': (1, 11), 'B13': (1, 12), 'B14': (1, 13), 'B15': (1, 14), 'B16': (1, 15), 'B17': (1, 16), 'B18': (1, 17), 'C1': (2, 0), 'C2': (2, 1), 'C3': (2, 2), 'C4': (2, 3), 'C5': (2, 4), 'C6': (2, 5), 'C7': (2, 6), 'C8': (2, 7), 'C9': (2, 8), 'C10': (2, 9), 'C11': (2, 10), 'C12': (2, 11), 'C13': (2, 12), 'C14': (2, 13), 'C15': (2, 14), 'C16': (2, 15), 'C17': (2, 16), 'C18': (2, 17), 'D1': (3, 0), 'D2': (3, 1), 'D3': (3, 2), 'D4': (3, 3), 'D5': (3, 4), 'D6': (3, 5), 'D7': (3, 6), 'D8': (3, 7), 'D9': (3, 8), 'D10': (3, 9), 'D11': (3, 10), 'D12': (3, 11), 'D13': (3, 12), 'D14': (3, 13), 'D15': (3, 14), 'D16': (3, 15), 'D17': (3, 16), 'D18': (3, 17), 'E1': (4, 0), 'E2': (4, 1), 'E3': (4, 2), 'E4': (4, 3), 'E5': (4, 4), 'E6': (4, 5), 'E7': (4, 6), 'E8': (4, 7), 'E9': (4, 8), 'E10': (4, 9), 'E11': (4, 10), 'E12': (4, 11), 'E13': (4, 12), 'E14': (4, 13), 'E15': (4, 14), 'E16': (4, 15), 'E17': (4, 16), 'E18': (4, 17), 'F1': (5, 0), 'F2': (5, 1), 'F3': (5, 2), 'F4': (5, 3), 'F5': (5, 4), 'F6': (5, 5), 'F7': (5, 6), 'F8': (5, 7), 'F9': (5, 8), 'F10': (5, 9), 'F11': (5, 10), 'F12': (5, 11), 'F13': (5, 12), 'F14': (5, 13), 'F15': (5, 14), 'F16': (5, 15), 'F17': (5, 16), 'F18': (5, 17), 'G1': (6, 0), 'G2': (6, 1), 'G3': (6, 2), 'G4': (6, 3), 'G5': (6, 4), 'G6': (6, 5), 'G7': (6, 6), 'G8': (6, 7), 'G9': (6, 8), 'G10': (6, 9), 'G11': (6, 10), 'G12': (6, 11), 'G13': (6, 12), 'G14': (6, 13), 'G15': (6, 14), 'G16': (6, 15), 'G17': (6, 16), 'G18': (6, 17), 'H1': (7, 0), 'H2': (7, 1), 'H3': (7, 2), 'H4': (7, 3), 'H5': (7, 4), 'H6': (7, 5), 'H7': (7, 6), 'H8': (7, 7), 'H9': (7, 8), 'H10': (7, 9), 'H11': (7, 10), 'H12': (7, 11), 'H13': (7, 12), 'H14': (7, 13), 'H15': (7, 14), 'H16': (7, 15), 'H17': (7, 16), 'H18': (7, 17), 'I1': (8, 0), 'I2': (8, 1), 'I3': (8, 2), 'I4': (8, 3), 'I5': (8, 4), 'I6': (8, 5), 'I7': (8, 6), 'I8': (8, 7), 'I9': (8, 8), 'I10': (8, 9), 'I11': (8, 10), 'I12': (8, 11), 'I13': (8, 12), 'I14': (8, 13), 'I15': (8, 14), 'I16': (8, 15), 'I17': (8, 16), 'I18': (8, 17), 'J1': (9, 0), 'J2': (9, 1), 'J3': (9, 2), 'J4': (9, 3), 'J5': (9, 4), 'J6': (9, 5), 'J7': (9, 6), 'J8': (9, 7), 'J9': (9, 8), 'J10': (9, 9), 'J11': (9, 10), 'J12': (9, 11), 'J13': (9, 12), 'J14': (9, 13), 'J15': (9, 14), 'J16': (9, 15), 'J17': (9, 16), 'J18': (9, 17), 'K1': (10, 0), 'K2': (10, 1), 'K3': (10, 2), 'K4': (10, 3), 'K5': (10, 4), 'K6': (10, 5), 'K7': (10, 6), 'K8': (10, 7), 'K9': (10, 8), 'K10': (10, 9), 'K11': (10, 10), 'K12': (10, 11), 'K13': (10, 12), 'K14': (10, 13), 'K15': (10, 14), 'K16': (10, 15), 'K17': (10, 16), 'K18': (10, 17)}
Using device: cuda
------------------------------------iteration no 1------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7754
Epoch 02 — loss: 1.6386
Epoch 03 — loss: 1.5778
Epoch 04 — loss: 1.5401
Epoch 05 — loss: 1.5048
Epoch 06 — loss: 1.4789
Epoch 07 — loss: 1.4413
Epoch 08 — loss: 1.4077
Epoch 09 — loss: 1.3792
Epoch 10 — loss: 1.3486
Epoch 11 — loss: 1.3231
Epoch 12 — loss: 1.2800
Epoch 13 — loss: 1.2459
Epoch 14 — loss: 1.2120
Epoch 15 — loss: 1.1742
Epoch 16 — loss: 1.1506
Epoch 17 — loss: 1.1052
Epoch 18 — loss: 1.0784
Epoch 19 — loss: 1.0339
Epoch 20 — loss: 0.9966
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8977
Epoch 02 — loss: 1.6162
Epoch 03 — loss: 1.5471
Epoch 04 — loss: 1.5272
Epoch 05 — loss: 1.5028
Epoch 06 — loss: 1.4932
Epoch 07 — loss: 1.4817
Epoch 08 — loss: 1.4721
Epoch 09 — loss: 1.4699
Epoch 10 — loss: 1.4666
Epoch 11 — loss: 1.4581
Epoch 12 — loss: 1.4455
Epoch 13 — loss: 1.4558
Epoch 14 — loss: 1.4522
Epoch 15 — loss: 1.4507
Epoch 16 — loss: 1.4477
Epoch 17 — loss: 1.4479
Epoch 18 — loss: 1.4377
Epoch 19 — loss: 1.4397
Epoch 20 — loss: 1.4365
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7067
Epoch 02 — loss: 1.5813
Epoch 03 — loss: 1.5345
Epoch 04 — loss: 1.5110
Epoch 05 — loss: 1.4702
Epoch 06 — loss: 1.4481
Epoch 07 — loss: 1.4082
Epoch 08 — loss: 1.3808
Epoch 09 — loss: 1.3415
Epoch 10 — loss: 1.3173
Epoch 11 — loss: 1.2830
Epoch 12 — loss: 1.2523
Epoch 13 — loss: 1.2129
Epoch 14 — loss: 1.1737
Epoch 15 — loss: 1.1464
Epoch 16 — loss: 1.1074
Epoch 17 — loss: 1.0718
Epoch 18 — loss: 1.0352
Epoch 19 — loss: 1.0082
Epoch 20 — loss: 0.9682
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8635
Epoch 02 — loss: 1.5955
Epoch 03 — loss: 1.5438
Epoch 04 — loss: 1.5116
Epoch 05 — loss: 1.5038
Epoch 06 — loss: 1.4877
Epoch 07 — loss: 1.4757
Epoch 08 — loss: 1.4751
Epoch 09 — loss: 1.4703
Epoch 10 — loss: 1.4585
Epoch 11 — loss: 1.4612
Epoch 12 — loss: 1.4532
Epoch 13 — loss: 1.4529
Epoch 14 — loss: 1.4478
Epoch 15 — loss: 1.4516
Epoch 16 — loss: 1.4531/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 17 — loss: 1.4469
Epoch 18 — loss: 1.4474
Epoch 19 — loss: 1.4457
Epoch 20 — loss: 1.4454
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7328
Epoch 02 — loss: 1.6159
Epoch 03 — loss: 1.5699
Epoch 04 — loss: 1.5429
Epoch 05 — loss: 1.5264
Epoch 06 — loss: 1.4974
Epoch 07 — loss: 1.4783
Epoch 08 — loss: 1.4549
Epoch 09 — loss: 1.4259
Epoch 10 — loss: 1.4021
Epoch 11 — loss: 1.3748
Epoch 12 — loss: 1.3569
Epoch 13 — loss: 1.3242
Epoch 14 — loss: 1.3073
Epoch 15 — loss: 1.2892
Epoch 16 — loss: 1.2613
Epoch 17 — loss: 1.2358
Epoch 18 — loss: 1.2169
Epoch 19 — loss: 1.1927
Epoch 20 — loss: 1.1523
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7885
Epoch 02 — loss: 1.6149
Epoch 03 — loss: 1.5849
Epoch 04 — loss: 1.5528
Epoch 05 — loss: 1.5370
Epoch 06 — loss: 1.5213
Epoch 07 — loss: 1.5050
Epoch 08 — loss: 1.5005
Epoch 09 — loss: 1.4836
Epoch 10 — loss: 1.4759
Epoch 11 — loss: 1.4693
Epoch 12 — loss: 1.4631
Epoch 13 — loss: 1.4679
Epoch 14 — loss: 1.4567
Epoch 15 — loss: 1.4445
Epoch 16 — loss: 1.4436
Epoch 17 — loss: 1.4481
Epoch 18 — loss: 1.4456
Epoch 19 — loss: 1.4427
Epoch 20 — loss: 1.4481
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6168
Epoch 02 — loss: 1.3732
Epoch 03 — loss: 1.2994
Stage 1: Error=0.5327, Alpha=1.6606
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7787
Epoch 02 — loss: 1.5735
Epoch 03 — loss: 1.5209
Stage 2: Error=0.6391, Alpha=1.2202
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8335
Epoch 02 — loss: 1.6678
Epoch 03 — loss: 1.5986
Stage 3: Error=0.6644, Alpha=1.1088
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8822
Epoch 02 — loss: 1.7854
Epoch 03 — loss: 1.7041
Stage 4: Error=0.7602, Alpha=0.6377
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8922
Epoch 02 — loss: 1.8143
Epoch 03 — loss: 1.7368
Stage 5: Error=0.7446, Alpha=0.7215
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5581
Epoch 02 — loss: 1.3623
Epoch 03 — loss: 1.3410
Stage 1: Error=0.5321, Alpha=1.6631
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7624
Epoch 02 — loss: 1.6065
Epoch 03 — loss: 1.5527
Stage 2: Error=0.6566, Alpha=1.1434
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8010
Epoch 02 — loss: 1.6702
Epoch 03 — loss: 1.6177
Stage 3: Error=0.7207, Alpha=0.8441
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8061
Epoch 02 — loss: 1.7094
Epoch 03 — loss: 1.6643
Stage 4: Error=0.6965, Alpha=0.9610
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8435
Epoch 02 — loss: 1.7675
Epoch 03 — loss: 1.7253
Stage 5: Error=0.7135, Alpha=0.8791
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6225
Epoch 02 — loss: 1.3521
Epoch 03 — loss: 1.3024
Stage 1: Error=0.5280, Alpha=1.6795
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7823
Epoch 02 — loss: 1.5813
Epoch 03 — loss: 1.5070
Stage 2: Error=0.6510, Alpha=1.1684
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7942
Epoch 02 — loss: 1.6162
Epoch 03 — loss: 1.5550
Stage 3: Error=0.6291, Alpha=1.2634
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8576
Epoch 02 — loss: 1.7647
Epoch 03 — loss: 1.7009
Stage 4: Error=0.7366, Alpha=0.7636
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8780
Epoch 02 — loss: 1.8240
Epoch 03 — loss: 1.7565
Stage 5: Error=0.7597, Alpha=0.6407
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4661
Epoch 02 — loss: 1.3398
Epoch 03 — loss: 1.3060
Stage 1: Error=0.5284, Alpha=1.6780
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6779
Epoch 02 — loss: 1.5670
Epoch 03 — loss: 1.5156
Stage 2: Error=0.6270, Alpha=1.2725
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7454
Epoch 02 — loss: 1.6297
Epoch 03 — loss: 1.5498
Stage 3: Error=0.6273, Alpha=1.2711
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7761
Epoch 02 — loss: 1.6834
Epoch 03 — loss: 1.6214
Stage 4: Error=0.6280, Alpha=1.2683
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8310
Epoch 02 — loss: 1.7399
Epoch 03 — loss: 1.6807
Stage 5: Error=0.6956, Alpha=0.9656
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4381
Epoch 02 — loss: 1.3515
Epoch 03 — loss: 1.3058
Stage 1: Error=0.5289, Alpha=1.6761
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6413
Epoch 02 — loss: 1.5485
Epoch 03 — loss: 1.5111
Stage 2: Error=0.6414, Alpha=1.2105
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6804
Epoch 02 — loss: 1.6087
Epoch 03 — loss: 1.5717
Stage 3: Error=0.6678, Alpha=1.0933
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7580
Epoch 02 — loss: 1.6862
Epoch 03 — loss: 1.6351
Stage 4: Error=0.6529, Alpha=1.1599
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8466
Epoch 02 — loss: 1.7850
Epoch 03 — loss: 1.7407
Stage 5: Error=0.7214, Alpha=0.8406
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:34:56] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:37:07] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:38:59] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.3976
Epoch 02 — loss: 1.3099
Epoch 03 — loss: 1.2713
Stage 1: Error=0.5090, Alpha=1.7557
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6422
Epoch 02 — loss: 1.5247
Epoch 03 — loss: 1.4864
Stage 2: Error=0.6017, Alpha=1.3793
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7164
Epoch 02 — loss: 1.6086
Epoch 03 — loss: 1.5498
Stage 3: Error=0.6310, Alpha=1.2555
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7821
Epoch 02 — loss: 1.6945
Epoch 03 — loss: 1.6127
Stage 4: Error=0.6487, Alpha=1.1785
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8019
Epoch 02 — loss: 1.7047
Epoch 03 — loss: 1.6363
Stage 5: Error=0.6797, Alpha=1.0395
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8733
Stacking meta epoch 2: loss=0.7425
Stacking meta epoch 3: loss=0.7139
Stacking meta epoch 4: loss=0.6980
Stacking meta epoch 5: loss=0.6869
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8602
Stacking meta epoch 2: loss=0.7431
Stacking meta epoch 3: loss=0.7149
Stacking meta epoch 4: loss=0.6990
Stacking meta epoch 5: loss=0.6880
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2547
Stacking meta epoch 2: loss=1.2416
Stacking meta epoch 3: loss=1.2392
Stacking meta epoch 4: loss=1.2376
Stacking meta epoch 5: loss=1.2363
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      79.26
1                                deepset  ...                      80.08
2                     set_transformer_xy  ...                      81.18
3                             deepset_xy  ...                      80.41
4               set_transformer_additive  ...                      80.32
5                    deepset_xy_additive  ...                      82.44
6                       adaboost_deepset  ...                      81.04
7           adaboost_deepset_xy_additive  ...                      79.98
8                    adaboost_deepset_xy  ...                      82.53
9               adaboost_set_transformer  ...                      80.41
10     adaboost_set_transformer_additive  ...                      80.08
11           adaboost_set_transformer_xy  ...                      82.44
12              soft_voting_ensemble_all  ...                      82.92
13                 stacking_ensemble_all  ...                      83.78
14                      gbm_ensemble_all  ...                      83.78
15                  xgboost_ensemble_all  ...                      84.12
16  soft_voting_ensemble_set_transformer  ...                      82.63
17     stacking_ensemble_set_transformer  ...                      83.73
18          gbm_ensemble_set_transformer  ...                      84.17
19      xgboost_ensemble_set_transformer  ...                      84.17
20          soft_voting_ensemble_deepset  ...                      81.23
21             stacking_ensemble_deepset  ...                      83.16
22                  gbm_ensemble_deepset  ...                      81.18
23              xgboost_ensemble_deepset  ...                      80.99

[24 rows x 5 columns]
------------------------------------iteration no 2------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7556
Epoch 02 — loss: 1.6218
Epoch 03 — loss: 1.5762
Epoch 04 — loss: 1.5345
Epoch 05 — loss: 1.5029
Epoch 06 — loss: 1.4673
Epoch 07 — loss: 1.4389
Epoch 08 — loss: 1.4080
Epoch 09 — loss: 1.3575
Epoch 10 — loss: 1.3316
Epoch 11 — loss: 1.3014
Epoch 12 — loss: 1.2639
Epoch 13 — loss: 1.2180
Epoch 14 — loss: 1.2060
Epoch 15 — loss: 1.1627
Epoch 16 — loss: 1.1154
Epoch 17 — loss: 1.0813
Epoch 18 — loss: 1.0546
Epoch 19 — loss: 1.0089
Epoch 20 — loss: 0.9714
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8956
Epoch 02 — loss: 1.6260
Epoch 03 — loss: 1.5389
Epoch 04 — loss: 1.5101
Epoch 05 — loss: 1.4899
Epoch 06 — loss: 1.4914
Epoch 07 — loss: 1.4745
Epoch 08 — loss: 1.4644
Epoch 09 — loss: 1.4632
Epoch 10 — loss: 1.4638
Epoch 11 — loss: 1.4562
Epoch 12 — loss: 1.4528
Epoch 13 — loss: 1.4525
Epoch 14 — loss: 1.4478
Epoch 15 — loss: 1.4544
Epoch 16 — loss: 1.4542
Epoch 17 — loss: 1.4501
Epoch 18 — loss: 1.4383
Epoch 19 — loss: 1.4517
Epoch 20 — loss: 1.4493
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7757
Epoch 02 — loss: 1.6276
Epoch 03 — loss: 1.5666
Epoch 04 — loss: 1.5255
Epoch 05 — loss: 1.4930
Epoch 06 — loss: 1.4630
Epoch 07 — loss: 1.4267
Epoch 08 — loss: 1.3963
Epoch 09 — loss: 1.3650
Epoch 10 — loss: 1.3343
Epoch 11 — loss: 1.3013
Epoch 12 — loss: 1.2698/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 13 — loss: 1.2289
Epoch 14 — loss: 1.1974
Epoch 15 — loss: 1.1655
Epoch 16 — loss: 1.1268
Epoch 17 — loss: 1.0855
Epoch 18 — loss: 1.0467
Epoch 19 — loss: 1.0183
Epoch 20 — loss: 0.9799
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8663
Epoch 02 — loss: 1.5847
Epoch 03 — loss: 1.5316
Epoch 04 — loss: 1.5026
Epoch 05 — loss: 1.4912
Epoch 06 — loss: 1.4837
Epoch 07 — loss: 1.4748
Epoch 08 — loss: 1.4792
Epoch 09 — loss: 1.4655
Epoch 10 — loss: 1.4587
Epoch 11 — loss: 1.4572
Epoch 12 — loss: 1.4552
Epoch 13 — loss: 1.4537
Epoch 14 — loss: 1.4512
Epoch 15 — loss: 1.4400
Epoch 16 — loss: 1.4444
Epoch 17 — loss: 1.4452
Epoch 18 — loss: 1.4433
Epoch 19 — loss: 1.4352
Epoch 20 — loss: 1.4405
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7058
Epoch 02 — loss: 1.6102
Epoch 03 — loss: 1.5805
Epoch 04 — loss: 1.5586
Epoch 05 — loss: 1.5361
Epoch 06 — loss: 1.5075
Epoch 07 — loss: 1.5006
Epoch 08 — loss: 1.4749
Epoch 09 — loss: 1.4583
Epoch 10 — loss: 1.4332
Epoch 11 — loss: 1.4145
Epoch 12 — loss: 1.3931
Epoch 13 — loss: 1.3693
Epoch 14 — loss: 1.3392
Epoch 15 — loss: 1.3214
Epoch 16 — loss: 1.2893
Epoch 17 — loss: 1.2685
Epoch 18 — loss: 1.2370
Epoch 19 — loss: 1.2063
Epoch 20 — loss: 1.1848
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8054
Epoch 02 — loss: 1.6158
Epoch 03 — loss: 1.5713
Epoch 04 — loss: 1.5328
Epoch 05 — loss: 1.5314
Epoch 06 — loss: 1.5143
Epoch 07 — loss: 1.4964
Epoch 08 — loss: 1.4897
Epoch 09 — loss: 1.4827
Epoch 10 — loss: 1.4777
Epoch 11 — loss: 1.4778
Epoch 12 — loss: 1.4639
Epoch 13 — loss: 1.4694
Epoch 14 — loss: 1.4561
Epoch 15 — loss: 1.4582
Epoch 16 — loss: 1.4503
Epoch 17 — loss: 1.4540
Epoch 18 — loss: 1.4468
Epoch 19 — loss: 1.4431
Epoch 20 — loss: 1.4437
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6191
Epoch 02 — loss: 1.3725
Epoch 03 — loss: 1.3126
Stage 1: Error=0.5368, Alpha=1.6442
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7878
Epoch 02 — loss: 1.5826
Epoch 03 — loss: 1.5091
Stage 2: Error=0.6107, Alpha=1.3413
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8468
Epoch 02 — loss: 1.7073
Epoch 03 — loss: 1.6159
Stage 3: Error=0.6721, Alpha=1.0741
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8598
Epoch 02 — loss: 1.7865
Epoch 03 — loss: 1.7104
Stage 4: Error=0.7569, Alpha=0.6559
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8739
Epoch 02 — loss: 1.8014
Epoch 03 — loss: 1.7569
Stage 5: Error=0.7766, Alpha=0.5459
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5680
Epoch 02 — loss: 1.3770
Epoch 03 — loss: 1.3181
Stage 1: Error=0.5425, Alpha=1.6214
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7364
Epoch 02 — loss: 1.5600
Epoch 03 — loss: 1.5044
Stage 2: Error=0.6087, Alpha=1.3497
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8167
Epoch 02 — loss: 1.7044
Epoch 03 — loss: 1.6472
Stage 3: Error=0.6859, Alpha=1.0105
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8445
Epoch 02 — loss: 1.7624
Epoch 03 — loss: 1.7162
Stage 4: Error=0.7231, Alpha=0.8318
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8818
Epoch 02 — loss: 1.8125
Epoch 03 — loss: 1.7580
Stage 5: Error=0.7403, Alpha=0.7444
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6038
Epoch 02 — loss: 1.3420
Epoch 03 — loss: 1.2928
Stage 1: Error=0.5245, Alpha=1.6935
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7891
Epoch 02 — loss: 1.5925
Epoch 03 — loss: 1.5244
Stage 2: Error=0.6549, Alpha=1.1513
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8087
Epoch 02 — loss: 1.6423
Epoch 03 — loss: 1.5775
Stage 3: Error=0.6384, Alpha=1.2232
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8700
Epoch 02 — loss: 1.7766
Epoch 03 — loss: 1.7225
Stage 4: Error=0.7265, Alpha=0.8148
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8828
Epoch 02 — loss: 1.8169
Epoch 03 — loss: 1.7582
Stage 5: Error=0.7995, Alpha=0.4089
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4709
Epoch 02 — loss: 1.3192
Epoch 03 — loss: 1.3126
Stage 1: Error=0.5208, Alpha=1.7084
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6428
Epoch 02 — loss: 1.5530
Epoch 03 — loss: 1.5094
Stage 2: Error=0.6224, Alpha=1.2919
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7501
Epoch 02 — loss: 1.6531
Epoch 03 — loss: 1.5865
Stage 3: Error=0.6573, Alpha=1.1406
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7979
Epoch 02 — loss: 1.6856
Epoch 03 — loss: 1.6272
Stage 4: Error=0.6955, Alpha=0.9656
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7839
Epoch 02 — loss: 1.6876
Epoch 03 — loss: 1.6324
Stage 5: Error=0.6417, Alpha=1.2090
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4418
Epoch 02 — loss: 1.3370
Epoch 03 — loss: 1.3062
Stage 1: Error=0.5345, Alpha=1.6534
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6393
Epoch 02 — loss: 1.5507/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:20:32] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:22:44] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:24:38] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 03 — loss: 1.4920
Stage 2: Error=0.6200, Alpha=1.3023
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7179
Epoch 02 — loss: 1.6357
Epoch 03 — loss: 1.6048
Stage 3: Error=0.6624, Alpha=1.1176
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7774
Epoch 02 — loss: 1.7240
Epoch 03 — loss: 1.6975
Stage 4: Error=0.6975, Alpha=0.9563
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8159
Epoch 02 — loss: 1.7375
Epoch 03 — loss: 1.7111
Stage 5: Error=0.7160, Alpha=0.8670
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4615
Epoch 02 — loss: 1.3370
Epoch 03 — loss: 1.2854
Stage 1: Error=0.5318, Alpha=1.6645
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6548
Epoch 02 — loss: 1.5221
Epoch 03 — loss: 1.4657
Stage 2: Error=0.5971, Alpha=1.3982
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7332
Epoch 02 — loss: 1.6294
Epoch 03 — loss: 1.5791
Stage 3: Error=0.6552, Alpha=1.1496
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7424
Epoch 02 — loss: 1.6522
Epoch 03 — loss: 1.5937
Stage 4: Error=0.6435, Alpha=1.2012
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8034
Epoch 02 — loss: 1.7154
Epoch 03 — loss: 1.6237
Stage 5: Error=0.6441, Alpha=1.1985
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8769
Stacking meta epoch 2: loss=0.7430
Stacking meta epoch 3: loss=0.7115
Stacking meta epoch 4: loss=0.6943
Stacking meta epoch 5: loss=0.6828
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8750
Stacking meta epoch 2: loss=0.7469
Stacking meta epoch 3: loss=0.7162
Stacking meta epoch 4: loss=0.6992
Stacking meta epoch 5: loss=0.6876
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2553
Stacking meta epoch 2: loss=1.2424
Stacking meta epoch 3: loss=1.2405
Stacking meta epoch 4: loss=1.2391
Stacking meta epoch 5: loss=1.2380
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      79.31
1                                deepset  ...                      80.37
2                     set_transformer_xy  ...                      78.68
3                             deepset_xy  ...                      80.61
4               set_transformer_additive  ...                      81.76
5                    deepset_xy_additive  ...                      82.10
6                       adaboost_deepset  ...                      80.99
7           adaboost_deepset_xy_additive  ...                      80.17
8                    adaboost_deepset_xy  ...                      82.10
9               adaboost_set_transformer  ...                      81.86
10     adaboost_set_transformer_additive  ...                      81.09
11           adaboost_set_transformer_xy  ...                      82.05
12              soft_voting_ensemble_all  ...                      82.58
13                 stacking_ensemble_all  ...                      82.48
14                      gbm_ensemble_all  ...                      81.57
15                  xgboost_ensemble_all  ...                      81.95
16  soft_voting_ensemble_set_transformer  ...                      82.05
17     stacking_ensemble_set_transformer  ...                      82.19
18          gbm_ensemble_set_transformer  ...                      81.91
19      xgboost_ensemble_set_transformer  ...                      81.71
20          soft_voting_ensemble_deepset  ...                      81.42
21             stacking_ensemble_deepset  ...                      82.63
22                  gbm_ensemble_deepset  ...                      81.38
23              xgboost_ensemble_deepset  ...                      81.23

[24 rows x 5 columns]
------------------------------------iteration no 3------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7685
Epoch 02 — loss: 1.6278
Epoch 03 — loss: 1.5677
Epoch 04 — loss: 1.5370
Epoch 05 — loss: 1.4981
Epoch 06 — loss: 1.4657
Epoch 07 — loss: 1.4315
Epoch 08 — loss: 1.3955
Epoch 09 — loss: 1.3658
Epoch 10 — loss: 1.3294
Epoch 11 — loss: 1.2952
Epoch 12 — loss: 1.2651
Epoch 13 — loss: 1.2247
Epoch 14 — loss: 1.1890
Epoch 15 — loss: 1.1550
Epoch 16 — loss: 1.1049
Epoch 17 — loss: 1.0833
Epoch 18 — loss: 1.0498
Epoch 19 — loss: 1.0019
Epoch 20 — loss: 0.9671
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.9024
Epoch 02 — loss: 1.6374
Epoch 03 — loss: 1.5442
Epoch 04 — loss: 1.5114
Epoch 05 — loss: 1.5019
Epoch 06 — loss: 1.4889
Epoch 07 — loss: 1.4818
Epoch 08 — loss: 1.4748/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 09 — loss: 1.4670
Epoch 10 — loss: 1.4625
Epoch 11 — loss: 1.4554
Epoch 12 — loss: 1.4545
Epoch 13 — loss: 1.4482
Epoch 14 — loss: 1.4472
Epoch 15 — loss: 1.4484
Epoch 16 — loss: 1.4401
Epoch 17 — loss: 1.4453
Epoch 18 — loss: 1.4418
Epoch 19 — loss: 1.4439
Epoch 20 — loss: 1.4398
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7273
Epoch 02 — loss: 1.6029
Epoch 03 — loss: 1.5529
Epoch 04 — loss: 1.5266
Epoch 05 — loss: 1.5002
Epoch 06 — loss: 1.4732
Epoch 07 — loss: 1.4435
Epoch 08 — loss: 1.4131
Epoch 09 — loss: 1.3813
Epoch 10 — loss: 1.3554
Epoch 11 — loss: 1.3138
Epoch 12 — loss: 1.2929
Epoch 13 — loss: 1.2549
Epoch 14 — loss: 1.2238
Epoch 15 — loss: 1.1930
Epoch 16 — loss: 1.1598
Epoch 17 — loss: 1.1206
Epoch 18 — loss: 1.0929
Epoch 19 — loss: 1.0540
Epoch 20 — loss: 1.0324
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8693
Epoch 02 — loss: 1.6034
Epoch 03 — loss: 1.5403
Epoch 04 — loss: 1.5152
Epoch 05 — loss: 1.4977
Epoch 06 — loss: 1.4908
Epoch 07 — loss: 1.4859
Epoch 08 — loss: 1.4709
Epoch 09 — loss: 1.4634
Epoch 10 — loss: 1.4577
Epoch 11 — loss: 1.4604
Epoch 12 — loss: 1.4546
Epoch 13 — loss: 1.4491
Epoch 14 — loss: 1.4505
Epoch 15 — loss: 1.4464
Epoch 16 — loss: 1.4518
Epoch 17 — loss: 1.4475
Epoch 18 — loss: 1.4441
Epoch 19 — loss: 1.4444
Epoch 20 — loss: 1.4393
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7089
Epoch 02 — loss: 1.6134
Epoch 03 — loss: 1.5767
Epoch 04 — loss: 1.5526
Epoch 05 — loss: 1.5409
Epoch 06 — loss: 1.5055
Epoch 07 — loss: 1.5016
Epoch 08 — loss: 1.4828
Epoch 09 — loss: 1.4636
Epoch 10 — loss: 1.4439
Epoch 11 — loss: 1.4271
Epoch 12 — loss: 1.4107
Epoch 13 — loss: 1.3870
Epoch 14 — loss: 1.3660
Epoch 15 — loss: 1.3340
Epoch 16 — loss: 1.3265
Epoch 17 — loss: 1.2933
Epoch 18 — loss: 1.2624
Epoch 19 — loss: 1.2482
Epoch 20 — loss: 1.2257
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8086
Epoch 02 — loss: 1.6183
Epoch 03 — loss: 1.5719
Epoch 04 — loss: 1.5383
Epoch 05 — loss: 1.5198
Epoch 06 — loss: 1.5023
Epoch 07 — loss: 1.4914
Epoch 08 — loss: 1.4793
Epoch 09 — loss: 1.4809
Epoch 10 — loss: 1.4829
Epoch 11 — loss: 1.4707
Epoch 12 — loss: 1.4679
Epoch 13 — loss: 1.4616
Epoch 14 — loss: 1.4577
Epoch 15 — loss: 1.4538
Epoch 16 — loss: 1.4597
Epoch 17 — loss: 1.4525
Epoch 18 — loss: 1.4539
Epoch 19 — loss: 1.4432
Epoch 20 — loss: 1.4415
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6332
Epoch 02 — loss: 1.3738
Epoch 03 — loss: 1.3188
Stage 1: Error=0.5302, Alpha=1.6708
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8088
Epoch 02 — loss: 1.6154
Epoch 03 — loss: 1.5388
Stage 2: Error=0.6601, Alpha=1.1280
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8083
Epoch 02 — loss: 1.7014
Epoch 03 — loss: 1.6048
Stage 3: Error=0.6658, Alpha=1.1026
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8455
Epoch 02 — loss: 1.7624
Epoch 03 — loss: 1.6894
Stage 4: Error=0.7400, Alpha=0.7458
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8718
Epoch 02 — loss: 1.8106
Epoch 03 — loss: 1.7567
Stage 5: Error=0.7540, Alpha=0.6717
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5621
Epoch 02 — loss: 1.3800
Epoch 03 — loss: 1.3520
Stage 1: Error=0.5427, Alpha=1.6205
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7443
Epoch 02 — loss: 1.5824
Epoch 03 — loss: 1.5441
Stage 2: Error=0.6447, Alpha=1.1959
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7621
Epoch 02 — loss: 1.6393
Epoch 03 — loss: 1.6079
Stage 3: Error=0.6227, Alpha=1.2909
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8411
Epoch 02 — loss: 1.7665
Epoch 03 — loss: 1.7203
Stage 4: Error=0.7514, Alpha=0.6858
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8608
Epoch 02 — loss: 1.7920
Epoch 03 — loss: 1.7473
Stage 5: Error=0.7449, Alpha=0.7202
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6120
Epoch 02 — loss: 1.3432
Epoch 03 — loss: 1.3001
Stage 1: Error=0.5233, Alpha=1.6983
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7860
Epoch 02 — loss: 1.6008
Epoch 03 — loss: 1.5402
Stage 2: Error=0.6693, Alpha=1.0868
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8126
Epoch 02 — loss: 1.6654
Epoch 03 — loss: 1.5895
Stage 3: Error=0.6591, Alpha=1.1326
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8395
Epoch 02 — loss: 1.7423
Epoch 03 — loss: 1.6870
Stage 4: Error=0.7256, Alpha=0.8192
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8610
Epoch 02 — loss: 1.7993
Epoch 03 — loss: 1.7459
Stage 5: Error=0.7609, Alpha=0.6342
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4872
Epoch 02 — loss: 1.3530
Epoch 03 — loss: 1.2986
Stage 1: Error=0.5404, Alpha=1.6297
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6419
Epoch 02 — loss: 1.5310
Epoch 03 — loss: 1.4608
Stage 2: Error=0.6061, Alpha=1.3610
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7447
Epoch 02 — loss: 1.6327
Epoch 03 — loss: 1.5739
Stage 3: Error=0.6342, Alpha=1.2414
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7790
Epoch 02 — loss: 1.6856
Epoch 03 — loss: 1.6276/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:06:22] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:08:44] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:10:38] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Stage 4: Error=0.6451, Alpha=1.1944
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8031
Epoch 02 — loss: 1.6959
Epoch 03 — loss: 1.6099
Stage 5: Error=0.6193, Alpha=1.3050
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4269
Epoch 02 — loss: 1.3446
Epoch 03 — loss: 1.2921
Stage 1: Error=0.5291, Alpha=1.6751
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6310
Epoch 02 — loss: 1.5349
Epoch 03 — loss: 1.4855
Stage 2: Error=0.6124, Alpha=1.3345
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7245
Epoch 02 — loss: 1.6331
Epoch 03 — loss: 1.5874
Stage 3: Error=0.6648, Alpha=1.1072
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7849
Epoch 02 — loss: 1.7062
Epoch 03 — loss: 1.6764
Stage 4: Error=0.7164, Alpha=0.8652
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7949
Epoch 02 — loss: 1.7318
Epoch 03 — loss: 1.6938
Stage 5: Error=0.7061, Alpha=0.9150
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4669
Epoch 02 — loss: 1.3187
Epoch 03 — loss: 1.2921
Stage 1: Error=0.5236, Alpha=1.6973
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6351
Epoch 02 — loss: 1.5230
Epoch 03 — loss: 1.4708
Stage 2: Error=0.6037, Alpha=1.3708
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7263
Epoch 02 — loss: 1.6153
Epoch 03 — loss: 1.5757
Stage 3: Error=0.6476, Alpha=1.1834
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7757
Epoch 02 — loss: 1.6834
Epoch 03 — loss: 1.6348
Stage 4: Error=0.6597, Alpha=1.1299
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8211
Epoch 02 — loss: 1.7216
Epoch 03 — loss: 1.6532
Stage 5: Error=0.6745, Alpha=1.0631
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8900
Stacking meta epoch 2: loss=0.7582
Stacking meta epoch 3: loss=0.7282
Stacking meta epoch 4: loss=0.7114
Stacking meta epoch 5: loss=0.6997
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8828
Stacking meta epoch 2: loss=0.7600
Stacking meta epoch 3: loss=0.7308
Stacking meta epoch 4: loss=0.7142
Stacking meta epoch 5: loss=0.7026
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2581
Stacking meta epoch 2: loss=1.2428
Stacking meta epoch 3: loss=1.2408
Stacking meta epoch 4: loss=1.2392
Stacking meta epoch 5: loss=1.2378
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      78.15
1                                deepset  ...                      78.92
2                     set_transformer_xy  ...                      79.55
3                             deepset_xy  ...                      81.57
4               set_transformer_additive  ...                      79.60
5                    deepset_xy_additive  ...                      80.90
6                       adaboost_deepset  ...                      82.05
7           adaboost_deepset_xy_additive  ...                      79.60
8                    adaboost_deepset_xy  ...                      81.04
9               adaboost_set_transformer  ...                      81.76
10     adaboost_set_transformer_additive  ...                      80.13
11           adaboost_set_transformer_xy  ...                      81.62
12              soft_voting_ensemble_all  ...                      82.72
13                 stacking_ensemble_all  ...                      82.48
14                      gbm_ensemble_all  ...                      82.82
15                  xgboost_ensemble_all  ...                      82.77
16  soft_voting_ensemble_set_transformer  ...                      81.91
17     stacking_ensemble_set_transformer  ...                      82.72
18          gbm_ensemble_set_transformer  ...                      82.87
19      xgboost_ensemble_set_transformer  ...                      83.21
20          soft_voting_ensemble_deepset  ...                      80.61
21             stacking_ensemble_deepset  ...                      82.77
22                  gbm_ensemble_deepset  ...                      80.03
23              xgboost_ensemble_deepset  ...                      81.52

[24 rows x 5 columns]
------------------------------------iteration no 4------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7403
Epoch 02 — loss: 1.6197
Epoch 03 — loss: 1.5705/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 04 — loss: 1.5308
Epoch 05 — loss: 1.5079
Epoch 06 — loss: 1.4641
Epoch 07 — loss: 1.4319
Epoch 08 — loss: 1.4000
Epoch 09 — loss: 1.3677
Epoch 10 — loss: 1.3376
Epoch 11 — loss: 1.3016
Epoch 12 — loss: 1.2677
Epoch 13 — loss: 1.2284
Epoch 14 — loss: 1.2099
Epoch 15 — loss: 1.1590
Epoch 16 — loss: 1.1217
Epoch 17 — loss: 1.0889
Epoch 18 — loss: 1.0512
Epoch 19 — loss: 1.0248
Epoch 20 — loss: 0.9779
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8666
Epoch 02 — loss: 1.6230
Epoch 03 — loss: 1.5480
Epoch 04 — loss: 1.5259
Epoch 05 — loss: 1.5043
Epoch 06 — loss: 1.4918
Epoch 07 — loss: 1.4893
Epoch 08 — loss: 1.4785
Epoch 09 — loss: 1.4736
Epoch 10 — loss: 1.4724
Epoch 11 — loss: 1.4651
Epoch 12 — loss: 1.4631
Epoch 13 — loss: 1.4554
Epoch 14 — loss: 1.4602
Epoch 15 — loss: 1.4514
Epoch 16 — loss: 1.4493
Epoch 17 — loss: 1.4400
Epoch 18 — loss: 1.4403
Epoch 19 — loss: 1.4447
Epoch 20 — loss: 1.4447
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7418
Epoch 02 — loss: 1.6016
Epoch 03 — loss: 1.5595
Epoch 04 — loss: 1.5260
Epoch 05 — loss: 1.4894
Epoch 06 — loss: 1.4571
Epoch 07 — loss: 1.4282
Epoch 08 — loss: 1.4059
Epoch 09 — loss: 1.3710
Epoch 10 — loss: 1.3405
Epoch 11 — loss: 1.3189
Epoch 12 — loss: 1.2768
Epoch 13 — loss: 1.2453
Epoch 14 — loss: 1.2059
Epoch 15 — loss: 1.1716
Epoch 16 — loss: 1.1422
Epoch 17 — loss: 1.1063
Epoch 18 — loss: 1.0637
Epoch 19 — loss: 1.0430
Epoch 20 — loss: 0.9943
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8710
Epoch 02 — loss: 1.5797
Epoch 03 — loss: 1.5359
Epoch 04 — loss: 1.5023
Epoch 05 — loss: 1.4917
Epoch 06 — loss: 1.4797
Epoch 07 — loss: 1.4788
Epoch 08 — loss: 1.4638
Epoch 09 — loss: 1.4613
Epoch 10 — loss: 1.4545
Epoch 11 — loss: 1.4564
Epoch 12 — loss: 1.4555
Epoch 13 — loss: 1.4512
Epoch 14 — loss: 1.4453
Epoch 15 — loss: 1.4401
Epoch 16 — loss: 1.4397
Epoch 17 — loss: 1.4428
Epoch 18 — loss: 1.4326
Epoch 19 — loss: 1.4354
Epoch 20 — loss: 1.4278
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.6989
Epoch 02 — loss: 1.6046
Epoch 03 — loss: 1.5679
Epoch 04 — loss: 1.5430
Epoch 05 — loss: 1.5131
Epoch 06 — loss: 1.4912
Epoch 07 — loss: 1.4670
Epoch 08 — loss: 1.4491
Epoch 09 — loss: 1.4152
Epoch 10 — loss: 1.3956
Epoch 11 — loss: 1.3743
Epoch 12 — loss: 1.3480
Epoch 13 — loss: 1.3218
Epoch 14 — loss: 1.2931
Epoch 15 — loss: 1.2777
Epoch 16 — loss: 1.2488
Epoch 17 — loss: 1.2268
Epoch 18 — loss: 1.1894
Epoch 19 — loss: 1.1789
Epoch 20 — loss: 1.1443
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7915
Epoch 02 — loss: 1.6187
Epoch 03 — loss: 1.5687
Epoch 04 — loss: 1.5468
Epoch 05 — loss: 1.5269
Epoch 06 — loss: 1.5100
Epoch 07 — loss: 1.5043
Epoch 08 — loss: 1.4869
Epoch 09 — loss: 1.4812
Epoch 10 — loss: 1.4786
Epoch 11 — loss: 1.4738
Epoch 12 — loss: 1.4668
Epoch 13 — loss: 1.4629
Epoch 14 — loss: 1.4626
Epoch 15 — loss: 1.4566
Epoch 16 — loss: 1.4605
Epoch 17 — loss: 1.4476
Epoch 18 — loss: 1.4512
Epoch 19 — loss: 1.4481
Epoch 20 — loss: 1.4481
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6178
Epoch 02 — loss: 1.3690
Epoch 03 — loss: 1.3266
Stage 1: Error=0.5353, Alpha=1.6505
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7992
Epoch 02 — loss: 1.5905
Epoch 03 — loss: 1.5116
Stage 2: Error=0.6576, Alpha=1.1390
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8372
Epoch 02 — loss: 1.6920
Epoch 03 — loss: 1.5963
Stage 3: Error=0.6780, Alpha=1.0472
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8669
Epoch 02 — loss: 1.7607
Epoch 03 — loss: 1.6981
Stage 4: Error=0.7232, Alpha=0.8312
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8817
Epoch 02 — loss: 1.8136
Epoch 03 — loss: 1.7422
Stage 5: Error=0.7741, Alpha=0.5602
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5556
Epoch 02 — loss: 1.3931
Epoch 03 — loss: 1.3426
Stage 1: Error=0.5353, Alpha=1.6505
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7382
Epoch 02 — loss: 1.5836
Epoch 03 — loss: 1.5403
Stage 2: Error=0.6191, Alpha=1.3060
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8208
Epoch 02 — loss: 1.6951
Epoch 03 — loss: 1.6343
Stage 3: Error=0.6837, Alpha=1.0211
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8443
Epoch 02 — loss: 1.7599
Epoch 03 — loss: 1.7110
Stage 4: Error=0.7459, Alpha=0.7151
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8627
Epoch 02 — loss: 1.7918
Epoch 03 — loss: 1.7480
Stage 5: Error=0.7625, Alpha=0.6251
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6315
Epoch 02 — loss: 1.3589
Epoch 03 — loss: 1.3017
Stage 1: Error=0.5277, Alpha=1.6809
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7812
Epoch 02 — loss: 1.5931
Epoch 03 — loss: 1.5323
Stage 2: Error=0.6414, Alpha=1.2105
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8245
Epoch 02 — loss: 1.6549
Epoch 03 — loss: 1.5926
Stage 3: Error=0.6574, Alpha=1.1400
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8644
Epoch 02 — loss: 1.7414
Epoch 03 — loss: 1.6994
Stage 4: Error=0.7262, Alpha=0.8163
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8764
Epoch 02 — loss: 1.8088
Epoch 03 — loss: 1.7468
Stage 5: Error=0.7888, Alpha=0.4739
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:52:18] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:54:33] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:56:35] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4666
Epoch 02 — loss: 1.3508
Epoch 03 — loss: 1.3013
Stage 1: Error=0.5223, Alpha=1.7027
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6680
Epoch 02 — loss: 1.5648
Epoch 03 — loss: 1.5009
Stage 2: Error=0.6156, Alpha=1.3210
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7271
Epoch 02 — loss: 1.6329
Epoch 03 — loss: 1.5612
Stage 3: Error=0.6405, Alpha=1.2144
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7774
Epoch 02 — loss: 1.6832
Epoch 03 — loss: 1.6100
Stage 4: Error=0.6426, Alpha=1.2051
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8236
Epoch 02 — loss: 1.7354
Epoch 03 — loss: 1.6701
Stage 5: Error=0.6656, Alpha=1.1032
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4274
Epoch 02 — loss: 1.3262
Epoch 03 — loss: 1.3041
Stage 1: Error=0.5258, Alpha=1.6887
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6521
Epoch 02 — loss: 1.5571
Epoch 03 — loss: 1.5218
Stage 2: Error=0.6605, Alpha=1.1262
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6583
Epoch 02 — loss: 1.5958
Epoch 03 — loss: 1.5686
Stage 3: Error=0.6260, Alpha=1.2768
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7738
Epoch 02 — loss: 1.7180
Epoch 03 — loss: 1.6717
Stage 4: Error=0.7244, Alpha=0.8253
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7875
Epoch 02 — loss: 1.7418
Epoch 03 — loss: 1.6758
Stage 5: Error=0.7004, Alpha=0.9427
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4420
Epoch 02 — loss: 1.3165
Epoch 03 — loss: 1.2748
Stage 1: Error=0.5137, Alpha=1.7369
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6574
Epoch 02 — loss: 1.5588
Epoch 03 — loss: 1.4949
Stage 2: Error=0.6230, Alpha=1.2897
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7041
Epoch 02 — loss: 1.6154
Epoch 03 — loss: 1.5373
Stage 3: Error=0.6269, Alpha=1.2729
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7818
Epoch 02 — loss: 1.6762
Epoch 03 — loss: 1.6051
Stage 4: Error=0.6475, Alpha=1.1838
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8003
Epoch 02 — loss: 1.7156
Epoch 03 — loss: 1.6564
Stage 5: Error=0.6980, Alpha=0.9541
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8792
Stacking meta epoch 2: loss=0.7425
Stacking meta epoch 3: loss=0.7093
Stacking meta epoch 4: loss=0.6911
Stacking meta epoch 5: loss=0.6788
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8817
Stacking meta epoch 2: loss=0.7488
Stacking meta epoch 3: loss=0.7145
Stacking meta epoch 4: loss=0.6956
Stacking meta epoch 5: loss=0.6830
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2572
Stacking meta epoch 2: loss=1.2418
Stacking meta epoch 3: loss=1.2392
Stacking meta epoch 4: loss=1.2375
Stacking meta epoch 5: loss=1.2361
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      79.69
1                                deepset  ...                      79.21
2                     set_transformer_xy  ...                      76.66
3                             deepset_xy  ...                      82.19
4               set_transformer_additive  ...                      81.33
5                    deepset_xy_additive  ...                      83.59
6                       adaboost_deepset  ...                      81.76
7           adaboost_deepset_xy_additive  ...                      80.27
8                    adaboost_deepset_xy  ...                      81.33
9               adaboost_set_transformer  ...                      81.33
10     adaboost_set_transformer_additive  ...                      80.80
11           adaboost_set_transformer_xy  ...                      81.14
12              soft_voting_ensemble_all  ...                      82.53
13                 stacking_ensemble_all  ...                      83.11
14                      gbm_ensemble_all  ...                      83.11
15                  xgboost_ensemble_all  ...                      83.21
16  soft_voting_ensemble_set_transformer  ...                      81.18
17     stacking_ensemble_set_transformer  ...                      82.77
18          gbm_ensemble_set_transformer  ...                      83.01
19      xgboost_ensemble_set_transformer  ...                      83.16
20          soft_voting_ensemble_deepset  ...                      82.34
21             stacking_ensemble_deepset  ...                      82.19
22                  gbm_ensemble_deepset  ...                      80.75
23              xgboost_ensemble_deepset  ...                      81.42

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 5------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7602
Epoch 02 — loss: 1.6226
Epoch 03 — loss: 1.5593
Epoch 04 — loss: 1.5256
Epoch 05 — loss: 1.4881
Epoch 06 — loss: 1.4515
Epoch 07 — loss: 1.4167
Epoch 08 — loss: 1.3822
Epoch 09 — loss: 1.3615
Epoch 10 — loss: 1.3258
Epoch 11 — loss: 1.3017
Epoch 12 — loss: 1.2749
Epoch 13 — loss: 1.2343
Epoch 14 — loss: 1.2125
Epoch 15 — loss: 1.1811
Epoch 16 — loss: 1.1377
Epoch 17 — loss: 1.0959
Epoch 18 — loss: 1.0618
Epoch 19 — loss: 1.0460
Epoch 20 — loss: 0.9872
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8631
Epoch 02 — loss: 1.6080
Epoch 03 — loss: 1.5601
Epoch 04 — loss: 1.5200
Epoch 05 — loss: 1.5019
Epoch 06 — loss: 1.4948
Epoch 07 — loss: 1.4832
Epoch 08 — loss: 1.4734
Epoch 09 — loss: 1.4676
Epoch 10 — loss: 1.4702
Epoch 11 — loss: 1.4642
Epoch 12 — loss: 1.4623
Epoch 13 — loss: 1.4627
Epoch 14 — loss: 1.4495
Epoch 15 — loss: 1.4524
Epoch 16 — loss: 1.4530
Epoch 17 — loss: 1.4491
Epoch 18 — loss: 1.4469
Epoch 19 — loss: 1.4489
Epoch 20 — loss: 1.4497
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7187
Epoch 02 — loss: 1.5838
Epoch 03 — loss: 1.5313
Epoch 04 — loss: 1.5005
Epoch 05 — loss: 1.4759
Epoch 06 — loss: 1.4476
Epoch 07 — loss: 1.4204
Epoch 08 — loss: 1.3777
Epoch 09 — loss: 1.3558
Epoch 10 — loss: 1.3376
Epoch 11 — loss: 1.3008
Epoch 12 — loss: 1.2619
Epoch 13 — loss: 1.2206
Epoch 14 — loss: 1.1980
Epoch 15 — loss: 1.1737
Epoch 16 — loss: 1.1289
Epoch 17 — loss: 1.0941
Epoch 18 — loss: 1.0585
Epoch 19 — loss: 1.0255
Epoch 20 — loss: 1.0036
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8778
Epoch 02 — loss: 1.6042
Epoch 03 — loss: 1.5469
Epoch 04 — loss: 1.5227
Epoch 05 — loss: 1.5089
Epoch 06 — loss: 1.4946
Epoch 07 — loss: 1.4846
Epoch 08 — loss: 1.4741
Epoch 09 — loss: 1.4695
Epoch 10 — loss: 1.4695
Epoch 11 — loss: 1.4588
Epoch 12 — loss: 1.4544
Epoch 13 — loss: 1.4562
Epoch 14 — loss: 1.4486
Epoch 15 — loss: 1.4514
Epoch 16 — loss: 1.4580
Epoch 17 — loss: 1.4485
Epoch 18 — loss: 1.4452
Epoch 19 — loss: 1.4445
Epoch 20 — loss: 1.4460
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.6892
Epoch 02 — loss: 1.5982
Epoch 03 — loss: 1.5647
Epoch 04 — loss: 1.5338
Epoch 05 — loss: 1.5234
Epoch 06 — loss: 1.4989
Epoch 07 — loss: 1.4716
Epoch 08 — loss: 1.4577
Epoch 09 — loss: 1.4506
Epoch 10 — loss: 1.4269
Epoch 11 — loss: 1.3960
Epoch 12 — loss: 1.3859
Epoch 13 — loss: 1.3650
Epoch 14 — loss: 1.3454
Epoch 15 — loss: 1.3152
Epoch 16 — loss: 1.2871
Epoch 17 — loss: 1.2732
Epoch 18 — loss: 1.2557
Epoch 19 — loss: 1.2295
Epoch 20 — loss: 1.1947
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8142
Epoch 02 — loss: 1.6044
Epoch 03 — loss: 1.5680
Epoch 04 — loss: 1.5466
Epoch 05 — loss: 1.5314
Epoch 06 — loss: 1.5054
Epoch 07 — loss: 1.4984
Epoch 08 — loss: 1.4962
Epoch 09 — loss: 1.4787
Epoch 10 — loss: 1.4711
Epoch 11 — loss: 1.4738
Epoch 12 — loss: 1.4659
Epoch 13 — loss: 1.4664
Epoch 14 — loss: 1.4632
Epoch 15 — loss: 1.4549
Epoch 16 — loss: 1.4532
Epoch 17 — loss: 1.4571
Epoch 18 — loss: 1.4568
Epoch 19 — loss: 1.4488
Epoch 20 — loss: 1.4460
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6486
Epoch 02 — loss: 1.4282
Epoch 03 — loss: 1.3243
Stage 1: Error=0.5239, Alpha=1.6959
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7817
Epoch 02 — loss: 1.6063
Epoch 03 — loss: 1.5512
Stage 2: Error=0.6733, Alpha=1.0685
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8159
Epoch 02 — loss: 1.6432
Epoch 03 — loss: 1.5776
Stage 3: Error=0.6433, Alpha=1.2022
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8721
Epoch 02 — loss: 1.8036
Epoch 03 — loss: 1.7204
Stage 4: Error=0.7481, Alpha=0.7033
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8767
Epoch 02 — loss: 1.8103
Epoch 03 — loss: 1.7514
Stage 5: Error=0.7533, Alpha=0.6757
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5604
Epoch 02 — loss: 1.3754
Epoch 03 — loss: 1.3235
Stage 1: Error=0.5345, Alpha=1.6534
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7324
Epoch 02 — loss: 1.5981
Epoch 03 — loss: 1.5265
Stage 2: Error=0.6553, Alpha=1.1492
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7980
Epoch 02 — loss: 1.6469
Epoch 03 — loss: 1.5891
Stage 3: Error=0.6360, Alpha=1.2338
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8598
Epoch 02 — loss: 1.7562
Epoch 03 — loss: 1.6973
Stage 4: Error=0.7310, Alpha=0.7921
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8788
Epoch 02 — loss: 1.7945
Epoch 03 — loss: 1.7491
Stage 5: Error=0.7400, Alpha=0.7459
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6027
Epoch 02 — loss: 1.3670
Epoch 03 — loss: 1.3193
Stage 1: Error=0.5256, Alpha=1.6891
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8031
Epoch 02 — loss: 1.6015
Epoch 03 — loss: 1.5235
Stage 2: Error=0.6719, Alpha=1.0751
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8034
Epoch 02 — loss: 1.6566
Epoch 03 — loss: 1.5700
Stage 3: Error=0.6598, Alpha=1.1296
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8615
Epoch 02 — loss: 1.7622
Epoch 03 — loss: 1.7012
Stage 4: Error=0.7473, Alpha=0.7073
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8663/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [04:36:59] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [04:39:15] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [04:41:09] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8004
Epoch 03 — loss: 1.7466
Stage 5: Error=0.7787, Alpha=0.5339
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4490
Epoch 02 — loss: 1.3487
Epoch 03 — loss: 1.2958
Stage 1: Error=0.5274, Alpha=1.6819
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6553
Epoch 02 — loss: 1.5443
Epoch 03 — loss: 1.4814
Stage 2: Error=0.5927, Alpha=1.4165
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7268
Epoch 02 — loss: 1.6238
Epoch 03 — loss: 1.5664
Stage 3: Error=0.6471, Alpha=1.1854
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7973
Epoch 02 — loss: 1.6927
Epoch 03 — loss: 1.6244
Stage 4: Error=0.6659, Alpha=1.1018
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8014
Epoch 02 — loss: 1.7285
Epoch 03 — loss: 1.6603
Stage 5: Error=0.6651, Alpha=1.1057
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4571
Epoch 02 — loss: 1.3636
Epoch 03 — loss: 1.3263
Stage 1: Error=0.5344, Alpha=1.6539
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6552
Epoch 02 — loss: 1.5751
Epoch 03 — loss: 1.5493
Stage 2: Error=0.6379, Alpha=1.2253
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7151
Epoch 02 — loss: 1.6331
Epoch 03 — loss: 1.5856
Stage 3: Error=0.6628, Alpha=1.1160
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7622
Epoch 02 — loss: 1.7071
Epoch 03 — loss: 1.6629
Stage 4: Error=0.7095, Alpha=0.8989
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8087
Epoch 02 — loss: 1.7432
Epoch 03 — loss: 1.7029
Stage 5: Error=0.7181, Alpha=0.8567
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4358
Epoch 02 — loss: 1.3152
Epoch 03 — loss: 1.2881
Stage 1: Error=0.5301, Alpha=1.6713
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6469
Epoch 02 — loss: 1.5486
Epoch 03 — loss: 1.5013
Stage 2: Error=0.6234, Alpha=1.2880
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7338
Epoch 02 — loss: 1.6172
Epoch 03 — loss: 1.5635
Stage 3: Error=0.6250, Alpha=1.2808
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7925
Epoch 02 — loss: 1.6938
Epoch 03 — loss: 1.6182
Stage 4: Error=0.6384, Alpha=1.2232
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8054
Epoch 02 — loss: 1.7088
Epoch 03 — loss: 1.6503
Stage 5: Error=0.6835, Alpha=1.0219
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8825
Stacking meta epoch 2: loss=0.7601
Stacking meta epoch 3: loss=0.7314
Stacking meta epoch 4: loss=0.7145
Stacking meta epoch 5: loss=0.7025
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8744
Stacking meta epoch 2: loss=0.7611
Stacking meta epoch 3: loss=0.7320
Stacking meta epoch 4: loss=0.7149
Stacking meta epoch 5: loss=0.7028
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2616
Stacking meta epoch 2: loss=1.2479
Stacking meta epoch 3: loss=1.2455
Stacking meta epoch 4: loss=1.2438
Stacking meta epoch 5: loss=1.2425
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      80.75
1                                deepset  ...                      80.90
2                     set_transformer_xy  ...                      81.71
3                             deepset_xy  ...                      79.55
4               set_transformer_additive  ...                      78.06
5                    deepset_xy_additive  ...                      81.52
6                       adaboost_deepset  ...                      81.91
7           adaboost_deepset_xy_additive  ...                      80.70
8                    adaboost_deepset_xy  ...                      82.10
9               adaboost_set_transformer  ...                      80.56
10     adaboost_set_transformer_additive  ...                      81.28
11           adaboost_set_transformer_xy  ...                      81.86
12              soft_voting_ensemble_all  ...                      82.53
13                 stacking_ensemble_all  ...                      84.36
14                      gbm_ensemble_all  ...                      84.55
15                  xgboost_ensemble_all  ...                      84.31
16  soft_voting_ensemble_set_transformer  ...                      82.39
17     stacking_ensemble_set_transformer  ...                      84.07
18          gbm_ensemble_set_transformer  ...                      84.50
19      xgboost_ensemble_set_transformer  ...                      84.17
20          soft_voting_ensemble_deepset  ...                      81.14
21             stacking_ensemble_deepset  ...                      82.82
22                  gbm_ensemble_deepset  ...                      80.90
23              xgboost_ensemble_deepset  ...                      80.32

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 6------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7648
Epoch 02 — loss: 1.6257
Epoch 03 — loss: 1.5807
Epoch 04 — loss: 1.5457
Epoch 05 — loss: 1.5073
Epoch 06 — loss: 1.4849
Epoch 07 — loss: 1.4469
Epoch 08 — loss: 1.4171
Epoch 09 — loss: 1.3803
Epoch 10 — loss: 1.3511
Epoch 11 — loss: 1.3107
Epoch 12 — loss: 1.2843
Epoch 13 — loss: 1.2475
Epoch 14 — loss: 1.2115
Epoch 15 — loss: 1.1716
Epoch 16 — loss: 1.1365
Epoch 17 — loss: 1.0996
Epoch 18 — loss: 1.0688
Epoch 19 — loss: 1.0163
Epoch 20 — loss: 0.9772
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8846
Epoch 02 — loss: 1.6208
Epoch 03 — loss: 1.5434
Epoch 04 — loss: 1.5191
Epoch 05 — loss: 1.5015
Epoch 06 — loss: 1.4947
Epoch 07 — loss: 1.4791
Epoch 08 — loss: 1.4747
Epoch 09 — loss: 1.4649
Epoch 10 — loss: 1.4714
Epoch 11 — loss: 1.4618
Epoch 12 — loss: 1.4537
Epoch 13 — loss: 1.4534
Epoch 14 — loss: 1.4463
Epoch 15 — loss: 1.4505
Epoch 16 — loss: 1.4488
Epoch 17 — loss: 1.4545
Epoch 18 — loss: 1.4350
Epoch 19 — loss: 1.4428
Epoch 20 — loss: 1.4410
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7261
Epoch 02 — loss: 1.5927
Epoch 03 — loss: 1.5495
Epoch 04 — loss: 1.5187
Epoch 05 — loss: 1.4883
Epoch 06 — loss: 1.4610
Epoch 07 — loss: 1.4360
Epoch 08 — loss: 1.4049
Epoch 09 — loss: 1.3623
Epoch 10 — loss: 1.3505
Epoch 11 — loss: 1.3117
Epoch 12 — loss: 1.2836
Epoch 13 — loss: 1.2479
Epoch 14 — loss: 1.2114
Epoch 15 — loss: 1.1699
Epoch 16 — loss: 1.1427
Epoch 17 — loss: 1.1105
Epoch 18 — loss: 1.0760
Epoch 19 — loss: 1.0312
Epoch 20 — loss: 0.9951
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8641
Epoch 02 — loss: 1.5918
Epoch 03 — loss: 1.5224
Epoch 04 — loss: 1.5061
Epoch 05 — loss: 1.4830
Epoch 06 — loss: 1.4717
Epoch 07 — loss: 1.4727
Epoch 08 — loss: 1.4626
Epoch 09 — loss: 1.4615
Epoch 10 — loss: 1.4594
Epoch 11 — loss: 1.4580
Epoch 12 — loss: 1.4593
Epoch 13 — loss: 1.4509
Epoch 14 — loss: 1.4474
Epoch 15 — loss: 1.4468
Epoch 16 — loss: 1.4421
Epoch 17 — loss: 1.4423
Epoch 18 — loss: 1.4440
Epoch 19 — loss: 1.4425
Epoch 20 — loss: 1.4345
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7328
Epoch 02 — loss: 1.6237
Epoch 03 — loss: 1.5900
Epoch 04 — loss: 1.5628
Epoch 05 — loss: 1.5270
Epoch 06 — loss: 1.5025
Epoch 07 — loss: 1.4996
Epoch 08 — loss: 1.4612
Epoch 09 — loss: 1.4469
Epoch 10 — loss: 1.4276
Epoch 11 — loss: 1.4091
Epoch 12 — loss: 1.3721
Epoch 13 — loss: 1.3465
Epoch 14 — loss: 1.3257
Epoch 15 — loss: 1.2965
Epoch 16 — loss: 1.2615
Epoch 17 — loss: 1.2342
Epoch 18 — loss: 1.1982
Epoch 19 — loss: 1.1755
Epoch 20 — loss: 1.1434
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7954
Epoch 02 — loss: 1.6150
Epoch 03 — loss: 1.5733
Epoch 04 — loss: 1.5384
Epoch 05 — loss: 1.5229
Epoch 06 — loss: 1.5120
Epoch 07 — loss: 1.4981
Epoch 08 — loss: 1.4868
Epoch 09 — loss: 1.4884
Epoch 10 — loss: 1.4770
Epoch 11 — loss: 1.4739
Epoch 12 — loss: 1.4657
Epoch 13 — loss: 1.4625
Epoch 14 — loss: 1.4605
Epoch 15 — loss: 1.4554
Epoch 16 — loss: 1.4522
Epoch 17 — loss: 1.4504
Epoch 18 — loss: 1.4562
Epoch 19 — loss: 1.4522
Epoch 20 — loss: 1.4464
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6661
Epoch 02 — loss: 1.3893
Epoch 03 — loss: 1.2905
Stage 1: Error=0.5304, Alpha=1.6698
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7846
Epoch 02 — loss: 1.5897
Epoch 03 — loss: 1.5069
Stage 2: Error=0.6188, Alpha=1.3072
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8326
Epoch 02 — loss: 1.6971
Epoch 03 — loss: 1.6203
Stage 3: Error=0.6737, Alpha=1.0668
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8659
Epoch 02 — loss: 1.7790
Epoch 03 — loss: 1.7043
Stage 4: Error=0.7180, Alpha=0.8570
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8765
Epoch 02 — loss: 1.8100
Epoch 03 — loss: 1.7644
Stage 5: Error=0.7724, Alpha=0.5699
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5503
Epoch 02 — loss: 1.3775
Epoch 03 — loss: 1.3400
Stage 1: Error=0.5387, Alpha=1.6365
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7427
Epoch 02 — loss: 1.5794
Epoch 03 — loss: 1.5335
Stage 2: Error=0.6515, Alpha=1.1660
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7621
Epoch 02 — loss: 1.6163
Epoch 03 — loss: 1.5767
Stage 3: Error=0.6368, Alpha=1.2302
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8399
Epoch 02 — loss: 1.7495
Epoch 03 — loss: 1.7031
Stage 4: Error=0.7210, Alpha=0.8422
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8472
Epoch 02 — loss: 1.7925
Epoch 03 — loss: 1.7473
Stage 5: Error=0.7485, Alpha=0.7011
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6531
Epoch 02 — loss: 1.3986
Epoch 03 — loss: 1.3192
Stage 1: Error=0.5496, Alpha=1.5928
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8137
Epoch 02 — loss: 1.6065
Epoch 03 — loss: 1.5240
Stage 2: Error=0.6171, Alpha=1.3143
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8375
Epoch 02 — loss: 1.6842
Epoch 03 — loss: 1.6130
Stage 3: Error=0.6654, Alpha=1.1043
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8576
Epoch 02 — loss: 1.7846
Epoch 03 — loss: 1.7049
Stage 4: Error=0.7684, Alpha=0.5925
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8823/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [05:23:07] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [05:25:21] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [05:27:16] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8105
Epoch 03 — loss: 1.7376
Stage 5: Error=0.8048, Alpha=0.3753
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4669
Epoch 02 — loss: 1.3556
Epoch 03 — loss: 1.3042
Stage 1: Error=0.5177, Alpha=1.7210
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6583
Epoch 02 — loss: 1.5336
Epoch 03 — loss: 1.4862
Stage 2: Error=0.6056, Alpha=1.3630
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7269
Epoch 02 — loss: 1.6363
Epoch 03 — loss: 1.5768
Stage 3: Error=0.6649, Alpha=1.1064
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7670
Epoch 02 — loss: 1.6742
Epoch 03 — loss: 1.6160
Stage 4: Error=0.6609, Alpha=1.1243
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8184
Epoch 02 — loss: 1.7382
Epoch 03 — loss: 1.6527
Stage 5: Error=0.6579, Alpha=1.1379
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4025
Epoch 02 — loss: 1.3277
Epoch 03 — loss: 1.3236
Stage 1: Error=0.5286, Alpha=1.6771
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6459
Epoch 02 — loss: 1.5534
Epoch 03 — loss: 1.5238
Stage 2: Error=0.6492, Alpha=1.1762
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7279
Epoch 02 — loss: 1.6534
Epoch 03 — loss: 1.6149
Stage 3: Error=0.6658, Alpha=1.1025
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7893
Epoch 02 — loss: 1.6962
Epoch 03 — loss: 1.6563
Stage 4: Error=0.6802, Alpha=1.0369
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8165
Epoch 02 — loss: 1.7556
Epoch 03 — loss: 1.7183
Stage 5: Error=0.7230, Alpha=0.8322
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4628
Epoch 02 — loss: 1.3410
Epoch 03 — loss: 1.3176
Stage 1: Error=0.5288, Alpha=1.6766
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6583
Epoch 02 — loss: 1.5243
Epoch 03 — loss: 1.4739
Stage 2: Error=0.6268, Alpha=1.2731
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6838
Epoch 02 — loss: 1.5577
Epoch 03 — loss: 1.5276
Stage 3: Error=0.6035, Alpha=1.3715
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8297
Epoch 02 — loss: 1.7145
Epoch 03 — loss: 1.6566
Stage 4: Error=0.6626, Alpha=1.1170
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8133
Epoch 02 — loss: 1.7187
Epoch 03 — loss: 1.6393
Stage 5: Error=0.6678, Alpha=1.0934
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8719
Stacking meta epoch 2: loss=0.7398
Stacking meta epoch 3: loss=0.7094
Stacking meta epoch 4: loss=0.6924
Stacking meta epoch 5: loss=0.6807
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8648
Stacking meta epoch 2: loss=0.7404
Stacking meta epoch 3: loss=0.7099
Stacking meta epoch 4: loss=0.6924
Stacking meta epoch 5: loss=0.6804
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2598
Stacking meta epoch 2: loss=1.2432
Stacking meta epoch 3: loss=1.2407
Stacking meta epoch 4: loss=1.2389
Stacking meta epoch 5: loss=1.2375
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      77.33
1                                deepset  ...                      80.65
2                     set_transformer_xy  ...                      79.02
3                             deepset_xy  ...                      80.75
4               set_transformer_additive  ...                      76.85
5                    deepset_xy_additive  ...                      79.98
6                       adaboost_deepset  ...                      82.34
7           adaboost_deepset_xy_additive  ...                      78.87
8                    adaboost_deepset_xy  ...                      81.09
9               adaboost_set_transformer  ...                      80.94
10     adaboost_set_transformer_additive  ...                      80.41
11           adaboost_set_transformer_xy  ...                      81.76
12              soft_voting_ensemble_all  ...                      81.42
13                 stacking_ensemble_all  ...                      84.17
14                      gbm_ensemble_all  ...                      81.71
15                  xgboost_ensemble_all  ...                      81.81
16  soft_voting_ensemble_set_transformer  ...                      80.17
17     stacking_ensemble_set_transformer  ...                      83.78
18          gbm_ensemble_set_transformer  ...                      81.76
19      xgboost_ensemble_set_transformer  ...                      82.10
20          soft_voting_ensemble_deepset  ...                      80.90
21             stacking_ensemble_deepset  ...                      82.58
22                  gbm_ensemble_deepset  ...                      80.56
23              xgboost_ensemble_deepset  ...                      80.32

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 7------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7437
Epoch 02 — loss: 1.6121
Epoch 03 — loss: 1.5701
Epoch 04 — loss: 1.5348
Epoch 05 — loss: 1.4978
Epoch 06 — loss: 1.4676
Epoch 07 — loss: 1.4448
Epoch 08 — loss: 1.4062
Epoch 09 — loss: 1.3881
Epoch 10 — loss: 1.3460
Epoch 11 — loss: 1.3121
Epoch 12 — loss: 1.2882
Epoch 13 — loss: 1.2529
Epoch 14 — loss: 1.2132
Epoch 15 — loss: 1.1679
Epoch 16 — loss: 1.1462
Epoch 17 — loss: 1.0978
Epoch 18 — loss: 1.0705
Epoch 19 — loss: 1.0196
Epoch 20 — loss: 0.9913
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8878
Epoch 02 — loss: 1.6084
Epoch 03 — loss: 1.5407
Epoch 04 — loss: 1.5168
Epoch 05 — loss: 1.5042
Epoch 06 — loss: 1.4938
Epoch 07 — loss: 1.4830
Epoch 08 — loss: 1.4758
Epoch 09 — loss: 1.4702
Epoch 10 — loss: 1.4632
Epoch 11 — loss: 1.4609
Epoch 12 — loss: 1.4586
Epoch 13 — loss: 1.4525
Epoch 14 — loss: 1.4542
Epoch 15 — loss: 1.4472
Epoch 16 — loss: 1.4473
Epoch 17 — loss: 1.4390
Epoch 18 — loss: 1.4437
Epoch 19 — loss: 1.4375
Epoch 20 — loss: 1.4363
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7205
Epoch 02 — loss: 1.5925
Epoch 03 — loss: 1.5521
Epoch 04 — loss: 1.5204
Epoch 05 — loss: 1.4924
Epoch 06 — loss: 1.4680
Epoch 07 — loss: 1.4398
Epoch 08 — loss: 1.3961
Epoch 09 — loss: 1.3804
Epoch 10 — loss: 1.3513
Epoch 11 — loss: 1.3159
Epoch 12 — loss: 1.2831
Epoch 13 — loss: 1.2442
Epoch 14 — loss: 1.2160
Epoch 15 — loss: 1.1805
Epoch 16 — loss: 1.1475
Epoch 17 — loss: 1.1143
Epoch 18 — loss: 1.0810
Epoch 19 — loss: 1.0350
Epoch 20 — loss: 1.0032
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8601
Epoch 02 — loss: 1.5841
Epoch 03 — loss: 1.5362
Epoch 04 — loss: 1.5138
Epoch 05 — loss: 1.5006
Epoch 06 — loss: 1.4954
Epoch 07 — loss: 1.4783
Epoch 08 — loss: 1.4703
Epoch 09 — loss: 1.4687
Epoch 10 — loss: 1.4599
Epoch 11 — loss: 1.4545
Epoch 12 — loss: 1.4597
Epoch 13 — loss: 1.4532
Epoch 14 — loss: 1.4499
Epoch 15 — loss: 1.4422
Epoch 16 — loss: 1.4516
Epoch 17 — loss: 1.4458
Epoch 18 — loss: 1.4419
Epoch 19 — loss: 1.4414
Epoch 20 — loss: 1.4329
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7164
Epoch 02 — loss: 1.6255
Epoch 03 — loss: 1.5820
Epoch 04 — loss: 1.5562
Epoch 05 — loss: 1.5364
Epoch 06 — loss: 1.5117
Epoch 07 — loss: 1.4870
Epoch 08 — loss: 1.4772
Epoch 09 — loss: 1.4427
Epoch 10 — loss: 1.4251
Epoch 11 — loss: 1.3961
Epoch 12 — loss: 1.3710
Epoch 13 — loss: 1.3636
Epoch 14 — loss: 1.3326
Epoch 15 — loss: 1.3046
Epoch 16 — loss: 1.2759
Epoch 17 — loss: 1.2632
Epoch 18 — loss: 1.2223
Epoch 19 — loss: 1.1951
Epoch 20 — loss: 1.1663
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8045
Epoch 02 — loss: 1.6180
Epoch 03 — loss: 1.5742
Epoch 04 — loss: 1.5447
Epoch 05 — loss: 1.5293
Epoch 06 — loss: 1.5133
Epoch 07 — loss: 1.4977
Epoch 08 — loss: 1.4802
Epoch 09 — loss: 1.4864
Epoch 10 — loss: 1.4750
Epoch 11 — loss: 1.4672
Epoch 12 — loss: 1.4738
Epoch 13 — loss: 1.4665
Epoch 14 — loss: 1.4593
Epoch 15 — loss: 1.4580
Epoch 16 — loss: 1.4525
Epoch 17 — loss: 1.4557
Epoch 18 — loss: 1.4430
Epoch 19 — loss: 1.4541
Epoch 20 — loss: 1.4396
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6420
Epoch 02 — loss: 1.3769
Epoch 03 — loss: 1.2840
Stage 1: Error=0.5265, Alpha=1.6858
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7978
Epoch 02 — loss: 1.5974
Epoch 03 — loss: 1.5158
Stage 2: Error=0.6709, Alpha=1.0795
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7949
Epoch 02 — loss: 1.6408
Epoch 03 — loss: 1.5832
Stage 3: Error=0.6528, Alpha=1.1603
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8635
Epoch 02 — loss: 1.7945
Epoch 03 — loss: 1.7304
Stage 4: Error=0.7486, Alpha=0.7008
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8817
Epoch 02 — loss: 1.8242
Epoch 03 — loss: 1.7506
Stage 5: Error=0.7874, Alpha=0.4822
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5801
Epoch 02 — loss: 1.3603
Epoch 03 — loss: 1.3306
Stage 1: Error=0.5332, Alpha=1.6587
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7591
Epoch 02 — loss: 1.6086
Epoch 03 — loss: 1.5553
Stage 2: Error=0.6823, Alpha=1.0275
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7512
Epoch 02 — loss: 1.6340
Epoch 03 — loss: 1.5945
Stage 3: Error=0.6538, Alpha=1.1559
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8379
Epoch 02 — loss: 1.7503
Epoch 03 — loss: 1.7028
Stage 4: Error=0.7654, Alpha=0.6090
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8466
Epoch 02 — loss: 1.7767
Epoch 03 — loss: 1.7337
Stage 5: Error=0.7561, Alpha=0.6604
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6235
Epoch 02 — loss: 1.3696
Epoch 03 — loss: 1.2967
Stage 1: Error=0.5276, Alpha=1.6814
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7782
Epoch 02 — loss: 1.6107
Epoch 03 — loss: 1.5408
Stage 2: Error=0.7147, Alpha=0.8733
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8042
Epoch 02 — loss: 1.6565
Epoch 03 — loss: 1.5965
Stage 3: Error=0.7132, Alpha=0.8810
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8154
Epoch 02 — loss: 1.7042
Epoch 03 — loss: 1.6441
Stage 4: Error=0.7069, Alpha=0.9114
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8651/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [06:08:39] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [06:10:53] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [06:12:47] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.7985
Epoch 03 — loss: 1.7145
Stage 5: Error=0.7794, Alpha=0.5293
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4753
Epoch 02 — loss: 1.3438
Epoch 03 — loss: 1.2997
Stage 1: Error=0.5315, Alpha=1.6655
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6855
Epoch 02 — loss: 1.5792
Epoch 03 — loss: 1.5126
Stage 2: Error=0.6147, Alpha=1.3247
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7400
Epoch 02 — loss: 1.6208
Epoch 03 — loss: 1.5702
Stage 3: Error=0.6488, Alpha=1.1781
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7815
Epoch 02 — loss: 1.6860
Epoch 03 — loss: 1.6224
Stage 4: Error=0.6469, Alpha=1.1865
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8038
Epoch 02 — loss: 1.7049
Epoch 03 — loss: 1.6428
Stage 5: Error=0.6686, Alpha=1.0898
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4348
Epoch 02 — loss: 1.3443
Epoch 03 — loss: 1.3058
Stage 1: Error=0.5421, Alpha=1.6229
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6187
Epoch 02 — loss: 1.5124
Epoch 03 — loss: 1.4881
Stage 2: Error=0.6243, Alpha=1.2841
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7092
Epoch 02 — loss: 1.6279
Epoch 03 — loss: 1.5791
Stage 3: Error=0.6652, Alpha=1.1050
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7684
Epoch 02 — loss: 1.7082
Epoch 03 — loss: 1.6715
Stage 4: Error=0.6860, Alpha=1.0104
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8118
Epoch 02 — loss: 1.7482
Epoch 03 — loss: 1.7268
Stage 5: Error=0.7062, Alpha=0.9147
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4286
Epoch 02 — loss: 1.3161
Epoch 03 — loss: 1.2813
Stage 1: Error=0.5147, Alpha=1.7330
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6413
Epoch 02 — loss: 1.5230
Epoch 03 — loss: 1.4646
Stage 2: Error=0.6090, Alpha=1.3487
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6954
Epoch 02 — loss: 1.6042
Epoch 03 — loss: 1.5576
Stage 3: Error=0.6194, Alpha=1.3049
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7830
Epoch 02 — loss: 1.6994
Epoch 03 — loss: 1.6308
Stage 4: Error=0.6593, Alpha=1.1317
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8058
Epoch 02 — loss: 1.7242
Epoch 03 — loss: 1.6644
Stage 5: Error=0.6620, Alpha=1.1197
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8780
Stacking meta epoch 2: loss=0.7477
Stacking meta epoch 3: loss=0.7182
Stacking meta epoch 4: loss=0.7017
Stacking meta epoch 5: loss=0.6902
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8753
Stacking meta epoch 2: loss=0.7542
Stacking meta epoch 3: loss=0.7244
Stacking meta epoch 4: loss=0.7069
Stacking meta epoch 5: loss=0.6945
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2617
Stacking meta epoch 2: loss=1.2418
Stacking meta epoch 3: loss=1.2398
Stacking meta epoch 4: loss=1.2384
Stacking meta epoch 5: loss=1.2372
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      77.62
1                                deepset  ...                      80.85
2                     set_transformer_xy  ...                      79.69
3                             deepset_xy  ...                      78.87
4               set_transformer_additive  ...                      80.32
5                    deepset_xy_additive  ...                      80.94
6                       adaboost_deepset  ...                      82.44
7           adaboost_deepset_xy_additive  ...                      81.09
8                    adaboost_deepset_xy  ...                      81.86
9               adaboost_set_transformer  ...                      81.76
10     adaboost_set_transformer_additive  ...                      80.32
11           adaboost_set_transformer_xy  ...                      81.62
12              soft_voting_ensemble_all  ...                      82.44
13                 stacking_ensemble_all  ...                      84.07
14                      gbm_ensemble_all  ...                      83.35
15                  xgboost_ensemble_all  ...                      83.01
16  soft_voting_ensemble_set_transformer  ...                      82.10
17     stacking_ensemble_set_transformer  ...                      84.22
18          gbm_ensemble_set_transformer  ...                      83.21
19      xgboost_ensemble_set_transformer  ...                      83.16
20          soft_voting_ensemble_deepset  ...                      80.61
21             stacking_ensemble_deepset  ...                      82.92
22                  gbm_ensemble_deepset  ...                      79.55
23              xgboost_ensemble_deepset  ...                      81.04

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 8------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7527
Epoch 02 — loss: 1.6021
Epoch 03 — loss: 1.5715
Epoch 04 — loss: 1.5285
Epoch 05 — loss: 1.5000
Epoch 06 — loss: 1.4671
Epoch 07 — loss: 1.4374
Epoch 08 — loss: 1.4096
Epoch 09 — loss: 1.3775
Epoch 10 — loss: 1.3479
Epoch 11 — loss: 1.3170
Epoch 12 — loss: 1.2810
Epoch 13 — loss: 1.2597
Epoch 14 — loss: 1.2172
Epoch 15 — loss: 1.1904
Epoch 16 — loss: 1.1544
Epoch 17 — loss: 1.1299
Epoch 18 — loss: 1.0905
Epoch 19 — loss: 1.0542
Epoch 20 — loss: 1.0135
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8729
Epoch 02 — loss: 1.6063
Epoch 03 — loss: 1.5385
Epoch 04 — loss: 1.5151
Epoch 05 — loss: 1.4951
Epoch 06 — loss: 1.4792
Epoch 07 — loss: 1.4747
Epoch 08 — loss: 1.4698
Epoch 09 — loss: 1.4694
Epoch 10 — loss: 1.4602
Epoch 11 — loss: 1.4580
Epoch 12 — loss: 1.4500
Epoch 13 — loss: 1.4542
Epoch 14 — loss: 1.4498
Epoch 15 — loss: 1.4468
Epoch 16 — loss: 1.4453
Epoch 17 — loss: 1.4448
Epoch 18 — loss: 1.4469
Epoch 19 — loss: 1.4385
Epoch 20 — loss: 1.4338
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7359
Epoch 02 — loss: 1.6017
Epoch 03 — loss: 1.5613
Epoch 04 — loss: 1.5365
Epoch 05 — loss: 1.5122
Epoch 06 — loss: 1.4785
Epoch 07 — loss: 1.4584
Epoch 08 — loss: 1.4327
Epoch 09 — loss: 1.4012
Epoch 10 — loss: 1.3772
Epoch 11 — loss: 1.3550
Epoch 12 — loss: 1.3241
Epoch 13 — loss: 1.2897
Epoch 14 — loss: 1.2578
Epoch 15 — loss: 1.2165
Epoch 16 — loss: 1.1844
Epoch 17 — loss: 1.1531
Epoch 18 — loss: 1.1126
Epoch 19 — loss: 1.0798
Epoch 20 — loss: 1.0447
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8533
Epoch 02 — loss: 1.5913
Epoch 03 — loss: 1.5381
Epoch 04 — loss: 1.5108
Epoch 05 — loss: 1.5019
Epoch 06 — loss: 1.4909
Epoch 07 — loss: 1.4891
Epoch 08 — loss: 1.4720
Epoch 09 — loss: 1.4746
Epoch 10 — loss: 1.4636
Epoch 11 — loss: 1.4623
Epoch 12 — loss: 1.4580
Epoch 13 — loss: 1.4566
Epoch 14 — loss: 1.4494
Epoch 15 — loss: 1.4541
Epoch 16 — loss: 1.4464
Epoch 17 — loss: 1.4462
Epoch 18 — loss: 1.4453
Epoch 19 — loss: 1.4455
Epoch 20 — loss: 1.4363
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7322
Epoch 02 — loss: 1.6264
Epoch 03 — loss: 1.5854
Epoch 04 — loss: 1.5643
Epoch 05 — loss: 1.5373
Epoch 06 — loss: 1.5304
Epoch 07 — loss: 1.5031
Epoch 08 — loss: 1.4917
Epoch 09 — loss: 1.4734
Epoch 10 — loss: 1.4574
Epoch 11 — loss: 1.4508
Epoch 12 — loss: 1.4208
Epoch 13 — loss: 1.4065
Epoch 14 — loss: 1.3926
Epoch 15 — loss: 1.3723
Epoch 16 — loss: 1.3530
Epoch 17 — loss: 1.3275
Epoch 18 — loss: 1.3125
Epoch 19 — loss: 1.2857
Epoch 20 — loss: 1.2623
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8077
Epoch 02 — loss: 1.6302
Epoch 03 — loss: 1.5856
Epoch 04 — loss: 1.5501
Epoch 05 — loss: 1.5329
Epoch 06 — loss: 1.5046
Epoch 07 — loss: 1.4997
Epoch 08 — loss: 1.4897
Epoch 09 — loss: 1.4814
Epoch 10 — loss: 1.4751
Epoch 11 — loss: 1.4712
Epoch 12 — loss: 1.4593
Epoch 13 — loss: 1.4699
Epoch 14 — loss: 1.4546
Epoch 15 — loss: 1.4579
Epoch 16 — loss: 1.4580
Epoch 17 — loss: 1.4448
Epoch 18 — loss: 1.4522
Epoch 19 — loss: 1.4442
Epoch 20 — loss: 1.4463
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6309
Epoch 02 — loss: 1.3872
Epoch 03 — loss: 1.3011
Stage 1: Error=0.5283, Alpha=1.6785
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7994
Epoch 02 — loss: 1.6374
Epoch 03 — loss: 1.5477
Stage 2: Error=0.6751, Alpha=1.0602
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8381
Epoch 02 — loss: 1.7332
Epoch 03 — loss: 1.6429
Stage 3: Error=0.7176, Alpha=0.8590
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8572
Epoch 02 — loss: 1.7591
Epoch 03 — loss: 1.6518
Stage 4: Error=0.6896, Alpha=0.9935
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8746
Epoch 02 — loss: 1.8188
Epoch 03 — loss: 1.7582
Stage 5: Error=0.7516, Alpha=0.6846
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5540
Epoch 02 — loss: 1.3462
Epoch 03 — loss: 1.3316
Stage 1: Error=0.5323, Alpha=1.6626
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7354
Epoch 02 — loss: 1.5699
Epoch 03 — loss: 1.5370
Stage 2: Error=0.6632, Alpha=1.1141
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7993
Epoch 02 — loss: 1.6637
Epoch 03 — loss: 1.6300
Stage 3: Error=0.6710, Alpha=1.0790
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8320
Epoch 02 — loss: 1.7310
Epoch 03 — loss: 1.6801
Stage 4: Error=0.7239, Alpha=0.8280
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8575
Epoch 02 — loss: 1.7872
Epoch 03 — loss: 1.7560
Stage 5: Error=0.7944, Alpha=0.4401
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6293
Epoch 02 — loss: 1.3833
Epoch 03 — loss: 1.3034
Stage 1: Error=0.5239, Alpha=1.6959
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7881
Epoch 02 — loss: 1.5757
Epoch 03 — loss: 1.5331
Stage 2: Error=0.6750, Alpha=1.0610
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8059
Epoch 02 — loss: 1.6553
Epoch 03 — loss: 1.5813
Stage 3: Error=0.6385, Alpha=1.2229
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8782
Epoch 02 — loss: 1.7642
Epoch 03 — loss: 1.7149
Stage 4: Error=0.7422, Alpha=0.7343
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8764/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [06:54:47] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [06:57:00] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [06:58:55] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8341
Epoch 03 — loss: 1.7629
Stage 5: Error=0.7556, Alpha=0.6633
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4895
Epoch 02 — loss: 1.3631
Epoch 03 — loss: 1.3335
Stage 1: Error=0.5665, Alpha=1.5240
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6201
Epoch 02 — loss: 1.4993
Epoch 03 — loss: 1.4668
Stage 2: Error=0.5680, Alpha=1.5182
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7170
Epoch 02 — loss: 1.6371
Epoch 03 — loss: 1.5854
Stage 3: Error=0.6436, Alpha=1.2009
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7723
Epoch 02 — loss: 1.6757
Epoch 03 — loss: 1.6164
Stage 4: Error=0.6591, Alpha=1.1327
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8173
Epoch 02 — loss: 1.7260
Epoch 03 — loss: 1.6500
Stage 5: Error=0.6939, Alpha=0.9735
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4326
Epoch 02 — loss: 1.3469
Epoch 03 — loss: 1.3411
Stage 1: Error=0.5433, Alpha=1.6180
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6641
Epoch 02 — loss: 1.5717
Epoch 03 — loss: 1.5325
Stage 2: Error=0.6261, Alpha=1.2761
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7201
Epoch 02 — loss: 1.6474
Epoch 03 — loss: 1.6163
Stage 3: Error=0.7056, Alpha=0.9177
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7763
Epoch 02 — loss: 1.7158
Epoch 03 — loss: 1.6731
Stage 4: Error=0.7017, Alpha=0.9365
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8259
Epoch 02 — loss: 1.7518
Epoch 03 — loss: 1.6975
Stage 5: Error=0.7040, Alpha=0.9254
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4406
Epoch 02 — loss: 1.3153
Epoch 03 — loss: 1.2654
Stage 1: Error=0.5097, Alpha=1.7528
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6521
Epoch 02 — loss: 1.5435
Epoch 03 — loss: 1.4992
Stage 2: Error=0.6451, Alpha=1.1941
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7299
Epoch 02 — loss: 1.6005
Epoch 03 — loss: 1.5546
Stage 3: Error=0.6174, Alpha=1.3132
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7769
Epoch 02 — loss: 1.6983
Epoch 03 — loss: 1.6221
Stage 4: Error=0.6469, Alpha=1.1865
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8024
Epoch 02 — loss: 1.7137
Epoch 03 — loss: 1.6141
Stage 5: Error=0.6329, Alpha=1.2473
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.9117
Stacking meta epoch 2: loss=0.7801
Stacking meta epoch 3: loss=0.7487
Stacking meta epoch 4: loss=0.7314
Stacking meta epoch 5: loss=0.7196
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.9033
Stacking meta epoch 2: loss=0.7812
Stacking meta epoch 3: loss=0.7514
Stacking meta epoch 4: loss=0.7345
Stacking meta epoch 5: loss=0.7228
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2546
Stacking meta epoch 2: loss=1.2421
Stacking meta epoch 3: loss=1.2400
Stacking meta epoch 4: loss=1.2384
Stacking meta epoch 5: loss=1.2371
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      79.93
1                                deepset  ...                      78.78
2                     set_transformer_xy  ...                      81.57
3                             deepset_xy  ...                      80.75
4               set_transformer_additive  ...                      75.89
5                    deepset_xy_additive  ...                      81.14
6                       adaboost_deepset  ...                      82.05
7           adaboost_deepset_xy_additive  ...                      81.95
8                    adaboost_deepset_xy  ...                      82.15
9               adaboost_set_transformer  ...                      81.76
10     adaboost_set_transformer_additive  ...                      80.37
11           adaboost_set_transformer_xy  ...                      81.18
12              soft_voting_ensemble_all  ...                      83.30
13                 stacking_ensemble_all  ...                      82.72
14                      gbm_ensemble_all  ...                      82.82
15                  xgboost_ensemble_all  ...                      83.30
16  soft_voting_ensemble_set_transformer  ...                      82.34
17     stacking_ensemble_set_transformer  ...                      82.68
18          gbm_ensemble_set_transformer  ...                      83.25
19      xgboost_ensemble_set_transformer  ...                      83.30
20          soft_voting_ensemble_deepset  ...                      80.65
21             stacking_ensemble_deepset  ...                      82.15
22                  gbm_ensemble_deepset  ...                      80.46
23              xgboost_ensemble_deepset  ...                      79.55

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 9------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7588
Epoch 02 — loss: 1.6373
Epoch 03 — loss: 1.5792
Epoch 04 — loss: 1.5404
Epoch 05 — loss: 1.5019
Epoch 06 — loss: 1.4643
Epoch 07 — loss: 1.4213
Epoch 08 — loss: 1.3954
Epoch 09 — loss: 1.3543
Epoch 10 — loss: 1.3181
Epoch 11 — loss: 1.2864
Epoch 12 — loss: 1.2539
Epoch 13 — loss: 1.2107
Epoch 14 — loss: 1.1853
Epoch 15 — loss: 1.1346
Epoch 16 — loss: 1.1067
Epoch 17 — loss: 1.0528
Epoch 18 — loss: 1.0292
Epoch 19 — loss: 0.9906
Epoch 20 — loss: 0.9557
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8691
Epoch 02 — loss: 1.5976
Epoch 03 — loss: 1.5417
Epoch 04 — loss: 1.5182
Epoch 05 — loss: 1.5038
Epoch 06 — loss: 1.4922
Epoch 07 — loss: 1.4773
Epoch 08 — loss: 1.4755
Epoch 09 — loss: 1.4739
Epoch 10 — loss: 1.4683
Epoch 11 — loss: 1.4631
Epoch 12 — loss: 1.4596
Epoch 13 — loss: 1.4541
Epoch 14 — loss: 1.4550
Epoch 15 — loss: 1.4493
Epoch 16 — loss: 1.4490
Epoch 17 — loss: 1.4486
Epoch 18 — loss: 1.4460
Epoch 19 — loss: 1.4445
Epoch 20 — loss: 1.4421
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7093
Epoch 02 — loss: 1.5896
Epoch 03 — loss: 1.5337
Epoch 04 — loss: 1.5061
Epoch 05 — loss: 1.4771
Epoch 06 — loss: 1.4487
Epoch 07 — loss: 1.4238
Epoch 08 — loss: 1.3976
Epoch 09 — loss: 1.3701
Epoch 10 — loss: 1.3381
Epoch 11 — loss: 1.3141
Epoch 12 — loss: 1.2747
Epoch 13 — loss: 1.2422
Epoch 14 — loss: 1.2026
Epoch 15 — loss: 1.1880
Epoch 16 — loss: 1.1413
Epoch 17 — loss: 1.1164
Epoch 18 — loss: 1.0818
Epoch 19 — loss: 1.0398
Epoch 20 — loss: 0.9958
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8725
Epoch 02 — loss: 1.5901
Epoch 03 — loss: 1.5351
Epoch 04 — loss: 1.5125
Epoch 05 — loss: 1.4910
Epoch 06 — loss: 1.4860
Epoch 07 — loss: 1.4794
Epoch 08 — loss: 1.4706
Epoch 09 — loss: 1.4653
Epoch 10 — loss: 1.4647
Epoch 11 — loss: 1.4539
Epoch 12 — loss: 1.4531
Epoch 13 — loss: 1.4538
Epoch 14 — loss: 1.4528
Epoch 15 — loss: 1.4495
Epoch 16 — loss: 1.4408
Epoch 17 — loss: 1.4382
Epoch 18 — loss: 1.4438
Epoch 19 — loss: 1.4411
Epoch 20 — loss: 1.4318
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7035
Epoch 02 — loss: 1.5953
Epoch 03 — loss: 1.5618
Epoch 04 — loss: 1.5400
Epoch 05 — loss: 1.5121
Epoch 06 — loss: 1.5040
Epoch 07 — loss: 1.4904
Epoch 08 — loss: 1.4799
Epoch 09 — loss: 1.4555
Epoch 10 — loss: 1.4416
Epoch 11 — loss: 1.4232
Epoch 12 — loss: 1.4083
Epoch 13 — loss: 1.3886
Epoch 14 — loss: 1.3731
Epoch 15 — loss: 1.3379
Epoch 16 — loss: 1.3198
Epoch 17 — loss: 1.3041
Epoch 18 — loss: 1.2685
Epoch 19 — loss: 1.2405
Epoch 20 — loss: 1.2040
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8137
Epoch 02 — loss: 1.6414
Epoch 03 — loss: 1.5898
Epoch 04 — loss: 1.5638
Epoch 05 — loss: 1.5453
Epoch 06 — loss: 1.5224
Epoch 07 — loss: 1.5190
Epoch 08 — loss: 1.4963
Epoch 09 — loss: 1.4943
Epoch 10 — loss: 1.4814
Epoch 11 — loss: 1.4765
Epoch 12 — loss: 1.4576
Epoch 13 — loss: 1.4630
Epoch 14 — loss: 1.4668
Epoch 15 — loss: 1.4552
Epoch 16 — loss: 1.4545
Epoch 17 — loss: 1.4553
Epoch 18 — loss: 1.4548
Epoch 19 — loss: 1.4412
Epoch 20 — loss: 1.4475
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6403
Epoch 02 — loss: 1.3979
Epoch 03 — loss: 1.3115
Stage 1: Error=0.5294, Alpha=1.6742
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8110
Epoch 02 — loss: 1.6153
Epoch 03 — loss: 1.5281
Stage 2: Error=0.6498, Alpha=1.1734
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8374
Epoch 02 — loss: 1.6720
Epoch 03 — loss: 1.6046
Stage 3: Error=0.6537, Alpha=1.1562
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8559
Epoch 02 — loss: 1.7695
Epoch 03 — loss: 1.7210
Stage 4: Error=0.7379, Alpha=0.7566
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8947
Epoch 02 — loss: 1.8226
Epoch 03 — loss: 1.7527
Stage 5: Error=0.7418, Alpha=0.7366
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5591
Epoch 02 — loss: 1.3701
Epoch 03 — loss: 1.3499
Stage 1: Error=0.5357, Alpha=1.6486
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7582
Epoch 02 — loss: 1.5670
Epoch 03 — loss: 1.5300
Stage 2: Error=0.6320, Alpha=1.2509
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7823
Epoch 02 — loss: 1.6514
Epoch 03 — loss: 1.6331
Stage 3: Error=0.6721, Alpha=1.0739
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8415
Epoch 02 — loss: 1.7642
Epoch 03 — loss: 1.7176
Stage 4: Error=0.7501, Alpha=0.6925
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8550
Epoch 02 — loss: 1.7821
Epoch 03 — loss: 1.7427
Stage 5: Error=0.7465, Alpha=0.7116
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6039
Epoch 02 — loss: 1.3864
Epoch 03 — loss: 1.3133
Stage 1: Error=0.5298, Alpha=1.6722
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7904
Epoch 02 — loss: 1.5805
Epoch 03 — loss: 1.5101
Stage 2: Error=0.6327, Alpha=1.2478
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8092
Epoch 02 — loss: 1.6566
Epoch 03 — loss: 1.5782
Stage 3: Error=0.6318, Alpha=1.2518
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8655
Epoch 02 — loss: 1.7906
Epoch 03 — loss: 1.7241
Stage 4: Error=0.7253, Alpha=0.8209
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8782/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [07:40:52] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [07:43:05] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [07:44:59] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8039
Epoch 03 — loss: 1.7495
Stage 5: Error=0.7653, Alpha=0.6095
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4343
Epoch 02 — loss: 1.3186
Epoch 03 — loss: 1.2954
Stage 1: Error=0.5253, Alpha=1.6906
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6641
Epoch 02 — loss: 1.5601
Epoch 03 — loss: 1.5105
Stage 2: Error=0.6273, Alpha=1.2712
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6948
Epoch 02 — loss: 1.5913
Epoch 03 — loss: 1.5416
Stage 3: Error=0.6308, Alpha=1.2562
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7778
Epoch 02 — loss: 1.6941
Epoch 03 — loss: 1.6244
Stage 4: Error=0.6726, Alpha=1.0717
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7830
Epoch 02 — loss: 1.7039
Epoch 03 — loss: 1.6346
Stage 5: Error=0.6700, Alpha=1.0836
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4459
Epoch 02 — loss: 1.3402
Epoch 03 — loss: 1.3172
Stage 1: Error=0.5288, Alpha=1.6766
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6386
Epoch 02 — loss: 1.5525
Epoch 03 — loss: 1.5093
Stage 2: Error=0.6257, Alpha=1.2777
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7379
Epoch 02 — loss: 1.6509
Epoch 03 — loss: 1.6171
Stage 3: Error=0.6938, Alpha=0.9738
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7389
Epoch 02 — loss: 1.6675
Epoch 03 — loss: 1.6301
Stage 4: Error=0.6647, Alpha=1.1075
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8016
Epoch 02 — loss: 1.7475
Epoch 03 — loss: 1.7068
Stage 5: Error=0.7060, Alpha=0.9156
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4134
Epoch 02 — loss: 1.3311
Epoch 03 — loss: 1.2673
Stage 1: Error=0.5052, Alpha=1.7711
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6488
Epoch 02 — loss: 1.5592
Epoch 03 — loss: 1.4900
Stage 2: Error=0.6145, Alpha=1.3256
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7052
Epoch 02 — loss: 1.6090
Epoch 03 — loss: 1.5526
Stage 3: Error=0.6226, Alpha=1.2910
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7895
Epoch 02 — loss: 1.6833
Epoch 03 — loss: 1.6050
Stage 4: Error=0.6660, Alpha=1.1015
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8058
Epoch 02 — loss: 1.7189
Epoch 03 — loss: 1.6538
Stage 5: Error=0.6650, Alpha=1.1062
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8698
Stacking meta epoch 2: loss=0.7412
Stacking meta epoch 3: loss=0.7107
Stacking meta epoch 4: loss=0.6940
Stacking meta epoch 5: loss=0.6827
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8737
Stacking meta epoch 2: loss=0.7497
Stacking meta epoch 3: loss=0.7202
Stacking meta epoch 4: loss=0.7035
Stacking meta epoch 5: loss=0.6919
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2603
Stacking meta epoch 2: loss=1.2429
Stacking meta epoch 3: loss=1.2401
Stacking meta epoch 4: loss=1.2382
Stacking meta epoch 5: loss=1.2368
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      80.27
1                                deepset  ...                      81.86
2                     set_transformer_xy  ...                      80.61
3                             deepset_xy  ...                      81.76
4               set_transformer_additive  ...                      79.64
5                    deepset_xy_additive  ...                      81.57
6                       adaboost_deepset  ...                      81.18
7           adaboost_deepset_xy_additive  ...                      79.64
8                    adaboost_deepset_xy  ...                      80.37
9               adaboost_set_transformer  ...                      81.38
10     adaboost_set_transformer_additive  ...                      82.68
11           adaboost_set_transformer_xy  ...                      83.25
12              soft_voting_ensemble_all  ...                      82.63
13                 stacking_ensemble_all  ...                      82.34
14                      gbm_ensemble_all  ...                      82.24
15                  xgboost_ensemble_all  ...                      82.44
16  soft_voting_ensemble_set_transformer  ...                      82.39
17     stacking_ensemble_set_transformer  ...                      82.92
18          gbm_ensemble_set_transformer  ...                      82.29
19      xgboost_ensemble_set_transformer  ...                      82.05
20          soft_voting_ensemble_deepset  ...                      81.81
21             stacking_ensemble_deepset  ...                      82.87
22                  gbm_ensemble_deepset  ...                      80.17
23              xgboost_ensemble_deepset  ...                      80.27

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 10------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7722
Epoch 02 — loss: 1.6430
Epoch 03 — loss: 1.5960
Epoch 04 — loss: 1.5503
Epoch 05 — loss: 1.5048
Epoch 06 — loss: 1.4710
Epoch 07 — loss: 1.4312
Epoch 08 — loss: 1.3888
Epoch 09 — loss: 1.3539
Epoch 10 — loss: 1.3252
Epoch 11 — loss: 1.2779
Epoch 12 — loss: 1.2516
Epoch 13 — loss: 1.2062
Epoch 14 — loss: 1.1694
Epoch 15 — loss: 1.1356
Epoch 16 — loss: 1.1071
Epoch 17 — loss: 1.0626
Epoch 18 — loss: 1.0168
Epoch 19 — loss: 0.9826
Epoch 20 — loss: 0.9506
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8965
Epoch 02 — loss: 1.6370
Epoch 03 — loss: 1.5574
Epoch 04 — loss: 1.5203
Epoch 05 — loss: 1.5094
Epoch 06 — loss: 1.4843
Epoch 07 — loss: 1.4837
Epoch 08 — loss: 1.4771
Epoch 09 — loss: 1.4671
Epoch 10 — loss: 1.4648
Epoch 11 — loss: 1.4677
Epoch 12 — loss: 1.4563
Epoch 13 — loss: 1.4570
Epoch 14 — loss: 1.4549
Epoch 15 — loss: 1.4507
Epoch 16 — loss: 1.4453
Epoch 17 — loss: 1.4434
Epoch 18 — loss: 1.4429
Epoch 19 — loss: 1.4476
Epoch 20 — loss: 1.4432
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7322
Epoch 02 — loss: 1.5959
Epoch 03 — loss: 1.5520
Epoch 04 — loss: 1.5236
Epoch 05 — loss: 1.4909
Epoch 06 — loss: 1.4617
Epoch 07 — loss: 1.4369
Epoch 08 — loss: 1.3969
Epoch 09 — loss: 1.3750
Epoch 10 — loss: 1.3532
Epoch 11 — loss: 1.3154
Epoch 12 — loss: 1.2620
Epoch 13 — loss: 1.2439
Epoch 14 — loss: 1.2061
Epoch 15 — loss: 1.1662
Epoch 16 — loss: 1.1433
Epoch 17 — loss: 1.1005
Epoch 18 — loss: 1.0616
Epoch 19 — loss: 1.0301
Epoch 20 — loss: 0.9887
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8710
Epoch 02 — loss: 1.6010
Epoch 03 — loss: 1.5401
Epoch 04 — loss: 1.5214
Epoch 05 — loss: 1.5118
Epoch 06 — loss: 1.4884
Epoch 07 — loss: 1.4887
Epoch 08 — loss: 1.4813
Epoch 09 — loss: 1.4684
Epoch 10 — loss: 1.4682
Epoch 11 — loss: 1.4615
Epoch 12 — loss: 1.4606
Epoch 13 — loss: 1.4556
Epoch 14 — loss: 1.4526
Epoch 15 — loss: 1.4485
Epoch 16 — loss: 1.4454
Epoch 17 — loss: 1.4502
Epoch 18 — loss: 1.4447
Epoch 19 — loss: 1.4436
Epoch 20 — loss: 1.4375
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7226
Epoch 02 — loss: 1.6303
Epoch 03 — loss: 1.5871
Epoch 04 — loss: 1.5553
Epoch 05 — loss: 1.5306
Epoch 06 — loss: 1.5116
Epoch 07 — loss: 1.4773
Epoch 08 — loss: 1.4742
Epoch 09 — loss: 1.4456
Epoch 10 — loss: 1.4199
Epoch 11 — loss: 1.3970
Epoch 12 — loss: 1.3751
Epoch 13 — loss: 1.3523
Epoch 14 — loss: 1.3382
Epoch 15 — loss: 1.3083
Epoch 16 — loss: 1.2891
Epoch 17 — loss: 1.2619
Epoch 18 — loss: 1.2308
Epoch 19 — loss: 1.2029
Epoch 20 — loss: 1.1718
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7900
Epoch 02 — loss: 1.6193
Epoch 03 — loss: 1.5740
Epoch 04 — loss: 1.5351
Epoch 05 — loss: 1.5150
Epoch 06 — loss: 1.5035
Epoch 07 — loss: 1.4929
Epoch 08 — loss: 1.4853
Epoch 09 — loss: 1.4881
Epoch 10 — loss: 1.4762
Epoch 11 — loss: 1.4685
Epoch 12 — loss: 1.4578
Epoch 13 — loss: 1.4618
Epoch 14 — loss: 1.4619
Epoch 15 — loss: 1.4508
Epoch 16 — loss: 1.4570
Epoch 17 — loss: 1.4465
Epoch 18 — loss: 1.4444
Epoch 19 — loss: 1.4438
Epoch 20 — loss: 1.4482
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6456
Epoch 02 — loss: 1.3787
Epoch 03 — loss: 1.3132
Stage 1: Error=0.5309, Alpha=1.6679
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7876
Epoch 02 — loss: 1.5840
Epoch 03 — loss: 1.5244
Stage 2: Error=0.6404, Alpha=1.2148
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8081
Epoch 02 — loss: 1.6514
Epoch 03 — loss: 1.5951
Stage 3: Error=0.6403, Alpha=1.2149
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8556
Epoch 02 — loss: 1.7825
Epoch 03 — loss: 1.7175
Stage 4: Error=0.7505, Alpha=0.6907
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8921
Epoch 02 — loss: 1.8177
Epoch 03 — loss: 1.7574
Stage 5: Error=0.7654, Alpha=0.6095
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5367
Epoch 02 — loss: 1.3604
Epoch 03 — loss: 1.3494
Stage 1: Error=0.5354, Alpha=1.6500
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7414
Epoch 02 — loss: 1.5665
Epoch 03 — loss: 1.5118
Stage 2: Error=0.6699, Alpha=1.0843
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7494
Epoch 02 — loss: 1.5816
Epoch 03 — loss: 1.5610
Stage 3: Error=0.6337, Alpha=1.2438
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8283
Epoch 02 — loss: 1.7202
Epoch 03 — loss: 1.6791
Stage 4: Error=0.7034, Alpha=0.9282
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8585
Epoch 02 — loss: 1.7895
Epoch 03 — loss: 1.7252
Stage 5: Error=0.7348, Alpha=0.7724
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6053
Epoch 02 — loss: 1.3546
Epoch 03 — loss: 1.2943
Stage 1: Error=0.5266, Alpha=1.6853
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7825
Epoch 02 — loss: 1.5882
Epoch 03 — loss: 1.5237
Stage 2: Error=0.6914, Alpha=0.9852
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8110
Epoch 02 — loss: 1.6599
Epoch 03 — loss: 1.5900
Stage 3: Error=0.6646, Alpha=1.1079
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8658
Epoch 02 — loss: 1.7755
Epoch 03 — loss: 1.6935
Stage 4: Error=0.7573, Alpha=0.6539
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8482/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [08:26:28] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [08:28:41] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [08:30:34] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.7821
Epoch 03 — loss: 1.7180
Stage 5: Error=0.7907, Alpha=0.4625
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4466
Epoch 02 — loss: 1.3412
Epoch 03 — loss: 1.3045
Stage 1: Error=0.5172, Alpha=1.7229
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6889
Epoch 02 — loss: 1.5603
Epoch 03 — loss: 1.5154
Stage 2: Error=0.6406, Alpha=1.2136
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7306
Epoch 02 — loss: 1.6178
Epoch 03 — loss: 1.5608
Stage 3: Error=0.6584, Alpha=1.1357
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7532
Epoch 02 — loss: 1.6599
Epoch 03 — loss: 1.5937
Stage 4: Error=0.6325, Alpha=1.2488
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8308
Epoch 02 — loss: 1.7603
Epoch 03 — loss: 1.6847
Stage 5: Error=0.6736, Alpha=1.0670
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4096
Epoch 02 — loss: 1.3534
Epoch 03 — loss: 1.3276
Stage 1: Error=0.5403, Alpha=1.6302
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6248
Epoch 02 — loss: 1.5473
Epoch 03 — loss: 1.5026
Stage 2: Error=0.6294, Alpha=1.2621
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7219
Epoch 02 — loss: 1.6559
Epoch 03 — loss: 1.6097
Stage 3: Error=0.6736, Alpha=1.0670
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7572
Epoch 02 — loss: 1.6975
Epoch 03 — loss: 1.6673
Stage 4: Error=0.6827, Alpha=1.0256
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8236
Epoch 02 — loss: 1.7576
Epoch 03 — loss: 1.7016
Stage 5: Error=0.7106, Alpha=0.8934
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4501
Epoch 02 — loss: 1.3081
Epoch 03 — loss: 1.3033
Stage 1: Error=0.5053, Alpha=1.7706
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6687
Epoch 02 — loss: 1.5514
Epoch 03 — loss: 1.5047
Stage 2: Error=0.6293, Alpha=1.2626
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7065
Epoch 02 — loss: 1.5877
Epoch 03 — loss: 1.5283
Stage 3: Error=0.5898, Alpha=1.4287
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7875
Epoch 02 — loss: 1.6869
Epoch 03 — loss: 1.6072
Stage 4: Error=0.6540, Alpha=1.1551
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8085
Epoch 02 — loss: 1.7149
Epoch 03 — loss: 1.6358
Stage 5: Error=0.6467, Alpha=1.1872
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8523
Stacking meta epoch 2: loss=0.7114
Stacking meta epoch 3: loss=0.6810
Stacking meta epoch 4: loss=0.6647
Stacking meta epoch 5: loss=0.6535
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8513
Stacking meta epoch 2: loss=0.7185
Stacking meta epoch 3: loss=0.6884
Stacking meta epoch 4: loss=0.6718
Stacking meta epoch 5: loss=0.6605
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2570
Stacking meta epoch 2: loss=1.2440
Stacking meta epoch 3: loss=1.2415
Stacking meta epoch 4: loss=1.2397
Stacking meta epoch 5: loss=1.2383
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      78.78
1                                deepset  ...                      79.16
2                     set_transformer_xy  ...                      81.23
3                             deepset_xy  ...                      79.40
4               set_transformer_additive  ...                      77.57
5                    deepset_xy_additive  ...                      79.21
6                       adaboost_deepset  ...                      81.09
7           adaboost_deepset_xy_additive  ...                      80.41
8                    adaboost_deepset_xy  ...                      82.44
9               adaboost_set_transformer  ...                      81.52
10     adaboost_set_transformer_additive  ...                      80.56
11           adaboost_set_transformer_xy  ...                      82.29
12              soft_voting_ensemble_all  ...                      81.71
13                 stacking_ensemble_all  ...                      83.01
14                      gbm_ensemble_all  ...                      84.02
15                  xgboost_ensemble_all  ...                      82.82
16  soft_voting_ensemble_set_transformer  ...                      81.67
17     stacking_ensemble_set_transformer  ...                      83.25
18          gbm_ensemble_set_transformer  ...                      83.45
19      xgboost_ensemble_set_transformer  ...                      82.77
20          soft_voting_ensemble_deepset  ...                      79.26
21             stacking_ensemble_deepset  ...                      82.58
22                  gbm_ensemble_deepset  ...                      80.37
23              xgboost_ensemble_deepset  ...                      80.99

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 11------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7435
Epoch 02 — loss: 1.6238
Epoch 03 — loss: 1.5671
Epoch 04 — loss: 1.5302
Epoch 05 — loss: 1.5024
Epoch 06 — loss: 1.4573
Epoch 07 — loss: 1.4228
Epoch 08 — loss: 1.3911
Epoch 09 — loss: 1.3648
Epoch 10 — loss: 1.3243
Epoch 11 — loss: 1.2936
Epoch 12 — loss: 1.2562
Epoch 13 — loss: 1.2221
Epoch 14 — loss: 1.1914
Epoch 15 — loss: 1.1568
Epoch 16 — loss: 1.1101
Epoch 17 — loss: 1.0776
Epoch 18 — loss: 1.0347
Epoch 19 — loss: 0.9995
Epoch 20 — loss: 0.9807
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8748
Epoch 02 — loss: 1.6205
Epoch 03 — loss: 1.5389
Epoch 04 — loss: 1.5124
Epoch 05 — loss: 1.5016
Epoch 06 — loss: 1.4827
Epoch 07 — loss: 1.4793
Epoch 08 — loss: 1.4734
Epoch 09 — loss: 1.4553
Epoch 10 — loss: 1.4692
Epoch 11 — loss: 1.4550
Epoch 12 — loss: 1.4569
Epoch 13 — loss: 1.4553
Epoch 14 — loss: 1.4454
Epoch 15 — loss: 1.4534
Epoch 16 — loss: 1.4495
Epoch 17 — loss: 1.4434
Epoch 18 — loss: 1.4425
Epoch 19 — loss: 1.4433
Epoch 20 — loss: 1.4418
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7291
Epoch 02 — loss: 1.5998
Epoch 03 — loss: 1.5626
Epoch 04 — loss: 1.5341
Epoch 05 — loss: 1.4922
Epoch 06 — loss: 1.4689
Epoch 07 — loss: 1.4220
Epoch 08 — loss: 1.4035
Epoch 09 — loss: 1.3616
Epoch 10 — loss: 1.3254
Epoch 11 — loss: 1.2955
Epoch 12 — loss: 1.2700
Epoch 13 — loss: 1.2341
Epoch 14 — loss: 1.1996
Epoch 15 — loss: 1.1554
Epoch 16 — loss: 1.1223
Epoch 17 — loss: 1.0955
Epoch 18 — loss: 1.0494
Epoch 19 — loss: 1.0026
Epoch 20 — loss: 0.9689
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8454
Epoch 02 — loss: 1.5907
Epoch 03 — loss: 1.5419
Epoch 04 — loss: 1.5125
Epoch 05 — loss: 1.4999
Epoch 06 — loss: 1.4878
Epoch 07 — loss: 1.4841
Epoch 08 — loss: 1.4691
Epoch 09 — loss: 1.4681
Epoch 10 — loss: 1.4616
Epoch 11 — loss: 1.4647
Epoch 12 — loss: 1.4596
Epoch 13 — loss: 1.4523
Epoch 14 — loss: 1.4519
Epoch 15 — loss: 1.4535
Epoch 16 — loss: 1.4565
Epoch 17 — loss: 1.4503
Epoch 18 — loss: 1.4433
Epoch 19 — loss: 1.4435
Epoch 20 — loss: 1.4450
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7011
Epoch 02 — loss: 1.6165
Epoch 03 — loss: 1.5824
Epoch 04 — loss: 1.5619
Epoch 05 — loss: 1.5332
Epoch 06 — loss: 1.5095
Epoch 07 — loss: 1.4976
Epoch 08 — loss: 1.4881
Epoch 09 — loss: 1.4692
Epoch 10 — loss: 1.4487
Epoch 11 — loss: 1.4281
Epoch 12 — loss: 1.4195
Epoch 13 — loss: 1.3949
Epoch 14 — loss: 1.3745
Epoch 15 — loss: 1.3615
Epoch 16 — loss: 1.3387
Epoch 17 — loss: 1.3041
Epoch 18 — loss: 1.2966
Epoch 19 — loss: 1.2733
Epoch 20 — loss: 1.2557
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8121
Epoch 02 — loss: 1.6470
Epoch 03 — loss: 1.5947
Epoch 04 — loss: 1.5648
Epoch 05 — loss: 1.5378
Epoch 06 — loss: 1.5218
Epoch 07 — loss: 1.5098
Epoch 08 — loss: 1.4967
Epoch 09 — loss: 1.4833
Epoch 10 — loss: 1.4896
Epoch 11 — loss: 1.4805
Epoch 12 — loss: 1.4693
Epoch 13 — loss: 1.4719
Epoch 14 — loss: 1.4644
Epoch 15 — loss: 1.4641
Epoch 16 — loss: 1.4633
Epoch 17 — loss: 1.4616
Epoch 18 — loss: 1.4522
Epoch 19 — loss: 1.4449
Epoch 20 — loss: 1.4432
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6343
Epoch 02 — loss: 1.3914
Epoch 03 — loss: 1.3256
Stage 1: Error=0.5289, Alpha=1.6761
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7932
Epoch 02 — loss: 1.6235
Epoch 03 — loss: 1.5478
Stage 2: Error=0.6408, Alpha=1.2128
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8233
Epoch 02 — loss: 1.6731
Epoch 03 — loss: 1.5850
Stage 3: Error=0.6528, Alpha=1.1602
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8702
Epoch 02 — loss: 1.7701
Epoch 03 — loss: 1.7102
Stage 4: Error=0.7308, Alpha=0.7931
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8847
Epoch 02 — loss: 1.8450
Epoch 03 — loss: 1.7809
Stage 5: Error=0.7534, Alpha=0.6751
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5387
Epoch 02 — loss: 1.3672
Epoch 03 — loss: 1.3065
Stage 1: Error=0.5316, Alpha=1.6650
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7217
Epoch 02 — loss: 1.5810
Epoch 03 — loss: 1.5411
Stage 2: Error=0.6439, Alpha=1.1995
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7849
Epoch 02 — loss: 1.6498
Epoch 03 — loss: 1.6245
Stage 3: Error=0.6592, Alpha=1.1320
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8435
Epoch 02 — loss: 1.7488
Epoch 03 — loss: 1.7127
Stage 4: Error=0.7355, Alpha=0.7693
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8537
Epoch 02 — loss: 1.7950
Epoch 03 — loss: 1.7637
Stage 5: Error=0.7675, Alpha=0.5975
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6070
Epoch 02 — loss: 1.3599
Epoch 03 — loss: 1.3042
Stage 1: Error=0.5274, Alpha=1.6819
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7673
Epoch 02 — loss: 1.5872
Epoch 03 — loss: 1.5311
Stage 2: Error=0.6451, Alpha=1.1944
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8291
Epoch 02 — loss: 1.6616
Epoch 03 — loss: 1.6101
Stage 3: Error=0.6723, Alpha=1.0733
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8544
Epoch 02 — loss: 1.7644
Epoch 03 — loss: 1.7179
Stage 4: Error=0.7552, Alpha=0.6650
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8812/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [09:12:06] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [09:14:18] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [09:16:12] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8093
Epoch 03 — loss: 1.7456
Stage 5: Error=0.7509, Alpha=0.6882
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4753
Epoch 02 — loss: 1.3880
Epoch 03 — loss: 1.3239
Stage 1: Error=0.5278, Alpha=1.6805
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6854
Epoch 02 — loss: 1.5491
Epoch 03 — loss: 1.4792
Stage 2: Error=0.6302, Alpha=1.2587
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7211
Epoch 02 — loss: 1.6414
Epoch 03 — loss: 1.5541
Stage 3: Error=0.6445, Alpha=1.1968
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7928
Epoch 02 — loss: 1.6794
Epoch 03 — loss: 1.6216
Stage 4: Error=0.6498, Alpha=1.1734
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7947
Epoch 02 — loss: 1.7092
Epoch 03 — loss: 1.6458
Stage 5: Error=0.6830, Alpha=1.0241
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4428
Epoch 02 — loss: 1.3472
Epoch 03 — loss: 1.3353
Stage 1: Error=0.5404, Alpha=1.6297
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.5970
Epoch 02 — loss: 1.4971
Epoch 03 — loss: 1.4722
Stage 2: Error=0.6154, Alpha=1.3216
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7168
Epoch 02 — loss: 1.6208
Epoch 03 — loss: 1.5925
Stage 3: Error=0.6584, Alpha=1.1357
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7542
Epoch 02 — loss: 1.6918
Epoch 03 — loss: 1.6431
Stage 4: Error=0.6943, Alpha=0.9714
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8063
Epoch 02 — loss: 1.7291
Epoch 03 — loss: 1.7038
Stage 5: Error=0.6864, Alpha=1.0086
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4339
Epoch 02 — loss: 1.3313
Epoch 03 — loss: 1.2795
Stage 1: Error=0.5188, Alpha=1.7166
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6475
Epoch 02 — loss: 1.5271
Epoch 03 — loss: 1.4826
Stage 2: Error=0.5904, Alpha=1.4260
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7349
Epoch 02 — loss: 1.6276
Epoch 03 — loss: 1.5609
Stage 3: Error=0.6593, Alpha=1.1317
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7849
Epoch 02 — loss: 1.6841
Epoch 03 — loss: 1.6284
Stage 4: Error=0.6647, Alpha=1.1076
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8122
Epoch 02 — loss: 1.7150
Epoch 03 — loss: 1.6333
Stage 5: Error=0.6687, Alpha=1.0896
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8719
Stacking meta epoch 2: loss=0.7359
Stacking meta epoch 3: loss=0.7057
Stacking meta epoch 4: loss=0.6894
Stacking meta epoch 5: loss=0.6783
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8547
Stacking meta epoch 2: loss=0.7349
Stacking meta epoch 3: loss=0.7052
Stacking meta epoch 4: loss=0.6884
Stacking meta epoch 5: loss=0.6769
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2588
Stacking meta epoch 2: loss=1.2439
Stacking meta epoch 3: loss=1.2415
Stacking meta epoch 4: loss=1.2398
Stacking meta epoch 5: loss=1.2384
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      77.77
1                                deepset  ...                      81.52
2                     set_transformer_xy  ...                      77.19
3                             deepset_xy  ...                      80.85
4               set_transformer_additive  ...                      82.19
5                    deepset_xy_additive  ...                      80.65
6                       adaboost_deepset  ...                      82.44
7           adaboost_deepset_xy_additive  ...                      79.74
8                    adaboost_deepset_xy  ...                      81.62
9               adaboost_set_transformer  ...                      81.52
10     adaboost_set_transformer_additive  ...                      81.86
11           adaboost_set_transformer_xy  ...                      81.81
12              soft_voting_ensemble_all  ...                      82.19
13                 stacking_ensemble_all  ...                      83.01
14                      gbm_ensemble_all  ...                      82.68
15                  xgboost_ensemble_all  ...                      83.97
16  soft_voting_ensemble_set_transformer  ...                      81.67
17     stacking_ensemble_set_transformer  ...                      82.53
18          gbm_ensemble_set_transformer  ...                      83.06
19      xgboost_ensemble_set_transformer  ...                      83.30
20          soft_voting_ensemble_deepset  ...                      81.33
21             stacking_ensemble_deepset  ...                      82.92
22                  gbm_ensemble_deepset  ...                      81.38
23              xgboost_ensemble_deepset  ...                      81.23

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 12------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7847
Epoch 02 — loss: 1.6400
Epoch 03 — loss: 1.5781
Epoch 04 — loss: 1.5474
Epoch 05 — loss: 1.5074
Epoch 06 — loss: 1.4773
Epoch 07 — loss: 1.4405
Epoch 08 — loss: 1.4200
Epoch 09 — loss: 1.3780
Epoch 10 — loss: 1.3657
Epoch 11 — loss: 1.3179
Epoch 12 — loss: 1.2965
Epoch 13 — loss: 1.2594
Epoch 14 — loss: 1.2248
Epoch 15 — loss: 1.1936
Epoch 16 — loss: 1.1691
Epoch 17 — loss: 1.1223
Epoch 18 — loss: 1.0919
Epoch 19 — loss: 1.0514
Epoch 20 — loss: 1.0334
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8826
Epoch 02 — loss: 1.6009
Epoch 03 — loss: 1.5396
Epoch 04 — loss: 1.5184
Epoch 05 — loss: 1.5046
Epoch 06 — loss: 1.4898
Epoch 07 — loss: 1.4869
Epoch 08 — loss: 1.4698
Epoch 09 — loss: 1.4591
Epoch 10 — loss: 1.4578
Epoch 11 — loss: 1.4539
Epoch 12 — loss: 1.4521
Epoch 13 — loss: 1.4508
Epoch 14 — loss: 1.4506
Epoch 15 — loss: 1.4511
Epoch 16 — loss: 1.4442
Epoch 17 — loss: 1.4424
Epoch 18 — loss: 1.4486
Epoch 19 — loss: 1.4390
Epoch 20 — loss: 1.4357
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7249
Epoch 02 — loss: 1.5926
Epoch 03 — loss: 1.5529
Epoch 04 — loss: 1.5245
Epoch 05 — loss: 1.5005
Epoch 06 — loss: 1.4611
Epoch 07 — loss: 1.4203
Epoch 08 — loss: 1.4057
Epoch 09 — loss: 1.3814
Epoch 10 — loss: 1.3516
Epoch 11 — loss: 1.3135
Epoch 12 — loss: 1.2820
Epoch 13 — loss: 1.2549
Epoch 14 — loss: 1.2128
Epoch 15 — loss: 1.1946
Epoch 16 — loss: 1.1447
Epoch 17 — loss: 1.1090
Epoch 18 — loss: 1.0729
Epoch 19 — loss: 1.0330
Epoch 20 — loss: 0.9870
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8641
Epoch 02 — loss: 1.5943
Epoch 03 — loss: 1.5357
Epoch 04 — loss: 1.5187
Epoch 05 — loss: 1.5012
Epoch 06 — loss: 1.4919
Epoch 07 — loss: 1.4815
Epoch 08 — loss: 1.4798
Epoch 09 — loss: 1.4684
Epoch 10 — loss: 1.4669
Epoch 11 — loss: 1.4653
Epoch 12 — loss: 1.4529
Epoch 13 — loss: 1.4595
Epoch 14 — loss: 1.4485
Epoch 15 — loss: 1.4417
Epoch 16 — loss: 1.4445
Epoch 17 — loss: 1.4513
Epoch 18 — loss: 1.4432
Epoch 19 — loss: 1.4336
Epoch 20 — loss: 1.4355
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7173
Epoch 02 — loss: 1.6196
Epoch 03 — loss: 1.5746
Epoch 04 — loss: 1.5535
Epoch 05 — loss: 1.5281
Epoch 06 — loss: 1.5060
Epoch 07 — loss: 1.4834
Epoch 08 — loss: 1.4698
Epoch 09 — loss: 1.4426
Epoch 10 — loss: 1.4271
Epoch 11 — loss: 1.4112
Epoch 12 — loss: 1.3908
Epoch 13 — loss: 1.3645
Epoch 14 — loss: 1.3534
Epoch 15 — loss: 1.3350
Epoch 16 — loss: 1.3029
Epoch 17 — loss: 1.2736
Epoch 18 — loss: 1.2513
Epoch 19 — loss: 1.2194
Epoch 20 — loss: 1.1943
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8180
Epoch 02 — loss: 1.6340
Epoch 03 — loss: 1.5757
Epoch 04 — loss: 1.5479
Epoch 05 — loss: 1.5215
Epoch 06 — loss: 1.5053
Epoch 07 — loss: 1.4910
Epoch 08 — loss: 1.4840
Epoch 09 — loss: 1.4806
Epoch 10 — loss: 1.4758
Epoch 11 — loss: 1.4692
Epoch 12 — loss: 1.4719
Epoch 13 — loss: 1.4649
Epoch 14 — loss: 1.4639
Epoch 15 — loss: 1.4582
Epoch 16 — loss: 1.4557
Epoch 17 — loss: 1.4492
Epoch 18 — loss: 1.4539
Epoch 19 — loss: 1.4492
Epoch 20 — loss: 1.4482
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6223
Epoch 02 — loss: 1.3744
Epoch 03 — loss: 1.3133
Stage 1: Error=0.5262, Alpha=1.6867
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7917
Epoch 02 — loss: 1.5964
Epoch 03 — loss: 1.5225
Stage 2: Error=0.6397, Alpha=1.2176
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8117
Epoch 02 — loss: 1.6582
Epoch 03 — loss: 1.5904
Stage 3: Error=0.6556, Alpha=1.1478
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8388
Epoch 02 — loss: 1.7697
Epoch 03 — loss: 1.7261
Stage 4: Error=0.7578, Alpha=0.6510
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8657
Epoch 02 — loss: 1.8146
Epoch 03 — loss: 1.7709
Stage 5: Error=0.7757, Alpha=0.5509
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5710
Epoch 02 — loss: 1.3619
Epoch 03 — loss: 1.3473
Stage 1: Error=0.5360, Alpha=1.6476
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7453
Epoch 02 — loss: 1.5962
Epoch 03 — loss: 1.5393
Stage 2: Error=0.6592, Alpha=1.1318
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7794
Epoch 02 — loss: 1.6447
Epoch 03 — loss: 1.6049
Stage 3: Error=0.6471, Alpha=1.1853
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8342
Epoch 02 — loss: 1.7473
Epoch 03 — loss: 1.6967
Stage 4: Error=0.7464, Alpha=0.7123
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8256
Epoch 02 — loss: 1.7671
Epoch 03 — loss: 1.7229
Stage 5: Error=0.7070, Alpha=0.9111
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6370
Epoch 02 — loss: 1.3797
Epoch 03 — loss: 1.3134
Stage 1: Error=0.5336, Alpha=1.6573
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7683
Epoch 02 — loss: 1.5788
Epoch 03 — loss: 1.5151
Stage 2: Error=0.6184, Alpha=1.3092
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8355
Epoch 02 — loss: 1.6589
Epoch 03 — loss: 1.6024
Stage 3: Error=0.6632, Alpha=1.1141
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8641
Epoch 02 — loss: 1.7624
Epoch 03 — loss: 1.7016
Stage 4: Error=0.7305, Alpha=0.7948
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8919/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [09:57:40] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [09:59:55] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [10:01:50] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8280
Epoch 03 — loss: 1.7573
Stage 5: Error=0.7691, Alpha=0.5887
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4616
Epoch 02 — loss: 1.3346
Epoch 03 — loss: 1.2973
Stage 1: Error=0.5264, Alpha=1.6862
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6438
Epoch 02 — loss: 1.5221
Epoch 03 — loss: 1.4918
Stage 2: Error=0.6006, Alpha=1.3836
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7345
Epoch 02 — loss: 1.6417
Epoch 03 — loss: 1.5814
Stage 3: Error=0.6342, Alpha=1.2416
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8085
Epoch 02 — loss: 1.6923
Epoch 03 — loss: 1.6569
Stage 4: Error=0.6635, Alpha=1.1129
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8052
Epoch 02 — loss: 1.7190
Epoch 03 — loss: 1.6468
Stage 5: Error=0.6702, Alpha=1.0827
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4316
Epoch 02 — loss: 1.3484
Epoch 03 — loss: 1.2994
Stage 1: Error=0.5310, Alpha=1.6674
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6363
Epoch 02 — loss: 1.5471
Epoch 03 — loss: 1.5073
Stage 2: Error=0.6188, Alpha=1.3074
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7074
Epoch 02 — loss: 1.6195
Epoch 03 — loss: 1.5888
Stage 3: Error=0.6607, Alpha=1.1255
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7905
Epoch 02 — loss: 1.7306
Epoch 03 — loss: 1.6848
Stage 4: Error=0.6808, Alpha=1.0342
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8183
Epoch 02 — loss: 1.7517
Epoch 03 — loss: 1.7118
Stage 5: Error=0.7109, Alpha=0.8919
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4482
Epoch 02 — loss: 1.3387
Epoch 03 — loss: 1.2745
Stage 1: Error=0.5113, Alpha=1.7465
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6867
Epoch 02 — loss: 1.5603
Epoch 03 — loss: 1.5037
Stage 2: Error=0.6251, Alpha=1.2804
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7417
Epoch 02 — loss: 1.6205
Epoch 03 — loss: 1.5479
Stage 3: Error=0.6306, Alpha=1.2571
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7884
Epoch 02 — loss: 1.6852
Epoch 03 — loss: 1.6091
Stage 4: Error=0.6635, Alpha=1.1127
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8065
Epoch 02 — loss: 1.7198
Epoch 03 — loss: 1.6540
Stage 5: Error=0.6577, Alpha=1.1388
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8967
Stacking meta epoch 2: loss=0.7631
Stacking meta epoch 3: loss=0.7349
Stacking meta epoch 4: loss=0.7197
Stacking meta epoch 5: loss=0.7092
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8747
Stacking meta epoch 2: loss=0.7659
Stacking meta epoch 3: loss=0.7386
Stacking meta epoch 4: loss=0.7233
Stacking meta epoch 5: loss=0.7128
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2564
Stacking meta epoch 2: loss=1.2415
Stacking meta epoch 3: loss=1.2394
Stacking meta epoch 4: loss=1.2380
Stacking meta epoch 5: loss=1.2368
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      77.91
1                                deepset  ...                      77.62
2                     set_transformer_xy  ...                      79.98
3                             deepset_xy  ...                      81.14
4               set_transformer_additive  ...                      80.99
5                    deepset_xy_additive  ...                      80.65
6                       adaboost_deepset  ...                      82.19
7           adaboost_deepset_xy_additive  ...                      80.03
8                    adaboost_deepset_xy  ...                      82.29
9               adaboost_set_transformer  ...                      80.80
10     adaboost_set_transformer_additive  ...                      81.91
11           adaboost_set_transformer_xy  ...                      82.44
12              soft_voting_ensemble_all  ...                      82.10
13                 stacking_ensemble_all  ...                      83.35
14                      gbm_ensemble_all  ...                      83.73
15                  xgboost_ensemble_all  ...                      83.64
16  soft_voting_ensemble_set_transformer  ...                      82.77
17     stacking_ensemble_set_transformer  ...                      83.35
18          gbm_ensemble_set_transformer  ...                      83.88
19      xgboost_ensemble_set_transformer  ...                      83.49
20          soft_voting_ensemble_deepset  ...                      80.03
21             stacking_ensemble_deepset  ...                      83.01
22                  gbm_ensemble_deepset  ...                      80.37
23              xgboost_ensemble_deepset  ...                      80.85

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 13------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7517
Epoch 02 — loss: 1.6229
Epoch 03 — loss: 1.5699
Epoch 04 — loss: 1.5352
Epoch 05 — loss: 1.5015
Epoch 06 — loss: 1.4573
Epoch 07 — loss: 1.4369
Epoch 08 — loss: 1.3923
Epoch 09 — loss: 1.3656
Epoch 10 — loss: 1.3295
Epoch 11 — loss: 1.2954
Epoch 12 — loss: 1.2605
Epoch 13 — loss: 1.2201
Epoch 14 — loss: 1.1896
Epoch 15 — loss: 1.1503
Epoch 16 — loss: 1.1171
Epoch 17 — loss: 1.0931
Epoch 18 — loss: 1.0359
Epoch 19 — loss: 1.0022
Epoch 20 — loss: 0.9720
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8898
Epoch 02 — loss: 1.6183
Epoch 03 — loss: 1.5436
Epoch 04 — loss: 1.5158
Epoch 05 — loss: 1.5041
Epoch 06 — loss: 1.4960
Epoch 07 — loss: 1.4894
Epoch 08 — loss: 1.4798
Epoch 09 — loss: 1.4678
Epoch 10 — loss: 1.4646
Epoch 11 — loss: 1.4609
Epoch 12 — loss: 1.4591
Epoch 13 — loss: 1.4537
Epoch 14 — loss: 1.4470
Epoch 15 — loss: 1.4493
Epoch 16 — loss: 1.4438
Epoch 17 — loss: 1.4414
Epoch 18 — loss: 1.4370
Epoch 19 — loss: 1.4386
Epoch 20 — loss: 1.4384
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7303
Epoch 02 — loss: 1.6059
Epoch 03 — loss: 1.5775
Epoch 04 — loss: 1.5450
Epoch 05 — loss: 1.5046
Epoch 06 — loss: 1.4848
Epoch 07 — loss: 1.4529
Epoch 08 — loss: 1.4214
Epoch 09 — loss: 1.3931
Epoch 10 — loss: 1.3591
Epoch 11 — loss: 1.3391
Epoch 12 — loss: 1.2979
Epoch 13 — loss: 1.2704
Epoch 14 — loss: 1.2237
Epoch 15 — loss: 1.1871
Epoch 16 — loss: 1.1568
Epoch 17 — loss: 1.1202
Epoch 18 — loss: 1.0888
Epoch 19 — loss: 1.0542
Epoch 20 — loss: 1.0141
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8845
Epoch 02 — loss: 1.6053
Epoch 03 — loss: 1.5456
Epoch 04 — loss: 1.5190
Epoch 05 — loss: 1.4991
Epoch 06 — loss: 1.4857
Epoch 07 — loss: 1.4769
Epoch 08 — loss: 1.4709
Epoch 09 — loss: 1.4588
Epoch 10 — loss: 1.4596
Epoch 11 — loss: 1.4542
Epoch 12 — loss: 1.4596
Epoch 13 — loss: 1.4615
Epoch 14 — loss: 1.4436
Epoch 15 — loss: 1.4512
Epoch 16 — loss: 1.4442
Epoch 17 — loss: 1.4380
Epoch 18 — loss: 1.4492
Epoch 19 — loss: 1.4450
Epoch 20 — loss: 1.4338
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7011
Epoch 02 — loss: 1.5993
Epoch 03 — loss: 1.5701
Epoch 04 — loss: 1.5609
Epoch 05 — loss: 1.5384
Epoch 06 — loss: 1.5173
Epoch 07 — loss: 1.5011
Epoch 08 — loss: 1.4938
Epoch 09 — loss: 1.4593
Epoch 10 — loss: 1.4519
Epoch 11 — loss: 1.4315
Epoch 12 — loss: 1.4202
Epoch 13 — loss: 1.3990
Epoch 14 — loss: 1.3741
Epoch 15 — loss: 1.3501
Epoch 16 — loss: 1.3412
Epoch 17 — loss: 1.3147
Epoch 18 — loss: 1.2943
Epoch 19 — loss: 1.2619
Epoch 20 — loss: 1.2482
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7842
Epoch 02 — loss: 1.6217
Epoch 03 — loss: 1.5805
Epoch 04 — loss: 1.5503
Epoch 05 — loss: 1.5373
Epoch 06 — loss: 1.5123
Epoch 07 — loss: 1.5036
Epoch 08 — loss: 1.4893
Epoch 09 — loss: 1.4805
Epoch 10 — loss: 1.4822
Epoch 11 — loss: 1.4690
Epoch 12 — loss: 1.4645
Epoch 13 — loss: 1.4673
Epoch 14 — loss: 1.4633
Epoch 15 — loss: 1.4603
Epoch 16 — loss: 1.4509
Epoch 17 — loss: 1.4461
Epoch 18 — loss: 1.4514
Epoch 19 — loss: 1.4446
Epoch 20 — loss: 1.4506
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6138
Epoch 02 — loss: 1.3756
Epoch 03 — loss: 1.2971
Stage 1: Error=0.5259, Alpha=1.6882
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7825
Epoch 02 — loss: 1.6355
Epoch 03 — loss: 1.5368
Stage 2: Error=0.6512, Alpha=1.1672
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7910
Epoch 02 — loss: 1.6611
Epoch 03 — loss: 1.5808
Stage 3: Error=0.6494, Alpha=1.1754
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8654
Epoch 02 — loss: 1.7898
Epoch 03 — loss: 1.7367
Stage 4: Error=0.7610, Alpha=0.6333
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8787
Epoch 02 — loss: 1.8104
Epoch 03 — loss: 1.7659
Stage 5: Error=0.7796, Alpha=0.5283
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5920
Epoch 02 — loss: 1.3725
Epoch 03 — loss: 1.3447
Stage 1: Error=0.5341, Alpha=1.6553
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7296
Epoch 02 — loss: 1.5811
Epoch 03 — loss: 1.5322
Stage 2: Error=0.6345, Alpha=1.2400
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7679
Epoch 02 — loss: 1.6650
Epoch 03 — loss: 1.6212
Stage 3: Error=0.6584, Alpha=1.1354
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8373
Epoch 02 — loss: 1.7383
Epoch 03 — loss: 1.7030
Stage 4: Error=0.7255, Alpha=0.8200
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8686
Epoch 02 — loss: 1.7944
Epoch 03 — loss: 1.7476
Stage 5: Error=0.7524, Alpha=0.6801
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6346
Epoch 02 — loss: 1.3679
Epoch 03 — loss: 1.3074
Stage 1: Error=0.5243, Alpha=1.6945
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7866
Epoch 02 — loss: 1.5766
Epoch 03 — loss: 1.5237
Stage 2: Error=0.6278, Alpha=1.2692
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7882
Epoch 02 — loss: 1.6541
Epoch 03 — loss: 1.5941
Stage 3: Error=0.6817, Alpha=1.0301
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8397
Epoch 02 — loss: 1.7569
Epoch 03 — loss: 1.6926
Stage 4: Error=0.7225, Alpha=0.8347
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8539/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [10:43:03] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [10:45:18] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [10:47:12] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.7999
Epoch 03 — loss: 1.7481
Stage 5: Error=0.7578, Alpha=0.6511
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4622
Epoch 02 — loss: 1.3383
Epoch 03 — loss: 1.3220
Stage 1: Error=0.5332, Alpha=1.6587
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6681
Epoch 02 — loss: 1.5635
Epoch 03 — loss: 1.5101
Stage 2: Error=0.5992, Alpha=1.3896
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7425
Epoch 02 — loss: 1.6427
Epoch 03 — loss: 1.5822
Stage 3: Error=0.6558, Alpha=1.1470
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7747
Epoch 02 — loss: 1.6800
Epoch 03 — loss: 1.6246
Stage 4: Error=0.6737, Alpha=1.0668
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7995
Epoch 02 — loss: 1.7122
Epoch 03 — loss: 1.6449
Stage 5: Error=0.6551, Alpha=1.1502
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4563
Epoch 02 — loss: 1.3654
Epoch 03 — loss: 1.3274
Stage 1: Error=0.5395, Alpha=1.6335
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6552
Epoch 02 — loss: 1.5764
Epoch 03 — loss: 1.5269
Stage 2: Error=0.6442, Alpha=1.1981
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7002
Epoch 02 — loss: 1.6385
Epoch 03 — loss: 1.6118
Stage 3: Error=0.6607, Alpha=1.1254
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7902
Epoch 02 — loss: 1.7160
Epoch 03 — loss: 1.6747
Stage 4: Error=0.6949, Alpha=0.9686
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8055
Epoch 02 — loss: 1.7435
Epoch 03 — loss: 1.6996
Stage 5: Error=0.7087, Alpha=0.9028
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4301
Epoch 02 — loss: 1.3220
Epoch 03 — loss: 1.2815
Stage 1: Error=0.5188, Alpha=1.7166
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6944
Epoch 02 — loss: 1.5678
Epoch 03 — loss: 1.5140
Stage 2: Error=0.6310, Alpha=1.2554
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7044
Epoch 02 — loss: 1.5997
Epoch 03 — loss: 1.5411
Stage 3: Error=0.6166, Alpha=1.3168
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7891
Epoch 02 — loss: 1.6985
Epoch 03 — loss: 1.6302
Stage 4: Error=0.6624, Alpha=1.1178
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7919
Epoch 02 — loss: 1.6925
Epoch 03 — loss: 1.6297
Stage 5: Error=0.6547, Alpha=1.1518
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.9034
Stacking meta epoch 2: loss=0.7665
Stacking meta epoch 3: loss=0.7340
Stacking meta epoch 4: loss=0.7167
Stacking meta epoch 5: loss=0.7051
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8915
Stacking meta epoch 2: loss=0.7697
Stacking meta epoch 3: loss=0.7387
Stacking meta epoch 4: loss=0.7214
Stacking meta epoch 5: loss=0.7095
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2534
Stacking meta epoch 2: loss=1.2396
Stacking meta epoch 3: loss=1.2372
Stacking meta epoch 4: loss=1.2355
Stacking meta epoch 5: loss=1.2342
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      79.64
1                                deepset  ...                      80.17
2                     set_transformer_xy  ...                      80.90
3                             deepset_xy  ...                      80.17
4               set_transformer_additive  ...                      80.90
5                    deepset_xy_additive  ...                      82.68
6                       adaboost_deepset  ...                      81.86
7           adaboost_deepset_xy_additive  ...                      81.57
8                    adaboost_deepset_xy  ...                      82.82
9               adaboost_set_transformer  ...                      80.85
10     adaboost_set_transformer_additive  ...                      80.75
11           adaboost_set_transformer_xy  ...                      81.38
12              soft_voting_ensemble_all  ...                      83.54
13                 stacking_ensemble_all  ...                      84.02
14                      gbm_ensemble_all  ...                      83.06
15                  xgboost_ensemble_all  ...                      82.92
16  soft_voting_ensemble_set_transformer  ...                      83.54
17     stacking_ensemble_set_transformer  ...                      84.07
18          gbm_ensemble_set_transformer  ...                      83.30
19      xgboost_ensemble_set_transformer  ...                      83.69
20          soft_voting_ensemble_deepset  ...                      81.38
21             stacking_ensemble_deepset  ...                      82.87
22                  gbm_ensemble_deepset  ...                      80.94
23              xgboost_ensemble_deepset  ...                      81.04

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 14------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7409
Epoch 02 — loss: 1.6322
Epoch 03 — loss: 1.5879
Epoch 04 — loss: 1.5561
Epoch 05 — loss: 1.5231
Epoch 06 — loss: 1.4973
Epoch 07 — loss: 1.4595
Epoch 08 — loss: 1.4388
Epoch 09 — loss: 1.3978
Epoch 10 — loss: 1.3644
Epoch 11 — loss: 1.3379
Epoch 12 — loss: 1.3039
Epoch 13 — loss: 1.2764
Epoch 14 — loss: 1.2288
Epoch 15 — loss: 1.2004
Epoch 16 — loss: 1.1588
Epoch 17 — loss: 1.1380
Epoch 18 — loss: 1.0784
Epoch 19 — loss: 1.0435
Epoch 20 — loss: 1.0199
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8799
Epoch 02 — loss: 1.6002
Epoch 03 — loss: 1.5376
Epoch 04 — loss: 1.5158
Epoch 05 — loss: 1.4975
Epoch 06 — loss: 1.4916
Epoch 07 — loss: 1.4870
Epoch 08 — loss: 1.4747
Epoch 09 — loss: 1.4739
Epoch 10 — loss: 1.4686
Epoch 11 — loss: 1.4654
Epoch 12 — loss: 1.4588
Epoch 13 — loss: 1.4646
Epoch 14 — loss: 1.4564
Epoch 15 — loss: 1.4542
Epoch 16 — loss: 1.4505
Epoch 17 — loss: 1.4526
Epoch 18 — loss: 1.4441
Epoch 19 — loss: 1.4451
Epoch 20 — loss: 1.4457
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7191
Epoch 02 — loss: 1.5955
Epoch 03 — loss: 1.5532
Epoch 04 — loss: 1.5115
Epoch 05 — loss: 1.5038
Epoch 06 — loss: 1.4610
Epoch 07 — loss: 1.4169
Epoch 08 — loss: 1.4003
Epoch 09 — loss: 1.3545
Epoch 10 — loss: 1.3314
Epoch 11 — loss: 1.2998
Epoch 12 — loss: 1.2635
Epoch 13 — loss: 1.2414
Epoch 14 — loss: 1.1958
Epoch 15 — loss: 1.1563
Epoch 16 — loss: 1.1466
Epoch 17 — loss: 1.0926
Epoch 18 — loss: 1.0671
Epoch 19 — loss: 1.0140
Epoch 20 — loss: 0.9921
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8929
Epoch 02 — loss: 1.6047
Epoch 03 — loss: 1.5369
Epoch 04 — loss: 1.5140
Epoch 05 — loss: 1.4913
Epoch 06 — loss: 1.4848
Epoch 07 — loss: 1.4765
Epoch 08 — loss: 1.4725
Epoch 09 — loss: 1.4592
Epoch 10 — loss: 1.4581
Epoch 11 — loss: 1.4602
Epoch 12 — loss: 1.4506
Epoch 13 — loss: 1.4580
Epoch 14 — loss: 1.4471
Epoch 15 — loss: 1.4470
Epoch 16 — loss: 1.4405
Epoch 17 — loss: 1.4469
Epoch 18 — loss: 1.4454
Epoch 19 — loss: 1.4406
Epoch 20 — loss: 1.4432
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7098
Epoch 02 — loss: 1.6178
Epoch 03 — loss: 1.5794
Epoch 04 — loss: 1.5562
Epoch 05 — loss: 1.5375
Epoch 06 — loss: 1.5216
Epoch 07 — loss: 1.5025
Epoch 08 — loss: 1.4813
Epoch 09 — loss: 1.4711
Epoch 10 — loss: 1.4427
Epoch 11 — loss: 1.4307
Epoch 12 — loss: 1.4016
Epoch 13 — loss: 1.3824
Epoch 14 — loss: 1.3573
Epoch 15 — loss: 1.3307
Epoch 16 — loss: 1.3078
Epoch 17 — loss: 1.2902
Epoch 18 — loss: 1.2579
Epoch 19 — loss: 1.2382
Epoch 20 — loss: 1.2161
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8214
Epoch 02 — loss: 1.6264
Epoch 03 — loss: 1.5785
Epoch 04 — loss: 1.5400
Epoch 05 — loss: 1.5245
Epoch 06 — loss: 1.5087
Epoch 07 — loss: 1.4979
Epoch 08 — loss: 1.4870
Epoch 09 — loss: 1.4807
Epoch 10 — loss: 1.4744
Epoch 11 — loss: 1.4691
Epoch 12 — loss: 1.4640
Epoch 13 — loss: 1.4634
Epoch 14 — loss: 1.4567
Epoch 15 — loss: 1.4581
Epoch 16 — loss: 1.4548
Epoch 17 — loss: 1.4489
Epoch 18 — loss: 1.4466
Epoch 19 — loss: 1.4470
Epoch 20 — loss: 1.4406
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6200
Epoch 02 — loss: 1.3974
Epoch 03 — loss: 1.3315
Stage 1: Error=0.5250, Alpha=1.6916
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8049
Epoch 02 — loss: 1.6142
Epoch 03 — loss: 1.5225
Stage 2: Error=0.6446, Alpha=1.1964
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8064
Epoch 02 — loss: 1.6399
Epoch 03 — loss: 1.5646
Stage 3: Error=0.6262, Alpha=1.2758
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8777
Epoch 02 — loss: 1.7827
Epoch 03 — loss: 1.7193
Stage 4: Error=0.7426, Alpha=0.7324
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8951
Epoch 02 — loss: 1.8339
Epoch 03 — loss: 1.7599
Stage 5: Error=0.7911, Alpha=0.4602
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5824
Epoch 02 — loss: 1.3518
Epoch 03 — loss: 1.3035
Stage 1: Error=0.5344, Alpha=1.6539
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7655
Epoch 02 — loss: 1.5840
Epoch 03 — loss: 1.5501
Stage 2: Error=0.6441, Alpha=1.1983
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7823
Epoch 02 — loss: 1.6636
Epoch 03 — loss: 1.6108
Stage 3: Error=0.6451, Alpha=1.1944
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8583
Epoch 02 — loss: 1.7644
Epoch 03 — loss: 1.7254
Stage 4: Error=0.7302, Alpha=0.7962
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8710
Epoch 02 — loss: 1.8035
Epoch 03 — loss: 1.7438
Stage 5: Error=0.7481, Alpha=0.7030
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6159
Epoch 02 — loss: 1.3621
Epoch 03 — loss: 1.3020
Stage 1: Error=0.5249, Alpha=1.6920
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7842
Epoch 02 — loss: 1.6009
Epoch 03 — loss: 1.5133
Stage 2: Error=0.6362, Alpha=1.2331
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8055
Epoch 02 — loss: 1.6658
Epoch 03 — loss: 1.5928
Stage 3: Error=0.6726, Alpha=1.0718
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8711
Epoch 02 — loss: 1.7762
Epoch 03 — loss: 1.7069
Stage 4: Error=0.7491, Alpha=0.6977
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8755/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [11:28:24] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [11:30:38] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [11:32:33] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8035
Epoch 03 — loss: 1.7518
Stage 5: Error=0.7778, Alpha=0.5390
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4712
Epoch 02 — loss: 1.3646
Epoch 03 — loss: 1.3377
Stage 1: Error=0.5466, Alpha=1.6049
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6098
Epoch 02 — loss: 1.4882
Epoch 03 — loss: 1.4422
Stage 2: Error=0.5696, Alpha=1.5116
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7567
Epoch 02 — loss: 1.6538
Epoch 03 — loss: 1.5997
Stage 3: Error=0.6465, Alpha=1.1880
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7873
Epoch 02 — loss: 1.6940
Epoch 03 — loss: 1.6242
Stage 4: Error=0.6498, Alpha=1.1735
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8071
Epoch 02 — loss: 1.7237
Epoch 03 — loss: 1.6401
Stage 5: Error=0.6487, Alpha=1.1784
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4495
Epoch 02 — loss: 1.3481
Epoch 03 — loss: 1.3309
Stage 1: Error=0.5331, Alpha=1.6592
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6075
Epoch 02 — loss: 1.5502
Epoch 03 — loss: 1.4855
Stage 2: Error=0.6099, Alpha=1.3448
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7094
Epoch 02 — loss: 1.6396
Epoch 03 — loss: 1.5886
Stage 3: Error=0.6714, Alpha=1.0775
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7776
Epoch 02 — loss: 1.7070
Epoch 03 — loss: 1.6589
Stage 4: Error=0.6934, Alpha=0.9756
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8169
Epoch 02 — loss: 1.7672
Epoch 03 — loss: 1.7363
Stage 5: Error=0.7042, Alpha=0.9244
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4121
Epoch 02 — loss: 1.3344
Epoch 03 — loss: 1.2587
Stage 1: Error=0.5259, Alpha=1.6882
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6441
Epoch 02 — loss: 1.5431
Epoch 03 — loss: 1.5013
Stage 2: Error=0.5949, Alpha=1.4075
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7270
Epoch 02 — loss: 1.6206
Epoch 03 — loss: 1.5505
Stage 3: Error=0.6432, Alpha=1.2025
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7807
Epoch 02 — loss: 1.6700
Epoch 03 — loss: 1.6228
Stage 4: Error=0.6799, Alpha=1.0382
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7828
Epoch 02 — loss: 1.7083
Epoch 03 — loss: 1.6431
Stage 5: Error=0.6825, Alpha=1.0264
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8763
Stacking meta epoch 2: loss=0.7456
Stacking meta epoch 3: loss=0.7175
Stacking meta epoch 4: loss=0.7023
Stacking meta epoch 5: loss=0.6917
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8761
Stacking meta epoch 2: loss=0.7499
Stacking meta epoch 3: loss=0.7206
Stacking meta epoch 4: loss=0.7043
Stacking meta epoch 5: loss=0.6930
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2586
Stacking meta epoch 2: loss=1.2468
Stacking meta epoch 3: loss=1.2447
Stacking meta epoch 4: loss=1.2432
Stacking meta epoch 5: loss=1.2419
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      80.85
1                                deepset  ...                      80.80
2                     set_transformer_xy  ...                      80.80
3                             deepset_xy  ...                      80.75
4               set_transformer_additive  ...                      76.03
5                    deepset_xy_additive  ...                      81.09
6                       adaboost_deepset  ...                      81.38
7           adaboost_deepset_xy_additive  ...                      80.27
8                    adaboost_deepset_xy  ...                      81.52
9               adaboost_set_transformer  ...                      80.27
10     adaboost_set_transformer_additive  ...                      82.29
11           adaboost_set_transformer_xy  ...                      80.17
12              soft_voting_ensemble_all  ...                      82.24
13                 stacking_ensemble_all  ...                      84.02
14                      gbm_ensemble_all  ...                      83.25
15                  xgboost_ensemble_all  ...                      83.69
16  soft_voting_ensemble_set_transformer  ...                      82.00
17     stacking_ensemble_set_transformer  ...                      83.78
18          gbm_ensemble_set_transformer  ...                      83.83
19      xgboost_ensemble_set_transformer  ...                      83.83
20          soft_voting_ensemble_deepset  ...                      81.42
21             stacking_ensemble_deepset  ...                      81.95
22                  gbm_ensemble_deepset  ...                      80.08
23              xgboost_ensemble_deepset  ...                      81.14

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 15------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7719
Epoch 02 — loss: 1.6388
Epoch 03 — loss: 1.5909
Epoch 04 — loss: 1.5485
Epoch 05 — loss: 1.5239
Epoch 06 — loss: 1.4789
Epoch 07 — loss: 1.4434
Epoch 08 — loss: 1.4075
Epoch 09 — loss: 1.3675
Epoch 10 — loss: 1.3371
Epoch 11 — loss: 1.2973
Epoch 12 — loss: 1.2665
Epoch 13 — loss: 1.2336
Epoch 14 — loss: 1.2042
Epoch 15 — loss: 1.1617
Epoch 16 — loss: 1.1255
Epoch 17 — loss: 1.0990
Epoch 18 — loss: 1.0538
Epoch 19 — loss: 1.0201
Epoch 20 — loss: 0.9946
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.9036
Epoch 02 — loss: 1.6516
Epoch 03 — loss: 1.5506
Epoch 04 — loss: 1.5204
Epoch 05 — loss: 1.5066
Epoch 06 — loss: 1.4935
Epoch 07 — loss: 1.4797
Epoch 08 — loss: 1.4817
Epoch 09 — loss: 1.4739
Epoch 10 — loss: 1.4665
Epoch 11 — loss: 1.4632
Epoch 12 — loss: 1.4574
Epoch 13 — loss: 1.4547
Epoch 14 — loss: 1.4584
Epoch 15 — loss: 1.4539
Epoch 16 — loss: 1.4473
Epoch 17 — loss: 1.4538
Epoch 18 — loss: 1.4537
Epoch 19 — loss: 1.4454
Epoch 20 — loss: 1.4458
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7464
Epoch 02 — loss: 1.6067
Epoch 03 — loss: 1.5521
Epoch 04 — loss: 1.5144
Epoch 05 — loss: 1.4678
Epoch 06 — loss: 1.4399
Epoch 07 — loss: 1.4134
Epoch 08 — loss: 1.3785
Epoch 09 — loss: 1.3540
Epoch 10 — loss: 1.3163
Epoch 11 — loss: 1.2848
Epoch 12 — loss: 1.2539
Epoch 13 — loss: 1.2149
Epoch 14 — loss: 1.1941
Epoch 15 — loss: 1.1457
Epoch 16 — loss: 1.1162
Epoch 17 — loss: 1.0883
Epoch 18 — loss: 1.0465
Epoch 19 — loss: 1.0174
Epoch 20 — loss: 0.9878
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8770
Epoch 02 — loss: 1.6098
Epoch 03 — loss: 1.5550
Epoch 04 — loss: 1.5211
Epoch 05 — loss: 1.5072
Epoch 06 — loss: 1.5022
Epoch 07 — loss: 1.4872
Epoch 08 — loss: 1.4788
Epoch 09 — loss: 1.4722
Epoch 10 — loss: 1.4713
Epoch 11 — loss: 1.4620
Epoch 12 — loss: 1.4554
Epoch 13 — loss: 1.4583
Epoch 14 — loss: 1.4574
Epoch 15 — loss: 1.4483
Epoch 16 — loss: 1.4502
Epoch 17 — loss: 1.4454
Epoch 18 — loss: 1.4421
Epoch 19 — loss: 1.4424
Epoch 20 — loss: 1.4411
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7112
Epoch 02 — loss: 1.6125
Epoch 03 — loss: 1.5804
Epoch 04 — loss: 1.5499
Epoch 05 — loss: 1.5335
Epoch 06 — loss: 1.5010
Epoch 07 — loss: 1.4861
Epoch 08 — loss: 1.4602
Epoch 09 — loss: 1.4389
Epoch 10 — loss: 1.4346
Epoch 11 — loss: 1.4109
Epoch 12 — loss: 1.3791
Epoch 13 — loss: 1.3640
Epoch 14 — loss: 1.3387
Epoch 15 — loss: 1.3105
Epoch 16 — loss: 1.2875
Epoch 17 — loss: 1.2575
Epoch 18 — loss: 1.2361
Epoch 19 — loss: 1.2099
Epoch 20 — loss: 1.1888
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7831
Epoch 02 — loss: 1.6177
Epoch 03 — loss: 1.5845
Epoch 04 — loss: 1.5614
Epoch 05 — loss: 1.5403
Epoch 06 — loss: 1.5232
Epoch 07 — loss: 1.5097
Epoch 08 — loss: 1.4951
Epoch 09 — loss: 1.4878
Epoch 10 — loss: 1.4862
Epoch 11 — loss: 1.4752
Epoch 12 — loss: 1.4781
Epoch 13 — loss: 1.4713
Epoch 14 — loss: 1.4670
Epoch 15 — loss: 1.4666
Epoch 16 — loss: 1.4570
Epoch 17 — loss: 1.4611
Epoch 18 — loss: 1.4516
Epoch 19 — loss: 1.4535
Epoch 20 — loss: 1.4489
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6053
Epoch 02 — loss: 1.3639
Epoch 03 — loss: 1.3182
Stage 1: Error=0.5326, Alpha=1.6611
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7939
Epoch 02 — loss: 1.6076
Epoch 03 — loss: 1.5230
Stage 2: Error=0.6459, Alpha=1.1909
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8174
Epoch 02 — loss: 1.6596
Epoch 03 — loss: 1.5808
Stage 3: Error=0.6428, Alpha=1.2041
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8562
Epoch 02 — loss: 1.7656
Epoch 03 — loss: 1.6896
Stage 4: Error=0.7141, Alpha=0.8766
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8733
Epoch 02 — loss: 1.8029
Epoch 03 — loss: 1.7426
Stage 5: Error=0.7706, Alpha=0.5798
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5547
Epoch 02 — loss: 1.3399
Epoch 03 — loss: 1.3139
Stage 1: Error=0.5366, Alpha=1.6452
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7162
Epoch 02 — loss: 1.5910
Epoch 03 — loss: 1.5447
Stage 2: Error=0.6553, Alpha=1.1493
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7647
Epoch 02 — loss: 1.6420
Epoch 03 — loss: 1.5969
Stage 3: Error=0.6687, Alpha=1.0895
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8323
Epoch 02 — loss: 1.7664
Epoch 03 — loss: 1.7080
Stage 4: Error=0.7453, Alpha=0.7182
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8573
Epoch 02 — loss: 1.7885
Epoch 03 — loss: 1.7496
Stage 5: Error=0.7547, Alpha=0.6681
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6291
Epoch 02 — loss: 1.3924
Epoch 03 — loss: 1.3246
Stage 1: Error=0.5288, Alpha=1.6766
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8015
Epoch 02 — loss: 1.5924
Epoch 03 — loss: 1.5253
Stage 2: Error=0.6711, Alpha=1.0786
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7851
Epoch 02 — loss: 1.6477
Epoch 03 — loss: 1.5782
Stage 3: Error=0.6669, Alpha=1.0977
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8748
Epoch 02 — loss: 1.7838
Epoch 03 — loss: 1.7326
Stage 4: Error=0.7409, Alpha=0.7412
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8783/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [12:16:09] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [12:18:23] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [12:20:19] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8086
Epoch 03 — loss: 1.7277
Stage 5: Error=0.7344, Alpha=0.7749
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4728
Epoch 02 — loss: 1.3552
Epoch 03 — loss: 1.2985
Stage 1: Error=0.5231, Alpha=1.6993
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6519
Epoch 02 — loss: 1.5464
Epoch 03 — loss: 1.4901
Stage 2: Error=0.6092, Alpha=1.3479
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7503
Epoch 02 — loss: 1.6252
Epoch 03 — loss: 1.5409
Stage 3: Error=0.6262, Alpha=1.2758
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7739
Epoch 02 — loss: 1.6838
Epoch 03 — loss: 1.6149
Stage 4: Error=0.6566, Alpha=1.1438
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8041
Epoch 02 — loss: 1.7029
Epoch 03 — loss: 1.6251
Stage 5: Error=0.6634, Alpha=1.1134
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4341
Epoch 02 — loss: 1.3380
Epoch 03 — loss: 1.3153
Stage 1: Error=0.5294, Alpha=1.6742
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6180
Epoch 02 — loss: 1.5511
Epoch 03 — loss: 1.5202
Stage 2: Error=0.6374, Alpha=1.2277
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6893
Epoch 02 — loss: 1.6101
Epoch 03 — loss: 1.5706
Stage 3: Error=0.6410, Alpha=1.2123
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7739
Epoch 02 — loss: 1.6891
Epoch 03 — loss: 1.6469
Stage 4: Error=0.6807, Alpha=1.0346
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8238
Epoch 02 — loss: 1.7400
Epoch 03 — loss: 1.7092
Stage 5: Error=0.7355, Alpha=0.7692
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4308
Epoch 02 — loss: 1.3159
Epoch 03 — loss: 1.2715
Stage 1: Error=0.5097, Alpha=1.7528
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6494
Epoch 02 — loss: 1.5476
Epoch 03 — loss: 1.4975
Stage 2: Error=0.6219, Alpha=1.2942
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7307
Epoch 02 — loss: 1.6359
Epoch 03 — loss: 1.5539
Stage 3: Error=0.6309, Alpha=1.2559
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7879
Epoch 02 — loss: 1.6984
Epoch 03 — loss: 1.6148
Stage 4: Error=0.6525, Alpha=1.1615
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8324
Epoch 02 — loss: 1.7366
Epoch 03 — loss: 1.6683
Stage 5: Error=0.6792, Alpha=1.0415
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8683
Stacking meta epoch 2: loss=0.7309
Stacking meta epoch 3: loss=0.7002
Stacking meta epoch 4: loss=0.6834
Stacking meta epoch 5: loss=0.6720
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8688
Stacking meta epoch 2: loss=0.7329
Stacking meta epoch 3: loss=0.7016
Stacking meta epoch 4: loss=0.6843
Stacking meta epoch 5: loss=0.6724
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2584
Stacking meta epoch 2: loss=1.2460
Stacking meta epoch 3: loss=1.2431
Stacking meta epoch 4: loss=1.2412
Stacking meta epoch 5: loss=1.2398
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      79.98
1                                deepset  ...                      79.79
2                     set_transformer_xy  ...                      80.51
3                             deepset_xy  ...                      80.27
4               set_transformer_additive  ...                      78.54
5                    deepset_xy_additive  ...                      79.84
6                       adaboost_deepset  ...                      81.28
7           adaboost_deepset_xy_additive  ...                      81.14
8                    adaboost_deepset_xy  ...                      82.19
9               adaboost_set_transformer  ...                      82.05
10     adaboost_set_transformer_additive  ...                      80.32
11           adaboost_set_transformer_xy  ...                      81.91
12              soft_voting_ensemble_all  ...                      82.10
13                 stacking_ensemble_all  ...                      83.25
14                      gbm_ensemble_all  ...                      83.73
15                  xgboost_ensemble_all  ...                      82.96
16  soft_voting_ensemble_set_transformer  ...                      82.24
17     stacking_ensemble_set_transformer  ...                      83.49
18          gbm_ensemble_set_transformer  ...                      83.54
19      xgboost_ensemble_set_transformer  ...                      83.30
20          soft_voting_ensemble_deepset  ...                      80.51
21             stacking_ensemble_deepset  ...                      82.48
22                  gbm_ensemble_deepset  ...                      80.75
23              xgboost_ensemble_deepset  ...                      81.47

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 16------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7533
Epoch 02 — loss: 1.6183
Epoch 03 — loss: 1.5738
Epoch 04 — loss: 1.5433
Epoch 05 — loss: 1.4962
Epoch 06 — loss: 1.4691
Epoch 07 — loss: 1.4408
Epoch 08 — loss: 1.4158
Epoch 09 — loss: 1.3767
Epoch 10 — loss: 1.3497
Epoch 11 — loss: 1.3178
Epoch 12 — loss: 1.2874
Epoch 13 — loss: 1.2537
Epoch 14 — loss: 1.2349
Epoch 15 — loss: 1.1909
Epoch 16 — loss: 1.1559
Epoch 17 — loss: 1.1191
Epoch 18 — loss: 1.0867
Epoch 19 — loss: 1.0467
Epoch 20 — loss: 1.0146
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8855
Epoch 02 — loss: 1.6227
Epoch 03 — loss: 1.5521
Epoch 04 — loss: 1.5159
Epoch 05 — loss: 1.4984
Epoch 06 — loss: 1.4869
Epoch 07 — loss: 1.4777
Epoch 08 — loss: 1.4749
Epoch 09 — loss: 1.4727
Epoch 10 — loss: 1.4644
Epoch 11 — loss: 1.4656
Epoch 12 — loss: 1.4580
Epoch 13 — loss: 1.4528
Epoch 14 — loss: 1.4555
Epoch 15 — loss: 1.4542
Epoch 16 — loss: 1.4501
Epoch 17 — loss: 1.4492
Epoch 18 — loss: 1.4459
Epoch 19 — loss: 1.4377
Epoch 20 — loss: 1.4431
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7367
Epoch 02 — loss: 1.6108
Epoch 03 — loss: 1.5605
Epoch 04 — loss: 1.5292
Epoch 05 — loss: 1.4978
Epoch 06 — loss: 1.4762
Epoch 07 — loss: 1.4501
Epoch 08 — loss: 1.4139
Epoch 09 — loss: 1.3721
Epoch 10 — loss: 1.3503
Epoch 11 — loss: 1.3059
Epoch 12 — loss: 1.2791
Epoch 13 — loss: 1.2458
Epoch 14 — loss: 1.2072
Epoch 15 — loss: 1.1831
Epoch 16 — loss: 1.1579
Epoch 17 — loss: 1.1112
Epoch 18 — loss: 1.0774
Epoch 19 — loss: 1.0466
Epoch 20 — loss: 1.0138
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8616
Epoch 02 — loss: 1.6000
Epoch 03 — loss: 1.5426
Epoch 04 — loss: 1.5218
Epoch 05 — loss: 1.5038
Epoch 06 — loss: 1.4951
Epoch 07 — loss: 1.4746
Epoch 08 — loss: 1.4809
Epoch 09 — loss: 1.4739
Epoch 10 — loss: 1.4700
Epoch 11 — loss: 1.4666
Epoch 12 — loss: 1.4608
Epoch 13 — loss: 1.4529
Epoch 14 — loss: 1.4552
Epoch 15 — loss: 1.4466
Epoch 16 — loss: 1.4466
Epoch 17 — loss: 1.4446
Epoch 18 — loss: 1.4376
Epoch 19 — loss: 1.4397
Epoch 20 — loss: 1.4395
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7215
Epoch 02 — loss: 1.6260
Epoch 03 — loss: 1.5824
Epoch 04 — loss: 1.5520
Epoch 05 — loss: 1.5363
Epoch 06 — loss: 1.5081
Epoch 07 — loss: 1.5003
Epoch 08 — loss: 1.4786
Epoch 09 — loss: 1.4660
Epoch 10 — loss: 1.4348
Epoch 11 — loss: 1.4077
Epoch 12 — loss: 1.3914
Epoch 13 — loss: 1.3709
Epoch 14 — loss: 1.3505
Epoch 15 — loss: 1.3220
Epoch 16 — loss: 1.3003
Epoch 17 — loss: 1.2757
Epoch 18 — loss: 1.2467
Epoch 19 — loss: 1.2314
Epoch 20 — loss: 1.2133
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7984
Epoch 02 — loss: 1.6230
Epoch 03 — loss: 1.5797
Epoch 04 — loss: 1.5569
Epoch 05 — loss: 1.5391
Epoch 06 — loss: 1.5097
Epoch 07 — loss: 1.5035
Epoch 08 — loss: 1.4978
Epoch 09 — loss: 1.4896
Epoch 10 — loss: 1.4798
Epoch 11 — loss: 1.4765
Epoch 12 — loss: 1.4754
Epoch 13 — loss: 1.4695
Epoch 14 — loss: 1.4659
Epoch 15 — loss: 1.4604
Epoch 16 — loss: 1.4537
Epoch 17 — loss: 1.4532
Epoch 18 — loss: 1.4473
Epoch 19 — loss: 1.4487
Epoch 20 — loss: 1.4421
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6299
Epoch 02 — loss: 1.3834
Epoch 03 — loss: 1.3293
Stage 1: Error=0.5326, Alpha=1.6611
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7860
Epoch 02 — loss: 1.5748
Epoch 03 — loss: 1.5120
Stage 2: Error=0.6401, Alpha=1.2157
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8455
Epoch 02 — loss: 1.6778
Epoch 03 — loss: 1.5789
Stage 3: Error=0.6520, Alpha=1.1640
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8616
Epoch 02 — loss: 1.7638
Epoch 03 — loss: 1.6960
Stage 4: Error=0.7075, Alpha=0.9087
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8804
Epoch 02 — loss: 1.8207
Epoch 03 — loss: 1.7651
Stage 5: Error=0.7523, Alpha=0.6811
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5562
Epoch 02 — loss: 1.3645
Epoch 03 — loss: 1.3307
Stage 1: Error=0.5314, Alpha=1.6660
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7806
Epoch 02 — loss: 1.6083
Epoch 03 — loss: 1.5719
Stage 2: Error=0.6855, Alpha=1.0127
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7796
Epoch 02 — loss: 1.6372
Epoch 03 — loss: 1.6045
Stage 3: Error=0.6659, Alpha=1.1022
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8239
Epoch 02 — loss: 1.7430
Epoch 03 — loss: 1.7069
Stage 4: Error=0.7101, Alpha=0.8957
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8714
Epoch 02 — loss: 1.7955
Epoch 03 — loss: 1.7307
Stage 5: Error=0.7374, Alpha=0.7591
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6267
Epoch 02 — loss: 1.3910
Epoch 03 — loss: 1.3053
Stage 1: Error=0.5252, Alpha=1.6911
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7807
Epoch 02 — loss: 1.5919
Epoch 03 — loss: 1.5035
Stage 2: Error=0.6381, Alpha=1.2245
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8117
Epoch 02 — loss: 1.6871
Epoch 03 — loss: 1.5912
Stage 3: Error=0.6560, Alpha=1.1463
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8613
Epoch 02 — loss: 1.7914
Epoch 03 — loss: 1.7180
Stage 4: Error=0.7380, Alpha=0.7563
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8630/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [13:01:39] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [13:03:55] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [13:05:55] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8086
Epoch 03 — loss: 1.7568
Stage 5: Error=0.7781, Alpha=0.5374
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4580
Epoch 02 — loss: 1.3343
Epoch 03 — loss: 1.2983
Stage 1: Error=0.5212, Alpha=1.7070
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6604
Epoch 02 — loss: 1.5498
Epoch 03 — loss: 1.5023
Stage 2: Error=0.6308, Alpha=1.2562
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6924
Epoch 02 — loss: 1.5801
Epoch 03 — loss: 1.5365
Stage 3: Error=0.6108, Alpha=1.3410
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8045
Epoch 02 — loss: 1.6892
Epoch 03 — loss: 1.6057
Stage 4: Error=0.6616, Alpha=1.1212
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7989
Epoch 02 — loss: 1.7074
Epoch 03 — loss: 1.6495
Stage 5: Error=0.6777, Alpha=1.0486
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4207
Epoch 02 — loss: 1.3542
Epoch 03 — loss: 1.3042
Stage 1: Error=0.5290, Alpha=1.6756
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6608
Epoch 02 — loss: 1.5787
Epoch 03 — loss: 1.5139
Stage 2: Error=0.6309, Alpha=1.2555
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7188
Epoch 02 — loss: 1.6471
Epoch 03 — loss: 1.6092
Stage 3: Error=0.6798, Alpha=1.0390
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8021
Epoch 02 — loss: 1.7200
Epoch 03 — loss: 1.6646
Stage 4: Error=0.7149, Alpha=0.8724
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8231
Epoch 02 — loss: 1.7604
Epoch 03 — loss: 1.6914
Stage 5: Error=0.7104, Alpha=0.8946
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4226
Epoch 02 — loss: 1.3130
Epoch 03 — loss: 1.2825
Stage 1: Error=0.5271, Alpha=1.6833
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6243
Epoch 02 — loss: 1.4814
Epoch 03 — loss: 1.4376
Stage 2: Error=0.5777, Alpha=1.4785
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7137
Epoch 02 — loss: 1.6330
Epoch 03 — loss: 1.5689
Stage 3: Error=0.6066, Alpha=1.3585
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7855
Epoch 02 — loss: 1.6889
Epoch 03 — loss: 1.6269
Stage 4: Error=0.6524, Alpha=1.1622
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8125
Epoch 02 — loss: 1.7218
Epoch 03 — loss: 1.6448
Stage 5: Error=0.6619, Alpha=1.1201
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8922
Stacking meta epoch 2: loss=0.7586
Stacking meta epoch 3: loss=0.7262
Stacking meta epoch 4: loss=0.7084
Stacking meta epoch 5: loss=0.6961
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8907
Stacking meta epoch 2: loss=0.7670
Stacking meta epoch 3: loss=0.7342
Stacking meta epoch 4: loss=0.7157
Stacking meta epoch 5: loss=0.7031
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2609
Stacking meta epoch 2: loss=1.2407
Stacking meta epoch 3: loss=1.2381
Stacking meta epoch 4: loss=1.2363
Stacking meta epoch 5: loss=1.2348
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      80.17
1                                deepset  ...                      77.48
2                     set_transformer_xy  ...                      81.52
3                             deepset_xy  ...                      78.06
4               set_transformer_additive  ...                      80.46
5                    deepset_xy_additive  ...                      80.08
6                       adaboost_deepset  ...                      81.86
7           adaboost_deepset_xy_additive  ...                      80.90
8                    adaboost_deepset_xy  ...                      82.63
9               adaboost_set_transformer  ...                      80.90
10     adaboost_set_transformer_additive  ...                      80.99
11           adaboost_set_transformer_xy  ...                      82.00
12              soft_voting_ensemble_all  ...                      82.72
13                 stacking_ensemble_all  ...                      83.16
14                      gbm_ensemble_all  ...                      83.21
15                  xgboost_ensemble_all  ...                      83.40
16  soft_voting_ensemble_set_transformer  ...                      82.96
17     stacking_ensemble_set_transformer  ...                      83.35
18          gbm_ensemble_set_transformer  ...                      83.01
19      xgboost_ensemble_set_transformer  ...                      83.01
20          soft_voting_ensemble_deepset  ...                      78.73
21             stacking_ensemble_deepset  ...                      82.96
22                  gbm_ensemble_deepset  ...                      80.85
23              xgboost_ensemble_deepset  ...                      81.28

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 17------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7351
Epoch 02 — loss: 1.6065
Epoch 03 — loss: 1.5592
Epoch 04 — loss: 1.5219
Epoch 05 — loss: 1.4803
Epoch 06 — loss: 1.4516
Epoch 07 — loss: 1.4232
Epoch 08 — loss: 1.3864
Epoch 09 — loss: 1.3698
Epoch 10 — loss: 1.3232
Epoch 11 — loss: 1.3070
Epoch 12 — loss: 1.2629
Epoch 13 — loss: 1.2317
Epoch 14 — loss: 1.2103
Epoch 15 — loss: 1.1667
Epoch 16 — loss: 1.1381
Epoch 17 — loss: 1.0987
Epoch 18 — loss: 1.0671
Epoch 19 — loss: 1.0323
Epoch 20 — loss: 0.9910
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8935
Epoch 02 — loss: 1.6195
Epoch 03 — loss: 1.5442
Epoch 04 — loss: 1.5251
Epoch 05 — loss: 1.5046
Epoch 06 — loss: 1.4917
Epoch 07 — loss: 1.4804
Epoch 08 — loss: 1.4682
Epoch 09 — loss: 1.4773
Epoch 10 — loss: 1.4633
Epoch 11 — loss: 1.4554
Epoch 12 — loss: 1.4595
Epoch 13 — loss: 1.4607
Epoch 14 — loss: 1.4497
Epoch 15 — loss: 1.4518
Epoch 16 — loss: 1.4483
Epoch 17 — loss: 1.4485
Epoch 18 — loss: 1.4432
Epoch 19 — loss: 1.4418
Epoch 20 — loss: 1.4432
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7115
Epoch 02 — loss: 1.5925
Epoch 03 — loss: 1.5577
Epoch 04 — loss: 1.5272
Epoch 05 — loss: 1.4976
Epoch 06 — loss: 1.4785
Epoch 07 — loss: 1.4443
Epoch 08 — loss: 1.4053
Epoch 09 — loss: 1.3690
Epoch 10 — loss: 1.3345
Epoch 11 — loss: 1.3190
Epoch 12 — loss: 1.2774
Epoch 13 — loss: 1.2620
Epoch 14 — loss: 1.2279
Epoch 15 — loss: 1.1751
Epoch 16 — loss: 1.1456
Epoch 17 — loss: 1.1173
Epoch 18 — loss: 1.0695
Epoch 19 — loss: 1.0354
Epoch 20 — loss: 1.0004
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8823
Epoch 02 — loss: 1.6024
Epoch 03 — loss: 1.5370
Epoch 04 — loss: 1.5132
Epoch 05 — loss: 1.5049
Epoch 06 — loss: 1.4822
Epoch 07 — loss: 1.4779
Epoch 08 — loss: 1.4710
Epoch 09 — loss: 1.4609
Epoch 10 — loss: 1.4691
Epoch 11 — loss: 1.4592
Epoch 12 — loss: 1.4504
Epoch 13 — loss: 1.4581
Epoch 14 — loss: 1.4492
Epoch 15 — loss: 1.4492
Epoch 16 — loss: 1.4406
Epoch 17 — loss: 1.4381
Epoch 18 — loss: 1.4480
Epoch 19 — loss: 1.4391
Epoch 20 — loss: 1.4448
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7184
Epoch 02 — loss: 1.6237
Epoch 03 — loss: 1.5814
Epoch 04 — loss: 1.5538
Epoch 05 — loss: 1.5277
Epoch 06 — loss: 1.5121
Epoch 07 — loss: 1.4901
Epoch 08 — loss: 1.4754
Epoch 09 — loss: 1.4570
Epoch 10 — loss: 1.4281
Epoch 11 — loss: 1.4119
Epoch 12 — loss: 1.3774
Epoch 13 — loss: 1.3616
Epoch 14 — loss: 1.3320
Epoch 15 — loss: 1.3149
Epoch 16 — loss: 1.2826
Epoch 17 — loss: 1.2569
Epoch 18 — loss: 1.2341
Epoch 19 — loss: 1.2015
Epoch 20 — loss: 1.1779
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8174
Epoch 02 — loss: 1.6462
Epoch 03 — loss: 1.6005
Epoch 04 — loss: 1.5628
Epoch 05 — loss: 1.5352
Epoch 06 — loss: 1.5145
Epoch 07 — loss: 1.5009
Epoch 08 — loss: 1.4889
Epoch 09 — loss: 1.4865
Epoch 10 — loss: 1.4749
Epoch 11 — loss: 1.4733
Epoch 12 — loss: 1.4610
Epoch 13 — loss: 1.4624
Epoch 14 — loss: 1.4609
Epoch 15 — loss: 1.4513
Epoch 16 — loss: 1.4479
Epoch 17 — loss: 1.4486
Epoch 18 — loss: 1.4444
Epoch 19 — loss: 1.4393
Epoch 20 — loss: 1.4445
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6352
Epoch 02 — loss: 1.3688
Epoch 03 — loss: 1.3023
Stage 1: Error=0.5288, Alpha=1.6766
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7859
Epoch 02 — loss: 1.5812
Epoch 03 — loss: 1.5259
Stage 2: Error=0.6365, Alpha=1.2317
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8251
Epoch 02 — loss: 1.6929
Epoch 03 — loss: 1.6095
Stage 3: Error=0.6590, Alpha=1.1331
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8616
Epoch 02 — loss: 1.7744
Epoch 03 — loss: 1.7083
Stage 4: Error=0.7440, Alpha=0.7250
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8605
Epoch 02 — loss: 1.8088
Epoch 03 — loss: 1.7527
Stage 5: Error=0.7559, Alpha=0.6615
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5582
Epoch 02 — loss: 1.3661
Epoch 03 — loss: 1.3086
Stage 1: Error=0.5300, Alpha=1.6718
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7319
Epoch 02 — loss: 1.5901
Epoch 03 — loss: 1.5431
Stage 2: Error=0.6307, Alpha=1.2566
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7845
Epoch 02 — loss: 1.6794
Epoch 03 — loss: 1.6192
Stage 3: Error=0.6567, Alpha=1.1429
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8550
Epoch 02 — loss: 1.7685
Epoch 03 — loss: 1.7170
Stage 4: Error=0.7299, Alpha=0.7974
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8737
Epoch 02 — loss: 1.8009
Epoch 03 — loss: 1.7505
Stage 5: Error=0.7789, Alpha=0.5324
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6047
Epoch 02 — loss: 1.3448
Epoch 03 — loss: 1.2812
Stage 1: Error=0.5272, Alpha=1.6829
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7965
Epoch 02 — loss: 1.5924
Epoch 03 — loss: 1.5199
Stage 2: Error=0.6321, Alpha=1.2504
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8264
Epoch 02 — loss: 1.6822
Epoch 03 — loss: 1.6220
Stage 3: Error=0.6536, Alpha=1.1570
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8681
Epoch 02 — loss: 1.7760
Epoch 03 — loss: 1.7222
Stage 4: Error=0.7767, Alpha=0.5453
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8783/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [13:49:55] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [13:52:13] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [13:54:21] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8049
Epoch 03 — loss: 1.7323
Stage 5: Error=0.7465, Alpha=0.7118
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4912
Epoch 02 — loss: 1.3617
Epoch 03 — loss: 1.3145
Stage 1: Error=0.5347, Alpha=1.6529
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6671
Epoch 02 — loss: 1.5435
Epoch 03 — loss: 1.4893
Stage 2: Error=0.6133, Alpha=1.3304
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7124
Epoch 02 — loss: 1.6278
Epoch 03 — loss: 1.5753
Stage 3: Error=0.6595, Alpha=1.1309
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7790
Epoch 02 — loss: 1.6705
Epoch 03 — loss: 1.6088
Stage 4: Error=0.6334, Alpha=1.2449
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8281
Epoch 02 — loss: 1.7414
Epoch 03 — loss: 1.6439
Stage 5: Error=0.6582, Alpha=1.1364
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4355
Epoch 02 — loss: 1.3345
Epoch 03 — loss: 1.3018
Stage 1: Error=0.5268, Alpha=1.6843
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6498
Epoch 02 — loss: 1.5694
Epoch 03 — loss: 1.5528
Stage 2: Error=0.6478, Alpha=1.1822
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7036
Epoch 02 — loss: 1.6332
Epoch 03 — loss: 1.5978
Stage 3: Error=0.6542, Alpha=1.1540
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8042
Epoch 02 — loss: 1.7342
Epoch 03 — loss: 1.6919
Stage 4: Error=0.7152, Alpha=0.8711
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8024
Epoch 02 — loss: 1.7428
Epoch 03 — loss: 1.7128
Stage 5: Error=0.6895, Alpha=0.9938
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4320
Epoch 02 — loss: 1.3155
Epoch 03 — loss: 1.2892
Stage 1: Error=0.5112, Alpha=1.7470
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6693
Epoch 02 — loss: 1.5471
Epoch 03 — loss: 1.4948
Stage 2: Error=0.6119, Alpha=1.3365
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7131
Epoch 02 — loss: 1.6112
Epoch 03 — loss: 1.5577
Stage 3: Error=0.6194, Alpha=1.3048
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7953
Epoch 02 — loss: 1.6901
Epoch 03 — loss: 1.6190
Stage 4: Error=0.6727, Alpha=1.0715
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8261
Epoch 02 — loss: 1.7351
Epoch 03 — loss: 1.6692
Stage 5: Error=0.6695, Alpha=1.0859
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8898
Stacking meta epoch 2: loss=0.7594
Stacking meta epoch 3: loss=0.7306
Stacking meta epoch 4: loss=0.7148
Stacking meta epoch 5: loss=0.7038
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8897
Stacking meta epoch 2: loss=0.7674
Stacking meta epoch 3: loss=0.7384
Stacking meta epoch 4: loss=0.7219
Stacking meta epoch 5: loss=0.7104
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2560
Stacking meta epoch 2: loss=1.2410
Stacking meta epoch 3: loss=1.2387
Stacking meta epoch 4: loss=1.2370
Stacking meta epoch 5: loss=1.2356
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      78.83
1                                deepset  ...                      80.61
2                     set_transformer_xy  ...                      80.46
3                             deepset_xy  ...                      80.80
4               set_transformer_additive  ...                      82.58
5                    deepset_xy_additive  ...                      77.96
6                       adaboost_deepset  ...                      82.48
7           adaboost_deepset_xy_additive  ...                      80.37
8                    adaboost_deepset_xy  ...                      81.86
9               adaboost_set_transformer  ...                      81.91
10     adaboost_set_transformer_additive  ...                      80.70
11           adaboost_set_transformer_xy  ...                      82.00
12              soft_voting_ensemble_all  ...                      82.92
13                 stacking_ensemble_all  ...                      83.64
14                      gbm_ensemble_all  ...                      82.96
15                  xgboost_ensemble_all  ...                      83.45
16  soft_voting_ensemble_set_transformer  ...                      82.87
17     stacking_ensemble_set_transformer  ...                      83.25
18          gbm_ensemble_set_transformer  ...                      83.49
19      xgboost_ensemble_set_transformer  ...                      83.35
20          soft_voting_ensemble_deepset  ...                      80.13
21             stacking_ensemble_deepset  ...                      82.92
22                  gbm_ensemble_deepset  ...                      81.23
23              xgboost_ensemble_deepset  ...                      81.28

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 18------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7651
Epoch 02 — loss: 1.6271
Epoch 03 — loss: 1.5848
Epoch 04 — loss: 1.5420
Epoch 05 — loss: 1.5139
Epoch 06 — loss: 1.4851
Epoch 07 — loss: 1.4531
Epoch 08 — loss: 1.4178
Epoch 09 — loss: 1.3831
Epoch 10 — loss: 1.3538
Epoch 11 — loss: 1.3166
Epoch 12 — loss: 1.2858
Epoch 13 — loss: 1.2567
Epoch 14 — loss: 1.2271
Epoch 15 — loss: 1.1775
Epoch 16 — loss: 1.1462
Epoch 17 — loss: 1.1169
Epoch 18 — loss: 1.0779
Epoch 19 — loss: 1.0407
Epoch 20 — loss: 1.0073
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8846
Epoch 02 — loss: 1.6028
Epoch 03 — loss: 1.5387
Epoch 04 — loss: 1.5138
Epoch 05 — loss: 1.4982
Epoch 06 — loss: 1.4878
Epoch 07 — loss: 1.4717
Epoch 08 — loss: 1.4741
Epoch 09 — loss: 1.4680
Epoch 10 — loss: 1.4532
Epoch 11 — loss: 1.4613
Epoch 12 — loss: 1.4539
Epoch 13 — loss: 1.4501
Epoch 14 — loss: 1.4505
Epoch 15 — loss: 1.4442
Epoch 16 — loss: 1.4426
Epoch 17 — loss: 1.4444
Epoch 18 — loss: 1.4428
Epoch 19 — loss: 1.4394
Epoch 20 — loss: 1.4326
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7063
Epoch 02 — loss: 1.5943
Epoch 03 — loss: 1.5479
Epoch 04 — loss: 1.5140
Epoch 05 — loss: 1.4792
Epoch 06 — loss: 1.4556
Epoch 07 — loss: 1.4303
Epoch 08 — loss: 1.3917
Epoch 09 — loss: 1.3543
Epoch 10 — loss: 1.3313
Epoch 11 — loss: 1.2996
Epoch 12 — loss: 1.2617
Epoch 13 — loss: 1.2365
Epoch 14 — loss: 1.1951
Epoch 15 — loss: 1.1695
Epoch 16 — loss: 1.1342
Epoch 17 — loss: 1.0919
Epoch 18 — loss: 1.0569
Epoch 19 — loss: 1.0174
Epoch 20 — loss: 0.9823
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8880
Epoch 02 — loss: 1.6174
Epoch 03 — loss: 1.5437
Epoch 04 — loss: 1.5114
Epoch 05 — loss: 1.4969
Epoch 06 — loss: 1.4847
Epoch 07 — loss: 1.4745
Epoch 08 — loss: 1.4727
Epoch 09 — loss: 1.4656
Epoch 10 — loss: 1.4531
Epoch 11 — loss: 1.4521
Epoch 12 — loss: 1.4563
Epoch 13 — loss: 1.4527
Epoch 14 — loss: 1.4513
Epoch 15 — loss: 1.4437
Epoch 16 — loss: 1.4397
Epoch 17 — loss: 1.4364
Epoch 18 — loss: 1.4437
Epoch 19 — loss: 1.4421
Epoch 20 — loss: 1.4457
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7134
Epoch 02 — loss: 1.6310
Epoch 03 — loss: 1.5908
Epoch 04 — loss: 1.5635
Epoch 05 — loss: 1.5349
Epoch 06 — loss: 1.5222
Epoch 07 — loss: 1.5093
Epoch 08 — loss: 1.4806
Epoch 09 — loss: 1.4643
Epoch 10 — loss: 1.4490
Epoch 11 — loss: 1.4200
Epoch 12 — loss: 1.4069
Epoch 13 — loss: 1.3787
Epoch 14 — loss: 1.3520
Epoch 15 — loss: 1.3313
Epoch 16 — loss: 1.3068
Epoch 17 — loss: 1.2797
Epoch 18 — loss: 1.2569
Epoch 19 — loss: 1.2294
Epoch 20 — loss: 1.2035
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7882
Epoch 02 — loss: 1.6277
Epoch 03 — loss: 1.5874
Epoch 04 — loss: 1.5457
Epoch 05 — loss: 1.5307
Epoch 06 — loss: 1.5097
Epoch 07 — loss: 1.5046
Epoch 08 — loss: 1.4953
Epoch 09 — loss: 1.4799
Epoch 10 — loss: 1.4811
Epoch 11 — loss: 1.4721
Epoch 12 — loss: 1.4682
Epoch 13 — loss: 1.4691
Epoch 14 — loss: 1.4665
Epoch 15 — loss: 1.4618
Epoch 16 — loss: 1.4599
Epoch 17 — loss: 1.4536
Epoch 18 — loss: 1.4614
Epoch 19 — loss: 1.4526
Epoch 20 — loss: 1.4592
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6393
Epoch 02 — loss: 1.3762
Epoch 03 — loss: 1.3151
Stage 1: Error=0.5397, Alpha=1.6326
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7647
Epoch 02 — loss: 1.5986
Epoch 03 — loss: 1.5287
Stage 2: Error=0.6668, Alpha=1.0979
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7785
Epoch 02 — loss: 1.6514
Epoch 03 — loss: 1.5727
Stage 3: Error=0.6565, Alpha=1.1440
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8432
Epoch 02 — loss: 1.7615
Epoch 03 — loss: 1.7025
Stage 4: Error=0.7456, Alpha=0.7162
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8598
Epoch 02 — loss: 1.7981
Epoch 03 — loss: 1.7368
Stage 5: Error=0.7583, Alpha=0.6481
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5595
Epoch 02 — loss: 1.3554
Epoch 03 — loss: 1.3310
Stage 1: Error=0.5324, Alpha=1.6621
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7319
Epoch 02 — loss: 1.5911
Epoch 03 — loss: 1.5220
Stage 2: Error=0.6567, Alpha=1.1433
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7840
Epoch 02 — loss: 1.6476
Epoch 03 — loss: 1.5905
Stage 3: Error=0.6596, Alpha=1.1303
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8343
Epoch 02 — loss: 1.7501
Epoch 03 — loss: 1.7059
Stage 4: Error=0.7412, Alpha=0.7397
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8616
Epoch 02 — loss: 1.7919
Epoch 03 — loss: 1.7607
Stage 5: Error=0.7199, Alpha=0.8477
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5954
Epoch 02 — loss: 1.3400
Epoch 03 — loss: 1.2883
Stage 1: Error=0.5406, Alpha=1.6292
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7789
Epoch 02 — loss: 1.5666
Epoch 03 — loss: 1.4879
Stage 2: Error=0.5967, Alpha=1.4002
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8191
Epoch 02 — loss: 1.6742
Epoch 03 — loss: 1.6229
Stage 3: Error=0.6635, Alpha=1.1129
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8746
Epoch 02 — loss: 1.7686
Epoch 03 — loss: 1.6894
Stage 4: Error=0.7173, Alpha=0.8604
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8892/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [14:37:41] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [14:39:56] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [14:41:54] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8255
Epoch 03 — loss: 1.7565
Stage 5: Error=0.7156, Alpha=0.8693
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4374
Epoch 02 — loss: 1.3485
Epoch 03 — loss: 1.2993
Stage 1: Error=0.5230, Alpha=1.6998
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6608
Epoch 02 — loss: 1.5624
Epoch 03 — loss: 1.5235
Stage 2: Error=0.6154, Alpha=1.3216
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7483
Epoch 02 — loss: 1.6255
Epoch 03 — loss: 1.5728
Stage 3: Error=0.6426, Alpha=1.2053
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8142
Epoch 02 — loss: 1.6944
Epoch 03 — loss: 1.6168
Stage 4: Error=0.6473, Alpha=1.1844
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8248
Epoch 02 — loss: 1.7072
Epoch 03 — loss: 1.6386
Stage 5: Error=0.6588, Alpha=1.1336
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4311
Epoch 02 — loss: 1.3183
Epoch 03 — loss: 1.3017
Stage 1: Error=0.5361, Alpha=1.6471
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6529
Epoch 02 — loss: 1.5963
Epoch 03 — loss: 1.5100
Stage 2: Error=0.6257, Alpha=1.2780
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7300
Epoch 02 — loss: 1.6365
Epoch 03 — loss: 1.5935
Stage 3: Error=0.6639, Alpha=1.1109
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7648
Epoch 02 — loss: 1.7134
Epoch 03 — loss: 1.6845
Stage 4: Error=0.7036, Alpha=0.9271
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8059
Epoch 02 — loss: 1.7502
Epoch 03 — loss: 1.7270
Stage 5: Error=0.7478, Alpha=0.7051
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4377
Epoch 02 — loss: 1.3060
Epoch 03 — loss: 1.2990
Stage 1: Error=0.5168, Alpha=1.7243
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6532
Epoch 02 — loss: 1.5267
Epoch 03 — loss: 1.4783
Stage 2: Error=0.6018, Alpha=1.3789
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7350
Epoch 02 — loss: 1.6323
Epoch 03 — loss: 1.5565
Stage 3: Error=0.6674, Alpha=1.0952
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7584
Epoch 02 — loss: 1.6729
Epoch 03 — loss: 1.6096
Stage 4: Error=0.6428, Alpha=1.2042
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8076
Epoch 02 — loss: 1.7166
Epoch 03 — loss: 1.6547
Stage 5: Error=0.6748, Alpha=1.0619
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8832
Stacking meta epoch 2: loss=0.7508
Stacking meta epoch 3: loss=0.7198
Stacking meta epoch 4: loss=0.7030
Stacking meta epoch 5: loss=0.6915
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8741
Stacking meta epoch 2: loss=0.7554
Stacking meta epoch 3: loss=0.7257
Stacking meta epoch 4: loss=0.7089
Stacking meta epoch 5: loss=0.6973
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2576
Stacking meta epoch 2: loss=1.2417
Stacking meta epoch 3: loss=1.2392
Stacking meta epoch 4: loss=1.2374
Stacking meta epoch 5: loss=1.2360
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      79.98
1                                deepset  ...                      79.31
2                     set_transformer_xy  ...                      76.95
3                             deepset_xy  ...                      81.57
4               set_transformer_additive  ...                      80.85
5                    deepset_xy_additive  ...                      78.39
6                       adaboost_deepset  ...                      81.81
7           adaboost_deepset_xy_additive  ...                      80.75
8                    adaboost_deepset_xy  ...                      82.19
9               adaboost_set_transformer  ...                      80.32
10     adaboost_set_transformer_additive  ...                      81.76
11           adaboost_set_transformer_xy  ...                      80.80
12              soft_voting_ensemble_all  ...                      82.29
13                 stacking_ensemble_all  ...                      83.25
14                      gbm_ensemble_all  ...                      82.00
15                  xgboost_ensemble_all  ...                      81.95
16  soft_voting_ensemble_set_transformer  ...                      80.41
17     stacking_ensemble_set_transformer  ...                      82.58
18          gbm_ensemble_set_transformer  ...                      82.29
19      xgboost_ensemble_set_transformer  ...                      82.05
20          soft_voting_ensemble_deepset  ...                      80.46
21             stacking_ensemble_deepset  ...                      82.82
22                  gbm_ensemble_deepset  ...                      80.13
23              xgboost_ensemble_deepset  ...                      81.42

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 19------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7427
Epoch 02 — loss: 1.6164
Epoch 03 — loss: 1.5839
Epoch 04 — loss: 1.5466
Epoch 05 — loss: 1.5047
Epoch 06 — loss: 1.4861
Epoch 07 — loss: 1.4464
Epoch 08 — loss: 1.4072
Epoch 09 — loss: 1.3904
Epoch 10 — loss: 1.3548
Epoch 11 — loss: 1.3254
Epoch 12 — loss: 1.2907
Epoch 13 — loss: 1.2667
Epoch 14 — loss: 1.2198
Epoch 15 — loss: 1.1846
Epoch 16 — loss: 1.1647
Epoch 17 — loss: 1.1156
Epoch 18 — loss: 1.0914
Epoch 19 — loss: 1.0508
Epoch 20 — loss: 1.0147
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.9054
Epoch 02 — loss: 1.6362
Epoch 03 — loss: 1.5470
Epoch 04 — loss: 1.5242
Epoch 05 — loss: 1.5000
Epoch 06 — loss: 1.4891
Epoch 07 — loss: 1.4857
Epoch 08 — loss: 1.4735
Epoch 09 — loss: 1.4704
Epoch 10 — loss: 1.4716
Epoch 11 — loss: 1.4713
Epoch 12 — loss: 1.4596
Epoch 13 — loss: 1.4672
Epoch 14 — loss: 1.4490
Epoch 15 — loss: 1.4514
Epoch 16 — loss: 1.4524
Epoch 17 — loss: 1.4540
Epoch 18 — loss: 1.4492
Epoch 19 — loss: 1.4447
Epoch 20 — loss: 1.4455
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7327
Epoch 02 — loss: 1.6164
Epoch 03 — loss: 1.5746
Epoch 04 — loss: 1.5294
Epoch 05 — loss: 1.5101
Epoch 06 — loss: 1.4710
Epoch 07 — loss: 1.4358
Epoch 08 — loss: 1.4070
Epoch 09 — loss: 1.3735
Epoch 10 — loss: 1.3384
Epoch 11 — loss: 1.3106
Epoch 12 — loss: 1.2722
Epoch 13 — loss: 1.2460
Epoch 14 — loss: 1.2034
Epoch 15 — loss: 1.1688
Epoch 16 — loss: 1.1387
Epoch 17 — loss: 1.0967
Epoch 18 — loss: 1.0662
Epoch 19 — loss: 1.0122
Epoch 20 — loss: 0.9857
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8772
Epoch 02 — loss: 1.5885
Epoch 03 — loss: 1.5285
Epoch 04 — loss: 1.5055
Epoch 05 — loss: 1.4922
Epoch 06 — loss: 1.4904
Epoch 07 — loss: 1.4866
Epoch 08 — loss: 1.4720
Epoch 09 — loss: 1.4628
Epoch 10 — loss: 1.4543
Epoch 11 — loss: 1.4605
Epoch 12 — loss: 1.4575
Epoch 13 — loss: 1.4516
Epoch 14 — loss: 1.4450
Epoch 15 — loss: 1.4437
Epoch 16 — loss: 1.4415
Epoch 17 — loss: 1.4378
Epoch 18 — loss: 1.4441
Epoch 19 — loss: 1.4338
Epoch 20 — loss: 1.4333
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7116
Epoch 02 — loss: 1.6323
Epoch 03 — loss: 1.5880
Epoch 04 — loss: 1.5660
Epoch 05 — loss: 1.5461
Epoch 06 — loss: 1.5291
Epoch 07 — loss: 1.5047
Epoch 08 — loss: 1.4948
Epoch 09 — loss: 1.4660
Epoch 10 — loss: 1.4467
Epoch 11 — loss: 1.4304
Epoch 12 — loss: 1.4128
Epoch 13 — loss: 1.3847
Epoch 14 — loss: 1.3601
Epoch 15 — loss: 1.3435
Epoch 16 — loss: 1.3017
Epoch 17 — loss: 1.2855
Epoch 18 — loss: 1.2491
Epoch 19 — loss: 1.2381
Epoch 20 — loss: 1.1996
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8511
Epoch 02 — loss: 1.6341
Epoch 03 — loss: 1.5853
Epoch 04 — loss: 1.5458
Epoch 05 — loss: 1.5298
Epoch 06 — loss: 1.5058
Epoch 07 — loss: 1.4903
Epoch 08 — loss: 1.4894
Epoch 09 — loss: 1.4859
Epoch 10 — loss: 1.4777
Epoch 11 — loss: 1.4631
Epoch 12 — loss: 1.4654
Epoch 13 — loss: 1.4659
Epoch 14 — loss: 1.4586
Epoch 15 — loss: 1.4561
Epoch 16 — loss: 1.4603
Epoch 17 — loss: 1.4529
Epoch 18 — loss: 1.4478
Epoch 19 — loss: 1.4402
Epoch 20 — loss: 1.4403
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6378
Epoch 02 — loss: 1.3904
Epoch 03 — loss: 1.3104
Stage 1: Error=0.5271, Alpha=1.6834
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8069
Epoch 02 — loss: 1.6246
Epoch 03 — loss: 1.5213
Stage 2: Error=0.6516, Alpha=1.1655
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8192
Epoch 02 — loss: 1.6675
Epoch 03 — loss: 1.6077
Stage 3: Error=0.6393, Alpha=1.2193
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8599
Epoch 02 — loss: 1.7750
Epoch 03 — loss: 1.6921
Stage 4: Error=0.7267, Alpha=0.8139
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8808
Epoch 02 — loss: 1.8153
Epoch 03 — loss: 1.7643
Stage 5: Error=0.7266, Alpha=0.8143
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5288
Epoch 02 — loss: 1.3592
Epoch 03 — loss: 1.3295
Stage 1: Error=0.5357, Alpha=1.6486
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7422
Epoch 02 — loss: 1.5752
Epoch 03 — loss: 1.5380
Stage 2: Error=0.6344, Alpha=1.2406
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7618
Epoch 02 — loss: 1.6386
Epoch 03 — loss: 1.6164
Stage 3: Error=0.6604, Alpha=1.1265
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8237
Epoch 02 — loss: 1.7472
Epoch 03 — loss: 1.7156
Stage 4: Error=0.7369, Alpha=0.7619
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8456
Epoch 02 — loss: 1.7825
Epoch 03 — loss: 1.7465
Stage 5: Error=0.7616, Alpha=0.6302
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6154
Epoch 02 — loss: 1.3429
Epoch 03 — loss: 1.3004
Stage 1: Error=0.5232, Alpha=1.6988
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7784
Epoch 02 — loss: 1.5911
Epoch 03 — loss: 1.5159
Stage 2: Error=0.6604, Alpha=1.1268
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8001
Epoch 02 — loss: 1.6639
Epoch 03 — loss: 1.6045
Stage 3: Error=0.6642, Alpha=1.1097
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8652
Epoch 02 — loss: 1.7840
Epoch 03 — loss: 1.7076
Stage 4: Error=0.7505, Alpha=0.6903
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8713/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [15:29:48] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [15:33:16] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [15:36:08] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8059
Epoch 03 — loss: 1.7384
Stage 5: Error=0.7821, Alpha=0.5139
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4643
Epoch 02 — loss: 1.3536
Epoch 03 — loss: 1.3029
Stage 1: Error=0.5286, Alpha=1.6771
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6819
Epoch 02 — loss: 1.5664
Epoch 03 — loss: 1.5046
Stage 2: Error=0.6236, Alpha=1.2868
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7023
Epoch 02 — loss: 1.6050
Epoch 03 — loss: 1.5679
Stage 3: Error=0.6407, Alpha=1.2135
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7727
Epoch 02 — loss: 1.6663
Epoch 03 — loss: 1.6227
Stage 4: Error=0.6408, Alpha=1.2129
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8068
Epoch 02 — loss: 1.7218
Epoch 03 — loss: 1.6478
Stage 5: Error=0.6460, Alpha=1.1902
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4225
Epoch 02 — loss: 1.3316
Epoch 03 — loss: 1.3231
Stage 1: Error=0.5373, Alpha=1.6423
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6181
Epoch 02 — loss: 1.5238
Epoch 03 — loss: 1.4960
Stage 2: Error=0.6013, Alpha=1.3808
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7272
Epoch 02 — loss: 1.6559
Epoch 03 — loss: 1.6289
Stage 3: Error=0.6632, Alpha=1.1140
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7760
Epoch 02 — loss: 1.7049
Epoch 03 — loss: 1.6557
Stage 4: Error=0.6916, Alpha=0.9841
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8028
Epoch 02 — loss: 1.7399
Epoch 03 — loss: 1.7057
Stage 5: Error=0.7233, Alpha=0.8311
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4548
Epoch 02 — loss: 1.3382
Epoch 03 — loss: 1.3066
Stage 1: Error=0.5107, Alpha=1.7489
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6720
Epoch 02 — loss: 1.5627
Epoch 03 — loss: 1.5033
Stage 2: Error=0.6115, Alpha=1.3382
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7522
Epoch 02 — loss: 1.6180
Epoch 03 — loss: 1.5732
Stage 3: Error=0.6421, Alpha=1.2074
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7745
Epoch 02 — loss: 1.6740
Epoch 03 — loss: 1.5941
Stage 4: Error=0.6714, Alpha=1.0773
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8120
Epoch 02 — loss: 1.7183
Epoch 03 — loss: 1.6547
Stage 5: Error=0.6726, Alpha=1.0719
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8879
Stacking meta epoch 2: loss=0.7602
Stacking meta epoch 3: loss=0.7313
Stacking meta epoch 4: loss=0.7155
Stacking meta epoch 5: loss=0.7045
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8781
Stacking meta epoch 2: loss=0.7649
Stacking meta epoch 3: loss=0.7365
Stacking meta epoch 4: loss=0.7202
Stacking meta epoch 5: loss=0.7088
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2554
Stacking meta epoch 2: loss=1.2424
Stacking meta epoch 3: loss=1.2397
Stacking meta epoch 4: loss=1.2379
Stacking meta epoch 5: loss=1.2365
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      78.34
1                                deepset  ...                      79.16
2                     set_transformer_xy  ...                      81.86
3                             deepset_xy  ...                      78.92
4               set_transformer_additive  ...                      81.47
5                    deepset_xy_additive  ...                      81.04
6                       adaboost_deepset  ...                      81.67
7           adaboost_deepset_xy_additive  ...                      79.79
8                    adaboost_deepset_xy  ...                      82.53
9               adaboost_set_transformer  ...                      82.05
10     adaboost_set_transformer_additive  ...                      80.08
11           adaboost_set_transformer_xy  ...                      81.52
12              soft_voting_ensemble_all  ...                      83.11
13                 stacking_ensemble_all  ...                      83.35
14                      gbm_ensemble_all  ...                      83.01
15                  xgboost_ensemble_all  ...                      83.35
16  soft_voting_ensemble_set_transformer  ...                      82.63
17     stacking_ensemble_set_transformer  ...                      83.64
18          gbm_ensemble_set_transformer  ...                      83.30
19      xgboost_ensemble_set_transformer  ...                      83.69
20          soft_voting_ensemble_deepset  ...                      79.64
21             stacking_ensemble_deepset  ...                      82.77
22                  gbm_ensemble_deepset  ...                      81.76
23              xgboost_ensemble_deepset  ...                      81.14

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 20------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7584
Epoch 02 — loss: 1.6302
Epoch 03 — loss: 1.5731
Epoch 04 — loss: 1.5330
Epoch 05 — loss: 1.5060
Epoch 06 — loss: 1.4806
Epoch 07 — loss: 1.4473
Epoch 08 — loss: 1.4148
Epoch 09 — loss: 1.3882
Epoch 10 — loss: 1.3562
Epoch 11 — loss: 1.3291
Epoch 12 — loss: 1.2958
Epoch 13 — loss: 1.2590
Epoch 14 — loss: 1.2322
Epoch 15 — loss: 1.1891
Epoch 16 — loss: 1.1521
Epoch 17 — loss: 1.1116
Epoch 18 — loss: 1.0736
Epoch 19 — loss: 1.0322
Epoch 20 — loss: 0.9974
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8532
Epoch 02 — loss: 1.6180
Epoch 03 — loss: 1.5600
Epoch 04 — loss: 1.5275
Epoch 05 — loss: 1.5136
Epoch 06 — loss: 1.5010
Epoch 07 — loss: 1.4892
Epoch 08 — loss: 1.4797
Epoch 09 — loss: 1.4792
Epoch 10 — loss: 1.4662
Epoch 11 — loss: 1.4713
Epoch 12 — loss: 1.4609
Epoch 13 — loss: 1.4589
Epoch 14 — loss: 1.4539
Epoch 15 — loss: 1.4555
Epoch 16 — loss: 1.4498
Epoch 17 — loss: 1.4511
Epoch 18 — loss: 1.4533
Epoch 19 — loss: 1.4465
Epoch 20 — loss: 1.4417
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7103
Epoch 02 — loss: 1.5766
Epoch 03 — loss: 1.5332
Epoch 04 — loss: 1.4928
Epoch 05 — loss: 1.4773
Epoch 06 — loss: 1.4435
Epoch 07 — loss: 1.4215
Epoch 08 — loss: 1.3872
Epoch 09 — loss: 1.3565
Epoch 10 — loss: 1.3315
Epoch 11 — loss: 1.2914
Epoch 12 — loss: 1.2630
Epoch 13 — loss: 1.2324
Epoch 14 — loss: 1.1985
Epoch 15 — loss: 1.1615
Epoch 16 — loss: 1.1239
Epoch 17 — loss: 1.0958
Epoch 18 — loss: 1.0571
Epoch 19 — loss: 1.0260
Epoch 20 — loss: 0.9884
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8595
Epoch 02 — loss: 1.5990
Epoch 03 — loss: 1.5383
Epoch 04 — loss: 1.5210
Epoch 05 — loss: 1.5030
Epoch 06 — loss: 1.4810
Epoch 07 — loss: 1.4820
Epoch 08 — loss: 1.4725
Epoch 09 — loss: 1.4686
Epoch 10 — loss: 1.4651
Epoch 11 — loss: 1.4570
Epoch 12 — loss: 1.4597
Epoch 13 — loss: 1.4548
Epoch 14 — loss: 1.4451
Epoch 15 — loss: 1.4486
Epoch 16 — loss: 1.4463
Epoch 17 — loss: 1.4434
Epoch 18 — loss: 1.4467
Epoch 19 — loss: 1.4426
Epoch 20 — loss: 1.4443
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7105
Epoch 02 — loss: 1.6248
Epoch 03 — loss: 1.5753
Epoch 04 — loss: 1.5587
Epoch 05 — loss: 1.5321
Epoch 06 — loss: 1.5144
Epoch 07 — loss: 1.5008
Epoch 08 — loss: 1.4768
Epoch 09 — loss: 1.4507
Epoch 10 — loss: 1.4350
Epoch 11 — loss: 1.4099
Epoch 12 — loss: 1.3985
Epoch 13 — loss: 1.3662
Epoch 14 — loss: 1.3546
Epoch 15 — loss: 1.3296
Epoch 16 — loss: 1.3057
Epoch 17 — loss: 1.2869
Epoch 18 — loss: 1.2527
Epoch 19 — loss: 1.2236
Epoch 20 — loss: 1.2138
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7879
Epoch 02 — loss: 1.6087
Epoch 03 — loss: 1.5679
Epoch 04 — loss: 1.5406
Epoch 05 — loss: 1.5180
Epoch 06 — loss: 1.4968
Epoch 07 — loss: 1.4890
Epoch 08 — loss: 1.4817
Epoch 09 — loss: 1.4736
Epoch 10 — loss: 1.4684
Epoch 11 — loss: 1.4658
Epoch 12 — loss: 1.4601
Epoch 13 — loss: 1.4524
Epoch 14 — loss: 1.4549
Epoch 15 — loss: 1.4556
Epoch 16 — loss: 1.4607
Epoch 17 — loss: 1.4425
Epoch 18 — loss: 1.4394
Epoch 19 — loss: 1.4500
Epoch 20 — loss: 1.4333
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6242
Epoch 02 — loss: 1.3752
Epoch 03 — loss: 1.3199
Stage 1: Error=0.5312, Alpha=1.6669
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7988
Epoch 02 — loss: 1.6110
Epoch 03 — loss: 1.5351
Stage 2: Error=0.6269, Alpha=1.2728
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8177
Epoch 02 — loss: 1.6691
Epoch 03 — loss: 1.6097
Stage 3: Error=0.6470, Alpha=1.1861
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8585
Epoch 02 — loss: 1.7705
Epoch 03 — loss: 1.7183
Stage 4: Error=0.7404, Alpha=0.7437
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8895
Epoch 02 — loss: 1.8259
Epoch 03 — loss: 1.7611
Stage 5: Error=0.7863, Alpha=0.4893
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5400
Epoch 02 — loss: 1.3584
Epoch 03 — loss: 1.3259
Stage 1: Error=0.5474, Alpha=1.6015
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7222
Epoch 02 — loss: 1.5535
Epoch 03 — loss: 1.5116
Stage 2: Error=0.6020, Alpha=1.3779
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7966
Epoch 02 — loss: 1.6869
Epoch 03 — loss: 1.6390
Stage 3: Error=0.7031, Alpha=0.9298
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8363
Epoch 02 — loss: 1.7556
Epoch 03 — loss: 1.7114
Stage 4: Error=0.7419, Alpha=0.7357
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8672
Epoch 02 — loss: 1.7956
Epoch 03 — loss: 1.7529
Stage 5: Error=0.7795, Alpha=0.5290
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6336
Epoch 02 — loss: 1.3508
Epoch 03 — loss: 1.3082
Stage 1: Error=0.5341, Alpha=1.6553
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7931
Epoch 02 — loss: 1.5828
Epoch 03 — loss: 1.5196
Stage 2: Error=0.6099, Alpha=1.3448
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8412
Epoch 02 — loss: 1.6715
Epoch 03 — loss: 1.6058
Stage 3: Error=0.6498, Alpha=1.1737
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8665
Epoch 02 — loss: 1.7697
Epoch 03 — loss: 1.7079
Stage 4: Error=0.7176, Alpha=0.8589
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8840/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [16:24:27] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [16:26:42] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [16:28:37] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8146
Epoch 03 — loss: 1.7526
Stage 5: Error=0.7324, Alpha=0.7849
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4389
Epoch 02 — loss: 1.3482
Epoch 03 — loss: 1.2989
Stage 1: Error=0.5472, Alpha=1.6025
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6604
Epoch 02 — loss: 1.5458
Epoch 03 — loss: 1.4885
Stage 2: Error=0.6083, Alpha=1.3516
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7334
Epoch 02 — loss: 1.6370
Epoch 03 — loss: 1.5704
Stage 3: Error=0.6685, Alpha=1.0905
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7688
Epoch 02 — loss: 1.6898
Epoch 03 — loss: 1.6332
Stage 4: Error=0.6453, Alpha=1.1933
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8210
Epoch 02 — loss: 1.7259
Epoch 03 — loss: 1.6473
Stage 5: Error=0.6601, Alpha=1.1280
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4366
Epoch 02 — loss: 1.3443
Epoch 03 — loss: 1.3092
Stage 1: Error=0.5413, Alpha=1.6263
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6310
Epoch 02 — loss: 1.5442
Epoch 03 — loss: 1.4995
Stage 2: Error=0.6040, Alpha=1.3698
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7178
Epoch 02 — loss: 1.6287
Epoch 03 — loss: 1.6054
Stage 3: Error=0.6590, Alpha=1.1331
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7922
Epoch 02 — loss: 1.7264
Epoch 03 — loss: 1.6842
Stage 4: Error=0.7182, Alpha=0.8564
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8168
Epoch 02 — loss: 1.7575
Epoch 03 — loss: 1.7048
Stage 5: Error=0.7102, Alpha=0.8955
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4426
Epoch 02 — loss: 1.3084
Epoch 03 — loss: 1.2712
Stage 1: Error=0.5088, Alpha=1.7566
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6488
Epoch 02 — loss: 1.5466
Epoch 03 — loss: 1.4911
Stage 2: Error=0.6348, Alpha=1.2391
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7043
Epoch 02 — loss: 1.6011
Epoch 03 — loss: 1.5536
Stage 3: Error=0.6502, Alpha=1.1718
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7497
Epoch 02 — loss: 1.6444
Epoch 03 — loss: 1.5935
Stage 4: Error=0.6384, Alpha=1.2233
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7827
Epoch 02 — loss: 1.6912
Epoch 03 — loss: 1.6286
Stage 5: Error=0.6648, Alpha=1.1069
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8674
Stacking meta epoch 2: loss=0.7272
Stacking meta epoch 3: loss=0.6961
Stacking meta epoch 4: loss=0.6794
Stacking meta epoch 5: loss=0.6683
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8709
Stacking meta epoch 2: loss=0.7354
Stacking meta epoch 3: loss=0.7040
Stacking meta epoch 4: loss=0.6865
Stacking meta epoch 5: loss=0.6746
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2619
Stacking meta epoch 2: loss=1.2467
Stacking meta epoch 3: loss=1.2441
Stacking meta epoch 4: loss=1.2423
Stacking meta epoch 5: loss=1.2409
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      77.53
1                                deepset  ...                      81.38
2                     set_transformer_xy  ...                      79.11
3                             deepset_xy  ...                      79.45
4               set_transformer_additive  ...                      79.16
5                    deepset_xy_additive  ...                      81.28
6                       adaboost_deepset  ...                      80.22
7           adaboost_deepset_xy_additive  ...                      81.18
8                    adaboost_deepset_xy  ...                      81.52
9               adaboost_set_transformer  ...                      81.71
10     adaboost_set_transformer_additive  ...                      81.23
11           adaboost_set_transformer_xy  ...                      82.77
12              soft_voting_ensemble_all  ...                      82.87
13                 stacking_ensemble_all  ...                      83.69
14                      gbm_ensemble_all  ...                      82.82
15                  xgboost_ensemble_all  ...                      82.39
16  soft_voting_ensemble_set_transformer  ...                      81.71
17     stacking_ensemble_set_transformer  ...                      83.35
18          gbm_ensemble_set_transformer  ...                      83.06
19      xgboost_ensemble_set_transformer  ...                      83.59
20          soft_voting_ensemble_deepset  ...                      81.38
21             stacking_ensemble_deepset  ...                      82.58
22                  gbm_ensemble_deepset  ...                      81.33
23              xgboost_ensemble_deepset  ...                      80.61

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 21------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7563
Epoch 02 — loss: 1.6476
Epoch 03 — loss: 1.5882
Epoch 04 — loss: 1.5427
Epoch 05 — loss: 1.5092
Epoch 06 — loss: 1.4811
Epoch 07 — loss: 1.4488
Epoch 08 — loss: 1.4120
Epoch 09 — loss: 1.3828
Epoch 10 — loss: 1.3441
Epoch 11 — loss: 1.3197
Epoch 12 — loss: 1.2898
Epoch 13 — loss: 1.2614
Epoch 14 — loss: 1.2167
Epoch 15 — loss: 1.1874
Epoch 16 — loss: 1.1558
Epoch 17 — loss: 1.1159
Epoch 18 — loss: 1.0699
Epoch 19 — loss: 1.0326
Epoch 20 — loss: 0.9962
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8741
Epoch 02 — loss: 1.6085
Epoch 03 — loss: 1.5477
Epoch 04 — loss: 1.5202
Epoch 05 — loss: 1.5051
Epoch 06 — loss: 1.4896
Epoch 07 — loss: 1.4789
Epoch 08 — loss: 1.4817
Epoch 09 — loss: 1.4709
Epoch 10 — loss: 1.4678
Epoch 11 — loss: 1.4667
Epoch 12 — loss: 1.4575
Epoch 13 — loss: 1.4613
Epoch 14 — loss: 1.4591
Epoch 15 — loss: 1.4566
Epoch 16 — loss: 1.4520
Epoch 17 — loss: 1.4537
Epoch 18 — loss: 1.4429
Epoch 19 — loss: 1.4529
Epoch 20 — loss: 1.4473
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7208
Epoch 02 — loss: 1.6172
Epoch 03 — loss: 1.5675
Epoch 04 — loss: 1.5318
Epoch 05 — loss: 1.4994
Epoch 06 — loss: 1.4653
Epoch 07 — loss: 1.4438
Epoch 08 — loss: 1.4176
Epoch 09 — loss: 1.3869
Epoch 10 — loss: 1.3594
Epoch 11 — loss: 1.3181
Epoch 12 — loss: 1.2826
Epoch 13 — loss: 1.2479
Epoch 14 — loss: 1.2107
Epoch 15 — loss: 1.1794
Epoch 16 — loss: 1.1489
Epoch 17 — loss: 1.1007
Epoch 18 — loss: 1.0693
Epoch 19 — loss: 1.0346
Epoch 20 — loss: 0.9949
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8477
Epoch 02 — loss: 1.5957
Epoch 03 — loss: 1.5426
Epoch 04 — loss: 1.5155
Epoch 05 — loss: 1.5004
Epoch 06 — loss: 1.4906
Epoch 07 — loss: 1.4824
Epoch 08 — loss: 1.4757
Epoch 09 — loss: 1.4699
Epoch 10 — loss: 1.4650
Epoch 11 — loss: 1.4571
Epoch 12 — loss: 1.4522
Epoch 13 — loss: 1.4512
Epoch 14 — loss: 1.4509
Epoch 15 — loss: 1.4515
Epoch 16 — loss: 1.4557
Epoch 17 — loss: 1.4499
Epoch 18 — loss: 1.4426
Epoch 19 — loss: 1.4381
Epoch 20 — loss: 1.4382
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7257
Epoch 02 — loss: 1.6209
Epoch 03 — loss: 1.5749
Epoch 04 — loss: 1.5621
Epoch 05 — loss: 1.5288
Epoch 06 — loss: 1.5036
Epoch 07 — loss: 1.4770
Epoch 08 — loss: 1.4652
Epoch 09 — loss: 1.4399
Epoch 10 — loss: 1.4157
Epoch 11 — loss: 1.4013
Epoch 12 — loss: 1.3721
Epoch 13 — loss: 1.3524
Epoch 14 — loss: 1.3234
Epoch 15 — loss: 1.3051
Epoch 16 — loss: 1.2750
Epoch 17 — loss: 1.2448
Epoch 18 — loss: 1.2239
Epoch 19 — loss: 1.1958
Epoch 20 — loss: 1.1646
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7924
Epoch 02 — loss: 1.6234
Epoch 03 — loss: 1.5731
Epoch 04 — loss: 1.5422
Epoch 05 — loss: 1.5186
Epoch 06 — loss: 1.5048
Epoch 07 — loss: 1.5004
Epoch 08 — loss: 1.4818
Epoch 09 — loss: 1.4871
Epoch 10 — loss: 1.4741
Epoch 11 — loss: 1.4696
Epoch 12 — loss: 1.4650
Epoch 13 — loss: 1.4629
Epoch 14 — loss: 1.4576
Epoch 15 — loss: 1.4497
Epoch 16 — loss: 1.4475
Epoch 17 — loss: 1.4507
Epoch 18 — loss: 1.4414
Epoch 19 — loss: 1.4453
Epoch 20 — loss: 1.4434
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6227
Epoch 02 — loss: 1.3787
Epoch 03 — loss: 1.3123
Stage 1: Error=0.5363, Alpha=1.6461
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7951
Epoch 02 — loss: 1.5736
Epoch 03 — loss: 1.5055
Stage 2: Error=0.6206, Alpha=1.2996
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8334
Epoch 02 — loss: 1.6736
Epoch 03 — loss: 1.5882
Stage 3: Error=0.6489, Alpha=1.1777
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8694
Epoch 02 — loss: 1.7703
Epoch 03 — loss: 1.7046
Stage 4: Error=0.6996, Alpha=0.9462
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8994
Epoch 02 — loss: 1.8285
Epoch 03 — loss: 1.7511
Stage 5: Error=0.7551, Alpha=0.6657
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5548
Epoch 02 — loss: 1.3714
Epoch 03 — loss: 1.3307
Stage 1: Error=0.5413, Alpha=1.6263
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7241
Epoch 02 — loss: 1.5711
Epoch 03 — loss: 1.5436
Stage 2: Error=0.6264, Alpha=1.2749
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8002
Epoch 02 — loss: 1.6836
Epoch 03 — loss: 1.6268
Stage 3: Error=0.6935, Alpha=0.9752
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8417
Epoch 02 — loss: 1.7589
Epoch 03 — loss: 1.7061
Stage 4: Error=0.7460, Alpha=0.7145
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8623
Epoch 02 — loss: 1.7859
Epoch 03 — loss: 1.7534
Stage 5: Error=0.8030, Alpha=0.3869
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5989
Epoch 02 — loss: 1.3636
Epoch 03 — loss: 1.2955
Stage 1: Error=0.5272, Alpha=1.6829
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7984
Epoch 02 — loss: 1.6040
Epoch 03 — loss: 1.5056
Stage 2: Error=0.6308, Alpha=1.2561
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8058
Epoch 02 — loss: 1.6849
Epoch 03 — loss: 1.5986
Stage 3: Error=0.6716, Alpha=1.0762
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8642
Epoch 02 — loss: 1.7937
Epoch 03 — loss: 1.7329
Stage 4: Error=0.7632, Alpha=0.6213
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8742/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [17:10:33] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [17:12:47] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [17:14:43] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8264
Epoch 03 — loss: 1.7663
Stage 5: Error=0.8038, Alpha=0.3815
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4523
Epoch 02 — loss: 1.3273
Epoch 03 — loss: 1.2910
Stage 1: Error=0.5130, Alpha=1.7398
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6779
Epoch 02 — loss: 1.5788
Epoch 03 — loss: 1.4983
Stage 2: Error=0.6363, Alpha=1.2325
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7157
Epoch 02 — loss: 1.6108
Epoch 03 — loss: 1.5724
Stage 3: Error=0.6232, Alpha=1.2887
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7833
Epoch 02 — loss: 1.6784
Epoch 03 — loss: 1.6200
Stage 4: Error=0.6815, Alpha=1.0310
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7884
Epoch 02 — loss: 1.7091
Epoch 03 — loss: 1.6172
Stage 5: Error=0.6371, Alpha=1.2291
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4295
Epoch 02 — loss: 1.3493
Epoch 03 — loss: 1.3139
Stage 1: Error=0.5573, Alpha=1.5616
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6202
Epoch 02 — loss: 1.5270
Epoch 03 — loss: 1.4904
Stage 2: Error=0.6042, Alpha=1.3688
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7112
Epoch 02 — loss: 1.6292
Epoch 03 — loss: 1.5927
Stage 3: Error=0.6675, Alpha=1.0951
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7613
Epoch 02 — loss: 1.6851
Epoch 03 — loss: 1.6277
Stage 4: Error=0.6900, Alpha=0.9918
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7845
Epoch 02 — loss: 1.7267
Epoch 03 — loss: 1.6796
Stage 5: Error=0.6924, Alpha=0.9802
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4426
Epoch 02 — loss: 1.3165
Epoch 03 — loss: 1.2772
Stage 1: Error=0.5076, Alpha=1.7614
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6726
Epoch 02 — loss: 1.5532
Epoch 03 — loss: 1.4959
Stage 2: Error=0.6217, Alpha=1.2951
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7579
Epoch 02 — loss: 1.6332
Epoch 03 — loss: 1.5957
Stage 3: Error=0.6420, Alpha=1.2075
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7855
Epoch 02 — loss: 1.7003
Epoch 03 — loss: 1.6397
Stage 4: Error=0.6755, Alpha=1.0587
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7810
Epoch 02 — loss: 1.6923
Epoch 03 — loss: 1.6221
Stage 5: Error=0.6664, Alpha=1.0998
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8752
Stacking meta epoch 2: loss=0.7344
Stacking meta epoch 3: loss=0.7039
Stacking meta epoch 4: loss=0.6871
Stacking meta epoch 5: loss=0.6756
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8631
Stacking meta epoch 2: loss=0.7410
Stacking meta epoch 3: loss=0.7103
Stacking meta epoch 4: loss=0.6928
Stacking meta epoch 5: loss=0.6806
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2625
Stacking meta epoch 2: loss=1.2435
Stacking meta epoch 3: loss=1.2411
Stacking meta epoch 4: loss=1.2395
Stacking meta epoch 5: loss=1.2383
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      81.28
1                                deepset  ...                      80.41
2                     set_transformer_xy  ...                      80.03
3                             deepset_xy  ...                      82.44
4               set_transformer_additive  ...                      78.25
5                    deepset_xy_additive  ...                      78.83
6                       adaboost_deepset  ...                      81.23
7           adaboost_deepset_xy_additive  ...                      80.03
8                    adaboost_deepset_xy  ...                      83.01
9               adaboost_set_transformer  ...                      81.04
10     adaboost_set_transformer_additive  ...                      80.65
11           adaboost_set_transformer_xy  ...                      81.67
12              soft_voting_ensemble_all  ...                      82.00
13                 stacking_ensemble_all  ...                      83.11
14                      gbm_ensemble_all  ...                      82.53
15                  xgboost_ensemble_all  ...                      82.87
16  soft_voting_ensemble_set_transformer  ...                      81.91
17     stacking_ensemble_set_transformer  ...                      82.87
18          gbm_ensemble_set_transformer  ...                      82.24
19      xgboost_ensemble_set_transformer  ...                      82.44
20          soft_voting_ensemble_deepset  ...                      80.94
21             stacking_ensemble_deepset  ...                      83.01
22                  gbm_ensemble_deepset  ...                      80.99
23              xgboost_ensemble_deepset  ...                      81.42

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 22------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7834
Epoch 02 — loss: 1.6264
Epoch 03 — loss: 1.5645
Epoch 04 — loss: 1.5340
Epoch 05 — loss: 1.4975
Epoch 06 — loss: 1.4587
Epoch 07 — loss: 1.4293
Epoch 08 — loss: 1.4012
Epoch 09 — loss: 1.3745
Epoch 10 — loss: 1.3370
Epoch 11 — loss: 1.3036
Epoch 12 — loss: 1.2570
Epoch 13 — loss: 1.2324
Epoch 14 — loss: 1.2056
Epoch 15 — loss: 1.1596
Epoch 16 — loss: 1.1362
Epoch 17 — loss: 1.0951
Epoch 18 — loss: 1.0531
Epoch 19 — loss: 1.0304
Epoch 20 — loss: 0.9805
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8818
Epoch 02 — loss: 1.5948
Epoch 03 — loss: 1.5428
Epoch 04 — loss: 1.5127
Epoch 05 — loss: 1.4964
Epoch 06 — loss: 1.4839
Epoch 07 — loss: 1.4728
Epoch 08 — loss: 1.4731
Epoch 09 — loss: 1.4682
Epoch 10 — loss: 1.4672
Epoch 11 — loss: 1.4572
Epoch 12 — loss: 1.4578
Epoch 13 — loss: 1.4573
Epoch 14 — loss: 1.4535
Epoch 15 — loss: 1.4505
Epoch 16 — loss: 1.4524
Epoch 17 — loss: 1.4460
Epoch 18 — loss: 1.4420
Epoch 19 — loss: 1.4418
Epoch 20 — loss: 1.4435
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7419
Epoch 02 — loss: 1.6168
Epoch 03 — loss: 1.5727
Epoch 04 — loss: 1.5437
Epoch 05 — loss: 1.5071
Epoch 06 — loss: 1.4768
Epoch 07 — loss: 1.4491
Epoch 08 — loss: 1.4263
Epoch 09 — loss: 1.3785
Epoch 10 — loss: 1.3564
Epoch 11 — loss: 1.3176
Epoch 12 — loss: 1.2761
Epoch 13 — loss: 1.2601
Epoch 14 — loss: 1.2173
Epoch 15 — loss: 1.1883
Epoch 16 — loss: 1.1460
Epoch 17 — loss: 1.1260
Epoch 18 — loss: 1.0744
Epoch 19 — loss: 1.0431
Epoch 20 — loss: 1.0063
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8952
Epoch 02 — loss: 1.6105
Epoch 03 — loss: 1.5453
Epoch 04 — loss: 1.5219
Epoch 05 — loss: 1.5065
Epoch 06 — loss: 1.4935
Epoch 07 — loss: 1.4883
Epoch 08 — loss: 1.4745
Epoch 09 — loss: 1.4598
Epoch 10 — loss: 1.4598
Epoch 11 — loss: 1.4576
Epoch 12 — loss: 1.4562
Epoch 13 — loss: 1.4539
Epoch 14 — loss: 1.4561
Epoch 15 — loss: 1.4502
Epoch 16 — loss: 1.4533
Epoch 17 — loss: 1.4476
Epoch 18 — loss: 1.4435
Epoch 19 — loss: 1.4433
Epoch 20 — loss: 1.4430
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7037
Epoch 02 — loss: 1.6022
Epoch 03 — loss: 1.5676
Epoch 04 — loss: 1.5518
Epoch 05 — loss: 1.5201
Epoch 06 — loss: 1.5005
Epoch 07 — loss: 1.4790
Epoch 08 — loss: 1.4695
Epoch 09 — loss: 1.4499
Epoch 10 — loss: 1.4384
Epoch 11 — loss: 1.4145
Epoch 12 — loss: 1.3930
Epoch 13 — loss: 1.3751
Epoch 14 — loss: 1.3514
Epoch 15 — loss: 1.3268
Epoch 16 — loss: 1.3030
Epoch 17 — loss: 1.2807
Epoch 18 — loss: 1.2577
Epoch 19 — loss: 1.2359
Epoch 20 — loss: 1.2039
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8127
Epoch 02 — loss: 1.6211
Epoch 03 — loss: 1.5881
Epoch 04 — loss: 1.5525
Epoch 05 — loss: 1.5242
Epoch 06 — loss: 1.5026
Epoch 07 — loss: 1.5066
Epoch 08 — loss: 1.4857
Epoch 09 — loss: 1.4831
Epoch 10 — loss: 1.4664
Epoch 11 — loss: 1.4714
Epoch 12 — loss: 1.4680
Epoch 13 — loss: 1.4657
Epoch 14 — loss: 1.4559
Epoch 15 — loss: 1.4627
Epoch 16 — loss: 1.4546
Epoch 17 — loss: 1.4508
Epoch 18 — loss: 1.4554
Epoch 19 — loss: 1.4494
Epoch 20 — loss: 1.4525
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6269
Epoch 02 — loss: 1.3747
Epoch 03 — loss: 1.3111
Stage 1: Error=0.5330, Alpha=1.6597
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7979
Epoch 02 — loss: 1.6083
Epoch 03 — loss: 1.4956
Stage 2: Error=0.6191, Alpha=1.3062
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8478
Epoch 02 — loss: 1.7011
Epoch 03 — loss: 1.6174
Stage 3: Error=0.6656, Alpha=1.1033
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8763
Epoch 02 — loss: 1.8030
Epoch 03 — loss: 1.7120
Stage 4: Error=0.7226, Alpha=0.8342
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8822
Epoch 02 — loss: 1.8159
Epoch 03 — loss: 1.7577
Stage 5: Error=0.7576, Alpha=0.6521
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5555
Epoch 02 — loss: 1.3668
Epoch 03 — loss: 1.3057
Stage 1: Error=0.5422, Alpha=1.6224
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7227
Epoch 02 — loss: 1.5846
Epoch 03 — loss: 1.5318
Stage 2: Error=0.6235, Alpha=1.2874
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7827
Epoch 02 — loss: 1.6759
Epoch 03 — loss: 1.6363
Stage 3: Error=0.6536, Alpha=1.1569
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8302
Epoch 02 — loss: 1.7451
Epoch 03 — loss: 1.7083
Stage 4: Error=0.7222, Alpha=0.8362
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8584
Epoch 02 — loss: 1.7896
Epoch 03 — loss: 1.7573
Stage 5: Error=0.7747, Alpha=0.5568
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6268
Epoch 02 — loss: 1.3631
Epoch 03 — loss: 1.3143
Stage 1: Error=0.5292, Alpha=1.6747
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8162
Epoch 02 — loss: 1.5920
Epoch 03 — loss: 1.5323
Stage 2: Error=0.6466, Alpha=1.1878
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8154
Epoch 02 — loss: 1.6331
Epoch 03 — loss: 1.5690
Stage 3: Error=0.6371, Alpha=1.2288
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8842
Epoch 02 — loss: 1.8033
Epoch 03 — loss: 1.7168
Stage 4: Error=0.7096, Alpha=0.8984
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8886/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [17:56:37] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [17:58:52] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:00:47] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8033
Epoch 03 — loss: 1.7508
Stage 5: Error=0.7621, Alpha=0.6275
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4999
Epoch 02 — loss: 1.3570
Epoch 03 — loss: 1.3102
Stage 1: Error=0.5348, Alpha=1.6524
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6199
Epoch 02 — loss: 1.5303
Epoch 03 — loss: 1.4661
Stage 2: Error=0.5975, Alpha=1.3967
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7432
Epoch 02 — loss: 1.6411
Epoch 03 — loss: 1.5722
Stage 3: Error=0.6636, Alpha=1.1123
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7548
Epoch 02 — loss: 1.6728
Epoch 03 — loss: 1.6172
Stage 4: Error=0.6651, Alpha=1.1058
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8365
Epoch 02 — loss: 1.7439
Epoch 03 — loss: 1.6644
Stage 5: Error=0.6853, Alpha=1.0134
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4177
Epoch 02 — loss: 1.3323
Epoch 03 — loss: 1.3039
Stage 1: Error=0.5386, Alpha=1.6369
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6181
Epoch 02 — loss: 1.5383
Epoch 03 — loss: 1.5098
Stage 2: Error=0.6199, Alpha=1.3025
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6990
Epoch 02 — loss: 1.6391
Epoch 03 — loss: 1.5743
Stage 3: Error=0.6493, Alpha=1.1758
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7561
Epoch 02 — loss: 1.6986
Epoch 03 — loss: 1.6758
Stage 4: Error=0.6882, Alpha=1.0002
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7953
Epoch 02 — loss: 1.7546
Epoch 03 — loss: 1.7315
Stage 5: Error=0.7128, Alpha=0.8827
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4308
Epoch 02 — loss: 1.3283
Epoch 03 — loss: 1.2908
Stage 1: Error=0.5223, Alpha=1.7027
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6598
Epoch 02 — loss: 1.5333
Epoch 03 — loss: 1.4844
Stage 2: Error=0.6032, Alpha=1.3731
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7020
Epoch 02 — loss: 1.6014
Epoch 03 — loss: 1.5503
Stage 3: Error=0.6334, Alpha=1.2447
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7688
Epoch 02 — loss: 1.6651
Epoch 03 — loss: 1.5966
Stage 4: Error=0.6578, Alpha=1.1381
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7785
Epoch 02 — loss: 1.6903
Epoch 03 — loss: 1.6220
Stage 5: Error=0.6526, Alpha=1.1612
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8823
Stacking meta epoch 2: loss=0.7485
Stacking meta epoch 3: loss=0.7175
Stacking meta epoch 4: loss=0.7006
Stacking meta epoch 5: loss=0.6891
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8722
Stacking meta epoch 2: loss=0.7497
Stacking meta epoch 3: loss=0.7204
Stacking meta epoch 4: loss=0.7036
Stacking meta epoch 5: loss=0.6919
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2592
Stacking meta epoch 2: loss=1.2452
Stacking meta epoch 3: loss=1.2428
Stacking meta epoch 4: loss=1.2411
Stacking meta epoch 5: loss=1.2397
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      77.67
1                                deepset  ...                      79.45
2                     set_transformer_xy  ...                      81.04
3                             deepset_xy  ...                      81.28
4               set_transformer_additive  ...                      77.86
5                    deepset_xy_additive  ...                      83.01
6                       adaboost_deepset  ...                      82.92
7           adaboost_deepset_xy_additive  ...                      78.49
8                    adaboost_deepset_xy  ...                      81.23
9               adaboost_set_transformer  ...                      80.65
10     adaboost_set_transformer_additive  ...                      81.18
11           adaboost_set_transformer_xy  ...                      81.95
12              soft_voting_ensemble_all  ...                      82.58
13                 stacking_ensemble_all  ...                      84.36
14                      gbm_ensemble_all  ...                      83.78
15                  xgboost_ensemble_all  ...                      84.02
16  soft_voting_ensemble_set_transformer  ...                      81.57
17     stacking_ensemble_set_transformer  ...                      84.02
18          gbm_ensemble_set_transformer  ...                      83.49
19      xgboost_ensemble_set_transformer  ...                      83.97
20          soft_voting_ensemble_deepset  ...                      81.57
21             stacking_ensemble_deepset  ...                      82.58
22                  gbm_ensemble_deepset  ...                      81.14
23              xgboost_ensemble_deepset  ...                      80.90

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 23------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7345
Epoch 02 — loss: 1.6164
Epoch 03 — loss: 1.5659
Epoch 04 — loss: 1.5321
Epoch 05 — loss: 1.4925
Epoch 06 — loss: 1.4621
Epoch 07 — loss: 1.4249
Epoch 08 — loss: 1.3969
Epoch 09 — loss: 1.3663
Epoch 10 — loss: 1.3375
Epoch 11 — loss: 1.3046
Epoch 12 — loss: 1.2622
Epoch 13 — loss: 1.2338
Epoch 14 — loss: 1.1949
Epoch 15 — loss: 1.1717
Epoch 16 — loss: 1.1356
Epoch 17 — loss: 1.0991
Epoch 18 — loss: 1.0648
Epoch 19 — loss: 1.0281
Epoch 20 — loss: 0.9968
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8813
Epoch 02 — loss: 1.6026
Epoch 03 — loss: 1.5488
Epoch 04 — loss: 1.5133
Epoch 05 — loss: 1.5029
Epoch 06 — loss: 1.4841
Epoch 07 — loss: 1.4807
Epoch 08 — loss: 1.4784
Epoch 09 — loss: 1.4703
Epoch 10 — loss: 1.4702
Epoch 11 — loss: 1.4654
Epoch 12 — loss: 1.4550
Epoch 13 — loss: 1.4586
Epoch 14 — loss: 1.4563
Epoch 15 — loss: 1.4458
Epoch 16 — loss: 1.4489
Epoch 17 — loss: 1.4466
Epoch 18 — loss: 1.4496
Epoch 19 — loss: 1.4427
Epoch 20 — loss: 1.4451
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7507
Epoch 02 — loss: 1.6208
Epoch 03 — loss: 1.5670
Epoch 04 — loss: 1.5207
Epoch 05 — loss: 1.4946
Epoch 06 — loss: 1.4729
Epoch 07 — loss: 1.4355
Epoch 08 — loss: 1.4035
Epoch 09 — loss: 1.3781
Epoch 10 — loss: 1.3462
Epoch 11 — loss: 1.3154
Epoch 12 — loss: 1.2837
Epoch 13 — loss: 1.2596
Epoch 14 — loss: 1.2131
Epoch 15 — loss: 1.1846
Epoch 16 — loss: 1.1455
Epoch 17 — loss: 1.1289
Epoch 18 — loss: 1.0941
Epoch 19 — loss: 1.0469
Epoch 20 — loss: 1.0112
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8836
Epoch 02 — loss: 1.6056
Epoch 03 — loss: 1.5373
Epoch 04 — loss: 1.5214
Epoch 05 — loss: 1.5085
Epoch 06 — loss: 1.4905
Epoch 07 — loss: 1.4801
Epoch 08 — loss: 1.4649
Epoch 09 — loss: 1.4715
Epoch 10 — loss: 1.4643
Epoch 11 — loss: 1.4558
Epoch 12 — loss: 1.4535
Epoch 13 — loss: 1.4503
Epoch 14 — loss: 1.4578
Epoch 15 — loss: 1.4473
Epoch 16 — loss: 1.4505
Epoch 17 — loss: 1.4497
Epoch 18 — loss: 1.4440
Epoch 19 — loss: 1.4365
Epoch 20 — loss: 1.4369
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7368
Epoch 02 — loss: 1.6252
Epoch 03 — loss: 1.5775
Epoch 04 — loss: 1.5522
Epoch 05 — loss: 1.5227
Epoch 06 — loss: 1.5073
Epoch 07 — loss: 1.4903
Epoch 08 — loss: 1.4747
Epoch 09 — loss: 1.4491
Epoch 10 — loss: 1.4302
Epoch 11 — loss: 1.4097
Epoch 12 — loss: 1.3941
Epoch 13 — loss: 1.3646
Epoch 14 — loss: 1.3560
Epoch 15 — loss: 1.3325
Epoch 16 — loss: 1.3040
Epoch 17 — loss: 1.2792
Epoch 18 — loss: 1.2564
Epoch 19 — loss: 1.2394
Epoch 20 — loss: 1.2153
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8177
Epoch 02 — loss: 1.6204
Epoch 03 — loss: 1.5746
Epoch 04 — loss: 1.5457
Epoch 05 — loss: 1.5200
Epoch 06 — loss: 1.5137
Epoch 07 — loss: 1.4969
Epoch 08 — loss: 1.4876
Epoch 09 — loss: 1.4794
Epoch 10 — loss: 1.4722
Epoch 11 — loss: 1.4683
Epoch 12 — loss: 1.4621
Epoch 13 — loss: 1.4682
Epoch 14 — loss: 1.4599
Epoch 15 — loss: 1.4581
Epoch 16 — loss: 1.4505
Epoch 17 — loss: 1.4511
Epoch 18 — loss: 1.4590
Epoch 19 — loss: 1.4484
Epoch 20 — loss: 1.4447
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6100
Epoch 02 — loss: 1.3654
Epoch 03 — loss: 1.3011
Stage 1: Error=0.5297, Alpha=1.6727
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8105
Epoch 02 — loss: 1.5916
Epoch 03 — loss: 1.5340
Stage 2: Error=0.6378, Alpha=1.2259
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8346
Epoch 02 — loss: 1.6536
Epoch 03 — loss: 1.5727
Stage 3: Error=0.6328, Alpha=1.2477
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8745
Epoch 02 — loss: 1.7957
Epoch 03 — loss: 1.7206
Stage 4: Error=0.7104, Alpha=0.8943
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8889
Epoch 02 — loss: 1.7974
Epoch 03 — loss: 1.7606
Stage 5: Error=0.7675, Alpha=0.5976
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5593
Epoch 02 — loss: 1.3901
Epoch 03 — loss: 1.3180
Stage 1: Error=0.5412, Alpha=1.6268
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7195
Epoch 02 — loss: 1.5928
Epoch 03 — loss: 1.5374
Stage 2: Error=0.6393, Alpha=1.2193
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7759
Epoch 02 — loss: 1.6562
Epoch 03 — loss: 1.6109
Stage 3: Error=0.6467, Alpha=1.1872
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8482
Epoch 02 — loss: 1.7683
Epoch 03 — loss: 1.7268
Stage 4: Error=0.7340, Alpha=0.7770
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8782
Epoch 02 — loss: 1.8170
Epoch 03 — loss: 1.7620
Stage 5: Error=0.7750, Alpha=0.5552
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6213
Epoch 02 — loss: 1.3654
Epoch 03 — loss: 1.3148
Stage 1: Error=0.5319, Alpha=1.6640
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8137
Epoch 02 — loss: 1.6063
Epoch 03 — loss: 1.5356
Stage 2: Error=0.6347, Alpha=1.2392
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8087
Epoch 02 — loss: 1.6436
Epoch 03 — loss: 1.5907
Stage 3: Error=0.6350, Alpha=1.2379
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8685
Epoch 02 — loss: 1.7682
Epoch 03 — loss: 1.7066
Stage 4: Error=0.7349, Alpha=0.7719
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8883/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:42:39] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:44:54] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:46:48] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8115
Epoch 03 — loss: 1.7412
Stage 5: Error=0.7552, Alpha=0.6652
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.3984
Epoch 02 — loss: 1.3302
Epoch 03 — loss: 1.3065
Stage 1: Error=0.5241, Alpha=1.6954
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6705
Epoch 02 — loss: 1.5408
Epoch 03 — loss: 1.5030
Stage 2: Error=0.6092, Alpha=1.3477
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7132
Epoch 02 — loss: 1.6147
Epoch 03 — loss: 1.5571
Stage 3: Error=0.6335, Alpha=1.2444
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7921
Epoch 02 — loss: 1.6853
Epoch 03 — loss: 1.6241
Stage 4: Error=0.6428, Alpha=1.2040
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8056
Epoch 02 — loss: 1.7165
Epoch 03 — loss: 1.6277
Stage 5: Error=0.6790, Alpha=1.0424
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4013
Epoch 02 — loss: 1.3451
Epoch 03 — loss: 1.3146
Stage 1: Error=0.5288, Alpha=1.6766
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6728
Epoch 02 — loss: 1.5686
Epoch 03 — loss: 1.5435
Stage 2: Error=0.6637, Alpha=1.1120
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7019
Epoch 02 — loss: 1.6258
Epoch 03 — loss: 1.5921
Stage 3: Error=0.6416, Alpha=1.2096
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7953
Epoch 02 — loss: 1.7325
Epoch 03 — loss: 1.7013
Stage 4: Error=0.7152, Alpha=0.8711
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8090
Epoch 02 — loss: 1.7488
Epoch 03 — loss: 1.7208
Stage 5: Error=0.7047, Alpha=0.9221
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4308
Epoch 02 — loss: 1.3128
Epoch 03 — loss: 1.2629
Stage 1: Error=0.5085, Alpha=1.7576
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6522
Epoch 02 — loss: 1.5164
Epoch 03 — loss: 1.4694
Stage 2: Error=0.5867, Alpha=1.4416
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7438
Epoch 02 — loss: 1.6424
Epoch 03 — loss: 1.5791
Stage 3: Error=0.6325, Alpha=1.2487
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8056
Epoch 02 — loss: 1.7068
Epoch 03 — loss: 1.6235
Stage 4: Error=0.6787, Alpha=1.0441
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7953
Epoch 02 — loss: 1.6887
Epoch 03 — loss: 1.6257
Stage 5: Error=0.6543, Alpha=1.1539
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8918
Stacking meta epoch 2: loss=0.7539
Stacking meta epoch 3: loss=0.7222
Stacking meta epoch 4: loss=0.7048
Stacking meta epoch 5: loss=0.6926
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8888
Stacking meta epoch 2: loss=0.7553
Stacking meta epoch 3: loss=0.7244
Stacking meta epoch 4: loss=0.7075
Stacking meta epoch 5: loss=0.6958
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2627
Stacking meta epoch 2: loss=1.2430
Stacking meta epoch 3: loss=1.2406
Stacking meta epoch 4: loss=1.2389
Stacking meta epoch 5: loss=1.2376
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      79.74
1                                deepset  ...                      81.33
2                     set_transformer_xy  ...                      79.40
3                             deepset_xy  ...                      81.57
4               set_transformer_additive  ...                      78.68
5                    deepset_xy_additive  ...                      82.05
6                       adaboost_deepset  ...                      81.52
7           adaboost_deepset_xy_additive  ...                      79.55
8                    adaboost_deepset_xy  ...                      80.99
9               adaboost_set_transformer  ...                      80.70
10     adaboost_set_transformer_additive  ...                      81.18
11           adaboost_set_transformer_xy  ...                      83.01
12              soft_voting_ensemble_all  ...                      82.77
13                 stacking_ensemble_all  ...                      83.21
14                      gbm_ensemble_all  ...                      83.49
15                  xgboost_ensemble_all  ...                      83.69
16  soft_voting_ensemble_set_transformer  ...                      82.29
17     stacking_ensemble_set_transformer  ...                      83.16
18          gbm_ensemble_set_transformer  ...                      83.69
19      xgboost_ensemble_set_transformer  ...                      83.64
20          soft_voting_ensemble_deepset  ...                      82.53
21             stacking_ensemble_deepset  ...                      82.87
22                  gbm_ensemble_deepset  ...                      80.51
23              xgboost_ensemble_deepset  ...                      80.65

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 24------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7433
Epoch 02 — loss: 1.6219
Epoch 03 — loss: 1.5750
Epoch 04 — loss: 1.5531
Epoch 05 — loss: 1.5146
Epoch 06 — loss: 1.4750
Epoch 07 — loss: 1.4446
Epoch 08 — loss: 1.4116
Epoch 09 — loss: 1.3760
Epoch 10 — loss: 1.3467
Epoch 11 — loss: 1.3120
Epoch 12 — loss: 1.2750
Epoch 13 — loss: 1.2368
Epoch 14 — loss: 1.2106
Epoch 15 — loss: 1.1687
Epoch 16 — loss: 1.1302
Epoch 17 — loss: 1.0879
Epoch 18 — loss: 1.0620
Epoch 19 — loss: 1.0237
Epoch 20 — loss: 0.9745
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8863
Epoch 02 — loss: 1.6280
Epoch 03 — loss: 1.5583
Epoch 04 — loss: 1.5314
Epoch 05 — loss: 1.5086
Epoch 06 — loss: 1.5032
Epoch 07 — loss: 1.4885
Epoch 08 — loss: 1.4806
Epoch 09 — loss: 1.4717
Epoch 10 — loss: 1.4692
Epoch 11 — loss: 1.4669
Epoch 12 — loss: 1.4600
Epoch 13 — loss: 1.4559
Epoch 14 — loss: 1.4542
Epoch 15 — loss: 1.4570
Epoch 16 — loss: 1.4458
Epoch 17 — loss: 1.4488
Epoch 18 — loss: 1.4378
Epoch 19 — loss: 1.4440
Epoch 20 — loss: 1.4349
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7373
Epoch 02 — loss: 1.6154
Epoch 03 — loss: 1.5719
Epoch 04 — loss: 1.5312
Epoch 05 — loss: 1.5037
Epoch 06 — loss: 1.4767
Epoch 07 — loss: 1.4406
Epoch 08 — loss: 1.3953
Epoch 09 — loss: 1.3668
Epoch 10 — loss: 1.3312
Epoch 11 — loss: 1.2970
Epoch 12 — loss: 1.2617
Epoch 13 — loss: 1.2142
Epoch 14 — loss: 1.1862
Epoch 15 — loss: 1.1588
Epoch 16 — loss: 1.1098
Epoch 17 — loss: 1.0827
Epoch 18 — loss: 1.0446
Epoch 19 — loss: 0.9978
Epoch 20 — loss: 0.9882
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8541
Epoch 02 — loss: 1.5917
Epoch 03 — loss: 1.5461
Epoch 04 — loss: 1.5203
Epoch 05 — loss: 1.5035
Epoch 06 — loss: 1.4982
Epoch 07 — loss: 1.4842
Epoch 08 — loss: 1.4814
Epoch 09 — loss: 1.4706
Epoch 10 — loss: 1.4624
Epoch 11 — loss: 1.4622
Epoch 12 — loss: 1.4600
Epoch 13 — loss: 1.4584
Epoch 14 — loss: 1.4538
Epoch 15 — loss: 1.4549
Epoch 16 — loss: 1.4429
Epoch 17 — loss: 1.4469
Epoch 18 — loss: 1.4445
Epoch 19 — loss: 1.4444
Epoch 20 — loss: 1.4356
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7127
Epoch 02 — loss: 1.6248
Epoch 03 — loss: 1.5860
Epoch 04 — loss: 1.5484
Epoch 05 — loss: 1.5392
Epoch 06 — loss: 1.5095
Epoch 07 — loss: 1.4986
Epoch 08 — loss: 1.4765
Epoch 09 — loss: 1.4447
Epoch 10 — loss: 1.4292
Epoch 11 — loss: 1.4132
Epoch 12 — loss: 1.3846
Epoch 13 — loss: 1.3581
Epoch 14 — loss: 1.3311
Epoch 15 — loss: 1.3124
Epoch 16 — loss: 1.2924
Epoch 17 — loss: 1.2581
Epoch 18 — loss: 1.2375
Epoch 19 — loss: 1.2117
Epoch 20 — loss: 1.1715
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7946
Epoch 02 — loss: 1.6286
Epoch 03 — loss: 1.5870
Epoch 04 — loss: 1.5644
Epoch 05 — loss: 1.5410
Epoch 06 — loss: 1.5205
Epoch 07 — loss: 1.5014
Epoch 08 — loss: 1.4913
Epoch 09 — loss: 1.4858
Epoch 10 — loss: 1.4749
Epoch 11 — loss: 1.4684
Epoch 12 — loss: 1.4663
Epoch 13 — loss: 1.4615
Epoch 14 — loss: 1.4598
Epoch 15 — loss: 1.4579
Epoch 16 — loss: 1.4502
Epoch 17 — loss: 1.4533
Epoch 18 — loss: 1.4566
Epoch 19 — loss: 1.4492
Epoch 20 — loss: 1.4482
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6416
Epoch 02 — loss: 1.3898
Epoch 03 — loss: 1.3147
Stage 1: Error=0.5336, Alpha=1.6573
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7886
Epoch 02 — loss: 1.5987
Epoch 03 — loss: 1.5192
Stage 2: Error=0.6276, Alpha=1.2697
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8340
Epoch 02 — loss: 1.7035
Epoch 03 — loss: 1.6090
Stage 3: Error=0.6732, Alpha=1.0692
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8555
Epoch 02 — loss: 1.7685
Epoch 03 — loss: 1.7184
Stage 4: Error=0.7725, Alpha=0.5690
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8709
Epoch 02 — loss: 1.8134
Epoch 03 — loss: 1.7541
Stage 5: Error=0.7346, Alpha=0.7739
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5515
Epoch 02 — loss: 1.3417
Epoch 03 — loss: 1.3051
Stage 1: Error=0.5335, Alpha=1.6577
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7539
Epoch 02 — loss: 1.5786
Epoch 03 — loss: 1.5448
Stage 2: Error=0.6657, Alpha=1.1030
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7948
Epoch 02 — loss: 1.6616
Epoch 03 — loss: 1.6161
Stage 3: Error=0.6730, Alpha=1.0698
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8432
Epoch 02 — loss: 1.7281
Epoch 03 — loss: 1.6895
Stage 4: Error=0.6898, Alpha=0.9926
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8617
Epoch 02 — loss: 1.7743
Epoch 03 — loss: 1.7441
Stage 5: Error=0.7253, Alpha=0.8208
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6091
Epoch 02 — loss: 1.3656
Epoch 03 — loss: 1.2948
Stage 1: Error=0.5372, Alpha=1.6427
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7810
Epoch 02 — loss: 1.5728
Epoch 03 — loss: 1.4920
Stage 2: Error=0.6094, Alpha=1.3469
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8375
Epoch 02 — loss: 1.6832
Epoch 03 — loss: 1.6322
Stage 3: Error=0.7006, Alpha=0.9414
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8729
Epoch 02 — loss: 1.7803
Epoch 03 — loss: 1.7117
Stage 4: Error=0.7335, Alpha=0.7791
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8746/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:28:36] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:30:52] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:32:48] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8065
Epoch 03 — loss: 1.7374
Stage 5: Error=0.7607, Alpha=0.6351
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4485
Epoch 02 — loss: 1.3554
Epoch 03 — loss: 1.3138
Stage 1: Error=0.5416, Alpha=1.6248
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6330
Epoch 02 — loss: 1.5375
Epoch 03 — loss: 1.4601
Stage 2: Error=0.5881, Alpha=1.4357
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7481
Epoch 02 — loss: 1.6201
Epoch 03 — loss: 1.5483
Stage 3: Error=0.6769, Alpha=1.0523
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7595
Epoch 02 — loss: 1.6473
Epoch 03 — loss: 1.5727
Stage 4: Error=0.6168, Alpha=1.3157
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7996
Epoch 02 — loss: 1.6920
Epoch 03 — loss: 1.6240
Stage 5: Error=0.6571, Alpha=1.1415
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4309
Epoch 02 — loss: 1.3466
Epoch 03 — loss: 1.3083
Stage 1: Error=0.5344, Alpha=1.6539
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6326
Epoch 02 — loss: 1.5466
Epoch 03 — loss: 1.5226
Stage 2: Error=0.6446, Alpha=1.1962
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7006
Epoch 02 — loss: 1.6474
Epoch 03 — loss: 1.6008
Stage 3: Error=0.6691, Alpha=1.0879
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7711
Epoch 02 — loss: 1.7055
Epoch 03 — loss: 1.6516
Stage 4: Error=0.6860, Alpha=1.0104
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8196
Epoch 02 — loss: 1.7439
Epoch 03 — loss: 1.7005
Stage 5: Error=0.7164, Alpha=0.8649
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4272
Epoch 02 — loss: 1.3431
Epoch 03 — loss: 1.3062
Stage 1: Error=0.5217, Alpha=1.7051
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6282
Epoch 02 — loss: 1.5311
Epoch 03 — loss: 1.4640
Stage 2: Error=0.5855, Alpha=1.4465
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7116
Epoch 02 — loss: 1.6329
Epoch 03 — loss: 1.5670
Stage 3: Error=0.6527, Alpha=1.1609
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7693
Epoch 02 — loss: 1.6805
Epoch 03 — loss: 1.5991
Stage 4: Error=0.6707, Alpha=1.0802
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8005
Epoch 02 — loss: 1.7137
Epoch 03 — loss: 1.6416
Stage 5: Error=0.6713, Alpha=1.0777
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8683
Stacking meta epoch 2: loss=0.7372
Stacking meta epoch 3: loss=0.7083
Stacking meta epoch 4: loss=0.6927
Stacking meta epoch 5: loss=0.6820
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8629
Stacking meta epoch 2: loss=0.7413
Stacking meta epoch 3: loss=0.7117
Stacking meta epoch 4: loss=0.6950
Stacking meta epoch 5: loss=0.6835
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2576
Stacking meta epoch 2: loss=1.2424
Stacking meta epoch 3: loss=1.2401
Stacking meta epoch 4: loss=1.2384
Stacking meta epoch 5: loss=1.2371
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      81.09
1                                deepset  ...                      80.17
2                     set_transformer_xy  ...                      80.32
3                             deepset_xy  ...                      79.84
4               set_transformer_additive  ...                      81.71
5                    deepset_xy_additive  ...                      80.80
6                       adaboost_deepset  ...                      81.14
7           adaboost_deepset_xy_additive  ...                      80.32
8                    adaboost_deepset_xy  ...                      82.53
9               adaboost_set_transformer  ...                      81.91
10     adaboost_set_transformer_additive  ...                      81.52
11           adaboost_set_transformer_xy  ...                      80.94
12              soft_voting_ensemble_all  ...                      83.06
13                 stacking_ensemble_all  ...                      82.82
14                      gbm_ensemble_all  ...                      82.48
15                  xgboost_ensemble_all  ...                      83.11
16  soft_voting_ensemble_set_transformer  ...                      82.68
17     stacking_ensemble_set_transformer  ...                      82.77
18          gbm_ensemble_set_transformer  ...                      82.72
19      xgboost_ensemble_set_transformer  ...                      83.21
20          soft_voting_ensemble_deepset  ...                      80.46
21             stacking_ensemble_deepset  ...                      82.77
22                  gbm_ensemble_deepset  ...                      80.56
23              xgboost_ensemble_deepset  ...                      80.80

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

------------------------------------iteration no 25------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7809
Epoch 02 — loss: 1.6487
Epoch 03 — loss: 1.5875
Epoch 04 — loss: 1.5499
Epoch 05 — loss: 1.5113
Epoch 06 — loss: 1.4769
Epoch 07 — loss: 1.4408
Epoch 08 — loss: 1.4167
Epoch 09 — loss: 1.3800
Epoch 10 — loss: 1.3546
Epoch 11 — loss: 1.3126
Epoch 12 — loss: 1.2851
Epoch 13 — loss: 1.2524
Epoch 14 — loss: 1.2147
Epoch 15 — loss: 1.1727
Epoch 16 — loss: 1.1465
Epoch 17 — loss: 1.1085
Epoch 18 — loss: 1.0613
Epoch 19 — loss: 1.0325
Epoch 20 — loss: 1.0036
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8739
Epoch 02 — loss: 1.6076
Epoch 03 — loss: 1.5433
Epoch 04 — loss: 1.5179
Epoch 05 — loss: 1.5029
Epoch 06 — loss: 1.4969
Epoch 07 — loss: 1.4861
Epoch 08 — loss: 1.4747
Epoch 09 — loss: 1.4700
Epoch 10 — loss: 1.4699
Epoch 11 — loss: 1.4582
Epoch 12 — loss: 1.4480
Epoch 13 — loss: 1.4498
Epoch 14 — loss: 1.4495
Epoch 15 — loss: 1.4441
Epoch 16 — loss: 1.4450
Epoch 17 — loss: 1.4449
Epoch 18 — loss: 1.4416
Epoch 19 — loss: 1.4398
Epoch 20 — loss: 1.4350
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7409
Epoch 02 — loss: 1.5858
Epoch 03 — loss: 1.5403
Epoch 04 — loss: 1.5108
Epoch 05 — loss: 1.4837
Epoch 06 — loss: 1.4451
Epoch 07 — loss: 1.4199
Epoch 08 — loss: 1.3912
Epoch 09 — loss: 1.3661
Epoch 10 — loss: 1.3315
Epoch 11 — loss: 1.3084
Epoch 12 — loss: 1.2643
Epoch 13 — loss: 1.2399
Epoch 14 — loss: 1.2130
Epoch 15 — loss: 1.1631
Epoch 16 — loss: 1.1315
Epoch 17 — loss: 1.0946
Epoch 18 — loss: 1.0596
Epoch 19 — loss: 1.0276
Epoch 20 — loss: 0.9909
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.9016
Epoch 02 — loss: 1.6117
Epoch 03 — loss: 1.5345
Epoch 04 — loss: 1.5196
Epoch 05 — loss: 1.4972
Epoch 06 — loss: 1.4875
Epoch 07 — loss: 1.4761
Epoch 08 — loss: 1.4705
Epoch 09 — loss: 1.4649
Epoch 10 — loss: 1.4566
Epoch 11 — loss: 1.4520
Epoch 12 — loss: 1.4480
Epoch 13 — loss: 1.4514
Epoch 14 — loss: 1.4513
Epoch 15 — loss: 1.4453
Epoch 16 — loss: 1.4425
Epoch 17 — loss: 1.4414
Epoch 18 — loss: 1.4477
Epoch 19 — loss: 1.4382
Epoch 20 — loss: 1.4405
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7091
Epoch 02 — loss: 1.6219
Epoch 03 — loss: 1.5936
Epoch 04 — loss: 1.5643
Epoch 05 — loss: 1.5500
Epoch 06 — loss: 1.5163
Epoch 07 — loss: 1.5046
Epoch 08 — loss: 1.4922
Epoch 09 — loss: 1.4685
Epoch 10 — loss: 1.4480
Epoch 11 — loss: 1.4339
Epoch 12 — loss: 1.4086
Epoch 13 — loss: 1.3903
Epoch 14 — loss: 1.3566
Epoch 15 — loss: 1.3397
Epoch 16 — loss: 1.3026
Epoch 17 — loss: 1.2867
Epoch 18 — loss: 1.2551
Epoch 19 — loss: 1.2312
Epoch 20 — loss: 1.1967
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7899
Epoch 02 — loss: 1.6319
Epoch 03 — loss: 1.5903
Epoch 04 — loss: 1.5564
Epoch 05 — loss: 1.5393
Epoch 06 — loss: 1.5211
Epoch 07 — loss: 1.4963
Epoch 08 — loss: 1.4979
Epoch 09 — loss: 1.4904
Epoch 10 — loss: 1.4793
Epoch 11 — loss: 1.4754
Epoch 12 — loss: 1.4720
Epoch 13 — loss: 1.4690
Epoch 14 — loss: 1.4661
Epoch 15 — loss: 1.4548
Epoch 16 — loss: 1.4686
Epoch 17 — loss: 1.4613
Epoch 18 — loss: 1.4616
Epoch 19 — loss: 1.4540
Epoch 20 — loss: 1.4467
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (deepset, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6143
Epoch 02 — loss: 1.3624
Epoch 03 — loss: 1.3003
Stage 1: Error=0.5284, Alpha=1.6780
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7877
Epoch 02 — loss: 1.6052
Epoch 03 — loss: 1.5307
Stage 2: Error=0.6499, Alpha=1.1729
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8266
Epoch 02 — loss: 1.6638
Epoch 03 — loss: 1.6077
Stage 3: Error=0.6485, Alpha=1.1793
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8692
Epoch 02 — loss: 1.8053
Epoch 03 — loss: 1.7298
Stage 4: Error=0.7581, Alpha=0.6497
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8806
Epoch 02 — loss: 1.8092
Epoch 03 — loss: 1.7392
Stage 5: Error=0.7667, Alpha=0.6020
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy_additive =====
===== Training Boosting Ensemble (deepset_xy_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5532
Epoch 02 — loss: 1.3569
Epoch 03 — loss: 1.3194
Stage 1: Error=0.5448, Alpha=1.6122
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7083
Epoch 02 — loss: 1.5896
Epoch 03 — loss: 1.5216
Stage 2: Error=0.6342, Alpha=1.2413
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7861
Epoch 02 — loss: 1.6717
Epoch 03 — loss: 1.6416
Stage 3: Error=0.6751, Alpha=1.0604
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8485
Epoch 02 — loss: 1.7529
Epoch 03 — loss: 1.7030
Stage 4: Error=0.7557, Alpha=0.6627
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8574
Epoch 02 — loss: 1.7843
Epoch 03 — loss: 1.7445
Stage 5: Error=0.7034, Alpha=0.9281
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset_xy =====
===== Training Boosting Ensemble (deepset_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6343
Epoch 02 — loss: 1.3679
Epoch 03 — loss: 1.3107
Stage 1: Error=0.5404, Alpha=1.6297
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7682
Epoch 02 — loss: 1.5667
Epoch 03 — loss: 1.4995
Stage 2: Error=0.6058, Alpha=1.3622
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8154
Epoch 02 — loss: 1.6572
Epoch 03 — loss: 1.6088
Stage 3: Error=0.6660, Alpha=1.1018
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8766
Epoch 02 — loss: 1.7839
Epoch 03 — loss: 1.7101
Stage 4: Error=0.7442, Alpha=0.7236
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8737/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:14:40] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:16:54] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:18:49] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 02 — loss: 1.8115
Epoch 03 — loss: 1.7614
Stage 5: Error=0.7783, Alpha=0.5362
Building final ensemble with 5 models.
Predictions for adaboost_deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (set_transformer, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4678
Epoch 02 — loss: 1.3501
Epoch 03 — loss: 1.3155
Stage 1: Error=0.5261, Alpha=1.6872
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6584
Epoch 02 — loss: 1.5534
Epoch 03 — loss: 1.4957
Stage 2: Error=0.6294, Alpha=1.2623
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7367
Epoch 02 — loss: 1.6285
Epoch 03 — loss: 1.5482
Stage 3: Error=0.6410, Alpha=1.2119
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7541
Epoch 02 — loss: 1.6562
Epoch 03 — loss: 1.6013
Stage 4: Error=0.6581, Alpha=1.1369
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8188
Epoch 02 — loss: 1.7404
Epoch 03 — loss: 1.6822
Stage 5: Error=0.6804, Alpha=1.0361
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_additive =====
===== Training Boosting Ensemble (set_transformer_additive, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4241
Epoch 02 — loss: 1.3508
Epoch 03 — loss: 1.3129
Stage 1: Error=0.5692, Alpha=1.5132
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.5919
Epoch 02 — loss: 1.5304
Epoch 03 — loss: 1.4909
Stage 2: Error=0.5843, Alpha=1.4512
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7084
Epoch 02 — loss: 1.6274
Epoch 03 — loss: 1.5892
Stage 3: Error=0.6789, Alpha=1.0432
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7709
Epoch 02 — loss: 1.7015
Epoch 03 — loss: 1.6752
Stage 4: Error=0.7091, Alpha=0.9006
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8058
Epoch 02 — loss: 1.7357
Epoch 03 — loss: 1.6969
Stage 5: Error=0.7148, Alpha=0.8729
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer_xy =====
===== Training Boosting Ensemble (set_transformer_xy, 5 stages) =====
--- Boosting Stage 1/5 ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4336
Epoch 02 — loss: 1.3592
Epoch 03 — loss: 1.2811
Stage 1: Error=0.5252, Alpha=1.6911
--- Boosting Stage 2/5 ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6294
Epoch 02 — loss: 1.5267
Epoch 03 — loss: 1.4578
Stage 2: Error=0.5998, Alpha=1.3871
--- Boosting Stage 3/5 ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6919
Epoch 02 — loss: 1.6039
Epoch 03 — loss: 1.5388
Stage 3: Error=0.6172, Alpha=1.3140
--- Boosting Stage 4/5 ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7440
Epoch 02 — loss: 1.6741
Epoch 03 — loss: 1.6067
Stage 4: Error=0.6434, Alpha=1.2016
--- Boosting Stage 5/5 ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8094
Epoch 02 — loss: 1.7070
Epoch 03 — loss: 1.6227
Stage 5: Error=0.6731, Alpha=1.0695
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8896
Stacking meta epoch 2: loss=0.7547
Stacking meta epoch 3: loss=0.7247
Stacking meta epoch 4: loss=0.7083
Stacking meta epoch 5: loss=0.6970
Predictions for soft_voting_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8792
Stacking meta epoch 2: loss=0.7595
Stacking meta epoch 3: loss=0.7299
Stacking meta epoch 4: loss=0.7129
Stacking meta epoch 5: loss=0.7012
Predictions for soft_voting_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2539
Stacking meta epoch 2: loss=1.2409
Stacking meta epoch 3: loss=1.2388
Stacking meta epoch 4: loss=1.2371
Stacking meta epoch 5: loss=1.2357
Predictions for soft_voting_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for gbm_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
Predictions for xgboost_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                              Model Type  ...  Val ±1 Grade Accuracy (%)
0                        set_transformer  ...                      78.10
1                                deepset  ...                      77.86
2                     set_transformer_xy  ...                      79.45
3                             deepset_xy  ...                      80.37
4               set_transformer_additive  ...                      80.41
5                    deepset_xy_additive  ...                      81.09
6                       adaboost_deepset  ...                      81.76
7           adaboost_deepset_xy_additive  ...                      78.97
8                    adaboost_deepset_xy  ...                      82.92
9               adaboost_set_transformer  ...                      81.86
10     adaboost_set_transformer_additive  ...                      80.41
11           adaboost_set_transformer_xy  ...                      80.85
12              soft_voting_ensemble_all  ...                      81.62
13                 stacking_ensemble_all  ...                      83.40
14                      gbm_ensemble_all  ...                      82.15
15                  xgboost_ensemble_all  ...                      82.15
16  soft_voting_ensemble_set_transformer  ...                      80.61
17     stacking_ensemble_set_transformer  ...                      84.12
18          gbm_ensemble_set_transformer  ...                      81.57
19      xgboost_ensemble_set_transformer  ...                      82.63
20          soft_voting_ensemble_deepset  ...                      80.17
21             stacking_ensemble_deepset  ...                      82.63
22                  gbm_ensemble_deepset  ...                      81.33
23              xgboost_ensemble_deepset  ...                      81.38

[24 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:42:35] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

=== Accuracy Stability Summary ===
                              Model Type  ...  Val ±1 Grade Std (%)
6                       adaboost_deepset  ...                 0.606
8                    adaboost_deepset_xy  ...                 0.685
7           adaboost_deepset_xy_additive  ...                 0.829
9               adaboost_set_transformer  ...                 0.589
10     adaboost_set_transformer_additive  ...                 0.709
11           adaboost_set_transformer_xy  ...                 0.713
1                                deepset  ...                 1.201
3                             deepset_xy  ...                 1.080
5                    deepset_xy_additive  ...                 1.384
14                      gbm_ensemble_all  ...                 0.734
22                  gbm_ensemble_deepset  ...                 0.533
18          gbm_ensemble_set_transformer  ...                 0.740
0                        set_transformer  ...                 1.221
4               set_transformer_additive  ...                 1.896
2                     set_transformer_xy  ...                 1.448
12              soft_voting_ensemble_all  ...                 0.518
20          soft_voting_ensemble_deepset  ...                 0.886
16  soft_voting_ensemble_set_transformer  ...                 0.802
13                 stacking_ensemble_all  ...                 0.582
21             stacking_ensemble_deepset  ...                 0.287
17     stacking_ensemble_set_transformer  ...                 0.578
15                  xgboost_ensemble_all  ...                 0.703
23              xgboost_ensemble_deepset  ...                 0.474
19      xgboost_ensemble_set_transformer  ...                 0.677

[24 rows x 9 columns]
----------------- Ordinal iteration 1/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3680
Epoch 02 — loss: 0.3182
Epoch 03 — loss: 0.3036
Epoch 04 — loss: 0.2939
Epoch 05 — loss: 0.2842
Epoch 06 — loss: 0.2767
Epoch 07 — loss: 0.2688
Epoch 08 — loss: 0.2625
Epoch 09 — loss: 0.2529
Epoch 10 — loss: 0.2483
Epoch 11 — loss: 0.2419
Epoch 12 — loss: 0.2344
Epoch 13 — loss: 0.2291
Epoch 14 — loss: 0.2227
Epoch 15 — loss: 0.2185
Epoch 16 — loss: 0.2110
Epoch 17 — loss: 0.2056
Epoch 18 — loss: 0.2007
Epoch 19 — loss: 0.1953
Epoch 20 — loss: 0.1891
  threshold   accuracy
0    P(>V4)  82.769997
1    P(>V5)  81.519997
2    P(>V6)  84.889999
3    P(>V7)  88.449997
4    P(>V8)  91.959999
5    P(>V9)  97.019997
Overall accuracy: 47.40%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3753
Epoch 02 — loss: 0.3164
Epoch 03 — loss: 0.2983
Epoch 04 — loss: 0.2895
Epoch 05 — loss: 0.2828
Epoch 06 — loss: 0.2719
Epoch 07 — loss: 0.2637
Epoch 08 — loss: 0.2555
Epoch 09 — loss: 0.2497
Epoch 10 — loss: 0.2420
Epoch 11 — loss: 0.2340
Epoch 12 — loss: 0.2278
Epoch 13 — loss: 0.2229
Epoch 14 — loss: 0.2170
Epoch 15 — loss: 0.2118
Epoch 16 — loss: 0.2054
Epoch 17 — loss: 0.2008
Epoch 18 — loss: 0.1953
Epoch 19 — loss: 0.1914
Epoch 20 — loss: 0.1845
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  81.089996
2    P(>V6)  83.930000
3    P(>V7)  87.919998
4    P(>V8)  92.589996
5    P(>V9)  96.680000
Overall accuracy: 48.51%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3587
Epoch 02 — loss: 0.3150
Epoch 03 — loss: 0.3022
Epoch 04 — loss: 0.2954
Epoch 05 — loss: 0.2873
Epoch 06 — loss: 0.2796
Epoch 07 — loss: 0.2740
Epoch 08 — loss: 0.2689
Epoch 09 — loss: 0.2611
Epoch 10 — loss: 0.2552
Epoch 11 — loss: 0.2505
Epoch 12 — loss: 0.2453
Epoch 13 — loss: 0.2401
Epoch 14 — loss: 0.2359
Epoch 15 — loss: 0.2322
Epoch 16 — loss: 0.2251
Epoch 17 — loss: 0.2211
Epoch 18 — loss: 0.2173
Epoch 19 — loss: 0.2121
Epoch 20 — loss: 0.2061
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  81.570000
2    P(>V6)  84.839996
3    P(>V7)  88.349998
4    P(>V8)  93.209999
5    P(>V9)  96.730003
Overall accuracy: 47.83%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4672
Epoch 02 — loss: 0.3572
Epoch 03 — loss: 0.3012
Epoch 04 — loss: 0.2923
Epoch 05 — loss: 0.2866
Epoch 06 — loss: 0.2826
Epoch 07 — loss: 0.2811
Epoch 08 — loss: 0.2781
Epoch 09 — loss: 0.2789
Epoch 10 — loss: 0.2753
Epoch 11 — loss: 0.2738
Epoch 12 — loss: 0.2753
Epoch 13 — loss: 0.2729
Epoch 14 — loss: 0.2720
Epoch 15 — loss: 0.2720
Epoch 16 — loss: 0.2702
Epoch 17 — loss: 0.2706
Epoch 18 — loss: 0.2714
Epoch 19 — loss: 0.2699
Epoch 20 — loss: 0.2693
  threshold   accuracy
0    P(>V4)  80.940002
1    P(>V5)  80.410004
2    P(>V6)  83.639999
3    P(>V7)  87.250000
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.54%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4719
Epoch 02 — loss: 0.3464
Epoch 03 — loss: 0.2973
Epoch 04 — loss: 0.2904
Epoch 05 — loss: 0.2831
Epoch 06 — loss: 0.2820
Epoch 07 — loss: 0.2790
Epoch 08 — loss: 0.2750
Epoch 09 — loss: 0.2754
Epoch 10 — loss: 0.2745
Epoch 11 — loss: 0.2736
Epoch 12 — loss: 0.2716
Epoch 13 — loss: 0.2739
Epoch 14 — loss: 0.2713
Epoch 15 — loss: 0.2728
Epoch 16 — loss: 0.2701
Epoch 17 — loss: 0.2696
Epoch 18 — loss: 0.2698
Epoch 19 — loss: 0.2695
Epoch 20 — loss: 0.2704
  threshold   accuracy
0    P(>V4)  81.139999
1    P(>V5)  80.610001
2    P(>V6)  83.589996
3    P(>V7)  87.779999
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 44.80%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4367
Epoch 02 — loss: 0.3274
Epoch 03 — loss: 0.3136
Epoch 04 — loss: 0.3045
Epoch 05 — loss: 0.2957
Epoch 06 — loss: 0.2905
Epoch 07 — loss: 0.2846
Epoch 08 — loss: 0.2845
Epoch 09 — loss: 0.2796
Epoch 10 — loss: 0.2806
Epoch 11 — loss: 0.2769
Epoch 12 — loss: 0.2766
Epoch 13 — loss: 0.2759
Epoch 14 — loss: 0.2754
Epoch 15 — loss: 0.2739
Epoch 16 — loss: 0.2729
Epoch 17 — loss: 0.2718
Epoch 18 — loss: 0.2728
Epoch 19 — loss: 0.2715
Epoch 20 — loss: 0.2707
  threshold   accuracy
0    P(>V4)  80.699997
1    P(>V5)  79.500000
2    P(>V6)  83.449997
3    P(>V7)  87.389999
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 46.20%
Ordinal stacking meta epoch 1: loss=0.2283
Ordinal stacking meta epoch 2: loss=0.1574
Ordinal stacking meta epoch 3: loss=0.1507
Ordinal stacking meta epoch 4: loss=0.1483
Ordinal stacking meta epoch 5: loss=0.1469
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001886 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.000000
2    P(>V6)  86.139999
3    P(>V7)  88.639999
4    P(>V8)  93.260002
5    P(>V9)  97.059998
Overall accuracy: 49.71%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.239998
2    P(>V6)  85.660004
3    P(>V7)  88.309998
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  82.769997
1    P(>V5)  81.040001
2    P(>V6)  85.370003
3    P(>V7)  88.400002
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 48.27%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.000000
2    P(>V6)  85.849998
3    P(>V7)  88.589996
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.440002
2    P(>V6)  86.089996
3    P(>V7)  89.120003
4    P(>V8)  93.169998
5    P(>V9)  96.870003
Overall accuracy: 49.33%/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [21:05:56] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.290001
2    P(>V6)  86.430000
3    P(>V7)  89.029999
4    P(>V8)  92.879997
5    P(>V9)  96.580002
Overall accuracy: 49.62%
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  82.000000
2    P(>V6)  85.709999
3    P(>V7)  88.160004
4    P(>V8)  92.730003
5    P(>V9)  96.730003
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.440002
2    P(>V6)  86.089996
3    P(>V7)  88.879997
4    P(>V8)  93.120003
5    P(>V9)  96.680000
Overall accuracy: 49.81%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.000000
2    P(>V6)  86.139999
3    P(>V7)  88.639999
4    P(>V8)  93.260002
5    P(>V9)  97.059998
Overall accuracy: 49.71%
----------------- Ordinal iteration 2/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3804
Epoch 02 — loss: 0.3183
Epoch 03 — loss: 0.3025
Epoch 04 — loss: 0.2906
Epoch 05 — loss: 0.2793
Epoch 06 — loss: 0.2724
Epoch 07 — loss: 0.2633
Epoch 08 — loss: 0.2517
Epoch 09 — loss: 0.2456
Epoch 10 — loss: 0.2387
Epoch 11 — loss: 0.2322
Epoch 12 — loss: 0.2262
Epoch 13 — loss: 0.2204
Epoch 14 — loss: 0.2152
Epoch 15 — loss: 0.2083
Epoch 16 — loss: 0.2031
Epoch 17 — loss: 0.1962
Epoch 18 — loss: 0.1922
Epoch 19 — loss: 0.1890
Epoch 20 — loss: 0.1810
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  81.379997
2    P(>V6)  84.309998
3    P(>V7)  87.150002
4    P(>V8)  91.480003
5    P(>V9)  95.480003
Overall accuracy: 46.73%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3738
Epoch 02 — loss: 0.3106
Epoch 03 — loss: 0.2969
Epoch 04 — loss: 0.2865
Epoch 05 — loss: 0.2791
Epoch 06 — loss: 0.2691
Epoch 07 — loss: 0.2638
Epoch 08 — loss: 0.2530
Epoch 09 — loss: 0.2466
Epoch 10 — loss: 0.2383
Epoch 11 — loss: 0.2342
Epoch 12 — loss: 0.2283
Epoch 13 — loss: 0.2230
Epoch 14 — loss: 0.2181
Epoch 15 — loss: 0.2119
Epoch 16 — loss: 0.2059
Epoch 17 — loss: 0.2016
Epoch 18 — loss: 0.1961
Epoch 19 — loss: 0.1923
Epoch 20 — loss: 0.1853
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  81.519997
2    P(>V6)  84.790001
3    P(>V7)  88.349998
4    P(>V8)  93.120003
5    P(>V9)  96.970001
Overall accuracy: 49.52%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3548
Epoch 02 — loss: 0.3109
Epoch 03 — loss: 0.3000
Epoch 04 — loss: 0.2910
Epoch 05 — loss: 0.2834
Epoch 06 — loss: 0.2776
Epoch 07 — loss: 0.2765
Epoch 08 — loss: 0.2698
Epoch 09 — loss: 0.2650
Epoch 10 — loss: 0.2592
Epoch 11 — loss: 0.2534
Epoch 12 — loss: 0.2474
Epoch 13 — loss: 0.2441
Epoch 14 — loss: 0.2371
Epoch 15 — loss: 0.2338
Epoch 16 — loss: 0.2281
Epoch 17 — loss: 0.2231
Epoch 18 — loss: 0.2181
Epoch 19 — loss: 0.2150
Epoch 20 — loss: 0.2098
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  83.010002
2    P(>V6)  85.029999
3    P(>V7)  88.790001
4    P(>V8)  93.410004
5    P(>V9)  96.870003
Overall accuracy: 49.71%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4778
Epoch 02 — loss: 0.3625
Epoch 03 — loss: 0.3005
Epoch 04 — loss: 0.2904
Epoch 05 — loss: 0.2846
Epoch 06 — loss: 0.2796
Epoch 07 — loss: 0.2772
Epoch 08 — loss: 0.2756
Epoch 09 — loss: 0.2745
Epoch 10 — loss: 0.2737
Epoch 11 — loss: 0.2716
Epoch 12 — loss: 0.2733
Epoch 13 — loss: 0.2713
Epoch 14 — loss: 0.2706
Epoch 15 — loss: 0.2708
Epoch 16 — loss: 0.2702
Epoch 17 — loss: 0.2694
Epoch 18 — loss: 0.2684
Epoch 19 — loss: 0.2706
Epoch 20 — loss: 0.2689
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.989998
2    P(>V6)  83.879997
3    P(>V7)  87.820000
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.05%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4653
Epoch 02 — loss: 0.3272
Epoch 03 — loss: 0.2942
Epoch 04 — loss: 0.2875
Epoch 05 — loss: 0.2823
Epoch 06 — loss: 0.2807
Epoch 07 — loss: 0.2770
Epoch 08 — loss: 0.2763
Epoch 09 — loss: 0.2746
Epoch 10 — loss: 0.2729
Epoch 11 — loss: 0.2747
Epoch 12 — loss: 0.2731
Epoch 13 — loss: 0.2707
Epoch 14 — loss: 0.2697
Epoch 15 — loss: 0.2708
Epoch 16 — loss: 0.2711
Epoch 17 — loss: 0.2709
Epoch 18 — loss: 0.2685
Epoch 19 — loss: 0.2691
Epoch 20 — loss: 0.2689
  threshold   accuracy
0    P(>V4)  82.000000
1    P(>V5)  80.849998
2    P(>V6)  83.930000
3    P(>V7)  87.489998
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.91%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4295
Epoch 02 — loss: 0.3222
Epoch 03 — loss: 0.3048
Epoch 04 — loss: 0.2974
Epoch 05 — loss: 0.2922
Epoch 06 — loss: 0.2887
Epoch 07 — loss: 0.2832
Epoch 08 — loss: 0.2817
Epoch 09 — loss: 0.2802
Epoch 10 — loss: 0.2789
Epoch 11 — loss: 0.2760
Epoch 12 — loss: 0.2756
Epoch 13 — loss: 0.2753
Epoch 14 — loss: 0.2747
Epoch 15 — loss: 0.2735
Epoch 16 — loss: 0.2733
Epoch 17 — loss: 0.2722
Epoch 18 — loss: 0.2713
Epoch 19 — loss: 0.2719
Epoch 20 — loss: 0.2704
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  80.370003
2    P(>V6)  83.779999
3    P(>V7)  87.150002
4    P(>V8)  92.489998
5    P(>V9)  97.019997
Overall accuracy: 45.28%
Ordinal stacking meta epoch 1: loss=0.2492
Ordinal stacking meta epoch 2: loss=0.1590
Ordinal stacking meta epoch 3: loss=0.1537
Ordinal stacking meta epoch 4: loss=0.1513
Ordinal stacking meta epoch 5: loss=0.1496
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001770 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  83.010002
2    P(>V6)  86.000000
3    P(>V7)  89.360001
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 50.29%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  83.250000
2    P(>V6)  86.480003
3    P(>V7)  88.980003
4    P(>V8)  93.169998
5    P(>V9)  97.019997
Overall accuracy: 50.72%
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  82.339996
2    P(>V6)  85.320000
3    P(>V7)  88.790001
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  83.059998
2    P(>V6)  85.660004
3    P(>V7)  88.839996
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 49.86%
  threshold   accuracy
0    P(>V4)  84.989998
1    P(>V5)  82.099998
2    P(>V6)  85.800003
3    P(>V7)  88.639999
4    P(>V8)  92.830002
5    P(>V9)  96.580002
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  85.419998
1    P(>V5)  82.580002
2    P(>V6)  86.239998
3    P(>V7)  88.879997
4    P(>V8)  92.540001
5    P(>V9)  96.250000
Overall accuracy: 49.52%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.290001
2    P(>V6)  85.610001
3    P(>V7)  88.639999
4    P(>V8)  92.540001
5    P(>V9)  96.339996
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  85.029999
1    P(>V5)  81.949997
2    P(>V6)  86.239998
3    P(>V7)  88.790001
4    P(>V8)  93.260002
5    P(>V9)  96.440002
Overall accuracy: 50.14%
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  83.010002
2    P(>V6)  86.000000
3    P(>V7)  89.360001
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 50.29%
----------------- Ordinal iteration 3/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3816
Epoch 02 — loss: 0.3220
Epoch 03 — loss: 0.3054
Epoch 04 — loss: 0.2950
Epoch 05 — loss: 0.2840
Epoch 06 — loss: 0.2772/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [21:29:01] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 07 — loss: 0.2667
Epoch 08 — loss: 0.2583
Epoch 09 — loss: 0.2511
Epoch 10 — loss: 0.2430
Epoch 11 — loss: 0.2366
Epoch 12 — loss: 0.2307
Epoch 13 — loss: 0.2252
Epoch 14 — loss: 0.2155
Epoch 15 — loss: 0.2122
Epoch 16 — loss: 0.2049
Epoch 17 — loss: 0.2002
Epoch 18 — loss: 0.1939
Epoch 19 — loss: 0.1877
Epoch 20 — loss: 0.1822
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  81.379997
2    P(>V6)  84.839996
3    P(>V7)  87.970001
4    P(>V8)  92.400002
5    P(>V9)  96.389999
Overall accuracy: 47.74%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3655
Epoch 02 — loss: 0.3083
Epoch 03 — loss: 0.2961
Epoch 04 — loss: 0.2886
Epoch 05 — loss: 0.2832
Epoch 06 — loss: 0.2731
Epoch 07 — loss: 0.2708
Epoch 08 — loss: 0.2639
Epoch 09 — loss: 0.2575
Epoch 10 — loss: 0.2513
Epoch 11 — loss: 0.2442
Epoch 12 — loss: 0.2387
Epoch 13 — loss: 0.2349
Epoch 14 — loss: 0.2277
Epoch 15 — loss: 0.2206
Epoch 16 — loss: 0.2166
Epoch 17 — loss: 0.2105
Epoch 18 — loss: 0.2054
Epoch 19 — loss: 0.1989
Epoch 20 — loss: 0.1947
  threshold   accuracy
0    P(>V4)  82.529999
1    P(>V5)  81.139999
2    P(>V6)  84.500000
3    P(>V7)  87.580002
4    P(>V8)  92.059998
5    P(>V9)  96.730003
Overall accuracy: 46.49%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3634
Epoch 02 — loss: 0.3117
Epoch 03 — loss: 0.3020
Epoch 04 — loss: 0.2929
Epoch 05 — loss: 0.2908
Epoch 06 — loss: 0.2880
Epoch 07 — loss: 0.2801
Epoch 08 — loss: 0.2779
Epoch 09 — loss: 0.2719
Epoch 10 — loss: 0.2650
Epoch 11 — loss: 0.2605
Epoch 12 — loss: 0.2567
Epoch 13 — loss: 0.2492
Epoch 14 — loss: 0.2437
Epoch 15 — loss: 0.2378
Epoch 16 — loss: 0.2359
Epoch 17 — loss: 0.2296
Epoch 18 — loss: 0.2230
Epoch 19 — loss: 0.2199
Epoch 20 — loss: 0.2152
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.680000
2    P(>V6)  84.940002
3    P(>V7)  87.580002
4    P(>V8)  92.349998
5    P(>V9)  96.820000
Overall accuracy: 47.74%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4716
Epoch 02 — loss: 0.3499
Epoch 03 — loss: 0.3036
Epoch 04 — loss: 0.2910
Epoch 05 — loss: 0.2860
Epoch 06 — loss: 0.2803
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2748
Epoch 09 — loss: 0.2738
Epoch 10 — loss: 0.2729
Epoch 11 — loss: 0.2731
Epoch 12 — loss: 0.2701
Epoch 13 — loss: 0.2712
Epoch 14 — loss: 0.2710
Epoch 15 — loss: 0.2691
Epoch 16 — loss: 0.2696
Epoch 17 — loss: 0.2707
Epoch 18 — loss: 0.2677
Epoch 19 — loss: 0.2693
Epoch 20 — loss: 0.2686
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.800003
2    P(>V6)  83.879997
3    P(>V7)  87.730003
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 44.51%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4617
Epoch 02 — loss: 0.3241
Epoch 03 — loss: 0.2980
Epoch 04 — loss: 0.2888
Epoch 05 — loss: 0.2835
Epoch 06 — loss: 0.2820
Epoch 07 — loss: 0.2782
Epoch 08 — loss: 0.2778
Epoch 09 — loss: 0.2750
Epoch 10 — loss: 0.2755
Epoch 11 — loss: 0.2733
Epoch 12 — loss: 0.2742
Epoch 13 — loss: 0.2727
Epoch 14 — loss: 0.2701
Epoch 15 — loss: 0.2717
Epoch 16 — loss: 0.2731
Epoch 17 — loss: 0.2685
Epoch 18 — loss: 0.2714
Epoch 19 — loss: 0.2703
Epoch 20 — loss: 0.2706
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.650002
2    P(>V6)  83.730003
3    P(>V7)  88.019997
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.29%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4484
Epoch 02 — loss: 0.3246
Epoch 03 — loss: 0.3079
Epoch 04 — loss: 0.2979
Epoch 05 — loss: 0.2928
Epoch 06 — loss: 0.2891
Epoch 07 — loss: 0.2833
Epoch 08 — loss: 0.2820
Epoch 09 — loss: 0.2816
Epoch 10 — loss: 0.2786
Epoch 11 — loss: 0.2773
Epoch 12 — loss: 0.2752
Epoch 13 — loss: 0.2761
Epoch 14 — loss: 0.2767
Epoch 15 — loss: 0.2742
Epoch 16 — loss: 0.2747
Epoch 17 — loss: 0.2735
Epoch 18 — loss: 0.2740
Epoch 19 — loss: 0.2720
Epoch 20 — loss: 0.2718
  threshold   accuracy
0    P(>V4)  80.989998
1    P(>V5)  80.320000
2    P(>V6)  84.260002
3    P(>V7)  87.440002
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 46.05%
Ordinal stacking meta epoch 1: loss=0.2158
Ordinal stacking meta epoch 2: loss=0.1569
Ordinal stacking meta epoch 3: loss=0.1509
Ordinal stacking meta epoch 4: loss=0.1483
Ordinal stacking meta epoch 5: loss=0.1465
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001673 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.769997
2    P(>V6)  85.800003
3    P(>V7)  88.690002
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.629997
2    P(>V6)  85.900002
3    P(>V7)  88.550003
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 49.66%
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  81.709999
2    P(>V6)  84.650002
3    P(>V7)  88.070000
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 48.08%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.480003
2    P(>V6)  85.419998
3    P(>V7)  88.690002
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.480003
2    P(>V6)  85.709999
3    P(>V7)  88.349998
4    P(>V8)  92.779999
5    P(>V9)  96.970001
Overall accuracy: 47.93%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  81.709999
2    P(>V6)  85.709999
3    P(>V7)  88.550003
4    P(>V8)  92.589996
5    P(>V9)  96.339996
Overall accuracy: 48.17%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.050003
2    P(>V6)  84.989998
3    P(>V7)  88.639999
4    P(>V8)  92.779999
5    P(>V9)  96.489998
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  82.820000
1    P(>V5)  81.809998
2    P(>V6)  85.470001
3    P(>V7)  88.160004
4    P(>V8)  92.160004
5    P(>V9)  96.440002
Overall accuracy: 47.64%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.769997
2    P(>V6)  85.800003
3    P(>V7)  88.690002
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 49.47%
----------------- Ordinal iteration 4/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3739
Epoch 02 — loss: 0.3137
Epoch 03 — loss: 0.3003
Epoch 04 — loss: 0.2910
Epoch 05 — loss: 0.2823
Epoch 06 — loss: 0.2701
Epoch 07 — loss: 0.2643
Epoch 08 — loss: 0.2580
Epoch 09 — loss: 0.2506
Epoch 10 — loss: 0.2455
Epoch 11 — loss: 0.2374
Epoch 12 — loss: 0.2330
Epoch 13 — loss: 0.2258
Epoch 14 — loss: 0.2215
Epoch 15 — loss: 0.2162
Epoch 16 — loss: 0.2104
Epoch 17 — loss: 0.2041
Epoch 18 — loss: 0.1994
Epoch 19 — loss: 0.1954
Epoch 20 — loss: 0.1901
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  80.699997
2    P(>V6)  84.739998
3    P(>V7)  88.260002
4    P(>V8)  92.489998
5    P(>V9)  96.820000
Overall accuracy: 48.51%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3668
Epoch 02 — loss: 0.3059
Epoch 03 — loss: 0.2932
Epoch 04 — loss: 0.2858
Epoch 05 — loss: 0.2772
Epoch 06 — loss: 0.2682
Epoch 07 — loss: 0.2613
Epoch 08 — loss: 0.2549
Epoch 09 — loss: 0.2473
Epoch 10 — loss: 0.2402
Epoch 11 — loss: 0.2362
Epoch 12 — loss: 0.2281
Epoch 13 — loss: 0.2229
Epoch 14 — loss: 0.2186
Epoch 15 — loss: 0.2122
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [21:51:31] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch 16 — loss: 0.2080
Epoch 17 — loss: 0.2017
Epoch 18 — loss: 0.1961
Epoch 19 — loss: 0.1906
Epoch 20 — loss: 0.1871
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  81.910004
2    P(>V6)  84.739998
3    P(>V7)  88.070000
4    P(>V8)  92.879997
5    P(>V9)  96.680000
Overall accuracy: 49.52%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3632
Epoch 02 — loss: 0.3106
Epoch 03 — loss: 0.3017
Epoch 04 — loss: 0.2928
Epoch 05 — loss: 0.2854
Epoch 06 — loss: 0.2806
Epoch 07 — loss: 0.2770
Epoch 08 — loss: 0.2702
Epoch 09 — loss: 0.2654
Epoch 10 — loss: 0.2594
Epoch 11 — loss: 0.2545
Epoch 12 — loss: 0.2502
Epoch 13 — loss: 0.2445
Epoch 14 — loss: 0.2392
Epoch 15 — loss: 0.2335
Epoch 16 — loss: 0.2316
Epoch 17 — loss: 0.2244
Epoch 18 — loss: 0.2197
Epoch 19 — loss: 0.2133
Epoch 20 — loss: 0.2080
  threshold   accuracy
0    P(>V4)  82.339996
1    P(>V5)  80.320000
2    P(>V6)  84.120003
3    P(>V7)  88.209999
4    P(>V8)  91.870003
5    P(>V9)  96.779999
Overall accuracy: 45.43%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4734
Epoch 02 — loss: 0.3480
Epoch 03 — loss: 0.3062
Epoch 04 — loss: 0.2939
Epoch 05 — loss: 0.2864
Epoch 06 — loss: 0.2821
Epoch 07 — loss: 0.2797
Epoch 08 — loss: 0.2789
Epoch 09 — loss: 0.2742
Epoch 10 — loss: 0.2747
Epoch 11 — loss: 0.2730
Epoch 12 — loss: 0.2733
Epoch 13 — loss: 0.2727
Epoch 14 — loss: 0.2720
Epoch 15 — loss: 0.2701
Epoch 16 — loss: 0.2708
Epoch 17 — loss: 0.2705
Epoch 18 — loss: 0.2701
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2678
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.650002
2    P(>V6)  83.930000
3    P(>V7)  87.489998
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 44.95%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4672
Epoch 02 — loss: 0.3326
Epoch 03 — loss: 0.3013
Epoch 04 — loss: 0.2917
Epoch 05 — loss: 0.2881
Epoch 06 — loss: 0.2843
Epoch 07 — loss: 0.2810
Epoch 08 — loss: 0.2775
Epoch 09 — loss: 0.2782
Epoch 10 — loss: 0.2759
Epoch 11 — loss: 0.2744
Epoch 12 — loss: 0.2739
Epoch 13 — loss: 0.2726
Epoch 14 — loss: 0.2723
Epoch 15 — loss: 0.2723
Epoch 16 — loss: 0.2716
Epoch 17 — loss: 0.2716
Epoch 18 — loss: 0.2714
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2699
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  81.230003
2    P(>V6)  83.970001
3    P(>V7)  87.919998
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.10%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4231
Epoch 02 — loss: 0.3323
Epoch 03 — loss: 0.3158
Epoch 04 — loss: 0.3035
Epoch 05 — loss: 0.2946
Epoch 06 — loss: 0.2882
Epoch 07 — loss: 0.2877
Epoch 08 — loss: 0.2860
Epoch 09 — loss: 0.2819
Epoch 10 — loss: 0.2806
Epoch 11 — loss: 0.2778
Epoch 12 — loss: 0.2783
Epoch 13 — loss: 0.2762
Epoch 14 — loss: 0.2745
Epoch 15 — loss: 0.2740
Epoch 16 — loss: 0.2743
Epoch 17 — loss: 0.2724
Epoch 18 — loss: 0.2722
Epoch 19 — loss: 0.2744
Epoch 20 — loss: 0.2734
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.699997
2    P(>V6)  83.779999
3    P(>V7)  88.070000
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 46.05%
Ordinal stacking meta epoch 1: loss=0.1850
Ordinal stacking meta epoch 2: loss=0.1525
Ordinal stacking meta epoch 3: loss=0.1486
Ordinal stacking meta epoch 4: loss=0.1468
Ordinal stacking meta epoch 5: loss=0.1456
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001847 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.339996
2    P(>V6)  86.089996
3    P(>V7)  88.739998
4    P(>V8)  93.169998
5    P(>V9)  97.110001
Overall accuracy: 49.42%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.720001
2    P(>V6)  85.900002
3    P(>V7)  88.879997
4    P(>V8)  93.019997
5    P(>V9)  97.059998
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  82.720001
1    P(>V5)  81.709999
2    P(>V6)  85.029999
3    P(>V7)  88.500000
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 48.08%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.389999
2    P(>V6)  85.559998
3    P(>V7)  88.589996
4    P(>V8)  92.930000
5    P(>V9)  97.110001
Overall accuracy: 48.89%
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  82.680000
2    P(>V6)  85.900002
3    P(>V7)  88.839996
4    P(>V8)  92.730003
5    P(>V9)  96.730003
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  83.690002
2    P(>V6)  86.190002
3    P(>V7)  88.160004
4    P(>V8)  92.639999
5    P(>V9)  96.540001
Overall accuracy: 49.90%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  83.010002
2    P(>V6)  86.139999
3    P(>V7)  88.260002
4    P(>V8)  93.169998
5    P(>V9)  96.489998
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  83.250000
2    P(>V6)  86.669998
3    P(>V7)  88.349998
4    P(>V8)  93.209999
5    P(>V9)  96.580002
Overall accuracy: 50.53%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.339996
2    P(>V6)  86.089996
3    P(>V7)  88.739998
4    P(>V8)  93.169998
5    P(>V9)  97.110001
Overall accuracy: 49.42%
----------------- Ordinal iteration 5/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3678
Epoch 02 — loss: 0.3119
Epoch 03 — loss: 0.2970
Epoch 04 — loss: 0.2861
Epoch 05 — loss: 0.2735
Epoch 06 — loss: 0.2639
Epoch 07 — loss: 0.2572
Epoch 08 — loss: 0.2494
Epoch 09 — loss: 0.2431
Epoch 10 — loss: 0.2370
Epoch 11 — loss: 0.2303
Epoch 12 — loss: 0.2232
Epoch 13 — loss: 0.2189
Epoch 14 — loss: 0.2122
Epoch 15 — loss: 0.2061
Epoch 16 — loss: 0.1987
Epoch 17 — loss: 0.1946
Epoch 18 — loss: 0.1906
Epoch 19 — loss: 0.1829
Epoch 20 — loss: 0.1785
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  81.860001
2    P(>V6)  84.309998
3    P(>V7)  87.970001
4    P(>V8)  92.690002
5    P(>V9)  96.680000
Overall accuracy: 48.32%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3715
Epoch 02 — loss: 0.3160
Epoch 03 — loss: 0.3009
Epoch 04 — loss: 0.2921
Epoch 05 — loss: 0.2806
Epoch 06 — loss: 0.2712
Epoch 07 — loss: 0.2619
Epoch 08 — loss: 0.2534
Epoch 09 — loss: 0.2462
Epoch 10 — loss: 0.2400
Epoch 11 — loss: 0.2335
Epoch 12 — loss: 0.2265
Epoch 13 — loss: 0.2216
Epoch 14 — loss: 0.2154
Epoch 15 — loss: 0.2107
Epoch 16 — loss: 0.2011
Epoch 17 — loss: 0.1991
Epoch 18 — loss: 0.1919
Epoch 19 — loss: 0.1880
Epoch 20 — loss: 0.1820
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  81.910004
2    P(>V6)  84.550003
3    P(>V7)  87.099998
4    P(>V8)  92.690002
5    P(>V9)  96.970001
Overall accuracy: 48.80%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3601
Epoch 02 — loss: 0.3110
Epoch 03 — loss: 0.3027
Epoch 04 — loss: 0.2952
Epoch 05 — loss: 0.2887
Epoch 06 — loss: 0.2821
Epoch 07 — loss: 0.2750
Epoch 08 — loss: 0.2680
Epoch 09 — loss: 0.2624
Epoch 10 — loss: 0.2552
Epoch 11 — loss: 0.2509
Epoch 12 — loss: 0.2447
Epoch 13 — loss: 0.2378
Epoch 14 — loss: 0.2335
Epoch 15 — loss: 0.2307
Epoch 16 — loss: 0.2236
Epoch 17 — loss: 0.2190
Epoch 18 — loss: 0.2161
Epoch 19 — loss: 0.2114
Epoch 20 — loss: 0.2048
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  83.540001
2    P(>V6)  85.320000
3    P(>V7)  87.970001
4    P(>V8)  91.959999
5    P(>V9)  96.779999/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [22:14:54] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 49.71%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4646
Epoch 02 — loss: 0.3321
Epoch 03 — loss: 0.2993
Epoch 04 — loss: 0.2888
Epoch 05 — loss: 0.2838
Epoch 06 — loss: 0.2807
Epoch 07 — loss: 0.2780
Epoch 08 — loss: 0.2764
Epoch 09 — loss: 0.2756
Epoch 10 — loss: 0.2735
Epoch 11 — loss: 0.2737
Epoch 12 — loss: 0.2720
Epoch 13 — loss: 0.2719
Epoch 14 — loss: 0.2717
Epoch 15 — loss: 0.2710
Epoch 16 — loss: 0.2712
Epoch 17 — loss: 0.2707
Epoch 18 — loss: 0.2701
Epoch 19 — loss: 0.2703
Epoch 20 — loss: 0.2692
  threshold   accuracy
0    P(>V4)  81.279999
1    P(>V5)  80.940002
2    P(>V6)  83.639999
3    P(>V7)  87.680000
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 45.09%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4612
Epoch 02 — loss: 0.3324
Epoch 03 — loss: 0.3013
Epoch 04 — loss: 0.2922
Epoch 05 — loss: 0.2851
Epoch 06 — loss: 0.2828
Epoch 07 — loss: 0.2798
Epoch 08 — loss: 0.2768
Epoch 09 — loss: 0.2765
Epoch 10 — loss: 0.2734
Epoch 11 — loss: 0.2729
Epoch 12 — loss: 0.2722
Epoch 13 — loss: 0.2719
Epoch 14 — loss: 0.2723
Epoch 15 — loss: 0.2702
Epoch 16 — loss: 0.2700
Epoch 17 — loss: 0.2703
Epoch 18 — loss: 0.2690
Epoch 19 — loss: 0.2684
Epoch 20 — loss: 0.2687
  threshold   accuracy
0    P(>V4)  80.800003
1    P(>V5)  80.800003
2    P(>V6)  83.830002
3    P(>V7)  87.680000
4    P(>V8)  92.730003
5    P(>V9)  97.059998
Overall accuracy: 46.29%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4285
Epoch 02 — loss: 0.3318
Epoch 03 — loss: 0.3137
Epoch 04 — loss: 0.3029
Epoch 05 — loss: 0.2934
Epoch 06 — loss: 0.2900
Epoch 07 — loss: 0.2862
Epoch 08 — loss: 0.2848
Epoch 09 — loss: 0.2825
Epoch 10 — loss: 0.2800
Epoch 11 — loss: 0.2779
Epoch 12 — loss: 0.2793
Epoch 13 — loss: 0.2760
Epoch 14 — loss: 0.2755
Epoch 15 — loss: 0.2745
Epoch 16 — loss: 0.2742
Epoch 17 — loss: 0.2729
Epoch 18 — loss: 0.2740
Epoch 19 — loss: 0.2734
Epoch 20 — loss: 0.2721
  threshold   accuracy
0    P(>V4)  81.910004
1    P(>V5)  80.989998
2    P(>V6)  83.690002
3    P(>V7)  87.440002
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 46.10%
Ordinal stacking meta epoch 1: loss=0.2131
Ordinal stacking meta epoch 2: loss=0.1523
Ordinal stacking meta epoch 3: loss=0.1458
Ordinal stacking meta epoch 4: loss=0.1428
Ordinal stacking meta epoch 5: loss=0.1409
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001564 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  83.199997
2    P(>V6)  85.510002
3    P(>V7)  88.690002
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 50.19%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  83.160004
2    P(>V6)  85.760002
3    P(>V7)  88.260002
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 50.10%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.389999
2    P(>V6)  84.599998
3    P(>V7)  88.070000
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 48.80%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.720001
2    P(>V6)  85.559998
3    P(>V7)  88.500000
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  83.639999
2    P(>V6)  85.559998
3    P(>V7)  88.449997
4    P(>V8)  93.360001
5    P(>V9)  96.680000
Overall accuracy: 49.86%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  83.540001
2    P(>V6)  85.760002
3    P(>V7)  88.400002
4    P(>V8)  92.930000
5    P(>V9)  96.680000
Overall accuracy: 50.58%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  83.400002
2    P(>V6)  85.129997
3    P(>V7)  88.260002
4    P(>V8)  93.070000
5    P(>V9)  96.440002
Overall accuracy: 49.90%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.959999
2    P(>V6)  85.800003
3    P(>V7)  88.160004
4    P(>V8)  92.970001
5    P(>V9)  96.440002
Overall accuracy: 49.66%
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  83.199997
2    P(>V6)  85.510002
3    P(>V7)  88.690002
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 50.19%
----------------- Ordinal iteration 6/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3702
Epoch 02 — loss: 0.3152
Epoch 03 — loss: 0.3002
Epoch 04 — loss: 0.2905
Epoch 05 — loss: 0.2827
Epoch 06 — loss: 0.2741
Epoch 07 — loss: 0.2643
Epoch 08 — loss: 0.2583
Epoch 09 — loss: 0.2514
Epoch 10 — loss: 0.2442
Epoch 11 — loss: 0.2375
Epoch 12 — loss: 0.2316
Epoch 13 — loss: 0.2261
Epoch 14 — loss: 0.2203
Epoch 15 — loss: 0.2139
Epoch 16 — loss: 0.2105
Epoch 17 — loss: 0.2028
Epoch 18 — loss: 0.1980
Epoch 19 — loss: 0.1911
Epoch 20 — loss: 0.1860
  threshold   accuracy
0    P(>V4)  82.529999
1    P(>V5)  81.330002
2    P(>V6)  84.599998
3    P(>V7)  87.629997
4    P(>V8)  92.540001
5    P(>V9)  96.440002
Overall accuracy: 47.59%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3650
Epoch 02 — loss: 0.3067
Epoch 03 — loss: 0.2948
Epoch 04 — loss: 0.2875
Epoch 05 — loss: 0.2803
Epoch 06 — loss: 0.2702
Epoch 07 — loss: 0.2627
Epoch 08 — loss: 0.2565
Epoch 09 — loss: 0.2487
Epoch 10 — loss: 0.2396
Epoch 11 — loss: 0.2357
Epoch 12 — loss: 0.2286
Epoch 13 — loss: 0.2219
Epoch 14 — loss: 0.2204
Epoch 15 — loss: 0.2107
Epoch 16 — loss: 0.2081
Epoch 17 — loss: 0.2037
Epoch 18 — loss: 0.1969
Epoch 19 — loss: 0.1918
Epoch 20 — loss: 0.1862
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  82.720001
2    P(>V6)  85.269997
3    P(>V7)  88.879997
4    P(>V8)  92.970001
5    P(>V9)  96.580002
Overall accuracy: 49.71%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3702
Epoch 02 — loss: 0.3145
Epoch 03 — loss: 0.3014
Epoch 04 — loss: 0.2956
Epoch 05 — loss: 0.2852
Epoch 06 — loss: 0.2831
Epoch 07 — loss: 0.2744
Epoch 08 — loss: 0.2709
Epoch 09 — loss: 0.2663
Epoch 10 — loss: 0.2611
Epoch 11 — loss: 0.2555
Epoch 12 — loss: 0.2515
Epoch 13 — loss: 0.2468
Epoch 14 — loss: 0.2439
Epoch 15 — loss: 0.2389
Epoch 16 — loss: 0.2329
Epoch 17 — loss: 0.2300
Epoch 18 — loss: 0.2245
Epoch 19 — loss: 0.2189
Epoch 20 — loss: 0.2135
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  81.470001
2    P(>V6)  85.129997
3    P(>V7)  87.489998
4    P(>V8)  92.400002
5    P(>V9)  97.059998
Overall accuracy: 48.12%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4595
Epoch 02 — loss: 0.3398
Epoch 03 — loss: 0.2986
Epoch 04 — loss: 0.2898
Epoch 05 — loss: 0.2828
Epoch 06 — loss: 0.2830
Epoch 07 — loss: 0.2778
Epoch 08 — loss: 0.2765
Epoch 09 — loss: 0.2755
Epoch 10 — loss: 0.2759
Epoch 11 — loss: 0.2734
Epoch 12 — loss: 0.2729
Epoch 13 — loss: 0.2722
Epoch 14 — loss: 0.2718
Epoch 15 — loss: 0.2712
Epoch 16 — loss: 0.2696
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2707
Epoch 19 — loss: 0.2692
Epoch 20 — loss: 0.2716
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.849998
2    P(>V6)  83.639999
3    P(>V7)  87.730003
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 45.04%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4678
Epoch 02 — loss: 0.3291
Epoch 03 — loss: 0.2949
Epoch 04 — loss: 0.2892
Epoch 05 — loss: 0.2839
Epoch 06 — loss: 0.2805
Epoch 07 — loss: 0.2789/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [22:38:08] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 08 — loss: 0.2770
Epoch 09 — loss: 0.2757
Epoch 10 — loss: 0.2734
Epoch 11 — loss: 0.2731
Epoch 12 — loss: 0.2726
Epoch 13 — loss: 0.2722
Epoch 14 — loss: 0.2716
Epoch 15 — loss: 0.2689
Epoch 16 — loss: 0.2690
Epoch 17 — loss: 0.2712
Epoch 18 — loss: 0.2695
Epoch 19 — loss: 0.2698
Epoch 20 — loss: 0.2688
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  81.040001
2    P(>V6)  83.879997
3    P(>V7)  87.779999
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 45.77%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4350
Epoch 02 — loss: 0.3246
Epoch 03 — loss: 0.3070
Epoch 04 — loss: 0.2980
Epoch 05 — loss: 0.2920
Epoch 06 — loss: 0.2873
Epoch 07 — loss: 0.2842
Epoch 08 — loss: 0.2803
Epoch 09 — loss: 0.2785
Epoch 10 — loss: 0.2777
Epoch 11 — loss: 0.2758
Epoch 12 — loss: 0.2778
Epoch 13 — loss: 0.2754
Epoch 14 — loss: 0.2751
Epoch 15 — loss: 0.2759
Epoch 16 — loss: 0.2730
Epoch 17 — loss: 0.2727
Epoch 18 — loss: 0.2723
Epoch 19 — loss: 0.2724
Epoch 20 — loss: 0.2707
  threshold   accuracy
0    P(>V4)  80.849998
1    P(>V5)  79.790001
2    P(>V6)  83.639999
3    P(>V7)  87.099998
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.38%
Ordinal stacking meta epoch 1: loss=0.2193
Ordinal stacking meta epoch 2: loss=0.1569
Ordinal stacking meta epoch 3: loss=0.1513
Ordinal stacking meta epoch 4: loss=0.1492
Ordinal stacking meta epoch 5: loss=0.1478
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001512 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.440002
2    P(>V6)  85.709999
3    P(>V7)  88.400002
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.290001
2    P(>V6)  85.029999
3    P(>V7)  88.449997
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  82.720001
1    P(>V5)  81.709999
2    P(>V6)  84.500000
3    P(>V7)  88.160004
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 47.98%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.529999
2    P(>V6)  85.610001
3    P(>V7)  88.110001
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.239998
2    P(>V6)  84.599998
3    P(>V7)  89.220001
4    P(>V8)  92.779999
5    P(>V9)  96.730003
Overall accuracy: 47.93%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  83.160004
2    P(>V6)  85.320000
3    P(>V7)  89.410004
4    P(>V8)  92.639999
5    P(>V9)  96.540001
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.389999
2    P(>V6)  84.650002
3    P(>V7)  88.790001
4    P(>V8)  92.879997
5    P(>V9)  96.540001
Overall accuracy: 48.22%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.769997
2    P(>V6)  84.699997
3    P(>V7)  88.739998
4    P(>V8)  92.690002
5    P(>V9)  96.540001
Overall accuracy: 48.60%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.440002
2    P(>V6)  85.709999
3    P(>V7)  88.400002
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 49.09%
----------------- Ordinal iteration 7/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3843
Epoch 02 — loss: 0.3175
Epoch 03 — loss: 0.3070
Epoch 04 — loss: 0.2958
Epoch 05 — loss: 0.2849
Epoch 06 — loss: 0.2777
Epoch 07 — loss: 0.2688
Epoch 08 — loss: 0.2603
Epoch 09 — loss: 0.2534
Epoch 10 — loss: 0.2484
Epoch 11 — loss: 0.2412
Epoch 12 — loss: 0.2351
Epoch 13 — loss: 0.2283
Epoch 14 — loss: 0.2243
Epoch 15 — loss: 0.2176
Epoch 16 — loss: 0.2107
Epoch 17 — loss: 0.2053
Epoch 18 — loss: 0.2017
Epoch 19 — loss: 0.1951
Epoch 20 — loss: 0.1915
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  81.709999
2    P(>V6)  84.790001
3    P(>V7)  87.870003
4    P(>V8)  92.400002
5    P(>V9)  96.870003
Overall accuracy: 48.99%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3711
Epoch 02 — loss: 0.3137
Epoch 03 — loss: 0.3017
Epoch 04 — loss: 0.2896
Epoch 05 — loss: 0.2821
Epoch 06 — loss: 0.2728
Epoch 07 — loss: 0.2684
Epoch 08 — loss: 0.2596
Epoch 09 — loss: 0.2535
Epoch 10 — loss: 0.2455
Epoch 11 — loss: 0.2389
Epoch 12 — loss: 0.2328
Epoch 13 — loss: 0.2257
Epoch 14 — loss: 0.2207
Epoch 15 — loss: 0.2147
Epoch 16 — loss: 0.2102
Epoch 17 — loss: 0.2050
Epoch 18 — loss: 0.2002
Epoch 19 — loss: 0.1922
Epoch 20 — loss: 0.1869
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  82.099998
2    P(>V6)  85.269997
3    P(>V7)  87.680000
4    P(>V8)  92.690002
5    P(>V9)  96.779999
Overall accuracy: 47.79%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3718
Epoch 02 — loss: 0.3191
Epoch 03 — loss: 0.3036
Epoch 04 — loss: 0.2978
Epoch 05 — loss: 0.2874
Epoch 06 — loss: 0.2830
Epoch 07 — loss: 0.2761
Epoch 08 — loss: 0.2674
Epoch 09 — loss: 0.2624
Epoch 10 — loss: 0.2582
Epoch 11 — loss: 0.2507
Epoch 12 — loss: 0.2440
Epoch 13 — loss: 0.2383
Epoch 14 — loss: 0.2327
Epoch 15 — loss: 0.2286
Epoch 16 — loss: 0.2223
Epoch 17 — loss: 0.2175
Epoch 18 — loss: 0.2153
Epoch 19 — loss: 0.2078
Epoch 20 — loss: 0.2043
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.919998
2    P(>V6)  86.000000
3    P(>V7)  88.400002
4    P(>V8)  92.779999
5    P(>V9)  96.970001
Overall accuracy: 49.23%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4647
Epoch 02 — loss: 0.3386
Epoch 03 — loss: 0.3029
Epoch 04 — loss: 0.2910
Epoch 05 — loss: 0.2850
Epoch 06 — loss: 0.2823
Epoch 07 — loss: 0.2776
Epoch 08 — loss: 0.2761
Epoch 09 — loss: 0.2756
Epoch 10 — loss: 0.2734
Epoch 11 — loss: 0.2739
Epoch 12 — loss: 0.2730
Epoch 13 — loss: 0.2716
Epoch 14 — loss: 0.2719
Epoch 15 — loss: 0.2704
Epoch 16 — loss: 0.2700
Epoch 17 — loss: 0.2690
Epoch 18 — loss: 0.2690
Epoch 19 — loss: 0.2711
Epoch 20 — loss: 0.2689
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  81.040001
2    P(>V6)  84.120003
3    P(>V7)  87.820000
4    P(>V8)  92.730003
5    P(>V9)  96.970001
Overall accuracy: 45.86%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4616
Epoch 02 — loss: 0.3286
Epoch 03 — loss: 0.2989
Epoch 04 — loss: 0.2913
Epoch 05 — loss: 0.2857
Epoch 06 — loss: 0.2804
Epoch 07 — loss: 0.2792
Epoch 08 — loss: 0.2785
Epoch 09 — loss: 0.2758
Epoch 10 — loss: 0.2758
Epoch 11 — loss: 0.2745
Epoch 12 — loss: 0.2741
Epoch 13 — loss: 0.2707
Epoch 14 — loss: 0.2720
Epoch 15 — loss: 0.2706
Epoch 16 — loss: 0.2714
Epoch 17 — loss: 0.2708
Epoch 18 — loss: 0.2713
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2691
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  80.849998
2    P(>V6)  84.260002
3    P(>V7)  87.919998
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 45.72%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4263
Epoch 02 — loss: 0.3314
Epoch 03 — loss: 0.3159
Epoch 04 — loss: 0.3044
Epoch 05 — loss: 0.2943
Epoch 06 — loss: 0.2884
Epoch 07 — loss: 0.2848
Epoch 08 — loss: 0.2838
Epoch 09 — loss: 0.2783
Epoch 10 — loss: 0.2792
Epoch 11 — loss: 0.2760
Epoch 12 — loss: 0.2770
Epoch 13 — loss: 0.2747
Epoch 14 — loss: 0.2744
Epoch 15 — loss: 0.2740
Epoch 16 — loss: 0.2722/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [23:01:27] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [23:24:50] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 17 — loss: 0.2723
Epoch 18 — loss: 0.2724
Epoch 19 — loss: 0.2728
Epoch 20 — loss: 0.2724
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.900002
2    P(>V6)  83.730003
3    P(>V7)  87.580002
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 45.96%
Ordinal stacking meta epoch 1: loss=0.2007
Ordinal stacking meta epoch 2: loss=0.1541
Ordinal stacking meta epoch 3: loss=0.1489
Ordinal stacking meta epoch 4: loss=0.1468
Ordinal stacking meta epoch 5: loss=0.1456
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001540 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.680000
2    P(>V6)  85.559998
3    P(>V7)  89.510002
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 49.52%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  83.010002
2    P(>V6)  86.089996
3    P(>V7)  88.690002
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 49.71%
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  82.339996
2    P(>V6)  85.029999
3    P(>V7)  88.639999
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.580002
2    P(>V6)  85.370003
3    P(>V7)  88.930000
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 49.42%
  threshold   accuracy
0    P(>V4)  84.599998
1    P(>V5)  83.250000
2    P(>V6)  85.949997
3    P(>V7)  88.739998
4    P(>V8)  92.879997
5    P(>V9)  96.870003
Overall accuracy: 48.41%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.720001
2    P(>V6)  86.040001
3    P(>V7)  88.739998
4    P(>V8)  92.160004
5    P(>V9)  96.540001
Overall accuracy: 48.32%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.820000
2    P(>V6)  86.000000
3    P(>V7)  88.839996
4    P(>V8)  92.830002
5    P(>V9)  96.919998
Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  84.599998
1    P(>V5)  83.400002
2    P(>V6)  85.419998
3    P(>V7)  88.879997
4    P(>V8)  92.440002
5    P(>V9)  96.779999
Overall accuracy: 49.81%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.680000
2    P(>V6)  85.559998
3    P(>V7)  89.510002
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 49.52%
----------------- Ordinal iteration 8/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3764
Epoch 02 — loss: 0.3164
Epoch 03 — loss: 0.3031
Epoch 04 — loss: 0.2914
Epoch 05 — loss: 0.2831
Epoch 06 — loss: 0.2729
Epoch 07 — loss: 0.2643
Epoch 08 — loss: 0.2588
Epoch 09 — loss: 0.2514
Epoch 10 — loss: 0.2459
Epoch 11 — loss: 0.2404
Epoch 12 — loss: 0.2354
Epoch 13 — loss: 0.2281
Epoch 14 — loss: 0.2231
Epoch 15 — loss: 0.2181
Epoch 16 — loss: 0.2113
Epoch 17 — loss: 0.2109
Epoch 18 — loss: 0.2036
Epoch 19 — loss: 0.1981
Epoch 20 — loss: 0.1927
  threshold   accuracy
0    P(>V4)  82.239998
1    P(>V5)  81.709999
2    P(>V6)  83.540001
3    P(>V7)  87.730003
4    P(>V8)  92.300003
5    P(>V9)  96.680000
Overall accuracy: 45.48%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3661
Epoch 02 — loss: 0.3118
Epoch 03 — loss: 0.2992
Epoch 04 — loss: 0.2874
Epoch 05 — loss: 0.2783
Epoch 06 — loss: 0.2710
Epoch 07 — loss: 0.2627
Epoch 08 — loss: 0.2581
Epoch 09 — loss: 0.2489
Epoch 10 — loss: 0.2428
Epoch 11 — loss: 0.2387
Epoch 12 — loss: 0.2309
Epoch 13 — loss: 0.2266
Epoch 14 — loss: 0.2199
Epoch 15 — loss: 0.2146
Epoch 16 — loss: 0.2088
Epoch 17 — loss: 0.2021
Epoch 18 — loss: 0.1994
Epoch 19 — loss: 0.1927
Epoch 20 — loss: 0.1883
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.389999
2    P(>V6)  85.269997
3    P(>V7)  87.870003
4    P(>V8)  92.440002
5    P(>V9)  96.870003
Overall accuracy: 49.09%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3584
Epoch 02 — loss: 0.3163
Epoch 03 — loss: 0.3046
Epoch 04 — loss: 0.2887
Epoch 05 — loss: 0.2840
Epoch 06 — loss: 0.2780
Epoch 07 — loss: 0.2716
Epoch 08 — loss: 0.2653
Epoch 09 — loss: 0.2601
Epoch 10 — loss: 0.2558
Epoch 11 — loss: 0.2488
Epoch 12 — loss: 0.2439
Epoch 13 — loss: 0.2404
Epoch 14 — loss: 0.2353
Epoch 15 — loss: 0.2304
Epoch 16 — loss: 0.2259
Epoch 17 — loss: 0.2229
Epoch 18 — loss: 0.2172
Epoch 19 — loss: 0.2127
Epoch 20 — loss: 0.2079
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.480003
2    P(>V6)  85.660004
3    P(>V7)  87.629997
4    P(>V8)  92.349998
5    P(>V9)  96.440002
Overall accuracy: 49.18%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4693
Epoch 02 — loss: 0.3388
Epoch 03 — loss: 0.3039
Epoch 04 — loss: 0.2925
Epoch 05 — loss: 0.2868
Epoch 06 — loss: 0.2826
Epoch 07 — loss: 0.2784
Epoch 08 — loss: 0.2773
Epoch 09 — loss: 0.2755
Epoch 10 — loss: 0.2748
Epoch 11 — loss: 0.2726
Epoch 12 — loss: 0.2720
Epoch 13 — loss: 0.2740
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2727
Epoch 16 — loss: 0.2706
Epoch 17 — loss: 0.2707
Epoch 18 — loss: 0.2685
Epoch 19 — loss: 0.2683
Epoch 20 — loss: 0.2707
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.459999
2    P(>V6)  84.019997
3    P(>V7)  88.070000
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.54%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4685
Epoch 02 — loss: 0.3355
Epoch 03 — loss: 0.2995
Epoch 04 — loss: 0.2886
Epoch 05 — loss: 0.2841
Epoch 06 — loss: 0.2804
Epoch 07 — loss: 0.2772
Epoch 08 — loss: 0.2759
Epoch 09 — loss: 0.2753
Epoch 10 — loss: 0.2735
Epoch 11 — loss: 0.2728
Epoch 12 — loss: 0.2740
Epoch 13 — loss: 0.2716
Epoch 14 — loss: 0.2717
Epoch 15 — loss: 0.2713
Epoch 16 — loss: 0.2701
Epoch 17 — loss: 0.2709
Epoch 18 — loss: 0.2700
Epoch 19 — loss: 0.2699
Epoch 20 — loss: 0.2690
  threshold   accuracy
0    P(>V4)  82.000000
1    P(>V5)  81.180000
2    P(>V6)  84.260002
3    P(>V7)  87.779999
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 46.92%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4294
Epoch 02 — loss: 0.3348
Epoch 03 — loss: 0.3162
Epoch 04 — loss: 0.3007
Epoch 05 — loss: 0.2960
Epoch 06 — loss: 0.2903
Epoch 07 — loss: 0.2871
Epoch 08 — loss: 0.2839
Epoch 09 — loss: 0.2828
Epoch 10 — loss: 0.2812
Epoch 11 — loss: 0.2788
Epoch 12 — loss: 0.2811
Epoch 13 — loss: 0.2789
Epoch 14 — loss: 0.2757
Epoch 15 — loss: 0.2754
Epoch 16 — loss: 0.2735
Epoch 17 — loss: 0.2741
Epoch 18 — loss: 0.2745
Epoch 19 — loss: 0.2732
Epoch 20 — loss: 0.2707
  threshold   accuracy
0    P(>V4)  81.040001
1    P(>V5)  80.029999
2    P(>V6)  83.059998
3    P(>V7)  87.199997
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 43.89%
Ordinal stacking meta epoch 1: loss=0.2516
Ordinal stacking meta epoch 2: loss=0.1639
Ordinal stacking meta epoch 3: loss=0.1551
Ordinal stacking meta epoch 4: loss=0.1519
Ordinal stacking meta epoch 5: loss=0.1501
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001497 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [23:48:05] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.339996
2    P(>V6)  85.949997
3    P(>V7)  88.309998
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 49.33%
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  82.580002
2    P(>V6)  85.849998
3    P(>V7)  88.449997
4    P(>V8)  93.169998
5    P(>V9)  97.019997
Overall accuracy: 50.29%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.290001
2    P(>V6)  84.550003
3    P(>V7)  87.919998
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 48.36%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.480003
2    P(>V6)  85.470001
3    P(>V7)  88.070000
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  84.599998
1    P(>V5)  82.820000
2    P(>V6)  85.800003
3    P(>V7)  89.120003
4    P(>V8)  93.260002
5    P(>V9)  96.820000
Overall accuracy: 49.37%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.389999
2    P(>V6)  85.419998
3    P(>V7)  88.839996
4    P(>V8)  92.690002
5    P(>V9)  96.580002
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  83.059998
2    P(>V6)  85.269997
3    P(>V7)  88.309998
4    P(>V8)  92.300003
5    P(>V9)  96.580002
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.769997
2    P(>V6)  85.180000
3    P(>V7)  88.639999
4    P(>V8)  92.489998
5    P(>V9)  96.580002
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.339996
2    P(>V6)  85.949997
3    P(>V7)  88.309998
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 49.33%
----------------- Ordinal iteration 9/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3872
Epoch 02 — loss: 0.3274
Epoch 03 — loss: 0.3104
Epoch 04 — loss: 0.2963
Epoch 05 — loss: 0.2854
Epoch 06 — loss: 0.2740
Epoch 07 — loss: 0.2668
Epoch 08 — loss: 0.2589
Epoch 09 — loss: 0.2516
Epoch 10 — loss: 0.2448
Epoch 11 — loss: 0.2387
Epoch 12 — loss: 0.2336
Epoch 13 — loss: 0.2267
Epoch 14 — loss: 0.2200
Epoch 15 — loss: 0.2146
Epoch 16 — loss: 0.2090
Epoch 17 — loss: 0.2012
Epoch 18 — loss: 0.1982
Epoch 19 — loss: 0.1923
Epoch 20 — loss: 0.1846
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  82.820000
2    P(>V6)  84.650002
3    P(>V7)  87.629997
4    P(>V8)  92.349998
5    P(>V9)  96.820000
Overall accuracy: 48.60%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3751
Epoch 02 — loss: 0.3166
Epoch 03 — loss: 0.3021
Epoch 04 — loss: 0.2893
Epoch 05 — loss: 0.2802
Epoch 06 — loss: 0.2691
Epoch 07 — loss: 0.2618
Epoch 08 — loss: 0.2538
Epoch 09 — loss: 0.2466
Epoch 10 — loss: 0.2399
Epoch 11 — loss: 0.2365
Epoch 12 — loss: 0.2298
Epoch 13 — loss: 0.2224
Epoch 14 — loss: 0.2198
Epoch 15 — loss: 0.2132
Epoch 16 — loss: 0.2065
Epoch 17 — loss: 0.2046
Epoch 18 — loss: 0.1985
Epoch 19 — loss: 0.1915
Epoch 20 — loss: 0.1873
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.440002
2    P(>V6)  83.830002
3    P(>V7)  87.339996
4    P(>V8)  92.349998
5    P(>V9)  96.779999
Overall accuracy: 47.50%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3589
Epoch 02 — loss: 0.3138
Epoch 03 — loss: 0.3007
Epoch 04 — loss: 0.2951
Epoch 05 — loss: 0.2883
Epoch 06 — loss: 0.2807
Epoch 07 — loss: 0.2798
Epoch 08 — loss: 0.2702
Epoch 09 — loss: 0.2656
Epoch 10 — loss: 0.2614
Epoch 11 — loss: 0.2548
Epoch 12 — loss: 0.2486
Epoch 13 — loss: 0.2439
Epoch 14 — loss: 0.2392
Epoch 15 — loss: 0.2326
Epoch 16 — loss: 0.2290
Epoch 17 — loss: 0.2240
Epoch 18 — loss: 0.2183
Epoch 19 — loss: 0.2136
Epoch 20 — loss: 0.2099
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.440002
2    P(>V6)  85.800003
3    P(>V7)  87.870003
4    P(>V8)  92.059998
5    P(>V9)  96.970001
Overall accuracy: 47.11%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4665
Epoch 02 — loss: 0.3394
Epoch 03 — loss: 0.3063
Epoch 04 — loss: 0.2940
Epoch 05 — loss: 0.2868
Epoch 06 — loss: 0.2818
Epoch 07 — loss: 0.2790
Epoch 08 — loss: 0.2755
Epoch 09 — loss: 0.2751
Epoch 10 — loss: 0.2749
Epoch 11 — loss: 0.2735
Epoch 12 — loss: 0.2714
Epoch 13 — loss: 0.2716
Epoch 14 — loss: 0.2690
Epoch 15 — loss: 0.2690
Epoch 16 — loss: 0.2712
Epoch 17 — loss: 0.2694
Epoch 18 — loss: 0.2693
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2700
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.940002
2    P(>V6)  83.690002
3    P(>V7)  87.779999
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.62%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4760
Epoch 02 — loss: 0.3463
Epoch 03 — loss: 0.2989
Epoch 04 — loss: 0.2881
Epoch 05 — loss: 0.2832
Epoch 06 — loss: 0.2815
Epoch 07 — loss: 0.2772
Epoch 08 — loss: 0.2778
Epoch 09 — loss: 0.2749
Epoch 10 — loss: 0.2738
Epoch 11 — loss: 0.2731
Epoch 12 — loss: 0.2727
Epoch 13 — loss: 0.2725
Epoch 14 — loss: 0.2714
Epoch 15 — loss: 0.2705
Epoch 16 — loss: 0.2713
Epoch 17 — loss: 0.2702
Epoch 18 — loss: 0.2707
Epoch 19 — loss: 0.2712
Epoch 20 — loss: 0.2699
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  81.230003
2    P(>V6)  83.930000
3    P(>V7)  88.160004
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 46.68%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4194
Epoch 02 — loss: 0.3239
Epoch 03 — loss: 0.3073
Epoch 04 — loss: 0.2993
Epoch 05 — loss: 0.2933
Epoch 06 — loss: 0.2873
Epoch 07 — loss: 0.2824
Epoch 08 — loss: 0.2824
Epoch 09 — loss: 0.2806
Epoch 10 — loss: 0.2773
Epoch 11 — loss: 0.2759
Epoch 12 — loss: 0.2763
Epoch 13 — loss: 0.2759
Epoch 14 — loss: 0.2737
Epoch 15 — loss: 0.2750
Epoch 16 — loss: 0.2735
Epoch 17 — loss: 0.2747
Epoch 18 — loss: 0.2715
Epoch 19 — loss: 0.2724
Epoch 20 — loss: 0.2712
  threshold   accuracy
0    P(>V4)  80.559998
1    P(>V5)  79.550003
2    P(>V6)  83.589996
3    P(>V7)  87.199997
4    P(>V8)  92.540001
5    P(>V9)  96.970001
Overall accuracy: 43.02%
Ordinal stacking meta epoch 1: loss=0.2162
Ordinal stacking meta epoch 2: loss=0.1560
Ordinal stacking meta epoch 3: loss=0.1509
Ordinal stacking meta epoch 4: loss=0.1480
Ordinal stacking meta epoch 5: loss=0.1461
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002291 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.529999
2    P(>V6)  86.330002
3    P(>V7)  88.790001
4    P(>V8)  93.169998
5    P(>V9)  97.059998
Overall accuracy: 49.13%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.440002
2    P(>V6)  86.430000
3    P(>V7)  88.739998
4    P(>V8)  93.019997
5    P(>V9)  97.059998
Overall accuracy: 49.66%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.239998
2    P(>V6)  85.080002
3    P(>V7)  87.870003
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 47.79%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.629997
2    P(>V6)  85.849998
3    P(>V7)  88.449997
4    P(>V8)  93.120003
5    P(>V9)  97.059998
Overall accuracy: 48.08%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.099998
2    P(>V6)  85.180000
3    P(>V7)  88.739998
4    P(>V8)  92.690002
5    P(>V9)  96.870003/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:11:29] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 47.64%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.870003
2    P(>V6)  85.370003
3    P(>V7)  88.500000
4    P(>V8)  93.070000
5    P(>V9)  96.730003
Overall accuracy: 49.13%
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  82.529999
2    P(>V6)  85.129997
3    P(>V7)  88.110001
4    P(>V8)  92.930000
5    P(>V9)  96.870003
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.820000
2    P(>V6)  85.760002
3    P(>V7)  88.309998
4    P(>V8)  92.639999
5    P(>V9)  96.820000
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.529999
2    P(>V6)  86.330002
3    P(>V7)  88.790001
4    P(>V8)  93.169998
5    P(>V9)  97.059998
Overall accuracy: 49.13%
----------------- Ordinal iteration 10/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3656
Epoch 02 — loss: 0.3121
Epoch 03 — loss: 0.2975
Epoch 04 — loss: 0.2895
Epoch 05 — loss: 0.2821
Epoch 06 — loss: 0.2735
Epoch 07 — loss: 0.2675
Epoch 08 — loss: 0.2603
Epoch 09 — loss: 0.2541
Epoch 10 — loss: 0.2466
Epoch 11 — loss: 0.2407
Epoch 12 — loss: 0.2363
Epoch 13 — loss: 0.2313
Epoch 14 — loss: 0.2241
Epoch 15 — loss: 0.2189
Epoch 16 — loss: 0.2132
Epoch 17 — loss: 0.2075
Epoch 18 — loss: 0.2035
Epoch 19 — loss: 0.1985
Epoch 20 — loss: 0.1944
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  81.620003
2    P(>V6)  84.410004
3    P(>V7)  86.860001
4    P(>V8)  92.540001
5    P(>V9)  96.629997
Overall accuracy: 47.55%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3686
Epoch 02 — loss: 0.3156
Epoch 03 — loss: 0.3019
Epoch 04 — loss: 0.2923
Epoch 05 — loss: 0.2841
Epoch 06 — loss: 0.2750
Epoch 07 — loss: 0.2662
Epoch 08 — loss: 0.2564
Epoch 09 — loss: 0.2496
Epoch 10 — loss: 0.2435
Epoch 11 — loss: 0.2377
Epoch 12 — loss: 0.2291
Epoch 13 — loss: 0.2218
Epoch 14 — loss: 0.2172
Epoch 15 — loss: 0.2115
Epoch 16 — loss: 0.2073
Epoch 17 — loss: 0.2006
Epoch 18 — loss: 0.1955
Epoch 19 — loss: 0.1894
Epoch 20 — loss: 0.1851
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  81.519997
2    P(>V6)  85.849998
3    P(>V7)  86.620003
4    P(>V8)  92.830002
5    P(>V9)  96.820000
Overall accuracy: 48.17%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3649
Epoch 02 — loss: 0.3131
Epoch 03 — loss: 0.3009
Epoch 04 — loss: 0.2887
Epoch 05 — loss: 0.2820
Epoch 06 — loss: 0.2720
Epoch 07 — loss: 0.2679
Epoch 08 — loss: 0.2598
Epoch 09 — loss: 0.2536
Epoch 10 — loss: 0.2503
Epoch 11 — loss: 0.2441
Epoch 12 — loss: 0.2387
Epoch 13 — loss: 0.2340
Epoch 14 — loss: 0.2300
Epoch 15 — loss: 0.2259
Epoch 16 — loss: 0.2225
Epoch 17 — loss: 0.2169
Epoch 18 — loss: 0.2138
Epoch 19 — loss: 0.2081
Epoch 20 — loss: 0.2034
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  81.949997
2    P(>V6)  86.000000
3    P(>V7)  88.160004
4    P(>V8)  93.550003
5    P(>V9)  96.730003
Overall accuracy: 48.70%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4717
Epoch 02 — loss: 0.3508
Epoch 03 — loss: 0.3035
Epoch 04 — loss: 0.2931
Epoch 05 — loss: 0.2870
Epoch 06 — loss: 0.2837
Epoch 07 — loss: 0.2796
Epoch 08 — loss: 0.2793
Epoch 09 — loss: 0.2789
Epoch 10 — loss: 0.2759
Epoch 11 — loss: 0.2733
Epoch 12 — loss: 0.2729
Epoch 13 — loss: 0.2737
Epoch 14 — loss: 0.2723
Epoch 15 — loss: 0.2724
Epoch 16 — loss: 0.2713
Epoch 17 — loss: 0.2708
Epoch 18 — loss: 0.2715
Epoch 19 — loss: 0.2695
Epoch 20 — loss: 0.2690
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  81.139999
2    P(>V6)  83.589996
3    P(>V7)  87.580002
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.72%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4578
Epoch 02 — loss: 0.3253
Epoch 03 — loss: 0.3015
Epoch 04 — loss: 0.2918
Epoch 05 — loss: 0.2878
Epoch 06 — loss: 0.2833
Epoch 07 — loss: 0.2788
Epoch 08 — loss: 0.2778
Epoch 09 — loss: 0.2767
Epoch 10 — loss: 0.2749
Epoch 11 — loss: 0.2743
Epoch 12 — loss: 0.2727
Epoch 13 — loss: 0.2721
Epoch 14 — loss: 0.2731
Epoch 15 — loss: 0.2726
Epoch 16 — loss: 0.2707
Epoch 17 — loss: 0.2698
Epoch 18 — loss: 0.2704
Epoch 19 — loss: 0.2694
Epoch 20 — loss: 0.2706
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  81.139999
2    P(>V6)  83.540001
3    P(>V7)  88.019997
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.05%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4285
Epoch 02 — loss: 0.3236
Epoch 03 — loss: 0.3071
Epoch 04 — loss: 0.2942
Epoch 05 — loss: 0.2894
Epoch 06 — loss: 0.2857
Epoch 07 — loss: 0.2835
Epoch 08 — loss: 0.2818
Epoch 09 — loss: 0.2784
Epoch 10 — loss: 0.2785
Epoch 11 — loss: 0.2762
Epoch 12 — loss: 0.2751
Epoch 13 — loss: 0.2745
Epoch 14 — loss: 0.2754
Epoch 15 — loss: 0.2733
Epoch 16 — loss: 0.2726
Epoch 17 — loss: 0.2722
Epoch 18 — loss: 0.2711
Epoch 19 — loss: 0.2720
Epoch 20 — loss: 0.2708
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.269997
2    P(>V6)  83.589996
3    P(>V7)  87.820000
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.29%
Ordinal stacking meta epoch 1: loss=0.2515
Ordinal stacking meta epoch 2: loss=0.1617
Ordinal stacking meta epoch 3: loss=0.1520
Ordinal stacking meta epoch 4: loss=0.1483
Ordinal stacking meta epoch 5: loss=0.1464
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001601 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.820000
2    P(>V6)  86.379997
3    P(>V7)  88.500000
4    P(>V8)  93.070000
5    P(>V9)  96.970001
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  83.110001
2    P(>V6)  86.000000
3    P(>V7)  88.160004
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 49.42%
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  81.470001
2    P(>V6)  84.459999
3    P(>V7)  88.260002
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 48.17%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.769997
2    P(>V6)  85.900002
3    P(>V7)  88.110001
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.190002
2    P(>V6)  86.430000
3    P(>V7)  87.970001
4    P(>V8)  93.070000
5    P(>V9)  96.870003
Overall accuracy: 48.22%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  81.519997
2    P(>V6)  86.720001
3    P(>V7)  87.820000
4    P(>V8)  93.070000
5    P(>V9)  96.339996
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  81.860001
2    P(>V6)  85.709999
3    P(>V7)  87.820000
4    P(>V8)  92.879997
5    P(>V9)  96.779999
Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  81.760002
2    P(>V6)  86.139999
3    P(>V7)  88.260002
4    P(>V8)  92.970001
5    P(>V9)  96.629997
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.820000
2    P(>V6)  86.379997
3    P(>V7)  88.500000
4    P(>V8)  93.070000
5    P(>V9)  96.970001
Overall accuracy: 49.47%
----------------- Ordinal iteration 11/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3780
Epoch 02 — loss: 0.3165
Epoch 03 — loss: 0.3005
Epoch 04 — loss: 0.2903
Epoch 05 — loss: 0.2818/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:34:56] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 06 — loss: 0.2767
Epoch 07 — loss: 0.2668
Epoch 08 — loss: 0.2619
Epoch 09 — loss: 0.2558
Epoch 10 — loss: 0.2483
Epoch 11 — loss: 0.2399
Epoch 12 — loss: 0.2352
Epoch 13 — loss: 0.2299
Epoch 14 — loss: 0.2232
Epoch 15 — loss: 0.2174
Epoch 16 — loss: 0.2109
Epoch 17 — loss: 0.2055
Epoch 18 — loss: 0.2001
Epoch 19 — loss: 0.1918
Epoch 20 — loss: 0.1868
  threshold   accuracy
0    P(>V4)  82.919998
1    P(>V5)  81.040001
2    P(>V6)  84.940002
3    P(>V7)  88.070000
4    P(>V8)  92.639999
5    P(>V9)  96.970001
Overall accuracy: 47.21%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3599
Epoch 02 — loss: 0.3070
Epoch 03 — loss: 0.2946
Epoch 04 — loss: 0.2852
Epoch 05 — loss: 0.2766
Epoch 06 — loss: 0.2702
Epoch 07 — loss: 0.2615
Epoch 08 — loss: 0.2546
Epoch 09 — loss: 0.2491
Epoch 10 — loss: 0.2435
Epoch 11 — loss: 0.2371
Epoch 12 — loss: 0.2312
Epoch 13 — loss: 0.2266
Epoch 14 — loss: 0.2212
Epoch 15 — loss: 0.2120
Epoch 16 — loss: 0.2091
Epoch 17 — loss: 0.2033
Epoch 18 — loss: 0.1991
Epoch 19 — loss: 0.1933
Epoch 20 — loss: 0.1872
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.290001
2    P(>V6)  84.309998
3    P(>V7)  87.389999
4    P(>V8)  92.779999
5    P(>V9)  96.820000
Overall accuracy: 47.88%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3673
Epoch 02 — loss: 0.3135
Epoch 03 — loss: 0.3019
Epoch 04 — loss: 0.2941
Epoch 05 — loss: 0.2875
Epoch 06 — loss: 0.2783
Epoch 07 — loss: 0.2725
Epoch 08 — loss: 0.2663
Epoch 09 — loss: 0.2596
Epoch 10 — loss: 0.2541
Epoch 11 — loss: 0.2459
Epoch 12 — loss: 0.2408
Epoch 13 — loss: 0.2375
Epoch 14 — loss: 0.2336
Epoch 15 — loss: 0.2282
Epoch 16 — loss: 0.2240
Epoch 17 — loss: 0.2209
Epoch 18 — loss: 0.2174
Epoch 19 — loss: 0.2110
Epoch 20 — loss: 0.2076
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  82.629997
2    P(>V6)  84.790001
3    P(>V7)  87.919998
4    P(>V8)  92.970001
5    P(>V9)  96.820000
Overall accuracy: 48.03%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4801
Epoch 02 — loss: 0.3581
Epoch 03 — loss: 0.3043
Epoch 04 — loss: 0.2911
Epoch 05 — loss: 0.2842
Epoch 06 — loss: 0.2808
Epoch 07 — loss: 0.2775
Epoch 08 — loss: 0.2770
Epoch 09 — loss: 0.2738
Epoch 10 — loss: 0.2738
Epoch 11 — loss: 0.2735
Epoch 12 — loss: 0.2727
Epoch 13 — loss: 0.2724
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2719
Epoch 16 — loss: 0.2702
Epoch 17 — loss: 0.2708
Epoch 18 — loss: 0.2694
Epoch 19 — loss: 0.2693
Epoch 20 — loss: 0.2685
  threshold   accuracy
0    P(>V4)  81.279999
1    P(>V5)  80.750000
2    P(>V6)  83.779999
3    P(>V7)  87.540001
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 44.71%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4735
Epoch 02 — loss: 0.3387
Epoch 03 — loss: 0.3016
Epoch 04 — loss: 0.2902
Epoch 05 — loss: 0.2853
Epoch 06 — loss: 0.2807
Epoch 07 — loss: 0.2779
Epoch 08 — loss: 0.2762
Epoch 09 — loss: 0.2760
Epoch 10 — loss: 0.2738
Epoch 11 — loss: 0.2728
Epoch 12 — loss: 0.2726
Epoch 13 — loss: 0.2720
Epoch 14 — loss: 0.2719
Epoch 15 — loss: 0.2715
Epoch 16 — loss: 0.2699
Epoch 17 — loss: 0.2706
Epoch 18 — loss: 0.2693
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2685
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  80.699997
2    P(>V6)  83.540001
3    P(>V7)  87.540001
4    P(>V8)  92.830002
5    P(>V9)  96.970001
Overall accuracy: 44.80%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4347
Epoch 02 — loss: 0.3220
Epoch 03 — loss: 0.3070
Epoch 04 — loss: 0.2986
Epoch 05 — loss: 0.2919
Epoch 06 — loss: 0.2853
Epoch 07 — loss: 0.2845
Epoch 08 — loss: 0.2808
Epoch 09 — loss: 0.2780
Epoch 10 — loss: 0.2770
Epoch 11 — loss: 0.2771
Epoch 12 — loss: 0.2767
Epoch 13 — loss: 0.2748
Epoch 14 — loss: 0.2743
Epoch 15 — loss: 0.2729
Epoch 16 — loss: 0.2742
Epoch 17 — loss: 0.2725
Epoch 18 — loss: 0.2706
Epoch 19 — loss: 0.2711
Epoch 20 — loss: 0.2719
  threshold   accuracy
0    P(>V4)  79.599998
1    P(>V5)  80.269997
2    P(>V6)  83.449997
3    P(>V7)  87.339996
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.29%
Ordinal stacking meta epoch 1: loss=0.2504
Ordinal stacking meta epoch 2: loss=0.1561
Ordinal stacking meta epoch 3: loss=0.1516
Ordinal stacking meta epoch 4: loss=0.1492
Ordinal stacking meta epoch 5: loss=0.1476
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001826 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  83.110001
2    P(>V6)  85.709999
3    P(>V7)  88.500000
4    P(>V8)  92.879997
5    P(>V9)  97.059998
Overall accuracy: 49.95%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.820000
2    P(>V6)  86.000000
3    P(>V7)  88.500000
4    P(>V8)  93.019997
5    P(>V9)  97.059998
Overall accuracy: 49.66%
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.680000
2    P(>V6)  84.940002
3    P(>V7)  88.070000
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 48.60%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  83.059998
2    P(>V6)  85.660004
3    P(>V7)  88.550003
4    P(>V8)  92.830002
5    P(>V9)  97.059998
Overall accuracy: 49.52%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.339996
2    P(>V6)  85.849998
3    P(>V7)  88.690002
4    P(>V8)  93.120003
5    P(>V9)  96.730003
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.769997
2    P(>V6)  86.239998
3    P(>V7)  88.739998
4    P(>V8)  92.730003
5    P(>V9)  96.870003
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.239998
2    P(>V6)  85.900002
3    P(>V7)  88.400002
4    P(>V8)  92.830002
5    P(>V9)  96.919998
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  84.650002
1    P(>V5)  82.339996
2    P(>V6)  86.089996
3    P(>V7)  88.070000
4    P(>V8)  92.690002
5    P(>V9)  96.779999
Overall accuracy: 50.19%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  83.110001
2    P(>V6)  85.709999
3    P(>V7)  88.500000
4    P(>V8)  92.879997
5    P(>V9)  97.059998
Overall accuracy: 49.95%
----------------- Ordinal iteration 12/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3740
Epoch 02 — loss: 0.3173
Epoch 03 — loss: 0.3063
Epoch 04 — loss: 0.2941
Epoch 05 — loss: 0.2854
Epoch 06 — loss: 0.2776
Epoch 07 — loss: 0.2670
Epoch 08 — loss: 0.2604
Epoch 09 — loss: 0.2527
Epoch 10 — loss: 0.2472
Epoch 11 — loss: 0.2394
Epoch 12 — loss: 0.2309
Epoch 13 — loss: 0.2293
Epoch 14 — loss: 0.2218
Epoch 15 — loss: 0.2177
Epoch 16 — loss: 0.2118
Epoch 17 — loss: 0.2070
Epoch 18 — loss: 0.2027
Epoch 19 — loss: 0.1953
Epoch 20 — loss: 0.1890
  threshold   accuracy
0    P(>V4)  82.769997
1    P(>V5)  80.220001
2    P(>V6)  83.059998
3    P(>V7)  88.209999
4    P(>V8)  92.010002
5    P(>V9)  96.580002
Overall accuracy: 45.38%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3670
Epoch 02 — loss: 0.3102
Epoch 03 — loss: 0.2973
Epoch 04 — loss: 0.2852
Epoch 05 — loss: 0.2749
Epoch 06 — loss: 0.2666
Epoch 07 — loss: 0.2590
Epoch 08 — loss: 0.2526
Epoch 09 — loss: 0.2440
Epoch 10 — loss: 0.2374
Epoch 11 — loss: 0.2303
Epoch 12 — loss: 0.2266
Epoch 13 — loss: 0.2208
Epoch 14 — loss: 0.2167/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:57:57] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 15 — loss: 0.2110
Epoch 16 — loss: 0.2065
Epoch 17 — loss: 0.1997
Epoch 18 — loss: 0.1943
Epoch 19 — loss: 0.1912
Epoch 20 — loss: 0.1837
  threshold   accuracy
0    P(>V4)  82.480003
1    P(>V5)  81.470001
2    P(>V6)  84.220001
3    P(>V7)  88.070000
4    P(>V8)  92.400002
5    P(>V9)  96.779999
Overall accuracy: 48.56%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3644
Epoch 02 — loss: 0.3151
Epoch 03 — loss: 0.3061
Epoch 04 — loss: 0.2947
Epoch 05 — loss: 0.2906
Epoch 06 — loss: 0.2826
Epoch 07 — loss: 0.2769
Epoch 08 — loss: 0.2700
Epoch 09 — loss: 0.2623
Epoch 10 — loss: 0.2551
Epoch 11 — loss: 0.2489
Epoch 12 — loss: 0.2454
Epoch 13 — loss: 0.2415
Epoch 14 — loss: 0.2337
Epoch 15 — loss: 0.2302
Epoch 16 — loss: 0.2250
Epoch 17 — loss: 0.2207
Epoch 18 — loss: 0.2136
Epoch 19 — loss: 0.2080
Epoch 20 — loss: 0.2039
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  79.160004
2    P(>V6)  84.360001
3    P(>V7)  86.620003
4    P(>V8)  91.580002
5    P(>V9)  96.820000
Overall accuracy: 44.03%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4657
Epoch 02 — loss: 0.3262
Epoch 03 — loss: 0.2969
Epoch 04 — loss: 0.2902
Epoch 05 — loss: 0.2855
Epoch 06 — loss: 0.2828
Epoch 07 — loss: 0.2807
Epoch 08 — loss: 0.2769
Epoch 09 — loss: 0.2760
Epoch 10 — loss: 0.2745
Epoch 11 — loss: 0.2726
Epoch 12 — loss: 0.2734
Epoch 13 — loss: 0.2713
Epoch 14 — loss: 0.2717
Epoch 15 — loss: 0.2704
Epoch 16 — loss: 0.2686
Epoch 17 — loss: 0.2707
Epoch 18 — loss: 0.2697
Epoch 19 — loss: 0.2705
Epoch 20 — loss: 0.2689
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.940002
2    P(>V6)  83.830002
3    P(>V7)  87.489998
4    P(>V8)  92.779999
5    P(>V9)  97.059998
Overall accuracy: 46.58%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4665
Epoch 02 — loss: 0.3262
Epoch 03 — loss: 0.2952
Epoch 04 — loss: 0.2881
Epoch 05 — loss: 0.2842
Epoch 06 — loss: 0.2811
Epoch 07 — loss: 0.2787
Epoch 08 — loss: 0.2765
Epoch 09 — loss: 0.2755
Epoch 10 — loss: 0.2732
Epoch 11 — loss: 0.2729
Epoch 12 — loss: 0.2718
Epoch 13 — loss: 0.2697
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2711
Epoch 16 — loss: 0.2693
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2685
Epoch 19 — loss: 0.2705
Epoch 20 — loss: 0.2681
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  81.180000
2    P(>V6)  83.730003
3    P(>V7)  87.540001
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.91%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4375
Epoch 02 — loss: 0.3306
Epoch 03 — loss: 0.3157
Epoch 04 — loss: 0.3024
Epoch 05 — loss: 0.2940
Epoch 06 — loss: 0.2894
Epoch 07 — loss: 0.2855
Epoch 08 — loss: 0.2829
Epoch 09 — loss: 0.2819
Epoch 10 — loss: 0.2792
Epoch 11 — loss: 0.2782
Epoch 12 — loss: 0.2762
Epoch 13 — loss: 0.2765
Epoch 14 — loss: 0.2746
Epoch 15 — loss: 0.2738
Epoch 16 — loss: 0.2730
Epoch 17 — loss: 0.2737
Epoch 18 — loss: 0.2737
Epoch 19 — loss: 0.2722
Epoch 20 — loss: 0.2713
  threshold   accuracy
0    P(>V4)  80.900002
1    P(>V5)  80.269997
2    P(>V6)  83.690002
3    P(>V7)  87.489998
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.96%
Ordinal stacking meta epoch 1: loss=0.1960
Ordinal stacking meta epoch 2: loss=0.1538
Ordinal stacking meta epoch 3: loss=0.1505
Ordinal stacking meta epoch 4: loss=0.1490
Ordinal stacking meta epoch 5: loss=0.1481
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001813 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.529999
2    P(>V6)  85.709999
3    P(>V7)  88.639999
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.529999
2    P(>V6)  85.029999
3    P(>V7)  88.160004
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  82.680000
1    P(>V5)  80.989998
2    P(>V6)  84.410004
3    P(>V7)  87.779999
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 47.35%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.190002
2    P(>V6)  85.370003
3    P(>V7)  88.550003
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.870003
2    P(>V6)  84.940002
3    P(>V7)  88.790001
4    P(>V8)  92.830002
5    P(>V9)  96.870003
Overall accuracy: 47.64%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.529999
2    P(>V6)  85.849998
3    P(>V7)  88.309998
4    P(>V8)  92.779999
5    P(>V9)  96.730003
Overall accuracy: 47.64%
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  82.629997
2    P(>V6)  85.849998
3    P(>V7)  88.589996
4    P(>V8)  92.779999
5    P(>V9)  96.629997
Overall accuracy: 48.03%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.190002
2    P(>V6)  85.470001
3    P(>V7)  88.209999
4    P(>V8)  92.830002
5    P(>V9)  96.540001
Overall accuracy: 47.69%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.529999
2    P(>V6)  85.709999
3    P(>V7)  88.639999
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 49.09%
----------------- Ordinal iteration 13/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3846
Epoch 02 — loss: 0.3223
Epoch 03 — loss: 0.3050
Epoch 04 — loss: 0.2975
Epoch 05 — loss: 0.2823
Epoch 06 — loss: 0.2721
Epoch 07 — loss: 0.2629
Epoch 08 — loss: 0.2528
Epoch 09 — loss: 0.2478
Epoch 10 — loss: 0.2398
Epoch 11 — loss: 0.2333
Epoch 12 — loss: 0.2271
Epoch 13 — loss: 0.2226
Epoch 14 — loss: 0.2163
Epoch 15 — loss: 0.2103
Epoch 16 — loss: 0.2052
Epoch 17 — loss: 0.2002
Epoch 18 — loss: 0.1952
Epoch 19 — loss: 0.1892
Epoch 20 — loss: 0.1839
  threshold   accuracy
0    P(>V4)  83.160004
1    P(>V5)  81.330002
2    P(>V6)  83.830002
3    P(>V7)  86.279999
4    P(>V8)  92.540001
5    P(>V9)  96.629997
Overall accuracy: 46.44%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3722
Epoch 02 — loss: 0.3108
Epoch 03 — loss: 0.2975
Epoch 04 — loss: 0.2887
Epoch 05 — loss: 0.2793
Epoch 06 — loss: 0.2713
Epoch 07 — loss: 0.2601
Epoch 08 — loss: 0.2540
Epoch 09 — loss: 0.2450
Epoch 10 — loss: 0.2386
Epoch 11 — loss: 0.2320
Epoch 12 — loss: 0.2284
Epoch 13 — loss: 0.2216
Epoch 14 — loss: 0.2152
Epoch 15 — loss: 0.2097
Epoch 16 — loss: 0.2054
Epoch 17 — loss: 0.2024
Epoch 18 — loss: 0.1962
Epoch 19 — loss: 0.1901
Epoch 20 — loss: 0.1872
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  80.320000
2    P(>V6)  83.830002
3    P(>V7)  86.959999
4    P(>V8)  92.730003
5    P(>V9)  96.870003
Overall accuracy: 45.81%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3682
Epoch 02 — loss: 0.3142
Epoch 03 — loss: 0.3001
Epoch 04 — loss: 0.2951
Epoch 05 — loss: 0.2884
Epoch 06 — loss: 0.2796
Epoch 07 — loss: 0.2771
Epoch 08 — loss: 0.2722
Epoch 09 — loss: 0.2667
Epoch 10 — loss: 0.2620
Epoch 11 — loss: 0.2569
Epoch 12 — loss: 0.2524
Epoch 13 — loss: 0.2491
Epoch 14 — loss: 0.2442
Epoch 15 — loss: 0.2400
Epoch 16 — loss: 0.2355
Epoch 17 — loss: 0.2303
Epoch 18 — loss: 0.2242
Epoch 19 — loss: 0.2206
Epoch 20 — loss: 0.2177
  threshold   accuracy
0    P(>V4)  82.050003
1    P(>V5)  81.910004
2    P(>V6)  85.129997
3    P(>V7)  88.400002
4    P(>V8)  92.930000
5    P(>V9)  96.919998/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:21:24] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 47.93%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4757
Epoch 02 — loss: 0.3477
Epoch 03 — loss: 0.3006
Epoch 04 — loss: 0.2916
Epoch 05 — loss: 0.2853
Epoch 06 — loss: 0.2827
Epoch 07 — loss: 0.2807
Epoch 08 — loss: 0.2771
Epoch 09 — loss: 0.2776
Epoch 10 — loss: 0.2743
Epoch 11 — loss: 0.2732
Epoch 12 — loss: 0.2720
Epoch 13 — loss: 0.2712
Epoch 14 — loss: 0.2719
Epoch 15 — loss: 0.2701
Epoch 16 — loss: 0.2705
Epoch 17 — loss: 0.2700
Epoch 18 — loss: 0.2698
Epoch 19 — loss: 0.2704
Epoch 20 — loss: 0.2689
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.410004
2    P(>V6)  83.970001
3    P(>V7)  87.580002
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 44.42%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4712
Epoch 02 — loss: 0.3335
Epoch 03 — loss: 0.3002
Epoch 04 — loss: 0.2905
Epoch 05 — loss: 0.2847
Epoch 06 — loss: 0.2792
Epoch 07 — loss: 0.2793
Epoch 08 — loss: 0.2768
Epoch 09 — loss: 0.2761
Epoch 10 — loss: 0.2738
Epoch 11 — loss: 0.2739
Epoch 12 — loss: 0.2725
Epoch 13 — loss: 0.2728
Epoch 14 — loss: 0.2706
Epoch 15 — loss: 0.2706
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2698
Epoch 18 — loss: 0.2701
Epoch 19 — loss: 0.2689
Epoch 20 — loss: 0.2691
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  81.379997
2    P(>V6)  83.779999
3    P(>V7)  87.489998
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 46.01%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4292
Epoch 02 — loss: 0.3217
Epoch 03 — loss: 0.3032
Epoch 04 — loss: 0.2972
Epoch 05 — loss: 0.2913
Epoch 06 — loss: 0.2870
Epoch 07 — loss: 0.2867
Epoch 08 — loss: 0.2834
Epoch 09 — loss: 0.2819
Epoch 10 — loss: 0.2795
Epoch 11 — loss: 0.2791
Epoch 12 — loss: 0.2798
Epoch 13 — loss: 0.2757
Epoch 14 — loss: 0.2748
Epoch 15 — loss: 0.2738
Epoch 16 — loss: 0.2739
Epoch 17 — loss: 0.2745
Epoch 18 — loss: 0.2734
Epoch 19 — loss: 0.2730
Epoch 20 — loss: 0.2714
  threshold   accuracy
0    P(>V4)  80.900002
1    P(>V5)  80.800003
2    P(>V6)  83.589996
3    P(>V7)  87.680000
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 44.90%
Ordinal stacking meta epoch 1: loss=0.2272
Ordinal stacking meta epoch 2: loss=0.1569
Ordinal stacking meta epoch 3: loss=0.1517
Ordinal stacking meta epoch 4: loss=0.1495
Ordinal stacking meta epoch 5: loss=0.1480
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001608 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.160004
1    P(>V5)  82.480003
2    P(>V6)  86.139999
3    P(>V7)  88.790001
4    P(>V8)  93.120003
5    P(>V9)  97.110001
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  82.959999
1    P(>V5)  82.440002
2    P(>V6)  86.239998
3    P(>V7)  88.070000
4    P(>V8)  92.930000
5    P(>V9)  97.110001
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  82.440002
1    P(>V5)  82.339996
2    P(>V6)  84.940002
3    P(>V7)  88.110001
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 47.59%
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  82.290001
2    P(>V6)  85.660004
3    P(>V7)  88.349998
4    P(>V8)  93.169998
5    P(>V9)  97.059998
Overall accuracy: 48.41%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  81.860001
2    P(>V6)  85.510002
3    P(>V7)  87.870003
4    P(>V8)  93.019997
5    P(>V9)  96.919998
Overall accuracy: 47.31%
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.440002
2    P(>V6)  85.900002
3    P(>V7)  88.349998
4    P(>V8)  92.779999
5    P(>V9)  96.629997
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  82.720001
1    P(>V5)  82.339996
2    P(>V6)  85.320000
3    P(>V7)  87.730003
4    P(>V8)  92.830002
5    P(>V9)  96.919998
Overall accuracy: 48.32%
  threshold   accuracy
0    P(>V4)  82.919998
1    P(>V5)  81.709999
2    P(>V6)  85.510002
3    P(>V7)  87.540001
4    P(>V8)  92.879997
5    P(>V9)  96.919998
Overall accuracy: 47.93%
  threshold   accuracy
0    P(>V4)  83.160004
1    P(>V5)  82.480003
2    P(>V6)  86.139999
3    P(>V7)  88.790001
4    P(>V8)  93.120003
5    P(>V9)  97.110001
Overall accuracy: 49.04%
----------------- Ordinal iteration 14/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3726
Epoch 02 — loss: 0.3169
Epoch 03 — loss: 0.3027
Epoch 04 — loss: 0.2932
Epoch 05 — loss: 0.2816
Epoch 06 — loss: 0.2763
Epoch 07 — loss: 0.2668
Epoch 08 — loss: 0.2587
Epoch 09 — loss: 0.2523
Epoch 10 — loss: 0.2452
Epoch 11 — loss: 0.2397
Epoch 12 — loss: 0.2334
Epoch 13 — loss: 0.2268
Epoch 14 — loss: 0.2200
Epoch 15 — loss: 0.2178
Epoch 16 — loss: 0.2104
Epoch 17 — loss: 0.2045
Epoch 18 — loss: 0.2008
Epoch 19 — loss: 0.1943
Epoch 20 — loss: 0.1888
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  81.709999
2    P(>V6)  85.709999
3    P(>V7)  88.209999
4    P(>V8)  92.779999
5    P(>V9)  96.870003
Overall accuracy: 49.76%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3685
Epoch 02 — loss: 0.3066
Epoch 03 — loss: 0.2950
Epoch 04 — loss: 0.2871
Epoch 05 — loss: 0.2796
Epoch 06 — loss: 0.2730
Epoch 07 — loss: 0.2651
Epoch 08 — loss: 0.2588
Epoch 09 — loss: 0.2530
Epoch 10 — loss: 0.2479
Epoch 11 — loss: 0.2424
Epoch 12 — loss: 0.2365
Epoch 13 — loss: 0.2308
Epoch 14 — loss: 0.2251
Epoch 15 — loss: 0.2194
Epoch 16 — loss: 0.2165
Epoch 17 — loss: 0.2097
Epoch 18 — loss: 0.2052
Epoch 19 — loss: 0.1990
Epoch 20 — loss: 0.1953
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  81.180000
2    P(>V6)  84.309998
3    P(>V7)  87.050003
4    P(>V8)  92.199997
5    P(>V9)  96.540001
Overall accuracy: 46.82%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3562
Epoch 02 — loss: 0.3142
Epoch 03 — loss: 0.3018
Epoch 04 — loss: 0.2957
Epoch 05 — loss: 0.2885
Epoch 06 — loss: 0.2835
Epoch 07 — loss: 0.2758
Epoch 08 — loss: 0.2728
Epoch 09 — loss: 0.2642
Epoch 10 — loss: 0.2589
Epoch 11 — loss: 0.2517
Epoch 12 — loss: 0.2458
Epoch 13 — loss: 0.2412
Epoch 14 — loss: 0.2351
Epoch 15 — loss: 0.2315
Epoch 16 — loss: 0.2253
Epoch 17 — loss: 0.2226
Epoch 18 — loss: 0.2162
Epoch 19 — loss: 0.2110
Epoch 20 — loss: 0.2085
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.870003
2    P(>V6)  85.559998
3    P(>V7)  88.019997
4    P(>V8)  92.830002
5    P(>V9)  96.629997
Overall accuracy: 48.85%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4608
Epoch 02 — loss: 0.3330
Epoch 03 — loss: 0.3043
Epoch 04 — loss: 0.2933
Epoch 05 — loss: 0.2857
Epoch 06 — loss: 0.2823
Epoch 07 — loss: 0.2796
Epoch 08 — loss: 0.2771
Epoch 09 — loss: 0.2764
Epoch 10 — loss: 0.2740
Epoch 11 — loss: 0.2745
Epoch 12 — loss: 0.2724
Epoch 13 — loss: 0.2728
Epoch 14 — loss: 0.2709
Epoch 15 — loss: 0.2707
Epoch 16 — loss: 0.2708
Epoch 17 — loss: 0.2713
Epoch 18 — loss: 0.2704
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2694
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  81.139999
2    P(>V6)  84.169998
3    P(>V7)  87.440002
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.09%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4620
Epoch 02 — loss: 0.3288
Epoch 03 — loss: 0.2956
Epoch 04 — loss: 0.2882
Epoch 05 — loss: 0.2852
Epoch 06 — loss: 0.2811
Epoch 07 — loss: 0.2802/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:44:04] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 08 — loss: 0.2766
Epoch 09 — loss: 0.2769
Epoch 10 — loss: 0.2742
Epoch 11 — loss: 0.2750
Epoch 12 — loss: 0.2737
Epoch 13 — loss: 0.2732
Epoch 14 — loss: 0.2711
Epoch 15 — loss: 0.2711
Epoch 16 — loss: 0.2710
Epoch 17 — loss: 0.2708
Epoch 18 — loss: 0.2708
Epoch 19 — loss: 0.2685
Epoch 20 — loss: 0.2684
  threshold   accuracy
0    P(>V4)  81.089996
1    P(>V5)  80.080002
2    P(>V6)  83.779999
3    P(>V7)  87.970001
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 43.70%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4345
Epoch 02 — loss: 0.3320
Epoch 03 — loss: 0.3165
Epoch 04 — loss: 0.3050
Epoch 05 — loss: 0.2974
Epoch 06 — loss: 0.2910
Epoch 07 — loss: 0.2876
Epoch 08 — loss: 0.2844
Epoch 09 — loss: 0.2825
Epoch 10 — loss: 0.2785
Epoch 11 — loss: 0.2768
Epoch 12 — loss: 0.2790
Epoch 13 — loss: 0.2751
Epoch 14 — loss: 0.2738
Epoch 15 — loss: 0.2750
Epoch 16 — loss: 0.2747
Epoch 17 — loss: 0.2742
Epoch 18 — loss: 0.2736
Epoch 19 — loss: 0.2730
Epoch 20 — loss: 0.2714
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.940002
2    P(>V6)  83.730003
3    P(>V7)  87.489998
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.48%
Ordinal stacking meta epoch 1: loss=0.2575
Ordinal stacking meta epoch 2: loss=0.1668
Ordinal stacking meta epoch 3: loss=0.1578
Ordinal stacking meta epoch 4: loss=0.1542
Ordinal stacking meta epoch 5: loss=0.1523
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002419 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.680000
2    P(>V6)  86.279999
3    P(>V7)  88.930000
4    P(>V8)  92.970001
5    P(>V9)  97.059998
Overall accuracy: 49.13%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.769997
2    P(>V6)  86.279999
3    P(>V7)  88.449997
4    P(>V8)  93.019997
5    P(>V9)  97.059998
Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  82.239998
2    P(>V6)  85.559998
3    P(>V7)  88.160004
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 48.32%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.239998
2    P(>V6)  86.139999
3    P(>V7)  88.839996
4    P(>V8)  92.930000
5    P(>V9)  97.059998
Overall accuracy: 48.99%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  83.400002
2    P(>V6)  85.849998
3    P(>V7)  88.879997
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 49.37%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  83.160004
2    P(>V6)  86.330002
3    P(>V7)  88.839996
4    P(>V8)  92.730003
5    P(>V9)  96.680000
Overall accuracy: 49.81%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.480003
2    P(>V6)  86.769997
3    P(>V7)  88.019997
4    P(>V8)  92.830002
5    P(>V9)  96.870003
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  83.250000
2    P(>V6)  86.570000
3    P(>V7)  88.160004
4    P(>V8)  92.879997
5    P(>V9)  96.680000
Overall accuracy: 50.48%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.680000
2    P(>V6)  86.279999
3    P(>V7)  88.930000
4    P(>V8)  92.970001
5    P(>V9)  97.059998
Overall accuracy: 49.13%
----------------- Ordinal iteration 15/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3711
Epoch 02 — loss: 0.3132
Epoch 03 — loss: 0.3011
Epoch 04 — loss: 0.2934
Epoch 05 — loss: 0.2861
Epoch 06 — loss: 0.2784
Epoch 07 — loss: 0.2707
Epoch 08 — loss: 0.2614
Epoch 09 — loss: 0.2580
Epoch 10 — loss: 0.2486
Epoch 11 — loss: 0.2431
Epoch 12 — loss: 0.2355
Epoch 13 — loss: 0.2307
Epoch 14 — loss: 0.2238
Epoch 15 — loss: 0.2180
Epoch 16 — loss: 0.2135
Epoch 17 — loss: 0.2090
Epoch 18 — loss: 0.2020
Epoch 19 — loss: 0.1969
Epoch 20 — loss: 0.1938
  threshold   accuracy
0    P(>V4)  82.720001
1    P(>V5)  80.650002
2    P(>V6)  82.099998
3    P(>V7)  86.769997
4    P(>V8)  92.010002
5    P(>V9)  96.199997
Overall accuracy: 44.27%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3590
Epoch 02 — loss: 0.3045
Epoch 03 — loss: 0.2962
Epoch 04 — loss: 0.2878
Epoch 05 — loss: 0.2764
Epoch 06 — loss: 0.2706
Epoch 07 — loss: 0.2621
Epoch 08 — loss: 0.2560
Epoch 09 — loss: 0.2477
Epoch 10 — loss: 0.2423
Epoch 11 — loss: 0.2346
Epoch 12 — loss: 0.2300
Epoch 13 — loss: 0.2236
Epoch 14 — loss: 0.2179
Epoch 15 — loss: 0.2139
Epoch 16 — loss: 0.2075
Epoch 17 — loss: 0.2017
Epoch 18 — loss: 0.1971
Epoch 19 — loss: 0.1911
Epoch 20 — loss: 0.1870
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.440002
2    P(>V6)  84.120003
3    P(>V7)  88.110001
4    P(>V8)  92.690002
5    P(>V9)  96.970001
Overall accuracy: 48.22%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3709
Epoch 02 — loss: 0.3161
Epoch 03 — loss: 0.3036
Epoch 04 — loss: 0.2954
Epoch 05 — loss: 0.2861
Epoch 06 — loss: 0.2819
Epoch 07 — loss: 0.2759
Epoch 08 — loss: 0.2683
Epoch 09 — loss: 0.2632
Epoch 10 — loss: 0.2574
Epoch 11 — loss: 0.2512
Epoch 12 — loss: 0.2458
Epoch 13 — loss: 0.2408
Epoch 14 — loss: 0.2372
Epoch 15 — loss: 0.2311
Epoch 16 — loss: 0.2253
Epoch 17 — loss: 0.2218
Epoch 18 — loss: 0.2160
Epoch 19 — loss: 0.2108
Epoch 20 — loss: 0.2046
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  80.800003
2    P(>V6)  85.419998
3    P(>V7)  87.820000
4    P(>V8)  92.730003
5    P(>V9)  96.730003
Overall accuracy: 48.56%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4690
Epoch 02 — loss: 0.3386
Epoch 03 — loss: 0.3004
Epoch 04 — loss: 0.2912
Epoch 05 — loss: 0.2843
Epoch 06 — loss: 0.2820
Epoch 07 — loss: 0.2797
Epoch 08 — loss: 0.2760
Epoch 09 — loss: 0.2771
Epoch 10 — loss: 0.2744
Epoch 11 — loss: 0.2744
Epoch 12 — loss: 0.2725
Epoch 13 — loss: 0.2713
Epoch 14 — loss: 0.2700
Epoch 15 — loss: 0.2702
Epoch 16 — loss: 0.2701
Epoch 17 — loss: 0.2696
Epoch 18 — loss: 0.2719
Epoch 19 — loss: 0.2711
Epoch 20 — loss: 0.2687
  threshold   accuracy
0    P(>V4)  81.860001
1    P(>V5)  81.330002
2    P(>V6)  83.970001
3    P(>V7)  87.300003
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 46.97%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4779
Epoch 02 — loss: 0.3445
Epoch 03 — loss: 0.2979
Epoch 04 — loss: 0.2897
Epoch 05 — loss: 0.2844
Epoch 06 — loss: 0.2819
Epoch 07 — loss: 0.2788
Epoch 08 — loss: 0.2779
Epoch 09 — loss: 0.2756
Epoch 10 — loss: 0.2748
Epoch 11 — loss: 0.2736
Epoch 12 — loss: 0.2732
Epoch 13 — loss: 0.2713
Epoch 14 — loss: 0.2722
Epoch 15 — loss: 0.2713
Epoch 16 — loss: 0.2712
Epoch 17 — loss: 0.2698
Epoch 18 — loss: 0.2697
Epoch 19 — loss: 0.2694
Epoch 20 — loss: 0.2695
  threshold   accuracy
0    P(>V4)  80.940002
1    P(>V5)  80.900002
2    P(>V6)  83.489998
3    P(>V7)  87.919998
4    P(>V8)  92.779999
5    P(>V9)  96.970001
Overall accuracy: 46.29%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4372
Epoch 02 — loss: 0.3212
Epoch 03 — loss: 0.3060
Epoch 04 — loss: 0.2969
Epoch 05 — loss: 0.2876
Epoch 06 — loss: 0.2827
Epoch 07 — loss: 0.2793
Epoch 08 — loss: 0.2790
Epoch 09 — loss: 0.2784
Epoch 10 — loss: 0.2741
Epoch 11 — loss: 0.2748
Epoch 12 — loss: 0.2749
Epoch 13 — loss: 0.2745
Epoch 14 — loss: 0.2719
Epoch 15 — loss: 0.2732
Epoch 16 — loss: 0.2727/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:07:29] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:30:55] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 17 — loss: 0.2718
Epoch 18 — loss: 0.2706
Epoch 19 — loss: 0.2712
Epoch 20 — loss: 0.2707
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.410004
2    P(>V6)  84.070000
3    P(>V7)  87.820000
4    P(>V8)  92.489998
5    P(>V9)  96.970001
Overall accuracy: 46.44%
Ordinal stacking meta epoch 1: loss=0.2287
Ordinal stacking meta epoch 2: loss=0.1593
Ordinal stacking meta epoch 3: loss=0.1532
Ordinal stacking meta epoch 4: loss=0.1506
Ordinal stacking meta epoch 5: loss=0.1492
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001542 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.050003
2    P(>V6)  85.320000
3    P(>V7)  88.550003
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.050003
2    P(>V6)  85.269997
3    P(>V7)  88.400002
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 48.12%
  threshold   accuracy
0    P(>V4)  82.529999
1    P(>V5)  81.279999
2    P(>V6)  84.120003
3    P(>V7)  87.730003
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 47.40%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  81.809998
2    P(>V6)  84.839996
3    P(>V7)  88.449997
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 47.69%
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  82.629997
2    P(>V6)  85.559998
3    P(>V7)  88.879997
4    P(>V8)  92.300003
5    P(>V9)  96.730003
Overall accuracy: 48.32%
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  82.190002
2    P(>V6)  84.989998
3    P(>V7)  88.449997
4    P(>V8)  92.199997
5    P(>V9)  96.150002
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  81.949997
2    P(>V6)  84.650002
3    P(>V7)  88.160004
4    P(>V8)  92.010002
5    P(>V9)  96.489998
Overall accuracy: 47.88%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.150002
2    P(>V6)  84.989998
3    P(>V7)  88.260002
4    P(>V8)  92.160004
5    P(>V9)  96.540001
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.050003
2    P(>V6)  85.320000
3    P(>V7)  88.550003
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 48.51%
----------------- Ordinal iteration 16/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3789
Epoch 02 — loss: 0.3200
Epoch 03 — loss: 0.3079
Epoch 04 — loss: 0.2931
Epoch 05 — loss: 0.2819
Epoch 06 — loss: 0.2725
Epoch 07 — loss: 0.2623
Epoch 08 — loss: 0.2566
Epoch 09 — loss: 0.2496
Epoch 10 — loss: 0.2407
Epoch 11 — loss: 0.2341
Epoch 12 — loss: 0.2275
Epoch 13 — loss: 0.2218
Epoch 14 — loss: 0.2163
Epoch 15 — loss: 0.2112
Epoch 16 — loss: 0.2049
Epoch 17 — loss: 0.1987
Epoch 18 — loss: 0.1934
Epoch 19 — loss: 0.1893
Epoch 20 — loss: 0.1841
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  81.139999
2    P(>V6)  83.449997
3    P(>V7)  86.769997
4    P(>V8)  92.930000
5    P(>V9)  96.919998
Overall accuracy: 48.85%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3729
Epoch 02 — loss: 0.3150
Epoch 03 — loss: 0.2986
Epoch 04 — loss: 0.2925
Epoch 05 — loss: 0.2821
Epoch 06 — loss: 0.2719
Epoch 07 — loss: 0.2673
Epoch 08 — loss: 0.2583
Epoch 09 — loss: 0.2510
Epoch 10 — loss: 0.2444
Epoch 11 — loss: 0.2399
Epoch 12 — loss: 0.2319
Epoch 13 — loss: 0.2263
Epoch 14 — loss: 0.2205
Epoch 15 — loss: 0.2142
Epoch 16 — loss: 0.2087
Epoch 17 — loss: 0.2034
Epoch 18 — loss: 0.1977
Epoch 19 — loss: 0.1923
Epoch 20 — loss: 0.1867
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  81.419998
2    P(>V6)  83.970001
3    P(>V7)  86.769997
4    P(>V8)  92.300003
5    P(>V9)  96.820000
Overall accuracy: 48.89%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3597
Epoch 02 — loss: 0.3110
Epoch 03 — loss: 0.3030
Epoch 04 — loss: 0.2946
Epoch 05 — loss: 0.2897
Epoch 06 — loss: 0.2843
Epoch 07 — loss: 0.2788
Epoch 08 — loss: 0.2716
Epoch 09 — loss: 0.2652
Epoch 10 — loss: 0.2627
Epoch 11 — loss: 0.2548
Epoch 12 — loss: 0.2493
Epoch 13 — loss: 0.2454
Epoch 14 — loss: 0.2397
Epoch 15 — loss: 0.2352
Epoch 16 — loss: 0.2294
Epoch 17 — loss: 0.2247
Epoch 18 — loss: 0.2204
Epoch 19 — loss: 0.2160
Epoch 20 — loss: 0.2113
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  81.760002
2    P(>V6)  85.559998
3    P(>V7)  88.449997
4    P(>V8)  92.639999
5    P(>V9)  96.540001
Overall accuracy: 48.65%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4659
Epoch 02 — loss: 0.3442
Epoch 03 — loss: 0.3013
Epoch 04 — loss: 0.2876
Epoch 05 — loss: 0.2835
Epoch 06 — loss: 0.2809
Epoch 07 — loss: 0.2786
Epoch 08 — loss: 0.2769
Epoch 09 — loss: 0.2762
Epoch 10 — loss: 0.2743
Epoch 11 — loss: 0.2733
Epoch 12 — loss: 0.2733
Epoch 13 — loss: 0.2726
Epoch 14 — loss: 0.2703
Epoch 15 — loss: 0.2716
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2705
Epoch 18 — loss: 0.2684
Epoch 19 — loss: 0.2688
Epoch 20 — loss: 0.2687
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.699997
2    P(>V6)  83.830002
3    P(>V7)  87.680000
4    P(>V8)  92.589996
5    P(>V9)  97.059998
Overall accuracy: 46.34%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4734
Epoch 02 — loss: 0.3465
Epoch 03 — loss: 0.2954
Epoch 04 — loss: 0.2886
Epoch 05 — loss: 0.2835
Epoch 06 — loss: 0.2801
Epoch 07 — loss: 0.2794
Epoch 08 — loss: 0.2771
Epoch 09 — loss: 0.2762
Epoch 10 — loss: 0.2747
Epoch 11 — loss: 0.2739
Epoch 12 — loss: 0.2730
Epoch 13 — loss: 0.2726
Epoch 14 — loss: 0.2710
Epoch 15 — loss: 0.2714
Epoch 16 — loss: 0.2702
Epoch 17 — loss: 0.2700
Epoch 18 — loss: 0.2697
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2688
  threshold   accuracy
0    P(>V4)  81.860001
1    P(>V5)  80.940002
2    P(>V6)  83.830002
3    P(>V7)  87.870003
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.96%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4285
Epoch 02 — loss: 0.3278
Epoch 03 — loss: 0.3120
Epoch 04 — loss: 0.3012
Epoch 05 — loss: 0.2922
Epoch 06 — loss: 0.2880
Epoch 07 — loss: 0.2830
Epoch 08 — loss: 0.2819
Epoch 09 — loss: 0.2792
Epoch 10 — loss: 0.2789
Epoch 11 — loss: 0.2773
Epoch 12 — loss: 0.2753
Epoch 13 — loss: 0.2783
Epoch 14 — loss: 0.2736
Epoch 15 — loss: 0.2740
Epoch 16 — loss: 0.2745
Epoch 17 — loss: 0.2728
Epoch 18 — loss: 0.2734
Epoch 19 — loss: 0.2737
Epoch 20 — loss: 0.2703
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  80.320000
2    P(>V6)  83.589996
3    P(>V7)  87.339996
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 44.85%
Ordinal stacking meta epoch 1: loss=0.2713
Ordinal stacking meta epoch 2: loss=0.1620
Ordinal stacking meta epoch 3: loss=0.1521
Ordinal stacking meta epoch 4: loss=0.1479
Ordinal stacking meta epoch 5: loss=0.1457
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001600 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:54:15] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.290001
2    P(>V6)  85.760002
3    P(>V7)  88.449997
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 48.89%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.290001
2    P(>V6)  85.230003
3    P(>V7)  88.110001
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 49.33%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  81.279999
2    P(>V6)  84.839996
3    P(>V7)  88.309998
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 48.60%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.290001
2    P(>V6)  85.709999
3    P(>V7)  88.209999
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  82.389999
2    P(>V6)  86.089996
3    P(>V7)  88.500000
4    P(>V8)  92.879997
5    P(>V9)  96.820000
Overall accuracy: 48.99%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.680000
2    P(>V6)  85.610001
3    P(>V7)  88.160004
4    P(>V8)  92.440002
5    P(>V9)  96.489998
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  82.680000
1    P(>V5)  82.680000
2    P(>V6)  85.660004
3    P(>V7)  88.209999
4    P(>V8)  92.589996
5    P(>V9)  96.440002
Overall accuracy: 48.41%
  threshold   accuracy
0    P(>V4)  82.480003
1    P(>V5)  82.820000
2    P(>V6)  85.320000
3    P(>V7)  88.550003
4    P(>V8)  92.730003
5    P(>V9)  96.339996
Overall accuracy: 48.08%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.290001
2    P(>V6)  85.760002
3    P(>V7)  88.449997
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 48.89%
----------------- Ordinal iteration 17/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3767
Epoch 02 — loss: 0.3173
Epoch 03 — loss: 0.3027
Epoch 04 — loss: 0.2884
Epoch 05 — loss: 0.2799
Epoch 06 — loss: 0.2697
Epoch 07 — loss: 0.2629
Epoch 08 — loss: 0.2535
Epoch 09 — loss: 0.2460
Epoch 10 — loss: 0.2369
Epoch 11 — loss: 0.2334
Epoch 12 — loss: 0.2275
Epoch 13 — loss: 0.2217
Epoch 14 — loss: 0.2163
Epoch 15 — loss: 0.2091
Epoch 16 — loss: 0.2032
Epoch 17 — loss: 0.1973
Epoch 18 — loss: 0.1945
Epoch 19 — loss: 0.1892
Epoch 20 — loss: 0.1823
  threshold   accuracy
0    P(>V4)  82.820000
1    P(>V5)  81.470001
2    P(>V6)  84.550003
3    P(>V7)  87.489998
4    P(>V8)  92.489998
5    P(>V9)  96.970001
Overall accuracy: 46.63%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3833
Epoch 02 — loss: 0.3195
Epoch 03 — loss: 0.3033
Epoch 04 — loss: 0.2941
Epoch 05 — loss: 0.2835
Epoch 06 — loss: 0.2780
Epoch 07 — loss: 0.2692
Epoch 08 — loss: 0.2597
Epoch 09 — loss: 0.2534
Epoch 10 — loss: 0.2453
Epoch 11 — loss: 0.2389
Epoch 12 — loss: 0.2306
Epoch 13 — loss: 0.2259
Epoch 14 — loss: 0.2186
Epoch 15 — loss: 0.2121
Epoch 16 — loss: 0.2075
Epoch 17 — loss: 0.2007
Epoch 18 — loss: 0.1952
Epoch 19 — loss: 0.1904
Epoch 20 — loss: 0.1836
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.339996
2    P(>V6)  84.360001
3    P(>V7)  87.150002
4    P(>V8)  92.830002
5    P(>V9)  96.870003
Overall accuracy: 48.22%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3596
Epoch 02 — loss: 0.3107
Epoch 03 — loss: 0.2980
Epoch 04 — loss: 0.2923
Epoch 05 — loss: 0.2874
Epoch 06 — loss: 0.2773
Epoch 07 — loss: 0.2713
Epoch 08 — loss: 0.2642
Epoch 09 — loss: 0.2581
Epoch 10 — loss: 0.2531
Epoch 11 — loss: 0.2465
Epoch 12 — loss: 0.2415
Epoch 13 — loss: 0.2353
Epoch 14 — loss: 0.2296
Epoch 15 — loss: 0.2264
Epoch 16 — loss: 0.2198
Epoch 17 — loss: 0.2150
Epoch 18 — loss: 0.2124
Epoch 19 — loss: 0.2071
Epoch 20 — loss: 0.2036
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.959999
2    P(>V6)  84.790001
3    P(>V7)  87.779999
4    P(>V8)  93.120003
5    P(>V9)  96.779999
Overall accuracy: 49.23%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4756
Epoch 02 — loss: 0.3540
Epoch 03 — loss: 0.3028
Epoch 04 — loss: 0.2914
Epoch 05 — loss: 0.2845
Epoch 06 — loss: 0.2819
Epoch 07 — loss: 0.2791
Epoch 08 — loss: 0.2781
Epoch 09 — loss: 0.2748
Epoch 10 — loss: 0.2738
Epoch 11 — loss: 0.2753
Epoch 12 — loss: 0.2731
Epoch 13 — loss: 0.2702
Epoch 14 — loss: 0.2707
Epoch 15 — loss: 0.2705
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2701
Epoch 18 — loss: 0.2699
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2694
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.699997
2    P(>V6)  84.220001
3    P(>V7)  87.870003
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 45.19%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4694
Epoch 02 — loss: 0.3360
Epoch 03 — loss: 0.2993
Epoch 04 — loss: 0.2896
Epoch 05 — loss: 0.2859
Epoch 06 — loss: 0.2827
Epoch 07 — loss: 0.2796
Epoch 08 — loss: 0.2778
Epoch 09 — loss: 0.2782
Epoch 10 — loss: 0.2764
Epoch 11 — loss: 0.2755
Epoch 12 — loss: 0.2727
Epoch 13 — loss: 0.2734
Epoch 14 — loss: 0.2718
Epoch 15 — loss: 0.2713
Epoch 16 — loss: 0.2723
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2702
Epoch 19 — loss: 0.2704
Epoch 20 — loss: 0.2687
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  80.699997
2    P(>V6)  83.830002
3    P(>V7)  87.680000
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.28%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4093
Epoch 02 — loss: 0.3236
Epoch 03 — loss: 0.3115
Epoch 04 — loss: 0.3014
Epoch 05 — loss: 0.2970
Epoch 06 — loss: 0.2904
Epoch 07 — loss: 0.2856
Epoch 08 — loss: 0.2841
Epoch 09 — loss: 0.2810
Epoch 10 — loss: 0.2797
Epoch 11 — loss: 0.2775
Epoch 12 — loss: 0.2766
Epoch 13 — loss: 0.2765
Epoch 14 — loss: 0.2749
Epoch 15 — loss: 0.2727
Epoch 16 — loss: 0.2738
Epoch 17 — loss: 0.2742
Epoch 18 — loss: 0.2724
Epoch 19 — loss: 0.2727
Epoch 20 — loss: 0.2714
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.650002
2    P(>V6)  84.019997
3    P(>V7)  87.820000
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.24%
Ordinal stacking meta epoch 1: loss=0.2327
Ordinal stacking meta epoch 2: loss=0.1566
Ordinal stacking meta epoch 3: loss=0.1510
Ordinal stacking meta epoch 4: loss=0.1486
Ordinal stacking meta epoch 5: loss=0.1470
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001528 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.720001
2    P(>V6)  86.139999
3    P(>V7)  88.500000
4    P(>V8)  92.879997
5    P(>V9)  97.059998
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  83.010002
2    P(>V6)  85.760002
3    P(>V7)  88.500000
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.239998
2    P(>V6)  85.129997
3    P(>V7)  88.019997
4    P(>V8)  92.879997
5    P(>V9)  97.059998
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.339996
2    P(>V6)  85.709999
3    P(>V7)  88.349998
4    P(>V8)  92.930000
5    P(>V9)  97.059998
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.919998
2    P(>V6)  85.800003
3    P(>V7)  88.550003
4    P(>V8)  93.309998
5    P(>V9)  96.540001/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:17:00] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.720001
2    P(>V6)  85.510002
3    P(>V7)  88.639999
4    P(>V8)  92.930000
5    P(>V9)  96.290001
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  82.820000
1    P(>V5)  82.580002
2    P(>V6)  85.510002
3    P(>V7)  88.639999
4    P(>V8)  93.360001
5    P(>V9)  96.540001
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.339996
2    P(>V6)  85.470001
3    P(>V7)  88.500000
4    P(>V8)  92.970001
5    P(>V9)  96.290001
Overall accuracy: 48.36%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.720001
2    P(>V6)  86.139999
3    P(>V7)  88.500000
4    P(>V8)  92.879997
5    P(>V9)  97.059998
Overall accuracy: 48.75%
----------------- Ordinal iteration 18/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3874
Epoch 02 — loss: 0.3248
Epoch 03 — loss: 0.3043
Epoch 04 — loss: 0.2925
Epoch 05 — loss: 0.2798
Epoch 06 — loss: 0.2701
Epoch 07 — loss: 0.2620
Epoch 08 — loss: 0.2555
Epoch 09 — loss: 0.2462
Epoch 10 — loss: 0.2399
Epoch 11 — loss: 0.2329
Epoch 12 — loss: 0.2261
Epoch 13 — loss: 0.2211
Epoch 14 — loss: 0.2148
Epoch 15 — loss: 0.2081
Epoch 16 — loss: 0.2030
Epoch 17 — loss: 0.1975
Epoch 18 — loss: 0.1907
Epoch 19 — loss: 0.1859
Epoch 20 — loss: 0.1808
  threshold   accuracy
0    P(>V4)  82.820000
1    P(>V5)  82.000000
2    P(>V6)  84.790001
3    P(>V7)  86.860001
4    P(>V8)  92.440002
5    P(>V9)  96.680000
Overall accuracy: 47.45%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3696
Epoch 02 — loss: 0.3151
Epoch 03 — loss: 0.2987
Epoch 04 — loss: 0.2887
Epoch 05 — loss: 0.2776
Epoch 06 — loss: 0.2719
Epoch 07 — loss: 0.2628
Epoch 08 — loss: 0.2530
Epoch 09 — loss: 0.2452
Epoch 10 — loss: 0.2389
Epoch 11 — loss: 0.2332
Epoch 12 — loss: 0.2261
Epoch 13 — loss: 0.2194
Epoch 14 — loss: 0.2135
Epoch 15 — loss: 0.2075
Epoch 16 — loss: 0.2043
Epoch 17 — loss: 0.1969
Epoch 18 — loss: 0.1909
Epoch 19 — loss: 0.1855
Epoch 20 — loss: 0.1802
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  82.480003
2    P(>V6)  85.370003
3    P(>V7)  88.070000
4    P(>V8)  92.440002
5    P(>V9)  96.820000
Overall accuracy: 48.08%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3721
Epoch 02 — loss: 0.3139
Epoch 03 — loss: 0.3018
Epoch 04 — loss: 0.2941
Epoch 05 — loss: 0.2888
Epoch 06 — loss: 0.2829
Epoch 07 — loss: 0.2751
Epoch 08 — loss: 0.2692
Epoch 09 — loss: 0.2606
Epoch 10 — loss: 0.2549
Epoch 11 — loss: 0.2480
Epoch 12 — loss: 0.2432
Epoch 13 — loss: 0.2377
Epoch 14 — loss: 0.2338
Epoch 15 — loss: 0.2283
Epoch 16 — loss: 0.2252
Epoch 17 — loss: 0.2185
Epoch 18 — loss: 0.2152
Epoch 19 — loss: 0.2143
Epoch 20 — loss: 0.2071
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.480003
2    P(>V6)  85.510002
3    P(>V7)  88.110001
4    P(>V8)  92.730003
5    P(>V9)  96.919998
Overall accuracy: 48.51%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4633
Epoch 02 — loss: 0.3346
Epoch 03 — loss: 0.3011
Epoch 04 — loss: 0.2894
Epoch 05 — loss: 0.2864
Epoch 06 — loss: 0.2815
Epoch 07 — loss: 0.2793
Epoch 08 — loss: 0.2770
Epoch 09 — loss: 0.2766
Epoch 10 — loss: 0.2748
Epoch 11 — loss: 0.2736
Epoch 12 — loss: 0.2721
Epoch 13 — loss: 0.2721
Epoch 14 — loss: 0.2724
Epoch 15 — loss: 0.2694
Epoch 16 — loss: 0.2712
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2695
Epoch 19 — loss: 0.2697
Epoch 20 — loss: 0.2692
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  81.230003
2    P(>V6)  84.220001
3    P(>V7)  87.919998
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 46.25%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4732
Epoch 02 — loss: 0.3319
Epoch 03 — loss: 0.2993
Epoch 04 — loss: 0.2907
Epoch 05 — loss: 0.2845
Epoch 06 — loss: 0.2815
Epoch 07 — loss: 0.2795
Epoch 08 — loss: 0.2756
Epoch 09 — loss: 0.2747
Epoch 10 — loss: 0.2737
Epoch 11 — loss: 0.2736
Epoch 12 — loss: 0.2728
Epoch 13 — loss: 0.2713
Epoch 14 — loss: 0.2704
Epoch 15 — loss: 0.2713
Epoch 16 — loss: 0.2712
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2702
Epoch 19 — loss: 0.2695
Epoch 20 — loss: 0.2695
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.940002
2    P(>V6)  84.070000
3    P(>V7)  87.730003
4    P(>V8)  92.400002
5    P(>V9)  97.019997
Overall accuracy: 45.33%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4202
Epoch 02 — loss: 0.3276
Epoch 03 — loss: 0.3110
Epoch 04 — loss: 0.2989
Epoch 05 — loss: 0.2916
Epoch 06 — loss: 0.2848
Epoch 07 — loss: 0.2836
Epoch 08 — loss: 0.2799
Epoch 09 — loss: 0.2784
Epoch 10 — loss: 0.2781
Epoch 11 — loss: 0.2763
Epoch 12 — loss: 0.2759
Epoch 13 — loss: 0.2739
Epoch 14 — loss: 0.2737
Epoch 15 — loss: 0.2727
Epoch 16 — loss: 0.2726
Epoch 17 — loss: 0.2743
Epoch 18 — loss: 0.2727
Epoch 19 — loss: 0.2724
Epoch 20 — loss: 0.2717
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  80.800003
2    P(>V6)  84.260002
3    P(>V7)  87.779999
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.25%
Ordinal stacking meta epoch 1: loss=0.2693
Ordinal stacking meta epoch 2: loss=0.1588
Ordinal stacking meta epoch 3: loss=0.1477
Ordinal stacking meta epoch 4: loss=0.1436
Ordinal stacking meta epoch 5: loss=0.1414
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001746 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.769997
2    P(>V6)  86.000000
3    P(>V7)  89.029999
4    P(>V8)  92.970001
5    P(>V9)  97.059998
Overall accuracy: 49.86%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  83.199997
2    P(>V6)  85.559998
3    P(>V7)  88.160004
4    P(>V8)  93.070000
5    P(>V9)  97.059998
Overall accuracy: 49.95%
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  82.190002
2    P(>V6)  84.790001
3    P(>V7)  88.019997
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 48.22%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.720001
2    P(>V6)  85.470001
3    P(>V7)  88.500000
4    P(>V8)  93.209999
5    P(>V9)  97.059998
Overall accuracy: 49.52%
  threshold   accuracy
0    P(>V4)  84.699997
1    P(>V5)  83.349998
2    P(>V6)  86.570000
3    P(>V7)  88.690002
4    P(>V8)  92.639999
5    P(>V9)  96.489998
Overall accuracy: 49.37%
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  83.730003
2    P(>V6)  85.510002
3    P(>V7)  88.400002
4    P(>V8)  92.879997
5    P(>V9)  96.489998
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  82.820000
1    P(>V5)  82.959999
2    P(>V6)  85.760002
3    P(>V7)  88.550003
4    P(>V8)  92.779999
5    P(>V9)  96.540001
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  83.110001
2    P(>V6)  86.139999
3    P(>V7)  87.970001
4    P(>V8)  92.540001
5    P(>V9)  96.680000
Overall accuracy: 49.52%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.769997
2    P(>V6)  86.000000
3    P(>V7)  89.029999
4    P(>V8)  92.970001
5    P(>V9)  97.059998
Overall accuracy: 49.86%
----------------- Ordinal iteration 19/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3808
Epoch 02 — loss: 0.3160/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:40:15] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 03 — loss: 0.2985
Epoch 04 — loss: 0.2904
Epoch 05 — loss: 0.2810
Epoch 06 — loss: 0.2701
Epoch 07 — loss: 0.2629
Epoch 08 — loss: 0.2588
Epoch 09 — loss: 0.2508
Epoch 10 — loss: 0.2425
Epoch 11 — loss: 0.2377
Epoch 12 — loss: 0.2326
Epoch 13 — loss: 0.2264
Epoch 14 — loss: 0.2201
Epoch 15 — loss: 0.2164
Epoch 16 — loss: 0.2084
Epoch 17 — loss: 0.2021
Epoch 18 — loss: 0.1983
Epoch 19 — loss: 0.1924
Epoch 20 — loss: 0.1863
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  80.940002
2    P(>V6)  84.019997
3    P(>V7)  88.209999
4    P(>V8)  92.930000
5    P(>V9)  96.820000
Overall accuracy: 49.04%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3690
Epoch 02 — loss: 0.3168
Epoch 03 — loss: 0.3028
Epoch 04 — loss: 0.2909
Epoch 05 — loss: 0.2826
Epoch 06 — loss: 0.2726
Epoch 07 — loss: 0.2627
Epoch 08 — loss: 0.2584
Epoch 09 — loss: 0.2484
Epoch 10 — loss: 0.2420
Epoch 11 — loss: 0.2350
Epoch 12 — loss: 0.2279
Epoch 13 — loss: 0.2233
Epoch 14 — loss: 0.2176
Epoch 15 — loss: 0.2113
Epoch 16 — loss: 0.2048
Epoch 17 — loss: 0.2000
Epoch 18 — loss: 0.1960
Epoch 19 — loss: 0.1886
Epoch 20 — loss: 0.1832
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.769997
2    P(>V6)  85.949997
3    P(>V7)  87.580002
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 50.67%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3721
Epoch 02 — loss: 0.3228
Epoch 03 — loss: 0.3092
Epoch 04 — loss: 0.2981
Epoch 05 — loss: 0.2936
Epoch 06 — loss: 0.2830
Epoch 07 — loss: 0.2771
Epoch 08 — loss: 0.2699
Epoch 09 — loss: 0.2649
Epoch 10 — loss: 0.2586
Epoch 11 — loss: 0.2524
Epoch 12 — loss: 0.2468
Epoch 13 — loss: 0.2390
Epoch 14 — loss: 0.2365
Epoch 15 — loss: 0.2316
Epoch 16 — loss: 0.2270
Epoch 17 — loss: 0.2215
Epoch 18 — loss: 0.2198
Epoch 19 — loss: 0.2159
Epoch 20 — loss: 0.2106
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  81.860001
2    P(>V6)  85.900002
3    P(>V7)  88.160004
4    P(>V8)  92.589996
5    P(>V9)  96.919998
Overall accuracy: 50.05%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4719
Epoch 02 — loss: 0.3499
Epoch 03 — loss: 0.3052
Epoch 04 — loss: 0.2942
Epoch 05 — loss: 0.2871
Epoch 06 — loss: 0.2829
Epoch 07 — loss: 0.2788
Epoch 08 — loss: 0.2769
Epoch 09 — loss: 0.2757
Epoch 10 — loss: 0.2730
Epoch 11 — loss: 0.2737
Epoch 12 — loss: 0.2720
Epoch 13 — loss: 0.2722
Epoch 14 — loss: 0.2711
Epoch 15 — loss: 0.2715
Epoch 16 — loss: 0.2713
Epoch 17 — loss: 0.2698
Epoch 18 — loss: 0.2709
Epoch 19 — loss: 0.2702
Epoch 20 — loss: 0.2699
  threshold   accuracy
0    P(>V4)  81.860001
1    P(>V5)  81.040001
2    P(>V6)  83.879997
3    P(>V7)  87.540001
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.10%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4641
Epoch 02 — loss: 0.3299
Epoch 03 — loss: 0.3022
Epoch 04 — loss: 0.2926
Epoch 05 — loss: 0.2880
Epoch 06 — loss: 0.2834
Epoch 07 — loss: 0.2808
Epoch 08 — loss: 0.2788
Epoch 09 — loss: 0.2771
Epoch 10 — loss: 0.2742
Epoch 11 — loss: 0.2737
Epoch 12 — loss: 0.2727
Epoch 13 — loss: 0.2725
Epoch 14 — loss: 0.2710
Epoch 15 — loss: 0.2707
Epoch 16 — loss: 0.2708
Epoch 17 — loss: 0.2709
Epoch 18 — loss: 0.2689
Epoch 19 — loss: 0.2692
Epoch 20 — loss: 0.2692
  threshold   accuracy
0    P(>V4)  81.910004
1    P(>V5)  81.040001
2    P(>V6)  84.070000
3    P(>V7)  87.680000
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.77%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4293
Epoch 02 — loss: 0.3263
Epoch 03 — loss: 0.3069
Epoch 04 — loss: 0.2972
Epoch 05 — loss: 0.2890
Epoch 06 — loss: 0.2836
Epoch 07 — loss: 0.2830
Epoch 08 — loss: 0.2793
Epoch 09 — loss: 0.2780
Epoch 10 — loss: 0.2777
Epoch 11 — loss: 0.2749
Epoch 12 — loss: 0.2755
Epoch 13 — loss: 0.2750
Epoch 14 — loss: 0.2746
Epoch 15 — loss: 0.2731
Epoch 16 — loss: 0.2726
Epoch 17 — loss: 0.2724
Epoch 18 — loss: 0.2724
Epoch 19 — loss: 0.2720
Epoch 20 — loss: 0.2709
  threshold   accuracy
0    P(>V4)  80.410004
1    P(>V5)  79.879997
2    P(>V6)  83.730003
3    P(>V7)  87.150002
4    P(>V8)  92.300003
5    P(>V9)  96.970001
Overall accuracy: 43.12%
Ordinal stacking meta epoch 1: loss=0.2162
Ordinal stacking meta epoch 2: loss=0.1545
Ordinal stacking meta epoch 3: loss=0.1478
Ordinal stacking meta epoch 4: loss=0.1450
Ordinal stacking meta epoch 5: loss=0.1431
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001588 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  83.010002
2    P(>V6)  86.000000
3    P(>V7)  88.839996
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 50.10%
  threshold   accuracy
0    P(>V4)  84.989998
1    P(>V5)  83.059998
2    P(>V6)  85.610001
3    P(>V7)  88.589996
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 50.14%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.150002
2    P(>V6)  85.470001
3    P(>V7)  88.260002
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.480003
2    P(>V6)  85.470001
3    P(>V7)  88.500000
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  84.599998
1    P(>V5)  82.239998
2    P(>V6)  85.709999
3    P(>V7)  88.550003
4    P(>V8)  93.070000
5    P(>V9)  96.779999
Overall accuracy: 48.80%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.050003
2    P(>V6)  86.239998
3    P(>V7)  88.639999
4    P(>V8)  93.550003
5    P(>V9)  96.629997
Overall accuracy: 50.10%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.580002
2    P(>V6)  85.949997
3    P(>V7)  88.930000
4    P(>V8)  93.459999
5    P(>V9)  96.540001
Overall accuracy: 50.58%
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  82.150002
2    P(>V6)  85.419998
3    P(>V7)  88.019997
4    P(>V8)  93.309998
5    P(>V9)  96.489998
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  83.010002
2    P(>V6)  86.000000
3    P(>V7)  88.839996
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 50.10%
----------------- Ordinal iteration 20/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3749
Epoch 02 — loss: 0.3132
Epoch 03 — loss: 0.3016
Epoch 04 — loss: 0.2929
Epoch 05 — loss: 0.2834
Epoch 06 — loss: 0.2773
Epoch 07 — loss: 0.2674
Epoch 08 — loss: 0.2608
Epoch 09 — loss: 0.2538
Epoch 10 — loss: 0.2496
Epoch 11 — loss: 0.2450
Epoch 12 — loss: 0.2389
Epoch 13 — loss: 0.2323
Epoch 14 — loss: 0.2278
Epoch 15 — loss: 0.2220
Epoch 16 — loss: 0.2170
Epoch 17 — loss: 0.2110
Epoch 18 — loss: 0.2059
Epoch 19 — loss: 0.2000
Epoch 20 — loss: 0.1946
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  81.470001
2    P(>V6)  84.650002
3    P(>V7)  86.379997
4    P(>V8)  92.540001
5    P(>V9)  96.680000
Overall accuracy: 48.22%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3643
Epoch 02 — loss: 0.3108
Epoch 03 — loss: 0.2979
Epoch 04 — loss: 0.2887
Epoch 05 — loss: 0.2779
Epoch 06 — loss: 0.2694
Epoch 07 — loss: 0.2637
Epoch 08 — loss: 0.2540
Epoch 09 — loss: 0.2468
Epoch 10 — loss: 0.2414
Epoch 11 — loss: 0.2323/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [04:03:46] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 12 — loss: 0.2279
Epoch 13 — loss: 0.2243
Epoch 14 — loss: 0.2165
Epoch 15 — loss: 0.2127
Epoch 16 — loss: 0.2076
Epoch 17 — loss: 0.2006
Epoch 18 — loss: 0.1961
Epoch 19 — loss: 0.1900
Epoch 20 — loss: 0.1884
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.440002
2    P(>V6)  84.070000
3    P(>V7)  86.860001
4    P(>V8)  92.160004
5    P(>V9)  96.150002
Overall accuracy: 47.83%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3711
Epoch 02 — loss: 0.3213
Epoch 03 — loss: 0.3054
Epoch 04 — loss: 0.2981
Epoch 05 — loss: 0.2870
Epoch 06 — loss: 0.2797
Epoch 07 — loss: 0.2712
Epoch 08 — loss: 0.2681
Epoch 09 — loss: 0.2595
Epoch 10 — loss: 0.2548
Epoch 11 — loss: 0.2489
Epoch 12 — loss: 0.2424
Epoch 13 — loss: 0.2394
Epoch 14 — loss: 0.2350
Epoch 15 — loss: 0.2309
Epoch 16 — loss: 0.2245
Epoch 17 — loss: 0.2213
Epoch 18 — loss: 0.2152
Epoch 19 — loss: 0.2117
Epoch 20 — loss: 0.2068
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  81.279999
2    P(>V6)  85.660004
3    P(>V7)  88.209999
4    P(>V8)  92.349998
5    P(>V9)  96.629997
Overall accuracy: 47.02%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4720
Epoch 02 — loss: 0.3554
Epoch 03 — loss: 0.3080
Epoch 04 — loss: 0.2940
Epoch 05 — loss: 0.2864
Epoch 06 — loss: 0.2828
Epoch 07 — loss: 0.2801
Epoch 08 — loss: 0.2776
Epoch 09 — loss: 0.2772
Epoch 10 — loss: 0.2744
Epoch 11 — loss: 0.2755
Epoch 12 — loss: 0.2716
Epoch 13 — loss: 0.2720
Epoch 14 — loss: 0.2709
Epoch 15 — loss: 0.2709
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2713
Epoch 18 — loss: 0.2691
Epoch 19 — loss: 0.2706
Epoch 20 — loss: 0.2677
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  81.040001
2    P(>V6)  83.730003
3    P(>V7)  87.779999
4    P(>V8)  92.730003
5    P(>V9)  97.059998
Overall accuracy: 45.62%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4718
Epoch 02 — loss: 0.3403
Epoch 03 — loss: 0.3003
Epoch 04 — loss: 0.2895
Epoch 05 — loss: 0.2832
Epoch 06 — loss: 0.2791
Epoch 07 — loss: 0.2771
Epoch 08 — loss: 0.2762
Epoch 09 — loss: 0.2755
Epoch 10 — loss: 0.2741
Epoch 11 — loss: 0.2732
Epoch 12 — loss: 0.2723
Epoch 13 — loss: 0.2704
Epoch 14 — loss: 0.2711
Epoch 15 — loss: 0.2705
Epoch 16 — loss: 0.2698
Epoch 17 — loss: 0.2708
Epoch 18 — loss: 0.2728
Epoch 19 — loss: 0.2688
Epoch 20 — loss: 0.2700
  threshold   accuracy
0    P(>V4)  80.220001
1    P(>V5)  80.080002
2    P(>V6)  83.449997
3    P(>V7)  87.580002
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 46.25%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4271
Epoch 02 — loss: 0.3253
Epoch 03 — loss: 0.3141
Epoch 04 — loss: 0.3029
Epoch 05 — loss: 0.2938
Epoch 06 — loss: 0.2882
Epoch 07 — loss: 0.2873
Epoch 08 — loss: 0.2820
Epoch 09 — loss: 0.2809
Epoch 10 — loss: 0.2774
Epoch 11 — loss: 0.2766
Epoch 12 — loss: 0.2744
Epoch 13 — loss: 0.2747
Epoch 14 — loss: 0.2729
Epoch 15 — loss: 0.2731
Epoch 16 — loss: 0.2740
Epoch 17 — loss: 0.2721
Epoch 18 — loss: 0.2751
Epoch 19 — loss: 0.2725
Epoch 20 — loss: 0.2713
  threshold   accuracy
0    P(>V4)  81.040001
1    P(>V5)  80.320000
2    P(>V6)  83.010002
3    P(>V7)  86.959999
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 44.27%
Ordinal stacking meta epoch 1: loss=0.2282
Ordinal stacking meta epoch 2: loss=0.1532
Ordinal stacking meta epoch 3: loss=0.1479
Ordinal stacking meta epoch 4: loss=0.1457
Ordinal stacking meta epoch 5: loss=0.1442
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001576 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.440002
2    P(>V6)  86.000000
3    P(>V7)  88.309998
4    P(>V8)  93.169998
5    P(>V9)  97.059998
Overall accuracy: 49.13%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.720001
2    P(>V6)  85.849998
3    P(>V7)  88.070000
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 49.62%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.239998
2    P(>V6)  85.419998
3    P(>V7)  88.070000
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.190002
2    P(>V6)  85.709999
3    P(>V7)  88.400002
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  83.300003
2    P(>V6)  85.129997
3    P(>V7)  87.580002
4    P(>V8)  92.589996
5    P(>V9)  96.339996
Overall accuracy: 47.35%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.720001
2    P(>V6)  85.419998
3    P(>V7)  87.580002
4    P(>V8)  92.879997
5    P(>V9)  96.440002
Overall accuracy: 49.52%
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  82.239998
2    P(>V6)  85.269997
3    P(>V7)  87.440002
4    P(>V8)  92.779999
5    P(>V9)  96.290001
Overall accuracy: 48.36%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.000000
2    P(>V6)  85.320000
3    P(>V7)  87.540001
4    P(>V8)  92.690002
5    P(>V9)  96.389999
Overall accuracy: 48.03%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.440002
2    P(>V6)  86.000000
3    P(>V7)  88.309998
4    P(>V8)  93.169998
5    P(>V9)  97.059998
Overall accuracy: 49.13%
----------------- Ordinal iteration 21/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3855
Epoch 02 — loss: 0.3229
Epoch 03 — loss: 0.3066
Epoch 04 — loss: 0.2959
Epoch 05 — loss: 0.2825
Epoch 06 — loss: 0.2735
Epoch 07 — loss: 0.2635
Epoch 08 — loss: 0.2549
Epoch 09 — loss: 0.2469
Epoch 10 — loss: 0.2382
Epoch 11 — loss: 0.2363
Epoch 12 — loss: 0.2284
Epoch 13 — loss: 0.2214
Epoch 14 — loss: 0.2151
Epoch 15 — loss: 0.2091
Epoch 16 — loss: 0.2038
Epoch 17 — loss: 0.1979
Epoch 18 — loss: 0.1932
Epoch 19 — loss: 0.1850
Epoch 20 — loss: 0.1813
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  81.139999
2    P(>V6)  83.830002
3    P(>V7)  87.779999
4    P(>V8)  92.250000
5    P(>V9)  96.290001
Overall accuracy: 47.64%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3725
Epoch 02 — loss: 0.3098
Epoch 03 — loss: 0.2968
Epoch 04 — loss: 0.2857
Epoch 05 — loss: 0.2816
Epoch 06 — loss: 0.2721
Epoch 07 — loss: 0.2604
Epoch 08 — loss: 0.2530
Epoch 09 — loss: 0.2462
Epoch 10 — loss: 0.2410
Epoch 11 — loss: 0.2312
Epoch 12 — loss: 0.2248
Epoch 13 — loss: 0.2208
Epoch 14 — loss: 0.2151
Epoch 15 — loss: 0.2077
Epoch 16 — loss: 0.2035
Epoch 17 — loss: 0.1954
Epoch 18 — loss: 0.1905
Epoch 19 — loss: 0.1869
Epoch 20 — loss: 0.1811
  threshold   accuracy
0    P(>V4)  82.919998
1    P(>V5)  81.620003
2    P(>V6)  84.790001
3    P(>V7)  88.160004
4    P(>V8)  92.440002
5    P(>V9)  96.820000
Overall accuracy: 48.70%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3723
Epoch 02 — loss: 0.3191
Epoch 03 — loss: 0.3071
Epoch 04 — loss: 0.2974
Epoch 05 — loss: 0.2906
Epoch 06 — loss: 0.2798
Epoch 07 — loss: 0.2753
Epoch 08 — loss: 0.2708
Epoch 09 — loss: 0.2617
Epoch 10 — loss: 0.2588
Epoch 11 — loss: 0.2531
Epoch 12 — loss: 0.2442
Epoch 13 — loss: 0.2416
Epoch 14 — loss: 0.2364
Epoch 15 — loss: 0.2309
Epoch 16 — loss: 0.2284
Epoch 17 — loss: 0.2236
Epoch 18 — loss: 0.2170
Epoch 19 — loss: 0.2120
Epoch 20 — loss: 0.2085/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [04:27:07] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.290001
2    P(>V6)  85.709999
3    P(>V7)  87.300003
4    P(>V8)  92.400002
5    P(>V9)  96.250000
Overall accuracy: 46.97%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4708
Epoch 02 — loss: 0.3514
Epoch 03 — loss: 0.3005
Epoch 04 — loss: 0.2892
Epoch 05 — loss: 0.2826
Epoch 06 — loss: 0.2810
Epoch 07 — loss: 0.2774
Epoch 08 — loss: 0.2749
Epoch 09 — loss: 0.2742
Epoch 10 — loss: 0.2739
Epoch 11 — loss: 0.2721
Epoch 12 — loss: 0.2738
Epoch 13 — loss: 0.2737
Epoch 14 — loss: 0.2709
Epoch 15 — loss: 0.2709
Epoch 16 — loss: 0.2719
Epoch 17 — loss: 0.2699
Epoch 18 — loss: 0.2699
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2706
  threshold   accuracy
0    P(>V4)  80.699997
1    P(>V5)  80.459999
2    P(>V6)  83.250000
3    P(>V7)  87.919998
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 46.49%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4708
Epoch 02 — loss: 0.3361
Epoch 03 — loss: 0.3020
Epoch 04 — loss: 0.2925
Epoch 05 — loss: 0.2871
Epoch 06 — loss: 0.2834
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2767
Epoch 09 — loss: 0.2761
Epoch 10 — loss: 0.2761
Epoch 11 — loss: 0.2748
Epoch 12 — loss: 0.2716
Epoch 13 — loss: 0.2740
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2710
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2696
Epoch 18 — loss: 0.2697
Epoch 19 — loss: 0.2693
Epoch 20 — loss: 0.2698
  threshold   accuracy
0    P(>V4)  81.910004
1    P(>V5)  80.900002
2    P(>V6)  84.019997
3    P(>V7)  87.629997
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.29%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4336
Epoch 02 — loss: 0.3289
Epoch 03 — loss: 0.3132
Epoch 04 — loss: 0.3023
Epoch 05 — loss: 0.2935
Epoch 06 — loss: 0.2874
Epoch 07 — loss: 0.2853
Epoch 08 — loss: 0.2825
Epoch 09 — loss: 0.2799
Epoch 10 — loss: 0.2800
Epoch 11 — loss: 0.2773
Epoch 12 — loss: 0.2745
Epoch 13 — loss: 0.2774
Epoch 14 — loss: 0.2737
Epoch 15 — loss: 0.2731
Epoch 16 — loss: 0.2719
Epoch 17 — loss: 0.2724
Epoch 18 — loss: 0.2736
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2710
  threshold   accuracy
0    P(>V4)  80.699997
1    P(>V5)  80.269997
2    P(>V6)  83.639999
3    P(>V7)  87.919998
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 46.29%
Ordinal stacking meta epoch 1: loss=0.2314
Ordinal stacking meta epoch 2: loss=0.1552
Ordinal stacking meta epoch 3: loss=0.1483
Ordinal stacking meta epoch 4: loss=0.1450
Ordinal stacking meta epoch 5: loss=0.1431
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001509 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.959999
2    P(>V6)  86.040001
3    P(>V7)  88.839996
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 50.34%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.820000
2    P(>V6)  85.660004
3    P(>V7)  88.550003
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 50.00%
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  81.279999
2    P(>V6)  84.650002
3    P(>V7)  88.070000
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.629997
2    P(>V6)  85.510002
3    P(>V7)  88.839996
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.099998
2    P(>V6)  85.470001
3    P(>V7)  88.589996
4    P(>V8)  92.730003
5    P(>V9)  96.779999
Overall accuracy: 47.74%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.580002
2    P(>V6)  85.709999
3    P(>V7)  88.400002
4    P(>V8)  92.300003
5    P(>V9)  96.339996
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  81.470001
2    P(>V6)  85.660004
3    P(>V7)  88.110001
4    P(>V8)  92.300003
5    P(>V9)  96.389999
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.389999
2    P(>V6)  85.180000
3    P(>V7)  88.349998
4    P(>V8)  92.400002
5    P(>V9)  96.540001
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.959999
2    P(>V6)  86.040001
3    P(>V7)  88.839996
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 50.34%
----------------- Ordinal iteration 22/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3721
Epoch 02 — loss: 0.3199
Epoch 03 — loss: 0.3050
Epoch 04 — loss: 0.2953
Epoch 05 — loss: 0.2869
Epoch 06 — loss: 0.2759
Epoch 07 — loss: 0.2682
Epoch 08 — loss: 0.2603
Epoch 09 — loss: 0.2540
Epoch 10 — loss: 0.2460
Epoch 11 — loss: 0.2405
Epoch 12 — loss: 0.2335
Epoch 13 — loss: 0.2264
Epoch 14 — loss: 0.2224
Epoch 15 — loss: 0.2157
Epoch 16 — loss: 0.2118
Epoch 17 — loss: 0.2044
Epoch 18 — loss: 0.1983
Epoch 19 — loss: 0.1935
Epoch 20 — loss: 0.1885
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  80.459999
2    P(>V6)  84.309998
3    P(>V7)  88.349998
4    P(>V8)  93.019997
5    P(>V9)  96.580002
Overall accuracy: 49.09%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3722
Epoch 02 — loss: 0.3080
Epoch 03 — loss: 0.2956
Epoch 04 — loss: 0.2866
Epoch 05 — loss: 0.2832
Epoch 06 — loss: 0.2750
Epoch 07 — loss: 0.2693
Epoch 08 — loss: 0.2614
Epoch 09 — loss: 0.2539
Epoch 10 — loss: 0.2483
Epoch 11 — loss: 0.2398
Epoch 12 — loss: 0.2378
Epoch 13 — loss: 0.2293
Epoch 14 — loss: 0.2237
Epoch 15 — loss: 0.2205
Epoch 16 — loss: 0.2129
Epoch 17 — loss: 0.2083
Epoch 18 — loss: 0.2034
Epoch 19 — loss: 0.1975
Epoch 20 — loss: 0.1913
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  81.470001
2    P(>V6)  85.470001
3    P(>V7)  87.820000
4    P(>V8)  93.070000
5    P(>V9)  97.110001
Overall accuracy: 50.05%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3487
Epoch 02 — loss: 0.3081
Epoch 03 — loss: 0.2990
Epoch 04 — loss: 0.2916
Epoch 05 — loss: 0.2877
Epoch 06 — loss: 0.2827
Epoch 07 — loss: 0.2786
Epoch 08 — loss: 0.2701
Epoch 09 — loss: 0.2637
Epoch 10 — loss: 0.2593
Epoch 11 — loss: 0.2532
Epoch 12 — loss: 0.2488
Epoch 13 — loss: 0.2429
Epoch 14 — loss: 0.2352
Epoch 15 — loss: 0.2308
Epoch 16 — loss: 0.2235
Epoch 17 — loss: 0.2199
Epoch 18 — loss: 0.2144
Epoch 19 — loss: 0.2104
Epoch 20 — loss: 0.2041
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.720001
2    P(>V6)  84.309998
3    P(>V7)  87.440002
4    P(>V8)  92.970001
5    P(>V9)  96.779999
Overall accuracy: 48.03%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4709
Epoch 02 — loss: 0.3408
Epoch 03 — loss: 0.2960
Epoch 04 — loss: 0.2880
Epoch 05 — loss: 0.2845
Epoch 06 — loss: 0.2815
Epoch 07 — loss: 0.2787
Epoch 08 — loss: 0.2769
Epoch 09 — loss: 0.2768
Epoch 10 — loss: 0.2747
Epoch 11 — loss: 0.2738
Epoch 12 — loss: 0.2734
Epoch 13 — loss: 0.2736
Epoch 14 — loss: 0.2711
Epoch 15 — loss: 0.2702
Epoch 16 — loss: 0.2712
Epoch 17 — loss: 0.2721
Epoch 18 — loss: 0.2701
Epoch 19 — loss: 0.2699
Epoch 20 — loss: 0.2701
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  80.410004
2    P(>V6)  83.639999
3    P(>V7)  87.730003
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.20%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4703/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [04:50:27] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 02 — loss: 0.3283
Epoch 03 — loss: 0.2979
Epoch 04 — loss: 0.2875
Epoch 05 — loss: 0.2841
Epoch 06 — loss: 0.2798
Epoch 07 — loss: 0.2789
Epoch 08 — loss: 0.2767
Epoch 09 — loss: 0.2745
Epoch 10 — loss: 0.2738
Epoch 11 — loss: 0.2731
Epoch 12 — loss: 0.2717
Epoch 13 — loss: 0.2716
Epoch 14 — loss: 0.2717
Epoch 15 — loss: 0.2710
Epoch 16 — loss: 0.2701
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2684
Epoch 19 — loss: 0.2698
Epoch 20 — loss: 0.2683
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.800003
2    P(>V6)  83.250000
3    P(>V7)  87.440002
4    P(>V8)  92.690002
5    P(>V9)  96.970001
Overall accuracy: 45.67%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4224
Epoch 02 — loss: 0.3208
Epoch 03 — loss: 0.3075
Epoch 04 — loss: 0.2995
Epoch 05 — loss: 0.2936
Epoch 06 — loss: 0.2875
Epoch 07 — loss: 0.2851
Epoch 08 — loss: 0.2807
Epoch 09 — loss: 0.2799
Epoch 10 — loss: 0.2771
Epoch 11 — loss: 0.2754
Epoch 12 — loss: 0.2764
Epoch 13 — loss: 0.2774
Epoch 14 — loss: 0.2745
Epoch 15 — loss: 0.2758
Epoch 16 — loss: 0.2730
Epoch 17 — loss: 0.2725
Epoch 18 — loss: 0.2710
Epoch 19 — loss: 0.2717
Epoch 20 — loss: 0.2709
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  81.089996
2    P(>V6)  83.639999
3    P(>V7)  88.209999
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.33%
Ordinal stacking meta epoch 1: loss=0.1936
Ordinal stacking meta epoch 2: loss=0.1513
Ordinal stacking meta epoch 3: loss=0.1473
Ordinal stacking meta epoch 4: loss=0.1455
Ordinal stacking meta epoch 5: loss=0.1443
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001596 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  82.480003
2    P(>V6)  85.510002
3    P(>V7)  88.839996
4    P(>V8)  93.169998
5    P(>V9)  97.019997
Overall accuracy: 50.29%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.000000
2    P(>V6)  85.230003
3    P(>V7)  88.349998
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  81.760002
2    P(>V6)  84.459999
3    P(>V7)  88.309998
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.389999
2    P(>V6)  85.269997
3    P(>V7)  88.930000
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 50.14%
  threshold   accuracy
0    P(>V4)  84.699997
1    P(>V5)  82.769997
2    P(>V6)  85.949997
3    P(>V7)  88.879997
4    P(>V8)  93.120003
5    P(>V9)  96.489998
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.389999
2    P(>V6)  86.000000
3    P(>V7)  88.500000
4    P(>V8)  93.070000
5    P(>V9)  96.629997
Overall accuracy: 49.81%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.339996
2    P(>V6)  85.709999
3    P(>V7)  88.500000
4    P(>V8)  93.120003
5    P(>V9)  96.580002
Overall accuracy: 50.00%
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.150002
2    P(>V6)  85.760002
3    P(>V7)  88.160004
4    P(>V8)  93.169998
5    P(>V9)  96.629997
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  82.480003
2    P(>V6)  85.510002
3    P(>V7)  88.839996
4    P(>V8)  93.169998
5    P(>V9)  97.019997
Overall accuracy: 50.29%
----------------- Ordinal iteration 23/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3688
Epoch 02 — loss: 0.3101
Epoch 03 — loss: 0.2955
Epoch 04 — loss: 0.2890
Epoch 05 — loss: 0.2792
Epoch 06 — loss: 0.2700
Epoch 07 — loss: 0.2636
Epoch 08 — loss: 0.2578
Epoch 09 — loss: 0.2485
Epoch 10 — loss: 0.2423
Epoch 11 — loss: 0.2360
Epoch 12 — loss: 0.2296
Epoch 13 — loss: 0.2250
Epoch 14 — loss: 0.2178
Epoch 15 — loss: 0.2154
Epoch 16 — loss: 0.2065
Epoch 17 — loss: 0.2012
Epoch 18 — loss: 0.1979
Epoch 19 — loss: 0.1912
Epoch 20 — loss: 0.1850
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  81.139999
2    P(>V6)  84.220001
3    P(>V7)  87.099998
4    P(>V8)  92.639999
5    P(>V9)  96.540001
Overall accuracy: 46.97%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3760
Epoch 02 — loss: 0.3160
Epoch 03 — loss: 0.2975
Epoch 04 — loss: 0.2902
Epoch 05 — loss: 0.2794
Epoch 06 — loss: 0.2696
Epoch 07 — loss: 0.2585
Epoch 08 — loss: 0.2528
Epoch 09 — loss: 0.2455
Epoch 10 — loss: 0.2416
Epoch 11 — loss: 0.2353
Epoch 12 — loss: 0.2288
Epoch 13 — loss: 0.2256
Epoch 14 — loss: 0.2184
Epoch 15 — loss: 0.2124
Epoch 16 — loss: 0.2101
Epoch 17 — loss: 0.2040
Epoch 18 — loss: 0.1983
Epoch 19 — loss: 0.1932
Epoch 20 — loss: 0.1900
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.720001
2    P(>V6)  85.849998
3    P(>V7)  87.489998
4    P(>V8)  93.169998
5    P(>V9)  96.870003
Overall accuracy: 48.32%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3639
Epoch 02 — loss: 0.3123
Epoch 03 — loss: 0.2990
Epoch 04 — loss: 0.2921
Epoch 05 — loss: 0.2853
Epoch 06 — loss: 0.2774
Epoch 07 — loss: 0.2743
Epoch 08 — loss: 0.2670
Epoch 09 — loss: 0.2617
Epoch 10 — loss: 0.2564
Epoch 11 — loss: 0.2503
Epoch 12 — loss: 0.2458
Epoch 13 — loss: 0.2413
Epoch 14 — loss: 0.2357
Epoch 15 — loss: 0.2309
Epoch 16 — loss: 0.2279
Epoch 17 — loss: 0.2187
Epoch 18 — loss: 0.2169
Epoch 19 — loss: 0.2090
Epoch 20 — loss: 0.2048
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  81.230003
2    P(>V6)  85.419998
3    P(>V7)  87.489998
4    P(>V8)  92.970001
5    P(>V9)  96.970001
Overall accuracy: 47.74%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4745
Epoch 02 — loss: 0.3467
Epoch 03 — loss: 0.3071
Epoch 04 — loss: 0.2945
Epoch 05 — loss: 0.2873
Epoch 06 — loss: 0.2830
Epoch 07 — loss: 0.2814
Epoch 08 — loss: 0.2775
Epoch 09 — loss: 0.2750
Epoch 10 — loss: 0.2743
Epoch 11 — loss: 0.2736
Epoch 12 — loss: 0.2721
Epoch 13 — loss: 0.2719
Epoch 14 — loss: 0.2714
Epoch 15 — loss: 0.2724
Epoch 16 — loss: 0.2694
Epoch 17 — loss: 0.2699
Epoch 18 — loss: 0.2704
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2698
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  80.900002
2    P(>V6)  83.730003
3    P(>V7)  87.389999
4    P(>V8)  92.779999
5    P(>V9)  97.059998
Overall accuracy: 46.05%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4574
Epoch 02 — loss: 0.3263
Epoch 03 — loss: 0.2970
Epoch 04 — loss: 0.2891
Epoch 05 — loss: 0.2833
Epoch 06 — loss: 0.2808
Epoch 07 — loss: 0.2794
Epoch 08 — loss: 0.2766
Epoch 09 — loss: 0.2756
Epoch 10 — loss: 0.2733
Epoch 11 — loss: 0.2730
Epoch 12 — loss: 0.2715
Epoch 13 — loss: 0.2701
Epoch 14 — loss: 0.2706
Epoch 15 — loss: 0.2718
Epoch 16 — loss: 0.2705
Epoch 17 — loss: 0.2696
Epoch 18 — loss: 0.2688
Epoch 19 — loss: 0.2690
Epoch 20 — loss: 0.2695
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  81.139999
2    P(>V6)  83.489998
3    P(>V7)  87.730003
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 45.38%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4165
Epoch 02 — loss: 0.3232
Epoch 03 — loss: 0.3063
Epoch 04 — loss: 0.2962
Epoch 05 — loss: 0.2868
Epoch 06 — loss: 0.2846
Epoch 07 — loss: 0.2824
Epoch 08 — loss: 0.2765
Epoch 09 — loss: 0.2766
Epoch 10 — loss: 0.2760/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [05:13:43] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [05:37:13] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 11 — loss: 0.2757
Epoch 12 — loss: 0.2741
Epoch 13 — loss: 0.2734
Epoch 14 — loss: 0.2732
Epoch 15 — loss: 0.2733
Epoch 16 — loss: 0.2711
Epoch 17 — loss: 0.2727
Epoch 18 — loss: 0.2718
Epoch 19 — loss: 0.2721
Epoch 20 — loss: 0.2703
  threshold   accuracy
0    P(>V4)  80.849998
1    P(>V5)  80.269997
2    P(>V6)  83.639999
3    P(>V7)  87.680000
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 43.55%
Ordinal stacking meta epoch 1: loss=0.2462
Ordinal stacking meta epoch 2: loss=0.1585
Ordinal stacking meta epoch 3: loss=0.1512
Ordinal stacking meta epoch 4: loss=0.1480
Ordinal stacking meta epoch 5: loss=0.1459
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001559 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.599998
1    P(>V5)  82.190002
2    P(>V6)  85.660004
3    P(>V7)  88.500000
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 49.13%
  threshold   accuracy
0    P(>V4)  84.790001
1    P(>V5)  82.290001
2    P(>V6)  85.709999
3    P(>V7)  88.500000
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.000000
2    P(>V6)  85.029999
3    P(>V7)  87.870003
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  82.099998
2    P(>V6)  85.470001
3    P(>V7)  88.160004
4    P(>V8)  93.169998
5    P(>V9)  97.019997
Overall accuracy: 48.80%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  83.250000
2    P(>V6)  85.849998
3    P(>V7)  88.209999
4    P(>V8)  92.730003
5    P(>V9)  96.820000
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  84.839996
1    P(>V5)  83.160004
2    P(>V6)  86.139999
3    P(>V7)  88.449997
4    P(>V8)  92.639999
5    P(>V9)  96.489998
Overall accuracy: 50.10%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  83.160004
2    P(>V6)  85.760002
3    P(>V7)  87.779999
4    P(>V8)  92.349998
5    P(>V9)  96.629997
Overall accuracy: 49.33%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  83.589996
2    P(>V6)  85.660004
3    P(>V7)  87.820000
4    P(>V8)  92.400002
5    P(>V9)  96.580002
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  84.599998
1    P(>V5)  82.190002
2    P(>V6)  85.660004
3    P(>V7)  88.500000
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 49.13%
----------------- Ordinal iteration 24/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3737
Epoch 02 — loss: 0.3189
Epoch 03 — loss: 0.3019
Epoch 04 — loss: 0.2899
Epoch 05 — loss: 0.2833
Epoch 06 — loss: 0.2738
Epoch 07 — loss: 0.2653
Epoch 08 — loss: 0.2579
Epoch 09 — loss: 0.2504
Epoch 10 — loss: 0.2434
Epoch 11 — loss: 0.2387
Epoch 12 — loss: 0.2315
Epoch 13 — loss: 0.2281
Epoch 14 — loss: 0.2227
Epoch 15 — loss: 0.2143
Epoch 16 — loss: 0.2113
Epoch 17 — loss: 0.2091
Epoch 18 — loss: 0.1997
Epoch 19 — loss: 0.1952
Epoch 20 — loss: 0.1914
  threshold   accuracy
0    P(>V4)  83.160004
1    P(>V5)  82.000000
2    P(>V6)  84.889999
3    P(>V7)  88.070000
4    P(>V8)  92.160004
5    P(>V9)  96.680000
Overall accuracy: 48.03%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3745
Epoch 02 — loss: 0.3142
Epoch 03 — loss: 0.2976
Epoch 04 — loss: 0.2877
Epoch 05 — loss: 0.2764
Epoch 06 — loss: 0.2693
Epoch 07 — loss: 0.2588
Epoch 08 — loss: 0.2506
Epoch 09 — loss: 0.2440
Epoch 10 — loss: 0.2374
Epoch 11 — loss: 0.2308
Epoch 12 — loss: 0.2262
Epoch 13 — loss: 0.2199
Epoch 14 — loss: 0.2144
Epoch 15 — loss: 0.2117
Epoch 16 — loss: 0.2038
Epoch 17 — loss: 0.1968
Epoch 18 — loss: 0.1926
Epoch 19 — loss: 0.1853
Epoch 20 — loss: 0.1826
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  81.620003
2    P(>V6)  85.470001
3    P(>V7)  87.680000
4    P(>V8)  91.580002
5    P(>V9)  95.570000
Overall accuracy: 48.03%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3748
Epoch 02 — loss: 0.3208
Epoch 03 — loss: 0.3063
Epoch 04 — loss: 0.2995
Epoch 05 — loss: 0.2902
Epoch 06 — loss: 0.2815
Epoch 07 — loss: 0.2779
Epoch 08 — loss: 0.2700
Epoch 09 — loss: 0.2638
Epoch 10 — loss: 0.2576
Epoch 11 — loss: 0.2514
Epoch 12 — loss: 0.2463
Epoch 13 — loss: 0.2413
Epoch 14 — loss: 0.2375
Epoch 15 — loss: 0.2325
Epoch 16 — loss: 0.2266
Epoch 17 — loss: 0.2227
Epoch 18 — loss: 0.2175
Epoch 19 — loss: 0.2124
Epoch 20 — loss: 0.2099
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  81.279999
2    P(>V6)  84.500000
3    P(>V7)  87.389999
4    P(>V8)  92.879997
5    P(>V9)  97.059998
Overall accuracy: 47.55%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4658
Epoch 02 — loss: 0.3439
Epoch 03 — loss: 0.3038
Epoch 04 — loss: 0.2914
Epoch 05 — loss: 0.2856
Epoch 06 — loss: 0.2821
Epoch 07 — loss: 0.2814
Epoch 08 — loss: 0.2784
Epoch 09 — loss: 0.2780
Epoch 10 — loss: 0.2758
Epoch 11 — loss: 0.2745
Epoch 12 — loss: 0.2743
Epoch 13 — loss: 0.2724
Epoch 14 — loss: 0.2728
Epoch 15 — loss: 0.2718
Epoch 16 — loss: 0.2718
Epoch 17 — loss: 0.2699
Epoch 18 — loss: 0.2706
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2691
  threshold   accuracy
0    P(>V4)  81.809998
1    P(>V5)  80.940002
2    P(>V6)  84.019997
3    P(>V7)  87.540001
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.82%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4658
Epoch 02 — loss: 0.3333
Epoch 03 — loss: 0.3009
Epoch 04 — loss: 0.2903
Epoch 05 — loss: 0.2859
Epoch 06 — loss: 0.2809
Epoch 07 — loss: 0.2801
Epoch 08 — loss: 0.2777
Epoch 09 — loss: 0.2746
Epoch 10 — loss: 0.2742
Epoch 11 — loss: 0.2750
Epoch 12 — loss: 0.2734
Epoch 13 — loss: 0.2720
Epoch 14 — loss: 0.2720
Epoch 15 — loss: 0.2726
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2700
Epoch 18 — loss: 0.2695
Epoch 19 — loss: 0.2697
Epoch 20 — loss: 0.2693
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  79.879997
2    P(>V6)  83.830002
3    P(>V7)  86.480003
4    P(>V8)  92.010002
5    P(>V9)  97.019997
Overall accuracy: 44.32%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4389
Epoch 02 — loss: 0.3243
Epoch 03 — loss: 0.3067
Epoch 04 — loss: 0.2972
Epoch 05 — loss: 0.2944
Epoch 06 — loss: 0.2893
Epoch 07 — loss: 0.2864
Epoch 08 — loss: 0.2811
Epoch 09 — loss: 0.2799
Epoch 10 — loss: 0.2781
Epoch 11 — loss: 0.2764
Epoch 12 — loss: 0.2768
Epoch 13 — loss: 0.2749
Epoch 14 — loss: 0.2750
Epoch 15 — loss: 0.2742
Epoch 16 — loss: 0.2730
Epoch 17 — loss: 0.2724
Epoch 18 — loss: 0.2735
Epoch 19 — loss: 0.2715
Epoch 20 — loss: 0.2709
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.510002
2    P(>V6)  83.779999
3    P(>V7)  87.489998
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 44.32%
Ordinal stacking meta epoch 1: loss=0.2610
Ordinal stacking meta epoch 2: loss=0.1627
Ordinal stacking meta epoch 3: loss=0.1521
Ordinal stacking meta epoch 4: loss=0.1478
Ordinal stacking meta epoch 5: loss=0.1456
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001530 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [06:00:41] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.440002
2    P(>V6)  86.089996
3    P(>V7)  88.500000
4    P(>V8)  92.879997
5    P(>V9)  97.059998
Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.290001
2    P(>V6)  85.849998
3    P(>V7)  88.739998
4    P(>V8)  93.260002
5    P(>V9)  97.019997
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.239998
2    P(>V6)  85.419998
3    P(>V7)  88.309998
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.239998
2    P(>V6)  85.760002
3    P(>V7)  88.260002
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  81.949997
2    P(>V6)  86.480003
3    P(>V7)  89.080002
4    P(>V8)  92.779999
5    P(>V9)  96.730003
Overall accuracy: 48.08%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.389999
2    P(>V6)  86.379997
3    P(>V7)  89.080002
4    P(>V8)  92.349998
5    P(>V9)  96.629997
Overall accuracy: 49.62%
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  82.190002
2    P(>V6)  86.620003
3    P(>V7)  89.029999
4    P(>V8)  92.489998
5    P(>V9)  96.540001
Overall accuracy: 48.99%
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  82.099998
2    P(>V6)  86.190002
3    P(>V7)  88.790001
4    P(>V8)  92.349998
5    P(>V9)  96.680000
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.440002
2    P(>V6)  86.089996
3    P(>V7)  88.500000
4    P(>V8)  92.879997
5    P(>V9)  97.059998
Overall accuracy: 49.18%
----------------- Ordinal iteration 25/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3717
Epoch 02 — loss: 0.3177
Epoch 03 — loss: 0.3036
Epoch 04 — loss: 0.2965
Epoch 05 — loss: 0.2857
Epoch 06 — loss: 0.2759
Epoch 07 — loss: 0.2678
Epoch 08 — loss: 0.2618
Epoch 09 — loss: 0.2525
Epoch 10 — loss: 0.2453
Epoch 11 — loss: 0.2397
Epoch 12 — loss: 0.2317
Epoch 13 — loss: 0.2265
Epoch 14 — loss: 0.2190
Epoch 15 — loss: 0.2127
Epoch 16 — loss: 0.2059
Epoch 17 — loss: 0.2021
Epoch 18 — loss: 0.1956
Epoch 19 — loss: 0.1899
Epoch 20 — loss: 0.1824
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  82.290001
2    P(>V6)  85.419998
3    P(>V7)  88.160004
4    P(>V8)  92.690002
5    P(>V9)  96.730003
Overall accuracy: 48.80%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3624
Epoch 02 — loss: 0.3107
Epoch 03 — loss: 0.2961
Epoch 04 — loss: 0.2878
Epoch 05 — loss: 0.2793
Epoch 06 — loss: 0.2707
Epoch 07 — loss: 0.2641
Epoch 08 — loss: 0.2540
Epoch 09 — loss: 0.2478
Epoch 10 — loss: 0.2408
Epoch 11 — loss: 0.2353
Epoch 12 — loss: 0.2310
Epoch 13 — loss: 0.2251
Epoch 14 — loss: 0.2187
Epoch 15 — loss: 0.2127
Epoch 16 — loss: 0.2081
Epoch 17 — loss: 0.2010
Epoch 18 — loss: 0.1965
Epoch 19 — loss: 0.1905
Epoch 20 — loss: 0.1873
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  81.230003
2    P(>V6)  85.180000
3    P(>V7)  86.959999
4    P(>V8)  92.879997
5    P(>V9)  97.059998
Overall accuracy: 48.65%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3668
Epoch 02 — loss: 0.3170
Epoch 03 — loss: 0.3039
Epoch 04 — loss: 0.2969
Epoch 05 — loss: 0.2869
Epoch 06 — loss: 0.2810
Epoch 07 — loss: 0.2744
Epoch 08 — loss: 0.2702
Epoch 09 — loss: 0.2642
Epoch 10 — loss: 0.2585
Epoch 11 — loss: 0.2530
Epoch 12 — loss: 0.2475
Epoch 13 — loss: 0.2432
Epoch 14 — loss: 0.2379
Epoch 15 — loss: 0.2343
Epoch 16 — loss: 0.2294
Epoch 17 — loss: 0.2240
Epoch 18 — loss: 0.2178
Epoch 19 — loss: 0.2137
Epoch 20 — loss: 0.2092
  threshold   accuracy
0    P(>V4)  83.160004
1    P(>V5)  83.110001
2    P(>V6)  85.900002
3    P(>V7)  87.970001
4    P(>V8)  92.639999
5    P(>V9)  96.970001
Overall accuracy: 49.18%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4594
Epoch 02 — loss: 0.3284
Epoch 03 — loss: 0.2984
Epoch 04 — loss: 0.2899
Epoch 05 — loss: 0.2832
Epoch 06 — loss: 0.2812
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2776
Epoch 09 — loss: 0.2758
Epoch 10 — loss: 0.2736
Epoch 11 — loss: 0.2734
Epoch 12 — loss: 0.2722
Epoch 13 — loss: 0.2715
Epoch 14 — loss: 0.2712
Epoch 15 — loss: 0.2702
Epoch 16 — loss: 0.2710
Epoch 17 — loss: 0.2703
Epoch 18 — loss: 0.2705
Epoch 19 — loss: 0.2693
Epoch 20 — loss: 0.2686
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  80.699997
2    P(>V6)  83.779999
3    P(>V7)  87.580002
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.44%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4688
Epoch 02 — loss: 0.3364
Epoch 03 — loss: 0.2987
Epoch 04 — loss: 0.2903
Epoch 05 — loss: 0.2844
Epoch 06 — loss: 0.2815
Epoch 07 — loss: 0.2784
Epoch 08 — loss: 0.2771
Epoch 09 — loss: 0.2747
Epoch 10 — loss: 0.2763
Epoch 11 — loss: 0.2744
Epoch 12 — loss: 0.2719
Epoch 13 — loss: 0.2723
Epoch 14 — loss: 0.2709
Epoch 15 — loss: 0.2713
Epoch 16 — loss: 0.2705
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2695
Epoch 19 — loss: 0.2695
Epoch 20 — loss: 0.2696
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.650002
2    P(>V6)  83.970001
3    P(>V7)  87.629997
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 45.48%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4375
Epoch 02 — loss: 0.3253
Epoch 03 — loss: 0.3089
Epoch 04 — loss: 0.2993
Epoch 05 — loss: 0.2928
Epoch 06 — loss: 0.2856
Epoch 07 — loss: 0.2834
Epoch 08 — loss: 0.2824
Epoch 09 — loss: 0.2762
Epoch 10 — loss: 0.2784
Epoch 11 — loss: 0.2758
Epoch 12 — loss: 0.2755
Epoch 13 — loss: 0.2747
Epoch 14 — loss: 0.2743
Epoch 15 — loss: 0.2734
Epoch 16 — loss: 0.2725
Epoch 17 — loss: 0.2724
Epoch 18 — loss: 0.2709
Epoch 19 — loss: 0.2720
Epoch 20 — loss: 0.2714
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  80.459999
2    P(>V6)  83.830002
3    P(>V7)  87.970001
4    P(>V8)  92.489998
5    P(>V9)  97.019997
Overall accuracy: 44.66%
Ordinal stacking meta epoch 1: loss=0.2292
Ordinal stacking meta epoch 2: loss=0.1551
Ordinal stacking meta epoch 3: loss=0.1488
Ordinal stacking meta epoch 4: loss=0.1459
Ordinal stacking meta epoch 5: loss=0.1442
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001571 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.629997
2    P(>V6)  85.949997
3    P(>V7)  88.449997
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 49.42%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.870003
2    P(>V6)  85.510002
3    P(>V7)  88.349998
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 49.81%
  threshold   accuracy
0    P(>V4)  83.160004
1    P(>V5)  82.290001
2    P(>V6)  85.029999
3    P(>V7)  88.309998
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.529999
2    P(>V6)  85.709999
3    P(>V7)  88.309998
4    P(>V8)  92.830002
5    P(>V9)  97.019997/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [06:23:52] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 48.99%
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  82.680000
2    P(>V6)  87.010002
3    P(>V7)  88.790001
4    P(>V8)  93.019997
5    P(>V9)  96.820000
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.529999
2    P(>V6)  86.959999
3    P(>V7)  89.360001
4    P(>V8)  93.070000
5    P(>V9)  96.779999
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.720001
2    P(>V6)  86.139999
3    P(>V7)  88.260002
4    P(>V8)  92.690002
5    P(>V9)  96.779999
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.389999
2    P(>V6)  86.720001
3    P(>V7)  89.029999
4    P(>V8)  93.019997
5    P(>V9)  96.870003
Overall accuracy: 49.37%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.629997
2    P(>V6)  85.949997
3    P(>V7)  88.449997
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 49.42%
Saved aggregated ordinal results to ./result/ordinal_result.xlsx
                                  model  ...  overall_accuracy_std
0                       deepset_ordinal  ...              0.746468
1                    deepset_ordinal_xy  ...              0.726973
2           deepset_ordinal_xy_additive  ...              1.048202
3             ordinal_adaboost_ensemble  ...              0.510899
4                  ordinal_gbm_ensemble  ...              0.687123
5       ordinal_geometric_mean_ensemble  ...              0.557931
6             ordinal_lightgbm_ensemble  ...              0.870697
7               ordinal_median_ensemble  ...              0.463817
8          ordinal_soft_voting_ensemble  ...              0.510899
9             ordinal_stacking_ensemble  ...              0.739037
10        ordinal_trimmed_mean_ensemble  ...              0.531160
11             ordinal_xgboost_ensemble  ...              0.648492
12              set_transformer_ordinal  ...              1.305972
13           set_transformer_ordinal_xy  ...              1.083039
14  set_transformer_ordinal_xy_additive  ...              1.342499

[15 rows x 3 columns]
----------------- Ordinal iteration 1/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3801
Epoch 02 — loss: 0.3172
Epoch 03 — loss: 0.3022
Epoch 04 — loss: 0.2899
Epoch 05 — loss: 0.2803
Epoch 06 — loss: 0.2691
Epoch 07 — loss: 0.2621
Epoch 08 — loss: 0.2526
Epoch 09 — loss: 0.2475
Epoch 10 — loss: 0.2426
Epoch 11 — loss: 0.2336
Epoch 12 — loss: 0.2288
Epoch 13 — loss: 0.2231
Epoch 14 — loss: 0.2191
Epoch 15 — loss: 0.2115
Epoch 16 — loss: 0.2061
Epoch 17 — loss: 0.2022
Epoch 18 — loss: 0.1951
Epoch 19 — loss: 0.1916
Epoch 20 — loss: 0.1851
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  81.709999
2    P(>V6)  85.370003
3    P(>V7)  88.260002
4    P(>V8)  93.209999
5    P(>V9)  96.730003
Overall accuracy: 48.60%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3684
Epoch 02 — loss: 0.3134
Epoch 03 — loss: 0.3015
Epoch 04 — loss: 0.2926
Epoch 05 — loss: 0.2834
Epoch 06 — loss: 0.2764
Epoch 07 — loss: 0.2678
Epoch 08 — loss: 0.2614
Epoch 09 — loss: 0.2529
Epoch 10 — loss: 0.2451
Epoch 11 — loss: 0.2377
Epoch 12 — loss: 0.2317
Epoch 13 — loss: 0.2251
Epoch 14 — loss: 0.2217
Epoch 15 — loss: 0.2153
Epoch 16 — loss: 0.2065
Epoch 17 — loss: 0.2029
Epoch 18 — loss: 0.1980
Epoch 19 — loss: 0.1918
Epoch 20 — loss: 0.1889
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  81.570000
2    P(>V6)  84.989998
3    P(>V7)  87.629997
4    P(>V8)  93.070000
5    P(>V9)  96.580002
Overall accuracy: 47.55%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3694
Epoch 02 — loss: 0.3176
Epoch 03 — loss: 0.3074
Epoch 04 — loss: 0.2959
Epoch 05 — loss: 0.2894
Epoch 06 — loss: 0.2859
Epoch 07 — loss: 0.2818
Epoch 08 — loss: 0.2709
Epoch 09 — loss: 0.2656
Epoch 10 — loss: 0.2598
Epoch 11 — loss: 0.2540
Epoch 12 — loss: 0.2482
Epoch 13 — loss: 0.2435
Epoch 14 — loss: 0.2399
Epoch 15 — loss: 0.2328
Epoch 16 — loss: 0.2284
Epoch 17 — loss: 0.2275
Epoch 18 — loss: 0.2199
Epoch 19 — loss: 0.2144
Epoch 20 — loss: 0.2103
  threshold   accuracy
0    P(>V4)  81.949997
1    P(>V5)  82.339996
2    P(>V6)  84.790001
3    P(>V7)  87.820000
4    P(>V8)  92.830002
5    P(>V9)  96.970001
Overall accuracy: 47.11%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4714
Epoch 02 — loss: 0.3376
Epoch 03 — loss: 0.3027
Epoch 04 — loss: 0.2905
Epoch 05 — loss: 0.2860
Epoch 06 — loss: 0.2802
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2767
Epoch 09 — loss: 0.2752
Epoch 10 — loss: 0.2730
Epoch 11 — loss: 0.2730
Epoch 12 — loss: 0.2719
Epoch 13 — loss: 0.2717
Epoch 14 — loss: 0.2707
Epoch 15 — loss: 0.2699
Epoch 16 — loss: 0.2695
Epoch 17 — loss: 0.2694
Epoch 18 — loss: 0.2707
Epoch 19 — loss: 0.2691
Epoch 20 — loss: 0.2693
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.370003
2    P(>V6)  83.639999
3    P(>V7)  87.540001
4    P(>V8)  92.489998
5    P(>V9)  97.019997
Overall accuracy: 46.10%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4601
Epoch 02 — loss: 0.3204
Epoch 03 — loss: 0.2993
Epoch 04 — loss: 0.2914
Epoch 05 — loss: 0.2856
Epoch 06 — loss: 0.2809
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2774
Epoch 09 — loss: 0.2762
Epoch 10 — loss: 0.2752
Epoch 11 — loss: 0.2732
Epoch 12 — loss: 0.2737
Epoch 13 — loss: 0.2729
Epoch 14 — loss: 0.2719
Epoch 15 — loss: 0.2716
Epoch 16 — loss: 0.2702
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2700
Epoch 19 — loss: 0.2694
Epoch 20 — loss: 0.2687
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.750000
2    P(>V6)  83.540001
3    P(>V7)  87.389999
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 45.72%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4224
Epoch 02 — loss: 0.3210
Epoch 03 — loss: 0.3056
Epoch 04 — loss: 0.2998
Epoch 05 — loss: 0.2954
Epoch 06 — loss: 0.2892
Epoch 07 — loss: 0.2876
Epoch 08 — loss: 0.2856
Epoch 09 — loss: 0.2824
Epoch 10 — loss: 0.2803
Epoch 11 — loss: 0.2775
Epoch 12 — loss: 0.2780
Epoch 13 — loss: 0.2780
Epoch 14 — loss: 0.2766
Epoch 15 — loss: 0.2750
Epoch 16 — loss: 0.2745
Epoch 17 — loss: 0.2753
Epoch 18 — loss: 0.2729
Epoch 19 — loss: 0.2734
Epoch 20 — loss: 0.2728
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.559998
2    P(>V6)  83.589996
3    P(>V7)  87.199997
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.48%
Ordinal stacking meta epoch 1: loss=0.1861
Ordinal stacking meta epoch 2: loss=0.1532
Ordinal stacking meta epoch 3: loss=0.1488
Ordinal stacking meta epoch 4: loss=0.1470
Ordinal stacking meta epoch 5: loss=0.1458
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001723 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.769997
2    P(>V6)  85.269997
3    P(>V7)  89.029999
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.870003
2    P(>V6)  85.419998
3    P(>V7)  88.550003
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  82.720001
1    P(>V5)  81.419998
2    P(>V6)  84.309998
3    P(>V7)  87.970001
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 47.64%
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [06:46:41] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.580002
2    P(>V6)  85.029999
3    P(>V7)  88.690002
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  81.760002
2    P(>V6)  86.190002
3    P(>V7)  88.690002
4    P(>V8)  93.699997
5    P(>V9)  96.629997
Overall accuracy: 47.64%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.099998
2    P(>V6)  86.000000
3    P(>V7)  88.639999
4    P(>V8)  93.550003
5    P(>V9)  96.250000
Overall accuracy: 48.36%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  81.709999
2    P(>V6)  85.610001
3    P(>V7)  88.550003
4    P(>V8)  93.169998
5    P(>V9)  96.290001
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  81.949997
2    P(>V6)  85.370003
3    P(>V7)  88.790001
4    P(>V8)  93.360001
5    P(>V9)  96.540001
Overall accuracy: 49.52%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.769997
2    P(>V6)  85.269997
3    P(>V7)  89.029999
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 48.94%
----------------- Ordinal iteration 2/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3696
Epoch 02 — loss: 0.3168
Epoch 03 — loss: 0.2991
Epoch 04 — loss: 0.2889
Epoch 05 — loss: 0.2815
Epoch 06 — loss: 0.2762
Epoch 07 — loss: 0.2643
Epoch 08 — loss: 0.2575
Epoch 09 — loss: 0.2497
Epoch 10 — loss: 0.2440
Epoch 11 — loss: 0.2385
Epoch 12 — loss: 0.2310
Epoch 13 — loss: 0.2262
Epoch 14 — loss: 0.2177
Epoch 15 — loss: 0.2137
Epoch 16 — loss: 0.2096
Epoch 17 — loss: 0.2042
Epoch 18 — loss: 0.1979
Epoch 19 — loss: 0.1933
Epoch 20 — loss: 0.1890
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  81.949997
2    P(>V6)  84.650002
3    P(>V7)  87.389999
4    P(>V8)  92.349998
5    P(>V9)  96.629997
Overall accuracy: 48.17%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3670
Epoch 02 — loss: 0.3098
Epoch 03 — loss: 0.2974
Epoch 04 — loss: 0.2903
Epoch 05 — loss: 0.2822
Epoch 06 — loss: 0.2745
Epoch 07 — loss: 0.2664
Epoch 08 — loss: 0.2601
Epoch 09 — loss: 0.2537
Epoch 10 — loss: 0.2447
Epoch 11 — loss: 0.2370
Epoch 12 — loss: 0.2329
Epoch 13 — loss: 0.2237
Epoch 14 — loss: 0.2173
Epoch 15 — loss: 0.2134
Epoch 16 — loss: 0.2074
Epoch 17 — loss: 0.2028
Epoch 18 — loss: 0.1951
Epoch 19 — loss: 0.1894
Epoch 20 — loss: 0.1858
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.870003
2    P(>V6)  85.080002
3    P(>V7)  87.580002
4    P(>V8)  92.349998
5    P(>V9)  96.580002
Overall accuracy: 47.98%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3580
Epoch 02 — loss: 0.3076
Epoch 03 — loss: 0.2972
Epoch 04 — loss: 0.2930
Epoch 05 — loss: 0.2848
Epoch 06 — loss: 0.2805
Epoch 07 — loss: 0.2752
Epoch 08 — loss: 0.2713
Epoch 09 — loss: 0.2662
Epoch 10 — loss: 0.2611
Epoch 11 — loss: 0.2545
Epoch 12 — loss: 0.2493
Epoch 13 — loss: 0.2456
Epoch 14 — loss: 0.2389
Epoch 15 — loss: 0.2346
Epoch 16 — loss: 0.2311
Epoch 17 — loss: 0.2242
Epoch 18 — loss: 0.2178
Epoch 19 — loss: 0.2157
Epoch 20 — loss: 0.2099
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.339996
2    P(>V6)  84.940002
3    P(>V7)  88.349998
4    P(>V8)  93.019997
5    P(>V9)  96.779999
Overall accuracy: 50.34%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4716
Epoch 02 — loss: 0.3382
Epoch 03 — loss: 0.2984
Epoch 04 — loss: 0.2904
Epoch 05 — loss: 0.2850
Epoch 06 — loss: 0.2820
Epoch 07 — loss: 0.2796
Epoch 08 — loss: 0.2781
Epoch 09 — loss: 0.2769
Epoch 10 — loss: 0.2755
Epoch 11 — loss: 0.2761
Epoch 12 — loss: 0.2726
Epoch 13 — loss: 0.2743
Epoch 14 — loss: 0.2713
Epoch 15 — loss: 0.2718
Epoch 16 — loss: 0.2710
Epoch 17 — loss: 0.2712
Epoch 18 — loss: 0.2685
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2696
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  81.040001
2    P(>V6)  83.690002
3    P(>V7)  87.870003
4    P(>V8)  92.349998
5    P(>V9)  97.019997
Overall accuracy: 45.24%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4638
Epoch 02 — loss: 0.3323
Epoch 03 — loss: 0.2968
Epoch 04 — loss: 0.2867
Epoch 05 — loss: 0.2834
Epoch 06 — loss: 0.2806
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2765
Epoch 09 — loss: 0.2747
Epoch 10 — loss: 0.2749
Epoch 11 — loss: 0.2728
Epoch 12 — loss: 0.2723
Epoch 13 — loss: 0.2729
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2721
Epoch 16 — loss: 0.2715
Epoch 17 — loss: 0.2708
Epoch 18 — loss: 0.2698
Epoch 19 — loss: 0.2696
Epoch 20 — loss: 0.2685
  threshold   accuracy
0    P(>V4)  81.910004
1    P(>V5)  80.750000
2    P(>V6)  84.120003
3    P(>V7)  87.730003
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.25%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4198
Epoch 02 — loss: 0.3207
Epoch 03 — loss: 0.3084
Epoch 04 — loss: 0.3012
Epoch 05 — loss: 0.2947
Epoch 06 — loss: 0.2919
Epoch 07 — loss: 0.2881
Epoch 08 — loss: 0.2842
Epoch 09 — loss: 0.2798
Epoch 10 — loss: 0.2821
Epoch 11 — loss: 0.2768
Epoch 12 — loss: 0.2759
Epoch 13 — loss: 0.2766
Epoch 14 — loss: 0.2746
Epoch 15 — loss: 0.2733
Epoch 16 — loss: 0.2740
Epoch 17 — loss: 0.2715
Epoch 18 — loss: 0.2724
Epoch 19 — loss: 0.2717
Epoch 20 — loss: 0.2707
  threshold   accuracy
0    P(>V4)  81.089996
1    P(>V5)  81.040001
2    P(>V6)  83.779999
3    P(>V7)  87.540001
4    P(>V8)  92.440002
5    P(>V9)  97.019997
Overall accuracy: 44.95%
Ordinal stacking meta epoch 1: loss=0.2494
Ordinal stacking meta epoch 2: loss=0.1554
Ordinal stacking meta epoch 3: loss=0.1482
Ordinal stacking meta epoch 4: loss=0.1455
Ordinal stacking meta epoch 5: loss=0.1441
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007799 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.480003
2    P(>V6)  85.709999
3    P(>V7)  88.930000
4    P(>V8)  92.930000
5    P(>V9)  97.059998
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  83.110001
2    P(>V6)  85.709999
3    P(>V7)  88.209999
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  82.190002
2    P(>V6)  84.699997
3    P(>V7)  88.500000
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 48.17%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.389999
2    P(>V6)  85.470001
3    P(>V7)  88.690002
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  84.699997
1    P(>V5)  83.970001
2    P(>V6)  86.040001
3    P(>V7)  88.879997
4    P(>V8)  92.540001
5    P(>V9)  96.580002
Overall accuracy: 50.19%
  threshold   accuracy
0    P(>V4)  85.080002
1    P(>V5)  83.489998
2    P(>V6)  85.610001
3    P(>V7)  88.739998
4    P(>V8)  92.779999
5    P(>V9)  96.540001
Overall accuracy: 50.91%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  83.199997
2    P(>V6)  85.230003
3    P(>V7)  88.309998
4    P(>V8)  92.199997
5    P(>V9)  96.339996
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.820000
2    P(>V6)  86.000000
3    P(>V7)  88.500000
4    P(>V8)  92.489998
5    P(>V9)  96.339996
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.480003
2    P(>V6)  85.709999
3    P(>V7)  88.930000
4    P(>V8)  92.930000
5    P(>V9)  97.059998/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [07:10:58] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 49.57%
----------------- Ordinal iteration 3/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3716
Epoch 02 — loss: 0.3088
Epoch 03 — loss: 0.2960
Epoch 04 — loss: 0.2861
Epoch 05 — loss: 0.2771
Epoch 06 — loss: 0.2696
Epoch 07 — loss: 0.2621
Epoch 08 — loss: 0.2549
Epoch 09 — loss: 0.2480
Epoch 10 — loss: 0.2436
Epoch 11 — loss: 0.2354
Epoch 12 — loss: 0.2299
Epoch 13 — loss: 0.2251
Epoch 14 — loss: 0.2196
Epoch 15 — loss: 0.2147
Epoch 16 — loss: 0.2104
Epoch 17 — loss: 0.2025
Epoch 18 — loss: 0.1991
Epoch 19 — loss: 0.1952
Epoch 20 — loss: 0.1899
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  80.699997
2    P(>V6)  83.300003
3    P(>V7)  86.959999
4    P(>V8)  92.059998
5    P(>V9)  96.680000
Overall accuracy: 47.64%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3767
Epoch 02 — loss: 0.3131
Epoch 03 — loss: 0.2999
Epoch 04 — loss: 0.2930
Epoch 05 — loss: 0.2830
Epoch 06 — loss: 0.2737
Epoch 07 — loss: 0.2690
Epoch 08 — loss: 0.2594
Epoch 09 — loss: 0.2556
Epoch 10 — loss: 0.2490
Epoch 11 — loss: 0.2411
Epoch 12 — loss: 0.2368
Epoch 13 — loss: 0.2315
Epoch 14 — loss: 0.2236
Epoch 15 — loss: 0.2204
Epoch 16 — loss: 0.2135
Epoch 17 — loss: 0.2080
Epoch 18 — loss: 0.2050
Epoch 19 — loss: 0.1971
Epoch 20 — loss: 0.1948
  threshold   accuracy
0    P(>V4)  82.919998
1    P(>V5)  81.470001
2    P(>V6)  84.790001
3    P(>V7)  87.099998
4    P(>V8)  92.400002
5    P(>V9)  96.290001
Overall accuracy: 46.20%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3561
Epoch 02 — loss: 0.3082
Epoch 03 — loss: 0.2984
Epoch 04 — loss: 0.2917
Epoch 05 — loss: 0.2826
Epoch 06 — loss: 0.2780
Epoch 07 — loss: 0.2746
Epoch 08 — loss: 0.2667
Epoch 09 — loss: 0.2615
Epoch 10 — loss: 0.2547
Epoch 11 — loss: 0.2500
Epoch 12 — loss: 0.2448
Epoch 13 — loss: 0.2399
Epoch 14 — loss: 0.2362
Epoch 15 — loss: 0.2295
Epoch 16 — loss: 0.2257
Epoch 17 — loss: 0.2215
Epoch 18 — loss: 0.2168
Epoch 19 — loss: 0.2113
Epoch 20 — loss: 0.2086
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  81.139999
2    P(>V6)  85.129997
3    P(>V7)  86.860001
4    P(>V8)  92.400002
5    P(>V9)  97.019997
Overall accuracy: 50.48%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4564
Epoch 02 — loss: 0.3272
Epoch 03 — loss: 0.2997
Epoch 04 — loss: 0.2903
Epoch 05 — loss: 0.2856
Epoch 06 — loss: 0.2830
Epoch 07 — loss: 0.2791
Epoch 08 — loss: 0.2767
Epoch 09 — loss: 0.2760
Epoch 10 — loss: 0.2770
Epoch 11 — loss: 0.2727
Epoch 12 — loss: 0.2735
Epoch 13 — loss: 0.2725
Epoch 14 — loss: 0.2717
Epoch 15 — loss: 0.2707
Epoch 16 — loss: 0.2711
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2710
Epoch 19 — loss: 0.2687
Epoch 20 — loss: 0.2687
  threshold   accuracy
0    P(>V4)  80.900002
1    P(>V5)  80.989998
2    P(>V6)  83.930000
3    P(>V7)  87.489998
4    P(>V8)  92.300003
5    P(>V9)  97.019997
Overall accuracy: 45.77%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4683
Epoch 02 — loss: 0.3268
Epoch 03 — loss: 0.2950
Epoch 04 — loss: 0.2884
Epoch 05 — loss: 0.2837
Epoch 06 — loss: 0.2806
Epoch 07 — loss: 0.2787
Epoch 08 — loss: 0.2754
Epoch 09 — loss: 0.2758
Epoch 10 — loss: 0.2741
Epoch 11 — loss: 0.2740
Epoch 12 — loss: 0.2724
Epoch 13 — loss: 0.2724
Epoch 14 — loss: 0.2707
Epoch 15 — loss: 0.2721
Epoch 16 — loss: 0.2692
Epoch 17 — loss: 0.2702
Epoch 18 — loss: 0.2712
Epoch 19 — loss: 0.2688
Epoch 20 — loss: 0.2700
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  80.900002
2    P(>V6)  83.639999
3    P(>V7)  87.580002
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 45.24%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4268
Epoch 02 — loss: 0.3271
Epoch 03 — loss: 0.3118
Epoch 04 — loss: 0.3000
Epoch 05 — loss: 0.2912
Epoch 06 — loss: 0.2862
Epoch 07 — loss: 0.2831
Epoch 08 — loss: 0.2812
Epoch 09 — loss: 0.2788
Epoch 10 — loss: 0.2774
Epoch 11 — loss: 0.2781
Epoch 12 — loss: 0.2757
Epoch 13 — loss: 0.2756
Epoch 14 — loss: 0.2740
Epoch 15 — loss: 0.2757
Epoch 16 — loss: 0.2726
Epoch 17 — loss: 0.2735
Epoch 18 — loss: 0.2721
Epoch 19 — loss: 0.2725
Epoch 20 — loss: 0.2710
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  80.750000
2    P(>V6)  84.220001
3    P(>V7)  87.730003
4    P(>V8)  92.400002
5    P(>V9)  97.019997
Overall accuracy: 45.81%
Ordinal stacking meta epoch 1: loss=0.2210
Ordinal stacking meta epoch 2: loss=0.1592
Ordinal stacking meta epoch 3: loss=0.1523
Ordinal stacking meta epoch 4: loss=0.1492
Ordinal stacking meta epoch 5: loss=0.1475
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001560 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.150002
2    P(>V6)  85.760002
3    P(>V7)  88.309998
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 49.33%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.339996
2    P(>V6)  85.419998
3    P(>V7)  88.209999
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 49.42%
  threshold   accuracy
0    P(>V4)  83.160004
1    P(>V5)  81.379997
2    P(>V6)  84.889999
3    P(>V7)  87.779999
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 47.69%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  81.470001
2    P(>V6)  85.230003
3    P(>V7)  88.110001
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  84.889999
1    P(>V5)  82.769997
2    P(>V6)  85.230003
3    P(>V7)  88.550003
4    P(>V8)  92.879997
5    P(>V9)  96.919998
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  85.029999
1    P(>V5)  82.919998
2    P(>V6)  85.610001
3    P(>V7)  88.309998
4    P(>V8)  92.970001
5    P(>V9)  96.870003
Overall accuracy: 51.20%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  81.809998
2    P(>V6)  85.370003
3    P(>V7)  88.160004
4    P(>V8)  92.930000
5    P(>V9)  96.580002
Overall accuracy: 49.71%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.339996
2    P(>V6)  85.419998
3    P(>V7)  88.110001
4    P(>V8)  92.830002
5    P(>V9)  96.870003
Overall accuracy: 50.53%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.150002
2    P(>V6)  85.760002
3    P(>V7)  88.309998
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 49.33%
----------------- Ordinal iteration 4/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3687
Epoch 02 — loss: 0.3103
Epoch 03 — loss: 0.2963
Epoch 04 — loss: 0.2861
Epoch 05 — loss: 0.2755
Epoch 06 — loss: 0.2671
Epoch 07 — loss: 0.2635
Epoch 08 — loss: 0.2548
Epoch 09 — loss: 0.2512
Epoch 10 — loss: 0.2438
Epoch 11 — loss: 0.2386
Epoch 12 — loss: 0.2320
Epoch 13 — loss: 0.2280
Epoch 14 — loss: 0.2226
Epoch 15 — loss: 0.2164
Epoch 16 — loss: 0.2121
Epoch 17 — loss: 0.2079
Epoch 18 — loss: 0.2019
Epoch 19 — loss: 0.1964
Epoch 20 — loss: 0.1911
  threshold   accuracy
0    P(>V4)  82.150002
1    P(>V5)  79.790001
2    P(>V6)  84.070000
3    P(>V7)  87.150002
4    P(>V8)  91.720001
5    P(>V9)  96.870003
Overall accuracy: 46.78%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3727
Epoch 02 — loss: 0.3095
Epoch 03 — loss: 0.2966
Epoch 04 — loss: 0.2911/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [07:35:21] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 05 — loss: 0.2830
Epoch 06 — loss: 0.2753
Epoch 07 — loss: 0.2659
Epoch 08 — loss: 0.2584
Epoch 09 — loss: 0.2533
Epoch 10 — loss: 0.2433
Epoch 11 — loss: 0.2381
Epoch 12 — loss: 0.2315
Epoch 13 — loss: 0.2269
Epoch 14 — loss: 0.2202
Epoch 15 — loss: 0.2168
Epoch 16 — loss: 0.2092
Epoch 17 — loss: 0.2057
Epoch 18 — loss: 0.1989
Epoch 19 — loss: 0.1946
Epoch 20 — loss: 0.1893
  threshold   accuracy
0    P(>V4)  82.529999
1    P(>V5)  82.339996
2    P(>V6)  84.839996
3    P(>V7)  87.050003
4    P(>V8)  93.070000
5    P(>V9)  96.870003
Overall accuracy: 48.12%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3546
Epoch 02 — loss: 0.3148
Epoch 03 — loss: 0.3014
Epoch 04 — loss: 0.2941
Epoch 05 — loss: 0.2879
Epoch 06 — loss: 0.2789
Epoch 07 — loss: 0.2734
Epoch 08 — loss: 0.2659
Epoch 09 — loss: 0.2611
Epoch 10 — loss: 0.2546
Epoch 11 — loss: 0.2510
Epoch 12 — loss: 0.2459
Epoch 13 — loss: 0.2423
Epoch 14 — loss: 0.2349
Epoch 15 — loss: 0.2304
Epoch 16 — loss: 0.2255
Epoch 17 — loss: 0.2235
Epoch 18 — loss: 0.2174
Epoch 19 — loss: 0.2143
Epoch 20 — loss: 0.2100
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.480003
2    P(>V6)  85.660004
3    P(>V7)  88.349998
4    P(>V8)  93.260002
5    P(>V9)  96.820000
Overall accuracy: 48.85%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4700
Epoch 02 — loss: 0.3528
Epoch 03 — loss: 0.3084
Epoch 04 — loss: 0.2933
Epoch 05 — loss: 0.2860
Epoch 06 — loss: 0.2816
Epoch 07 — loss: 0.2794
Epoch 08 — loss: 0.2781
Epoch 09 — loss: 0.2755
Epoch 10 — loss: 0.2747
Epoch 11 — loss: 0.2735
Epoch 12 — loss: 0.2724
Epoch 13 — loss: 0.2716
Epoch 14 — loss: 0.2708
Epoch 15 — loss: 0.2713
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2685
Epoch 18 — loss: 0.2711
Epoch 19 — loss: 0.2696
Epoch 20 — loss: 0.2704
  threshold   accuracy
0    P(>V4)  82.150002
1    P(>V5)  80.800003
2    P(>V6)  83.830002
3    P(>V7)  87.730003
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.73%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4602
Epoch 02 — loss: 0.3291
Epoch 03 — loss: 0.2992
Epoch 04 — loss: 0.2901
Epoch 05 — loss: 0.2848
Epoch 06 — loss: 0.2818
Epoch 07 — loss: 0.2770
Epoch 08 — loss: 0.2758
Epoch 09 — loss: 0.2751
Epoch 10 — loss: 0.2729
Epoch 11 — loss: 0.2710
Epoch 12 — loss: 0.2727
Epoch 13 — loss: 0.2710
Epoch 14 — loss: 0.2718
Epoch 15 — loss: 0.2702
Epoch 16 — loss: 0.2690
Epoch 17 — loss: 0.2691
Epoch 18 — loss: 0.2697
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2691
  threshold   accuracy
0    P(>V4)  81.139999
1    P(>V5)  80.900002
2    P(>V6)  83.489998
3    P(>V7)  87.779999
4    P(>V8)  92.730003
5    P(>V9)  96.970001
Overall accuracy: 44.71%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4222
Epoch 02 — loss: 0.3268
Epoch 03 — loss: 0.3091
Epoch 04 — loss: 0.2948
Epoch 05 — loss: 0.2908
Epoch 06 — loss: 0.2849
Epoch 07 — loss: 0.2856
Epoch 08 — loss: 0.2792
Epoch 09 — loss: 0.2793
Epoch 10 — loss: 0.2761
Epoch 11 — loss: 0.2760
Epoch 12 — loss: 0.2747
Epoch 13 — loss: 0.2743
Epoch 14 — loss: 0.2738
Epoch 15 — loss: 0.2732
Epoch 16 — loss: 0.2719
Epoch 17 — loss: 0.2735
Epoch 18 — loss: 0.2719
Epoch 19 — loss: 0.2713
Epoch 20 — loss: 0.2719
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.510002
2    P(>V6)  83.930000
3    P(>V7)  88.110001
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 46.58%
Ordinal stacking meta epoch 1: loss=0.2416
Ordinal stacking meta epoch 2: loss=0.1598
Ordinal stacking meta epoch 3: loss=0.1556
Ordinal stacking meta epoch 4: loss=0.1534
Ordinal stacking meta epoch 5: loss=0.1517
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001572 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.190002
2    P(>V6)  85.760002
3    P(>V7)  89.169998
4    P(>V8)  93.070000
5    P(>V9)  97.110001
Overall accuracy: 49.42%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.239998
2    P(>V6)  85.510002
3    P(>V7)  89.220001
4    P(>V8)  93.120003
5    P(>V9)  97.059998
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  81.470001
2    P(>V6)  84.790001
3    P(>V7)  88.690002
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 48.36%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  81.809998
2    P(>V6)  85.320000
3    P(>V7)  89.080002
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 48.89%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.239998
2    P(>V6)  85.470001
3    P(>V7)  88.309998
4    P(>V8)  93.070000
5    P(>V9)  96.580002
Overall accuracy: 48.36%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  81.949997
2    P(>V6)  85.370003
3    P(>V7)  88.260002
4    P(>V8)  92.730003
5    P(>V9)  96.779999
Overall accuracy: 48.41%
  threshold   accuracy
0    P(>V4)  82.580002
1    P(>V5)  81.809998
2    P(>V6)  85.320000
3    P(>V7)  87.250000
4    P(>V8)  93.019997
5    P(>V9)  96.680000
Overall accuracy: 47.93%
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  81.860001
2    P(>V6)  84.889999
3    P(>V7)  87.440002
4    P(>V8)  92.970001
5    P(>V9)  96.820000
Overall accuracy: 48.12%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.190002
2    P(>V6)  85.760002
3    P(>V7)  89.169998
4    P(>V8)  93.070000
5    P(>V9)  97.110001
Overall accuracy: 49.42%
----------------- Ordinal iteration 5/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3740
Epoch 02 — loss: 0.3225
Epoch 03 — loss: 0.3071
Epoch 04 — loss: 0.2932
Epoch 05 — loss: 0.2852
Epoch 06 — loss: 0.2742
Epoch 07 — loss: 0.2668
Epoch 08 — loss: 0.2575
Epoch 09 — loss: 0.2500
Epoch 10 — loss: 0.2422
Epoch 11 — loss: 0.2359
Epoch 12 — loss: 0.2306
Epoch 13 — loss: 0.2258
Epoch 14 — loss: 0.2205
Epoch 15 — loss: 0.2123
Epoch 16 — loss: 0.2059
Epoch 17 — loss: 0.2045
Epoch 18 — loss: 0.1979
Epoch 19 — loss: 0.1916
Epoch 20 — loss: 0.1871
  threshold   accuracy
0    P(>V4)  82.769997
1    P(>V5)  81.620003
2    P(>V6)  85.370003
3    P(>V7)  87.540001
4    P(>V8)  91.480003
5    P(>V9)  96.199997
Overall accuracy: 48.12%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3739
Epoch 02 — loss: 0.3152
Epoch 03 — loss: 0.3005
Epoch 04 — loss: 0.2952
Epoch 05 — loss: 0.2832
Epoch 06 — loss: 0.2748
Epoch 07 — loss: 0.2655
Epoch 08 — loss: 0.2567
Epoch 09 — loss: 0.2483
Epoch 10 — loss: 0.2420
Epoch 11 — loss: 0.2337
Epoch 12 — loss: 0.2278
Epoch 13 — loss: 0.2222
Epoch 14 — loss: 0.2162
Epoch 15 — loss: 0.2116
Epoch 16 — loss: 0.2027
Epoch 17 — loss: 0.2003
Epoch 18 — loss: 0.1934
Epoch 19 — loss: 0.1885
Epoch 20 — loss: 0.1844
  threshold   accuracy
0    P(>V4)  82.580002
1    P(>V5)  81.709999
2    P(>V6)  84.260002
3    P(>V7)  86.669998
4    P(>V8)  91.389999
5    P(>V9)  95.720001
Overall accuracy: 45.19%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3686
Epoch 02 — loss: 0.3165
Epoch 03 — loss: 0.3081
Epoch 04 — loss: 0.2961
Epoch 05 — loss: 0.2878
Epoch 06 — loss: 0.2807
Epoch 07 — loss: 0.2749
Epoch 08 — loss: 0.2672
Epoch 09 — loss: 0.2598
Epoch 10 — loss: 0.2533
Epoch 11 — loss: 0.2464
Epoch 12 — loss: 0.2404
Epoch 13 — loss: 0.2376/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [07:59:46] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 14 — loss: 0.2303
Epoch 15 — loss: 0.2241
Epoch 16 — loss: 0.2198
Epoch 17 — loss: 0.2156
Epoch 18 — loss: 0.2117
Epoch 19 — loss: 0.2059
Epoch 20 — loss: 0.2017
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.529999
2    P(>V6)  84.739998
3    P(>V7)  88.110001
4    P(>V8)  93.169998
5    P(>V9)  96.970001
Overall accuracy: 49.66%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4658
Epoch 02 — loss: 0.3412
Epoch 03 — loss: 0.3021
Epoch 04 — loss: 0.2917
Epoch 05 — loss: 0.2860
Epoch 06 — loss: 0.2803
Epoch 07 — loss: 0.2793
Epoch 08 — loss: 0.2767
Epoch 09 — loss: 0.2762
Epoch 10 — loss: 0.2752
Epoch 11 — loss: 0.2737
Epoch 12 — loss: 0.2732
Epoch 13 — loss: 0.2713
Epoch 14 — loss: 0.2724
Epoch 15 — loss: 0.2712
Epoch 16 — loss: 0.2710
Epoch 17 — loss: 0.2692
Epoch 18 — loss: 0.2678
Epoch 19 — loss: 0.2705
Epoch 20 — loss: 0.2681
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.849998
2    P(>V6)  83.930000
3    P(>V7)  87.730003
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 45.19%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4768
Epoch 02 — loss: 0.3403
Epoch 03 — loss: 0.2955
Epoch 04 — loss: 0.2879
Epoch 05 — loss: 0.2854
Epoch 06 — loss: 0.2817
Epoch 07 — loss: 0.2787
Epoch 08 — loss: 0.2781
Epoch 09 — loss: 0.2765
Epoch 10 — loss: 0.2749
Epoch 11 — loss: 0.2741
Epoch 12 — loss: 0.2720
Epoch 13 — loss: 0.2725
Epoch 14 — loss: 0.2711
Epoch 15 — loss: 0.2696
Epoch 16 — loss: 0.2698
Epoch 17 — loss: 0.2711
Epoch 18 — loss: 0.2708
Epoch 19 — loss: 0.2703
Epoch 20 — loss: 0.2696
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  80.849998
2    P(>V6)  83.730003
3    P(>V7)  87.580002
4    P(>V8)  92.930000
5    P(>V9)  97.059998
Overall accuracy: 45.09%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4178
Epoch 02 — loss: 0.3260
Epoch 03 — loss: 0.3094
Epoch 04 — loss: 0.2969
Epoch 05 — loss: 0.2907
Epoch 06 — loss: 0.2875
Epoch 07 — loss: 0.2841
Epoch 08 — loss: 0.2825
Epoch 09 — loss: 0.2797
Epoch 10 — loss: 0.2789
Epoch 11 — loss: 0.2774
Epoch 12 — loss: 0.2743
Epoch 13 — loss: 0.2739
Epoch 14 — loss: 0.2746
Epoch 15 — loss: 0.2717
Epoch 16 — loss: 0.2730
Epoch 17 — loss: 0.2702
Epoch 18 — loss: 0.2707
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2710
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.459999
2    P(>V6)  83.639999
3    P(>V7)  87.820000
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.24%
Ordinal stacking meta epoch 1: loss=0.2175
Ordinal stacking meta epoch 2: loss=0.1566
Ordinal stacking meta epoch 3: loss=0.1515
Ordinal stacking meta epoch 4: loss=0.1489
Ordinal stacking meta epoch 5: loss=0.1471
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001608 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.820000
2    P(>V6)  86.040001
3    P(>V7)  88.879997
4    P(>V8)  93.360001
5    P(>V9)  96.970001
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  83.160004
2    P(>V6)  86.480003
3    P(>V7)  88.550003
4    P(>V8)  93.209999
5    P(>V9)  96.970001
Overall accuracy: 50.48%
  threshold   accuracy
0    P(>V4)  82.480003
1    P(>V5)  82.480003
2    P(>V6)  85.320000
3    P(>V7)  88.349998
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 48.17%
  threshold   accuracy
0    P(>V4)  82.959999
1    P(>V5)  82.870003
2    P(>V6)  85.849998
3    P(>V7)  88.550003
4    P(>V8)  93.500000
5    P(>V9)  97.019997
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  84.839996
1    P(>V5)  83.160004
2    P(>V6)  86.959999
3    P(>V7)  89.080002
4    P(>V8)  92.730003
5    P(>V9)  96.389999
Overall accuracy: 50.10%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  83.010002
2    P(>V6)  87.199997
3    P(>V7)  89.080002
4    P(>V8)  92.300003
5    P(>V9)  96.389999
Overall accuracy: 49.42%
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  82.339996
2    P(>V6)  86.669998
3    P(>V7)  88.589996
4    P(>V8)  92.440002
5    P(>V9)  96.730003
Overall accuracy: 48.89%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.580002
2    P(>V6)  86.430000
3    P(>V7)  88.309998
4    P(>V8)  92.300003
5    P(>V9)  96.440002
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.820000
2    P(>V6)  86.040001
3    P(>V7)  88.879997
4    P(>V8)  93.360001
5    P(>V9)  96.970001
Overall accuracy: 49.76%
----------------- Ordinal iteration 6/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3771
Epoch 02 — loss: 0.3110
Epoch 03 — loss: 0.2958
Epoch 04 — loss: 0.2865
Epoch 05 — loss: 0.2765
Epoch 06 — loss: 0.2670
Epoch 07 — loss: 0.2609
Epoch 08 — loss: 0.2526
Epoch 09 — loss: 0.2459
Epoch 10 — loss: 0.2371
Epoch 11 — loss: 0.2348
Epoch 12 — loss: 0.2245
Epoch 13 — loss: 0.2211
Epoch 14 — loss: 0.2148
Epoch 15 — loss: 0.2082
Epoch 16 — loss: 0.2035
Epoch 17 — loss: 0.1980
Epoch 18 — loss: 0.1918
Epoch 19 — loss: 0.1864
Epoch 20 — loss: 0.1809
  threshold   accuracy
0    P(>V4)  82.580002
1    P(>V5)  81.519997
2    P(>V6)  84.500000
3    P(>V7)  87.010002
4    P(>V8)  92.110001
5    P(>V9)  96.680000
Overall accuracy: 46.39%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3669
Epoch 02 — loss: 0.3126
Epoch 03 — loss: 0.3009
Epoch 04 — loss: 0.2889
Epoch 05 — loss: 0.2810
Epoch 06 — loss: 0.2734
Epoch 07 — loss: 0.2675
Epoch 08 — loss: 0.2604
Epoch 09 — loss: 0.2534
Epoch 10 — loss: 0.2470
Epoch 11 — loss: 0.2408
Epoch 12 — loss: 0.2339
Epoch 13 — loss: 0.2285
Epoch 14 — loss: 0.2238
Epoch 15 — loss: 0.2182
Epoch 16 — loss: 0.2115
Epoch 17 — loss: 0.2062
Epoch 18 — loss: 0.2002
Epoch 19 — loss: 0.1955
Epoch 20 — loss: 0.1902
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  81.330002
2    P(>V6)  85.180000
3    P(>V7)  87.489998
4    P(>V8)  92.589996
5    P(>V9)  96.629997
Overall accuracy: 47.45%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3590
Epoch 02 — loss: 0.3111
Epoch 03 — loss: 0.3019
Epoch 04 — loss: 0.2951
Epoch 05 — loss: 0.2884
Epoch 06 — loss: 0.2838
Epoch 07 — loss: 0.2781
Epoch 08 — loss: 0.2705
Epoch 09 — loss: 0.2647
Epoch 10 — loss: 0.2580
Epoch 11 — loss: 0.2531
Epoch 12 — loss: 0.2486
Epoch 13 — loss: 0.2426
Epoch 14 — loss: 0.2358
Epoch 15 — loss: 0.2302
Epoch 16 — loss: 0.2284
Epoch 17 — loss: 0.2204
Epoch 18 — loss: 0.2160
Epoch 19 — loss: 0.2106
Epoch 20 — loss: 0.2061
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  81.379997
2    P(>V6)  84.940002
3    P(>V7)  87.730003
4    P(>V8)  91.580002
5    P(>V9)  96.870003
Overall accuracy: 47.59%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4672
Epoch 02 — loss: 0.3404
Epoch 03 — loss: 0.2982
Epoch 04 — loss: 0.2876
Epoch 05 — loss: 0.2832
Epoch 06 — loss: 0.2806
Epoch 07 — loss: 0.2780
Epoch 08 — loss: 0.2785
Epoch 09 — loss: 0.2763
Epoch 10 — loss: 0.2746
Epoch 11 — loss: 0.2740
Epoch 12 — loss: 0.2725
Epoch 13 — loss: 0.2723
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2699
Epoch 16 — loss: 0.2704
Epoch 17 — loss: 0.2707
Epoch 18 — loss: 0.2703
Epoch 19 — loss: 0.2690
Epoch 20 — loss: 0.2690
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  81.139999
2    P(>V6)  84.169998
3    P(>V7)  87.820000
4    P(>V8)  92.879997
5    P(>V9)  97.019997/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [08:24:16] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 45.77%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4726
Epoch 02 — loss: 0.3476
Epoch 03 — loss: 0.2998
Epoch 04 — loss: 0.2893
Epoch 05 — loss: 0.2848
Epoch 06 — loss: 0.2808
Epoch 07 — loss: 0.2785
Epoch 08 — loss: 0.2758
Epoch 09 — loss: 0.2753
Epoch 10 — loss: 0.2741
Epoch 11 — loss: 0.2742
Epoch 12 — loss: 0.2721
Epoch 13 — loss: 0.2725
Epoch 14 — loss: 0.2723
Epoch 15 — loss: 0.2694
Epoch 16 — loss: 0.2724
Epoch 17 — loss: 0.2697
Epoch 18 — loss: 0.2703
Epoch 19 — loss: 0.2705
Epoch 20 — loss: 0.2690
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.900002
2    P(>V6)  84.019997
3    P(>V7)  87.779999
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 45.33%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4219
Epoch 02 — loss: 0.3253
Epoch 03 — loss: 0.3112
Epoch 04 — loss: 0.3031
Epoch 05 — loss: 0.2966
Epoch 06 — loss: 0.2908
Epoch 07 — loss: 0.2865
Epoch 08 — loss: 0.2844
Epoch 09 — loss: 0.2824
Epoch 10 — loss: 0.2788
Epoch 11 — loss: 0.2794
Epoch 12 — loss: 0.2757
Epoch 13 — loss: 0.2767
Epoch 14 — loss: 0.2745
Epoch 15 — loss: 0.2750
Epoch 16 — loss: 0.2741
Epoch 17 — loss: 0.2730
Epoch 18 — loss: 0.2714
Epoch 19 — loss: 0.2721
Epoch 20 — loss: 0.2722
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  79.879997
2    P(>V6)  83.589996
3    P(>V7)  87.250000
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 46.10%
Ordinal stacking meta epoch 1: loss=0.2577
Ordinal stacking meta epoch 2: loss=0.1602
Ordinal stacking meta epoch 3: loss=0.1523
Ordinal stacking meta epoch 4: loss=0.1488
Ordinal stacking meta epoch 5: loss=0.1466
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001586 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.099998
2    P(>V6)  85.709999
3    P(>V7)  88.400002
4    P(>V8)  93.260002
5    P(>V9)  97.059998
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.290001
2    P(>V6)  85.470001
3    P(>V7)  88.160004
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  81.570000
2    P(>V6)  84.739998
3    P(>V7)  87.779999
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 47.93%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  81.709999
2    P(>V6)  85.320000
3    P(>V7)  88.160004
4    P(>V8)  92.970001
5    P(>V9)  97.059998
Overall accuracy: 47.50%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.000000
2    P(>V6)  86.279999
3    P(>V7)  89.120003
4    P(>V8)  92.730003
5    P(>V9)  96.440002
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  83.010002
2    P(>V6)  86.720001
3    P(>V7)  88.400002
4    P(>V8)  92.489998
5    P(>V9)  96.099998
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.239998
2    P(>V6)  85.559998
3    P(>V7)  88.309998
4    P(>V8)  92.400002
5    P(>V9)  96.290001
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.680000
2    P(>V6)  86.279999
3    P(>V7)  88.639999
4    P(>V8)  92.540001
5    P(>V9)  96.540001
Overall accuracy: 49.86%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.099998
2    P(>V6)  85.709999
3    P(>V7)  88.400002
4    P(>V8)  93.260002
5    P(>V9)  97.059998
Overall accuracy: 48.85%
----------------- Ordinal iteration 7/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3938
Epoch 02 — loss: 0.3287
Epoch 03 — loss: 0.3105
Epoch 04 — loss: 0.2963
Epoch 05 — loss: 0.2890
Epoch 06 — loss: 0.2756
Epoch 07 — loss: 0.2649
Epoch 08 — loss: 0.2566
Epoch 09 — loss: 0.2514
Epoch 10 — loss: 0.2417
Epoch 11 — loss: 0.2348
Epoch 12 — loss: 0.2292
Epoch 13 — loss: 0.2231
Epoch 14 — loss: 0.2166
Epoch 15 — loss: 0.2129
Epoch 16 — loss: 0.2059
Epoch 17 — loss: 0.1999
Epoch 18 — loss: 0.1962
Epoch 19 — loss: 0.1894
Epoch 20 — loss: 0.1853
  threshold   accuracy
0    P(>V4)  82.580002
1    P(>V5)  81.330002
2    P(>V6)  84.699997
3    P(>V7)  87.540001
4    P(>V8)  91.919998
5    P(>V9)  96.580002
Overall accuracy: 47.26%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3660
Epoch 02 — loss: 0.3078
Epoch 03 — loss: 0.2929
Epoch 04 — loss: 0.2849
Epoch 05 — loss: 0.2739
Epoch 06 — loss: 0.2652
Epoch 07 — loss: 0.2562
Epoch 08 — loss: 0.2525
Epoch 09 — loss: 0.2449
Epoch 10 — loss: 0.2391
Epoch 11 — loss: 0.2322
Epoch 12 — loss: 0.2256
Epoch 13 — loss: 0.2207
Epoch 14 — loss: 0.2158
Epoch 15 — loss: 0.2112
Epoch 16 — loss: 0.2036
Epoch 17 — loss: 0.1985
Epoch 18 — loss: 0.1953
Epoch 19 — loss: 0.1885
Epoch 20 — loss: 0.1845
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.820000
2    P(>V6)  85.709999
3    P(>V7)  88.209999
4    P(>V8)  92.830002
5    P(>V9)  96.970001
Overall accuracy: 49.57%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3617
Epoch 02 — loss: 0.3133
Epoch 03 — loss: 0.2981
Epoch 04 — loss: 0.2947
Epoch 05 — loss: 0.2882
Epoch 06 — loss: 0.2833
Epoch 07 — loss: 0.2756
Epoch 08 — loss: 0.2683
Epoch 09 — loss: 0.2643
Epoch 10 — loss: 0.2597
Epoch 11 — loss: 0.2543
Epoch 12 — loss: 0.2483
Epoch 13 — loss: 0.2449
Epoch 14 — loss: 0.2406
Epoch 15 — loss: 0.2347
Epoch 16 — loss: 0.2302
Epoch 17 — loss: 0.2249
Epoch 18 — loss: 0.2188
Epoch 19 — loss: 0.2166
Epoch 20 — loss: 0.2109
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  81.570000
2    P(>V6)  84.940002
3    P(>V7)  87.919998
4    P(>V8)  92.639999
5    P(>V9)  96.779999
Overall accuracy: 47.02%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4710
Epoch 02 — loss: 0.3508
Epoch 03 — loss: 0.3014
Epoch 04 — loss: 0.2936
Epoch 05 — loss: 0.2860
Epoch 06 — loss: 0.2800
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2750
Epoch 09 — loss: 0.2756
Epoch 10 — loss: 0.2740
Epoch 11 — loss: 0.2720
Epoch 12 — loss: 0.2742
Epoch 13 — loss: 0.2715
Epoch 14 — loss: 0.2702
Epoch 15 — loss: 0.2722
Epoch 16 — loss: 0.2712
Epoch 17 — loss: 0.2714
Epoch 18 — loss: 0.2703
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2675
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  81.089996
2    P(>V6)  83.639999
3    P(>V7)  87.730003
4    P(>V8)  92.250000
5    P(>V9)  97.019997
Overall accuracy: 45.67%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4639
Epoch 02 — loss: 0.3283
Epoch 03 — loss: 0.2977
Epoch 04 — loss: 0.2905
Epoch 05 — loss: 0.2839
Epoch 06 — loss: 0.2801
Epoch 07 — loss: 0.2798
Epoch 08 — loss: 0.2780
Epoch 09 — loss: 0.2732
Epoch 10 — loss: 0.2752
Epoch 11 — loss: 0.2723
Epoch 12 — loss: 0.2721
Epoch 13 — loss: 0.2717
Epoch 14 — loss: 0.2708
Epoch 15 — loss: 0.2720
Epoch 16 — loss: 0.2695
Epoch 17 — loss: 0.2708
Epoch 18 — loss: 0.2692
Epoch 19 — loss: 0.2677
Epoch 20 — loss: 0.2690
  threshold   accuracy
0    P(>V4)  80.129997
1    P(>V5)  80.220001
2    P(>V6)  83.589996
3    P(>V7)  87.540001
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 46.01%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4317
Epoch 02 — loss: 0.3272
Epoch 03 — loss: 0.3101
Epoch 04 — loss: 0.2958
Epoch 05 — loss: 0.2877
Epoch 06 — loss: 0.2850
Epoch 07 — loss: 0.2807/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [08:48:41] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [09:13:05] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 08 — loss: 0.2809
Epoch 09 — loss: 0.2773
Epoch 10 — loss: 0.2750
Epoch 11 — loss: 0.2757
Epoch 12 — loss: 0.2759
Epoch 13 — loss: 0.2740
Epoch 14 — loss: 0.2711
Epoch 15 — loss: 0.2741
Epoch 16 — loss: 0.2707
Epoch 17 — loss: 0.2695
Epoch 18 — loss: 0.2725
Epoch 19 — loss: 0.2714
Epoch 20 — loss: 0.2692
  threshold   accuracy
0    P(>V4)  80.080002
1    P(>V5)  80.169998
2    P(>V6)  83.589996
3    P(>V7)  88.019997
4    P(>V8)  92.639999
5    P(>V9)  96.970001
Overall accuracy: 43.55%
Ordinal stacking meta epoch 1: loss=0.2438
Ordinal stacking meta epoch 2: loss=0.1536
Ordinal stacking meta epoch 3: loss=0.1491
Ordinal stacking meta epoch 4: loss=0.1469
Ordinal stacking meta epoch 5: loss=0.1454
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001635 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.050003
2    P(>V6)  86.239998
3    P(>V7)  88.690002
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 48.89%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.529999
2    P(>V6)  86.040001
3    P(>V7)  88.550003
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  82.959999
1    P(>V5)  82.239998
2    P(>V6)  85.080002
3    P(>V7)  88.639999
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.050003
2    P(>V6)  85.800003
3    P(>V7)  88.839996
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.099998
2    P(>V6)  86.040001
3    P(>V7)  88.790001
4    P(>V8)  93.019997
5    P(>V9)  96.919998
Overall accuracy: 48.03%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.389999
2    P(>V6)  86.190002
3    P(>V7)  88.260002
4    P(>V8)  92.830002
5    P(>V9)  96.540001
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  81.470001
2    P(>V6)  85.610001
3    P(>V7)  87.680000
4    P(>V8)  92.690002
5    P(>V9)  96.440002
Overall accuracy: 47.55%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  81.860001
2    P(>V6)  85.610001
3    P(>V7)  87.919998
4    P(>V8)  92.489998
5    P(>V9)  96.489998
Overall accuracy: 48.17%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.050003
2    P(>V6)  86.239998
3    P(>V7)  88.690002
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 48.89%
----------------- Ordinal iteration 8/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3783
Epoch 02 — loss: 0.3196
Epoch 03 — loss: 0.3017
Epoch 04 — loss: 0.2929
Epoch 05 — loss: 0.2842
Epoch 06 — loss: 0.2733
Epoch 07 — loss: 0.2650
Epoch 08 — loss: 0.2622
Epoch 09 — loss: 0.2553
Epoch 10 — loss: 0.2473
Epoch 11 — loss: 0.2410
Epoch 12 — loss: 0.2340
Epoch 13 — loss: 0.2299
Epoch 14 — loss: 0.2242
Epoch 15 — loss: 0.2193
Epoch 16 — loss: 0.2136
Epoch 17 — loss: 0.2078
Epoch 18 — loss: 0.2030
Epoch 19 — loss: 0.1965
Epoch 20 — loss: 0.1906
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  81.949997
2    P(>V6)  84.120003
3    P(>V7)  86.809998
4    P(>V8)  92.440002
5    P(>V9)  96.199997
Overall accuracy: 47.79%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3672
Epoch 02 — loss: 0.3083
Epoch 03 — loss: 0.2991
Epoch 04 — loss: 0.2897
Epoch 05 — loss: 0.2827
Epoch 06 — loss: 0.2764
Epoch 07 — loss: 0.2671
Epoch 08 — loss: 0.2590
Epoch 09 — loss: 0.2532
Epoch 10 — loss: 0.2464
Epoch 11 — loss: 0.2376
Epoch 12 — loss: 0.2329
Epoch 13 — loss: 0.2274
Epoch 14 — loss: 0.2204
Epoch 15 — loss: 0.2153
Epoch 16 — loss: 0.2103
Epoch 17 — loss: 0.2039
Epoch 18 — loss: 0.1991
Epoch 19 — loss: 0.1917
Epoch 20 — loss: 0.1854
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  81.379997
2    P(>V6)  83.970001
3    P(>V7)  87.820000
4    P(>V8)  91.480003
5    P(>V9)  96.389999
Overall accuracy: 46.39%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3612
Epoch 02 — loss: 0.3134
Epoch 03 — loss: 0.3013
Epoch 04 — loss: 0.2945
Epoch 05 — loss: 0.2905
Epoch 06 — loss: 0.2870
Epoch 07 — loss: 0.2805
Epoch 08 — loss: 0.2739
Epoch 09 — loss: 0.2682
Epoch 10 — loss: 0.2621
Epoch 11 — loss: 0.2583
Epoch 12 — loss: 0.2529
Epoch 13 — loss: 0.2463
Epoch 14 — loss: 0.2406
Epoch 15 — loss: 0.2373
Epoch 16 — loss: 0.2302
Epoch 17 — loss: 0.2250
Epoch 18 — loss: 0.2196
Epoch 19 — loss: 0.2170
Epoch 20 — loss: 0.2111
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  81.709999
2    P(>V6)  85.320000
3    P(>V7)  87.629997
4    P(>V8)  90.900002
5    P(>V9)  96.199997
Overall accuracy: 47.45%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4693
Epoch 02 — loss: 0.3416
Epoch 03 — loss: 0.3004
Epoch 04 — loss: 0.2917
Epoch 05 — loss: 0.2848
Epoch 06 — loss: 0.2816
Epoch 07 — loss: 0.2786
Epoch 08 — loss: 0.2759
Epoch 09 — loss: 0.2752
Epoch 10 — loss: 0.2742
Epoch 11 — loss: 0.2731
Epoch 12 — loss: 0.2729
Epoch 13 — loss: 0.2723
Epoch 14 — loss: 0.2711
Epoch 15 — loss: 0.2706
Epoch 16 — loss: 0.2702
Epoch 17 — loss: 0.2701
Epoch 18 — loss: 0.2707
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2691
  threshold   accuracy
0    P(>V4)  81.279999
1    P(>V5)  80.849998
2    P(>V6)  84.120003
3    P(>V7)  87.730003
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.57%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4622
Epoch 02 — loss: 0.3284
Epoch 03 — loss: 0.2983
Epoch 04 — loss: 0.2884
Epoch 05 — loss: 0.2848
Epoch 06 — loss: 0.2808
Epoch 07 — loss: 0.2777
Epoch 08 — loss: 0.2789
Epoch 09 — loss: 0.2753
Epoch 10 — loss: 0.2731
Epoch 11 — loss: 0.2744
Epoch 12 — loss: 0.2731
Epoch 13 — loss: 0.2729
Epoch 14 — loss: 0.2718
Epoch 15 — loss: 0.2704
Epoch 16 — loss: 0.2701
Epoch 17 — loss: 0.2713
Epoch 18 — loss: 0.2699
Epoch 19 — loss: 0.2692
Epoch 20 — loss: 0.2701
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  80.320000
2    P(>V6)  83.160004
3    P(>V7)  87.199997
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 46.92%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4332
Epoch 02 — loss: 0.3292
Epoch 03 — loss: 0.3115
Epoch 04 — loss: 0.2973
Epoch 05 — loss: 0.2891
Epoch 06 — loss: 0.2879
Epoch 07 — loss: 0.2813
Epoch 08 — loss: 0.2801
Epoch 09 — loss: 0.2782
Epoch 10 — loss: 0.2784
Epoch 11 — loss: 0.2793
Epoch 12 — loss: 0.2765
Epoch 13 — loss: 0.2749
Epoch 14 — loss: 0.2757
Epoch 15 — loss: 0.2725
Epoch 16 — loss: 0.2725
Epoch 17 — loss: 0.2734
Epoch 18 — loss: 0.2708
Epoch 19 — loss: 0.2707
Epoch 20 — loss: 0.2723
  threshold   accuracy
0    P(>V4)  81.860001
1    P(>V5)  80.849998
2    P(>V6)  84.019997
3    P(>V7)  87.580002
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.15%
Ordinal stacking meta epoch 1: loss=0.1972
Ordinal stacking meta epoch 2: loss=0.1530
Ordinal stacking meta epoch 3: loss=0.1497
Ordinal stacking meta epoch 4: loss=0.1482
Ordinal stacking meta epoch 5: loss=0.1470
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001545 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [09:37:30] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.099998
2    P(>V6)  86.089996
3    P(>V7)  88.879997
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  82.529999
2    P(>V6)  85.800003
3    P(>V7)  88.839996
4    P(>V8)  92.930000
5    P(>V9)  97.059998
Overall accuracy: 49.90%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  81.910004
2    P(>V6)  85.230003
3    P(>V7)  88.160004
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.239998
2    P(>V6)  85.800003
3    P(>V7)  88.589996
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.720001
2    P(>V6)  85.080002
3    P(>V7)  89.269997
4    P(>V8)  92.589996
5    P(>V9)  96.489998
Overall accuracy: 48.12%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.389999
2    P(>V6)  85.320000
3    P(>V7)  88.739998
4    P(>V8)  92.830002
5    P(>V9)  96.489998
Overall accuracy: 48.17%
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  81.760002
2    P(>V6)  84.889999
3    P(>V7)  88.070000
4    P(>V8)  92.879997
5    P(>V9)  96.540001
Overall accuracy: 47.35%
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  82.000000
2    P(>V6)  84.790001
3    P(>V7)  88.589996
4    P(>V8)  92.489998
5    P(>V9)  96.440002
Overall accuracy: 47.50%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.099998
2    P(>V6)  86.089996
3    P(>V7)  88.879997
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 49.23%
----------------- Ordinal iteration 9/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3663
Epoch 02 — loss: 0.3145
Epoch 03 — loss: 0.2991
Epoch 04 — loss: 0.2869
Epoch 05 — loss: 0.2814
Epoch 06 — loss: 0.2765
Epoch 07 — loss: 0.2655
Epoch 08 — loss: 0.2611
Epoch 09 — loss: 0.2537
Epoch 10 — loss: 0.2481
Epoch 11 — loss: 0.2400
Epoch 12 — loss: 0.2359
Epoch 13 — loss: 0.2301
Epoch 14 — loss: 0.2248
Epoch 15 — loss: 0.2203
Epoch 16 — loss: 0.2137
Epoch 17 — loss: 0.2085
Epoch 18 — loss: 0.2039
Epoch 19 — loss: 0.1982
Epoch 20 — loss: 0.1939
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  80.029999
2    P(>V6)  83.250000
3    P(>V7)  86.860001
4    P(>V8)  91.820000
5    P(>V9)  96.730003
Overall accuracy: 45.67%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3691
Epoch 02 — loss: 0.3103
Epoch 03 — loss: 0.2985
Epoch 04 — loss: 0.2897
Epoch 05 — loss: 0.2818
Epoch 06 — loss: 0.2772
Epoch 07 — loss: 0.2690
Epoch 08 — loss: 0.2624
Epoch 09 — loss: 0.2545
Epoch 10 — loss: 0.2471
Epoch 11 — loss: 0.2412
Epoch 12 — loss: 0.2353
Epoch 13 — loss: 0.2302
Epoch 14 — loss: 0.2245
Epoch 15 — loss: 0.2182
Epoch 16 — loss: 0.2132
Epoch 17 — loss: 0.2070
Epoch 18 — loss: 0.2017
Epoch 19 — loss: 0.1956
Epoch 20 — loss: 0.1916
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  81.330002
2    P(>V6)  84.169998
3    P(>V7)  87.339996
4    P(>V8)  92.489998
5    P(>V9)  96.919998
Overall accuracy: 48.56%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3632
Epoch 02 — loss: 0.3109
Epoch 03 — loss: 0.3027
Epoch 04 — loss: 0.2936
Epoch 05 — loss: 0.2867
Epoch 06 — loss: 0.2794
Epoch 07 — loss: 0.2758
Epoch 08 — loss: 0.2721
Epoch 09 — loss: 0.2662
Epoch 10 — loss: 0.2612
Epoch 11 — loss: 0.2542
Epoch 12 — loss: 0.2469
Epoch 13 — loss: 0.2435
Epoch 14 — loss: 0.2402
Epoch 15 — loss: 0.2321
Epoch 16 — loss: 0.2262
Epoch 17 — loss: 0.2204
Epoch 18 — loss: 0.2171
Epoch 19 — loss: 0.2112
Epoch 20 — loss: 0.2065
  threshold   accuracy
0    P(>V4)  82.239998
1    P(>V5)  82.820000
2    P(>V6)  86.139999
3    P(>V7)  88.790001
4    P(>V8)  93.410004
5    P(>V9)  96.919998
Overall accuracy: 48.99%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4621
Epoch 02 — loss: 0.3306
Epoch 03 — loss: 0.3018
Epoch 04 — loss: 0.2914
Epoch 05 — loss: 0.2861
Epoch 06 — loss: 0.2816
Epoch 07 — loss: 0.2785
Epoch 08 — loss: 0.2784
Epoch 09 — loss: 0.2753
Epoch 10 — loss: 0.2743
Epoch 11 — loss: 0.2733
Epoch 12 — loss: 0.2724
Epoch 13 — loss: 0.2724
Epoch 14 — loss: 0.2708
Epoch 15 — loss: 0.2714
Epoch 16 — loss: 0.2707
Epoch 17 — loss: 0.2709
Epoch 18 — loss: 0.2703
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2693
  threshold   accuracy
0    P(>V4)  80.940002
1    P(>V5)  80.320000
2    P(>V6)  83.589996
3    P(>V7)  87.779999
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 46.05%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4663
Epoch 02 — loss: 0.3325
Epoch 03 — loss: 0.3014
Epoch 04 — loss: 0.2905
Epoch 05 — loss: 0.2852
Epoch 06 — loss: 0.2832
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2776
Epoch 09 — loss: 0.2758
Epoch 10 — loss: 0.2739
Epoch 11 — loss: 0.2709
Epoch 12 — loss: 0.2721
Epoch 13 — loss: 0.2711
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2716
Epoch 16 — loss: 0.2701
Epoch 17 — loss: 0.2715
Epoch 18 — loss: 0.2709
Epoch 19 — loss: 0.2693
Epoch 20 — loss: 0.2694
  threshold   accuracy
0    P(>V4)  82.000000
1    P(>V5)  80.750000
2    P(>V6)  83.879997
3    P(>V7)  88.070000
4    P(>V8)  92.589996
5    P(>V9)  96.970001
Overall accuracy: 46.29%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4245
Epoch 02 — loss: 0.3281
Epoch 03 — loss: 0.3130
Epoch 04 — loss: 0.3042
Epoch 05 — loss: 0.2939
Epoch 06 — loss: 0.2893
Epoch 07 — loss: 0.2859
Epoch 08 — loss: 0.2854
Epoch 09 — loss: 0.2804
Epoch 10 — loss: 0.2793
Epoch 11 — loss: 0.2765
Epoch 12 — loss: 0.2773
Epoch 13 — loss: 0.2753
Epoch 14 — loss: 0.2745
Epoch 15 — loss: 0.2752
Epoch 16 — loss: 0.2727
Epoch 17 — loss: 0.2718
Epoch 18 — loss: 0.2719
Epoch 19 — loss: 0.2711
Epoch 20 — loss: 0.2704
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.699997
2    P(>V6)  83.830002
3    P(>V7)  87.779999
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 46.49%
Ordinal stacking meta epoch 1: loss=0.3159
Ordinal stacking meta epoch 2: loss=0.1653
Ordinal stacking meta epoch 3: loss=0.1565
Ordinal stacking meta epoch 4: loss=0.1532
Ordinal stacking meta epoch 5: loss=0.1516
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003096 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.480003
2    P(>V6)  85.660004
3    P(>V7)  88.500000
4    P(>V8)  93.410004
5    P(>V9)  97.019997
Overall accuracy: 50.19%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.820000
2    P(>V6)  85.660004
3    P(>V7)  88.260002
4    P(>V8)  93.260002
5    P(>V9)  96.970001
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  82.580002
1    P(>V5)  81.570000
2    P(>V6)  84.650002
3    P(>V7)  88.309998
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 48.08%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.150002
2    P(>V6)  85.180000
3    P(>V7)  88.589996
4    P(>V8)  93.360001
5    P(>V9)  96.970001/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [10:01:52] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.339996
2    P(>V6)  84.699997
3    P(>V7)  88.739998
4    P(>V8)  92.970001
5    P(>V9)  96.870003
Overall accuracy: 47.83%
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  82.150002
2    P(>V6)  84.790001
3    P(>V7)  88.309998
4    P(>V8)  92.730003
5    P(>V9)  96.779999
Overall accuracy: 47.93%
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  81.620003
2    P(>V6)  85.230003
3    P(>V7)  88.260002
4    P(>V8)  92.830002
5    P(>V9)  96.919998
Overall accuracy: 47.69%
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  82.050003
2    P(>V6)  85.180000
3    P(>V7)  88.019997
4    P(>V8)  92.690002
5    P(>V9)  96.730003
Overall accuracy: 48.27%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.480003
2    P(>V6)  85.660004
3    P(>V7)  88.500000
4    P(>V8)  93.410004
5    P(>V9)  97.019997
Overall accuracy: 50.19%
----------------- Ordinal iteration 10/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3707
Epoch 02 — loss: 0.3127
Epoch 03 — loss: 0.3000
Epoch 04 — loss: 0.2924
Epoch 05 — loss: 0.2802
Epoch 06 — loss: 0.2739
Epoch 07 — loss: 0.2656
Epoch 08 — loss: 0.2616
Epoch 09 — loss: 0.2544
Epoch 10 — loss: 0.2488
Epoch 11 — loss: 0.2407
Epoch 12 — loss: 0.2358
Epoch 13 — loss: 0.2306
Epoch 14 — loss: 0.2246
Epoch 15 — loss: 0.2180
Epoch 16 — loss: 0.2145
Epoch 17 — loss: 0.2089
Epoch 18 — loss: 0.2036
Epoch 19 — loss: 0.1975
Epoch 20 — loss: 0.1926
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  81.860001
2    P(>V6)  83.730003
3    P(>V7)  86.720001
4    P(>V8)  93.169998
5    P(>V9)  97.019997
Overall accuracy: 47.02%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3714
Epoch 02 — loss: 0.3134
Epoch 03 — loss: 0.2981
Epoch 04 — loss: 0.2894
Epoch 05 — loss: 0.2824
Epoch 06 — loss: 0.2718
Epoch 07 — loss: 0.2618
Epoch 08 — loss: 0.2570
Epoch 09 — loss: 0.2501
Epoch 10 — loss: 0.2456
Epoch 11 — loss: 0.2361
Epoch 12 — loss: 0.2293
Epoch 13 — loss: 0.2241
Epoch 14 — loss: 0.2210
Epoch 15 — loss: 0.2175
Epoch 16 — loss: 0.2102
Epoch 17 — loss: 0.2050
Epoch 18 — loss: 0.2005
Epoch 19 — loss: 0.1949
Epoch 20 — loss: 0.1896
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  81.180000
2    P(>V6)  84.790001
3    P(>V7)  86.860001
4    P(>V8)  90.900002
5    P(>V9)  95.809998
Overall accuracy: 46.34%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3638
Epoch 02 — loss: 0.3106
Epoch 03 — loss: 0.2994
Epoch 04 — loss: 0.2920
Epoch 05 — loss: 0.2869
Epoch 06 — loss: 0.2833
Epoch 07 — loss: 0.2779
Epoch 08 — loss: 0.2715
Epoch 09 — loss: 0.2665
Epoch 10 — loss: 0.2603
Epoch 11 — loss: 0.2556
Epoch 12 — loss: 0.2508
Epoch 13 — loss: 0.2445
Epoch 14 — loss: 0.2420
Epoch 15 — loss: 0.2371
Epoch 16 — loss: 0.2350
Epoch 17 — loss: 0.2282
Epoch 18 — loss: 0.2244
Epoch 19 — loss: 0.2217
Epoch 20 — loss: 0.2161
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.050003
2    P(>V6)  84.650002
3    P(>V7)  88.400002
4    P(>V8)  92.930000
5    P(>V9)  96.680000
Overall accuracy: 47.74%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4753
Epoch 02 — loss: 0.3599
Epoch 03 — loss: 0.3053
Epoch 04 — loss: 0.2940
Epoch 05 — loss: 0.2862
Epoch 06 — loss: 0.2836
Epoch 07 — loss: 0.2804
Epoch 08 — loss: 0.2784
Epoch 09 — loss: 0.2758
Epoch 10 — loss: 0.2746
Epoch 11 — loss: 0.2748
Epoch 12 — loss: 0.2745
Epoch 13 — loss: 0.2743
Epoch 14 — loss: 0.2722
Epoch 15 — loss: 0.2732
Epoch 16 — loss: 0.2713
Epoch 17 — loss: 0.2696
Epoch 18 — loss: 0.2713
Epoch 19 — loss: 0.2694
Epoch 20 — loss: 0.2703
  threshold   accuracy
0    P(>V4)  82.000000
1    P(>V5)  80.900002
2    P(>V6)  83.690002
3    P(>V7)  87.389999
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.58%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4563
Epoch 02 — loss: 0.3182
Epoch 03 — loss: 0.2953
Epoch 04 — loss: 0.2868
Epoch 05 — loss: 0.2826
Epoch 06 — loss: 0.2809
Epoch 07 — loss: 0.2790
Epoch 08 — loss: 0.2766
Epoch 09 — loss: 0.2741
Epoch 10 — loss: 0.2750
Epoch 11 — loss: 0.2742
Epoch 12 — loss: 0.2736
Epoch 13 — loss: 0.2720
Epoch 14 — loss: 0.2712
Epoch 15 — loss: 0.2712
Epoch 16 — loss: 0.2699
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2703
Epoch 19 — loss: 0.2695
Epoch 20 — loss: 0.2707
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.650002
2    P(>V6)  84.169998
3    P(>V7)  87.629997
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.33%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4262
Epoch 02 — loss: 0.3274
Epoch 03 — loss: 0.3066
Epoch 04 — loss: 0.2958
Epoch 05 — loss: 0.2890
Epoch 06 — loss: 0.2878
Epoch 07 — loss: 0.2844
Epoch 08 — loss: 0.2809
Epoch 09 — loss: 0.2809
Epoch 10 — loss: 0.2775
Epoch 11 — loss: 0.2766
Epoch 12 — loss: 0.2768
Epoch 13 — loss: 0.2756
Epoch 14 — loss: 0.2752
Epoch 15 — loss: 0.2724
Epoch 16 — loss: 0.2718
Epoch 17 — loss: 0.2718
Epoch 18 — loss: 0.2719
Epoch 19 — loss: 0.2733
Epoch 20 — loss: 0.2710
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  81.180000
2    P(>V6)  83.730003
3    P(>V7)  88.019997
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.29%
Ordinal stacking meta epoch 1: loss=0.2475
Ordinal stacking meta epoch 2: loss=0.1632
Ordinal stacking meta epoch 3: loss=0.1563
Ordinal stacking meta epoch 4: loss=0.1532
Ordinal stacking meta epoch 5: loss=0.1513
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003625 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.629997
2    P(>V6)  85.470001
3    P(>V7)  88.790001
4    P(>V8)  93.309998
5    P(>V9)  97.019997
Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.870003
2    P(>V6)  85.510002
3    P(>V7)  88.639999
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  81.330002
2    P(>V6)  84.739998
3    P(>V7)  88.309998
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 47.79%
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  82.440002
2    P(>V6)  85.269997
3    P(>V7)  88.639999
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.720001
2    P(>V6)  85.419998
3    P(>V7)  88.070000
4    P(>V8)  92.589996
5    P(>V9)  96.730003
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.629997
2    P(>V6)  85.470001
3    P(>V7)  88.309998
4    P(>V8)  92.639999
5    P(>V9)  96.580002
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.050003
2    P(>V6)  84.599998
3    P(>V7)  87.820000
4    P(>V8)  92.400002
5    P(>V9)  96.580002
Overall accuracy: 46.92%
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  82.150002
2    P(>V6)  85.559998
3    P(>V7)  87.489998
4    P(>V8)  92.540001
5    P(>V9)  96.779999
Overall accuracy: 47.64%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.629997
2    P(>V6)  85.470001
3    P(>V7)  88.790001
4    P(>V8)  93.309998
5    P(>V9)  97.019997
Overall accuracy: 49.18%
----------------- Ordinal iteration 11/25 -----------------/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [10:24:58] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3919
Epoch 02 — loss: 0.3324
Epoch 03 — loss: 0.3146
Epoch 04 — loss: 0.2989
Epoch 05 — loss: 0.2865
Epoch 06 — loss: 0.2748
Epoch 07 — loss: 0.2645
Epoch 08 — loss: 0.2534
Epoch 09 — loss: 0.2485
Epoch 10 — loss: 0.2418
Epoch 11 — loss: 0.2338
Epoch 12 — loss: 0.2277
Epoch 13 — loss: 0.2216
Epoch 14 — loss: 0.2167
Epoch 15 — loss: 0.2113
Epoch 16 — loss: 0.2041
Epoch 17 — loss: 0.1977
Epoch 18 — loss: 0.1932
Epoch 19 — loss: 0.1881
Epoch 20 — loss: 0.1813
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  81.470001
2    P(>V6)  85.269997
3    P(>V7)  87.250000
4    P(>V8)  92.830002
5    P(>V9)  96.540001
Overall accuracy: 47.59%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3625
Epoch 02 — loss: 0.3068
Epoch 03 — loss: 0.2937
Epoch 04 — loss: 0.2844
Epoch 05 — loss: 0.2739
Epoch 06 — loss: 0.2654
Epoch 07 — loss: 0.2583
Epoch 08 — loss: 0.2513
Epoch 09 — loss: 0.2449
Epoch 10 — loss: 0.2364
Epoch 11 — loss: 0.2340
Epoch 12 — loss: 0.2266
Epoch 13 — loss: 0.2235
Epoch 14 — loss: 0.2173
Epoch 15 — loss: 0.2114
Epoch 16 — loss: 0.2074
Epoch 17 — loss: 0.2017
Epoch 18 — loss: 0.1987
Epoch 19 — loss: 0.1942
Epoch 20 — loss: 0.1873
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  81.470001
2    P(>V6)  85.660004
3    P(>V7)  87.919998
4    P(>V8)  92.540001
5    P(>V9)  96.820000
Overall accuracy: 47.40%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3605
Epoch 02 — loss: 0.3129
Epoch 03 — loss: 0.3024
Epoch 04 — loss: 0.2953
Epoch 05 — loss: 0.2885
Epoch 06 — loss: 0.2800
Epoch 07 — loss: 0.2776
Epoch 08 — loss: 0.2716
Epoch 09 — loss: 0.2656
Epoch 10 — loss: 0.2594
Epoch 11 — loss: 0.2535
Epoch 12 — loss: 0.2451
Epoch 13 — loss: 0.2404
Epoch 14 — loss: 0.2344
Epoch 15 — loss: 0.2302
Epoch 16 — loss: 0.2232
Epoch 17 — loss: 0.2194
Epoch 18 — loss: 0.2156
Epoch 19 — loss: 0.2088
Epoch 20 — loss: 0.2077
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  80.559998
2    P(>V6)  84.599998
3    P(>V7)  87.339996
4    P(>V8)  92.589996
5    P(>V9)  96.730003
Overall accuracy: 46.05%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4633
Epoch 02 — loss: 0.3300
Epoch 03 — loss: 0.3014
Epoch 04 — loss: 0.2920
Epoch 05 — loss: 0.2866
Epoch 06 — loss: 0.2830
Epoch 07 — loss: 0.2803
Epoch 08 — loss: 0.2775
Epoch 09 — loss: 0.2768
Epoch 10 — loss: 0.2759
Epoch 11 — loss: 0.2735
Epoch 12 — loss: 0.2743
Epoch 13 — loss: 0.2731
Epoch 14 — loss: 0.2728
Epoch 15 — loss: 0.2714
Epoch 16 — loss: 0.2704
Epoch 17 — loss: 0.2712
Epoch 18 — loss: 0.2698
Epoch 19 — loss: 0.2692
Epoch 20 — loss: 0.2680
  threshold   accuracy
0    P(>V4)  82.239998
1    P(>V5)  80.800003
2    P(>V6)  83.830002
3    P(>V7)  87.870003
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 46.29%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4716
Epoch 02 — loss: 0.3467
Epoch 03 — loss: 0.3025
Epoch 04 — loss: 0.2907
Epoch 05 — loss: 0.2851
Epoch 06 — loss: 0.2821
Epoch 07 — loss: 0.2788
Epoch 08 — loss: 0.2771
Epoch 09 — loss: 0.2753
Epoch 10 — loss: 0.2746
Epoch 11 — loss: 0.2745
Epoch 12 — loss: 0.2747
Epoch 13 — loss: 0.2722
Epoch 14 — loss: 0.2714
Epoch 15 — loss: 0.2690
Epoch 16 — loss: 0.2708
Epoch 17 — loss: 0.2708
Epoch 18 — loss: 0.2695
Epoch 19 — loss: 0.2690
Epoch 20 — loss: 0.2708
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.459999
2    P(>V6)  83.930000
3    P(>V7)  87.870003
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.05%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4157
Epoch 02 — loss: 0.3248
Epoch 03 — loss: 0.3115
Epoch 04 — loss: 0.2963
Epoch 05 — loss: 0.2907
Epoch 06 — loss: 0.2857
Epoch 07 — loss: 0.2835
Epoch 08 — loss: 0.2809
Epoch 09 — loss: 0.2790
Epoch 10 — loss: 0.2769
Epoch 11 — loss: 0.2782
Epoch 12 — loss: 0.2757
Epoch 13 — loss: 0.2746
Epoch 14 — loss: 0.2751
Epoch 15 — loss: 0.2736
Epoch 16 — loss: 0.2734
Epoch 17 — loss: 0.2740
Epoch 18 — loss: 0.2732
Epoch 19 — loss: 0.2717
Epoch 20 — loss: 0.2711
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  80.989998
2    P(>V6)  83.879997
3    P(>V7)  87.540001
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 46.78%
Ordinal stacking meta epoch 1: loss=0.2472
Ordinal stacking meta epoch 2: loss=0.1646
Ordinal stacking meta epoch 3: loss=0.1538
Ordinal stacking meta epoch 4: loss=0.1500
Ordinal stacking meta epoch 5: loss=0.1481
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002084 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.629997
2    P(>V6)  86.139999
3    P(>V7)  88.879997
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.239998
2    P(>V6)  86.190002
3    P(>V7)  88.639999
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  81.570000
2    P(>V6)  84.889999
3    P(>V7)  88.550003
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 47.98%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.239998
2    P(>V6)  85.849998
3    P(>V7)  88.500000
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.050003
2    P(>V6)  86.190002
3    P(>V7)  89.169998
4    P(>V8)  92.830002
5    P(>V9)  96.580002
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.440002
2    P(>V6)  86.669998
3    P(>V7)  88.500000
4    P(>V8)  92.830002
5    P(>V9)  96.680000
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  82.099998
2    P(>V6)  85.949997
3    P(>V7)  88.930000
4    P(>V8)  93.019997
5    P(>V9)  96.389999
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.050003
2    P(>V6)  86.239998
3    P(>V7)  89.080002
4    P(>V8)  93.169998
5    P(>V9)  96.489998
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.629997
2    P(>V6)  86.139999
3    P(>V7)  88.879997
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 49.57%
----------------- Ordinal iteration 12/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3832
Epoch 02 — loss: 0.3233
Epoch 03 — loss: 0.3054
Epoch 04 — loss: 0.2944
Epoch 05 — loss: 0.2816
Epoch 06 — loss: 0.2720
Epoch 07 — loss: 0.2645
Epoch 08 — loss: 0.2553
Epoch 09 — loss: 0.2461
Epoch 10 — loss: 0.2423
Epoch 11 — loss: 0.2339
Epoch 12 — loss: 0.2263
Epoch 13 — loss: 0.2206
Epoch 14 — loss: 0.2146
Epoch 15 — loss: 0.2084
Epoch 16 — loss: 0.2012
Epoch 17 — loss: 0.1964
Epoch 18 — loss: 0.1903
Epoch 19 — loss: 0.1848
Epoch 20 — loss: 0.1796
  threshold   accuracy
0    P(>V4)  82.529999
1    P(>V5)  82.190002
2    P(>V6)  84.459999
3    P(>V7)  88.019997
4    P(>V8)  93.169998
5    P(>V9)  96.730003
Overall accuracy: 48.36%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3751
Epoch 02 — loss: 0.3114
Epoch 03 — loss: 0.2997
Epoch 04 — loss: 0.2870
Epoch 05 — loss: 0.2801
Epoch 06 — loss: 0.2700
Epoch 07 — loss: 0.2616/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [10:48:24] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 08 — loss: 0.2525
Epoch 09 — loss: 0.2447
Epoch 10 — loss: 0.2388
Epoch 11 — loss: 0.2322
Epoch 12 — loss: 0.2256
Epoch 13 — loss: 0.2208
Epoch 14 — loss: 0.2155
Epoch 15 — loss: 0.2087
Epoch 16 — loss: 0.2033
Epoch 17 — loss: 0.1974
Epoch 18 — loss: 0.1923
Epoch 19 — loss: 0.1844
Epoch 20 — loss: 0.1817
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  82.769997
2    P(>V6)  85.470001
3    P(>V7)  88.160004
4    P(>V8)  93.260002
5    P(>V9)  96.779999
Overall accuracy: 49.28%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3661
Epoch 02 — loss: 0.3156
Epoch 03 — loss: 0.3013
Epoch 04 — loss: 0.2972
Epoch 05 — loss: 0.2923
Epoch 06 — loss: 0.2847
Epoch 07 — loss: 0.2819
Epoch 08 — loss: 0.2758
Epoch 09 — loss: 0.2719
Epoch 10 — loss: 0.2674
Epoch 11 — loss: 0.2596
Epoch 12 — loss: 0.2530
Epoch 13 — loss: 0.2501
Epoch 14 — loss: 0.2449
Epoch 15 — loss: 0.2385
Epoch 16 — loss: 0.2349
Epoch 17 — loss: 0.2291
Epoch 18 — loss: 0.2244
Epoch 19 — loss: 0.2195
Epoch 20 — loss: 0.2136
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  83.349998
2    P(>V6)  84.989998
3    P(>V7)  87.870003
4    P(>V8)  92.300003
5    P(>V9)  96.970001
Overall accuracy: 48.85%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4691
Epoch 02 — loss: 0.3477
Epoch 03 — loss: 0.3000
Epoch 04 — loss: 0.2872
Epoch 05 — loss: 0.2837
Epoch 06 — loss: 0.2805
Epoch 07 — loss: 0.2786
Epoch 08 — loss: 0.2766
Epoch 09 — loss: 0.2747
Epoch 10 — loss: 0.2744
Epoch 11 — loss: 0.2730
Epoch 12 — loss: 0.2737
Epoch 13 — loss: 0.2710
Epoch 14 — loss: 0.2706
Epoch 15 — loss: 0.2706
Epoch 16 — loss: 0.2713
Epoch 17 — loss: 0.2707
Epoch 18 — loss: 0.2702
Epoch 19 — loss: 0.2708
Epoch 20 — loss: 0.2709
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  80.989998
2    P(>V6)  84.019997
3    P(>V7)  87.870003
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.96%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4711
Epoch 02 — loss: 0.3301
Epoch 03 — loss: 0.2988
Epoch 04 — loss: 0.2897
Epoch 05 — loss: 0.2843
Epoch 06 — loss: 0.2817
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2762
Epoch 09 — loss: 0.2767
Epoch 10 — loss: 0.2740
Epoch 11 — loss: 0.2743
Epoch 12 — loss: 0.2717
Epoch 13 — loss: 0.2725
Epoch 14 — loss: 0.2719
Epoch 15 — loss: 0.2728
Epoch 16 — loss: 0.2712
Epoch 17 — loss: 0.2700
Epoch 18 — loss: 0.2678
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2697
  threshold   accuracy
0    P(>V4)  81.089996
1    P(>V5)  80.559998
2    P(>V6)  83.830002
3    P(>V7)  87.580002
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 45.67%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4297
Epoch 02 — loss: 0.3278
Epoch 03 — loss: 0.3088
Epoch 04 — loss: 0.2978
Epoch 05 — loss: 0.2911
Epoch 06 — loss: 0.2864
Epoch 07 — loss: 0.2819
Epoch 08 — loss: 0.2804
Epoch 09 — loss: 0.2807
Epoch 10 — loss: 0.2785
Epoch 11 — loss: 0.2774
Epoch 12 — loss: 0.2763
Epoch 13 — loss: 0.2756
Epoch 14 — loss: 0.2760
Epoch 15 — loss: 0.2741
Epoch 16 — loss: 0.2727
Epoch 17 — loss: 0.2740
Epoch 18 — loss: 0.2718
Epoch 19 — loss: 0.2732
Epoch 20 — loss: 0.2729
  threshold   accuracy
0    P(>V4)  81.089996
1    P(>V5)  80.900002
2    P(>V6)  84.739998
3    P(>V7)  87.820000
4    P(>V8)  92.540001
5    P(>V9)  96.970001
Overall accuracy: 45.77%
Ordinal stacking meta epoch 1: loss=0.1728
Ordinal stacking meta epoch 2: loss=0.1436
Ordinal stacking meta epoch 3: loss=0.1409
Ordinal stacking meta epoch 4: loss=0.1397
Ordinal stacking meta epoch 5: loss=0.1388
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001474 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  83.160004
2    P(>V6)  85.849998
3    P(>V7)  88.839996
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.769997
2    P(>V6)  85.760002
3    P(>V7)  87.919998
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  82.720001
1    P(>V5)  81.570000
2    P(>V6)  84.599998
3    P(>V7)  87.919998
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 47.31%
  threshold   accuracy
0    P(>V4)  83.160004
1    P(>V5)  82.870003
2    P(>V6)  85.610001
3    P(>V7)  88.639999
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  83.300003
2    P(>V6)  86.279999
3    P(>V7)  88.980003
4    P(>V8)  93.260002
5    P(>V9)  96.629997
Overall accuracy: 49.62%
  threshold   accuracy
0    P(>V4)  82.959999
1    P(>V5)  83.199997
2    P(>V6)  86.430000
3    P(>V7)  88.400002
4    P(>V8)  92.970001
5    P(>V9)  96.580002
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  82.959999
1    P(>V5)  83.160004
2    P(>V6)  86.529999
3    P(>V7)  88.019997
4    P(>V8)  93.019997
5    P(>V9)  96.440002
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  82.680000
1    P(>V5)  83.110001
2    P(>V6)  86.769997
3    P(>V7)  88.209999
4    P(>V8)  93.169998
5    P(>V9)  96.580002
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  83.160004
2    P(>V6)  85.849998
3    P(>V7)  88.839996
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 49.23%
----------------- Ordinal iteration 13/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3795
Epoch 02 — loss: 0.3245
Epoch 03 — loss: 0.3080
Epoch 04 — loss: 0.3000
Epoch 05 — loss: 0.2876
Epoch 06 — loss: 0.2788
Epoch 07 — loss: 0.2703
Epoch 08 — loss: 0.2627
Epoch 09 — loss: 0.2561
Epoch 10 — loss: 0.2497
Epoch 11 — loss: 0.2432
Epoch 12 — loss: 0.2362
Epoch 13 — loss: 0.2299
Epoch 14 — loss: 0.2243
Epoch 15 — loss: 0.2175
Epoch 16 — loss: 0.2128
Epoch 17 — loss: 0.2068
Epoch 18 — loss: 0.2041
Epoch 19 — loss: 0.1951
Epoch 20 — loss: 0.1894
  threshold   accuracy
0    P(>V4)  82.629997
1    P(>V5)  81.330002
2    P(>V6)  83.589996
3    P(>V7)  87.339996
4    P(>V8)  92.110001
5    P(>V9)  96.680000
Overall accuracy: 46.87%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3743
Epoch 02 — loss: 0.3134
Epoch 03 — loss: 0.2997
Epoch 04 — loss: 0.2908
Epoch 05 — loss: 0.2803
Epoch 06 — loss: 0.2730
Epoch 07 — loss: 0.2638
Epoch 08 — loss: 0.2581
Epoch 09 — loss: 0.2476
Epoch 10 — loss: 0.2419
Epoch 11 — loss: 0.2347
Epoch 12 — loss: 0.2279
Epoch 13 — loss: 0.2214
Epoch 14 — loss: 0.2191
Epoch 15 — loss: 0.2106
Epoch 16 — loss: 0.2028
Epoch 17 — loss: 0.1998
Epoch 18 — loss: 0.1938
Epoch 19 — loss: 0.1864
Epoch 20 — loss: 0.1829
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  82.389999
2    P(>V6)  84.410004
3    P(>V7)  88.400002
4    P(>V8)  92.010002
5    P(>V9)  96.389999
Overall accuracy: 47.93%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3684
Epoch 02 — loss: 0.3185
Epoch 03 — loss: 0.3028
Epoch 04 — loss: 0.2959
Epoch 05 — loss: 0.2885
Epoch 06 — loss: 0.2835
Epoch 07 — loss: 0.2789
Epoch 08 — loss: 0.2693
Epoch 09 — loss: 0.2663
Epoch 10 — loss: 0.2588
Epoch 11 — loss: 0.2526
Epoch 12 — loss: 0.2464
Epoch 13 — loss: 0.2424
Epoch 14 — loss: 0.2364
Epoch 15 — loss: 0.2295
Epoch 16 — loss: 0.2273/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [11:11:38] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 17 — loss: 0.2223
Epoch 18 — loss: 0.2177
Epoch 19 — loss: 0.2162
Epoch 20 — loss: 0.2109
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.480003
2    P(>V6)  84.790001
3    P(>V7)  88.349998
4    P(>V8)  93.019997
5    P(>V9)  96.779999
Overall accuracy: 49.47%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4689
Epoch 02 — loss: 0.3385
Epoch 03 — loss: 0.3054
Epoch 04 — loss: 0.2929
Epoch 05 — loss: 0.2860
Epoch 06 — loss: 0.2822
Epoch 07 — loss: 0.2791
Epoch 08 — loss: 0.2770
Epoch 09 — loss: 0.2764
Epoch 10 — loss: 0.2746
Epoch 11 — loss: 0.2737
Epoch 12 — loss: 0.2740
Epoch 13 — loss: 0.2715
Epoch 14 — loss: 0.2710
Epoch 15 — loss: 0.2711
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2691
Epoch 18 — loss: 0.2690
Epoch 19 — loss: 0.2706
Epoch 20 — loss: 0.2697
  threshold   accuracy
0    P(>V4)  81.089996
1    P(>V5)  80.029999
2    P(>V6)  83.199997
3    P(>V7)  87.300003
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 46.01%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4654
Epoch 02 — loss: 0.3349
Epoch 03 — loss: 0.2975
Epoch 04 — loss: 0.2883
Epoch 05 — loss: 0.2828
Epoch 06 — loss: 0.2791
Epoch 07 — loss: 0.2759
Epoch 08 — loss: 0.2763
Epoch 09 — loss: 0.2734
Epoch 10 — loss: 0.2723
Epoch 11 — loss: 0.2727
Epoch 12 — loss: 0.2731
Epoch 13 — loss: 0.2722
Epoch 14 — loss: 0.2706
Epoch 15 — loss: 0.2698
Epoch 16 — loss: 0.2696
Epoch 17 — loss: 0.2692
Epoch 18 — loss: 0.2694
Epoch 19 — loss: 0.2690
Epoch 20 — loss: 0.2689
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.849998
2    P(>V6)  83.489998
3    P(>V7)  87.820000
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.67%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4308
Epoch 02 — loss: 0.3275
Epoch 03 — loss: 0.3119
Epoch 04 — loss: 0.3021
Epoch 05 — loss: 0.2929
Epoch 06 — loss: 0.2887
Epoch 07 — loss: 0.2856
Epoch 08 — loss: 0.2822
Epoch 09 — loss: 0.2807
Epoch 10 — loss: 0.2762
Epoch 11 — loss: 0.2779
Epoch 12 — loss: 0.2753
Epoch 13 — loss: 0.2750
Epoch 14 — loss: 0.2759
Epoch 15 — loss: 0.2734
Epoch 16 — loss: 0.2749
Epoch 17 — loss: 0.2711
Epoch 18 — loss: 0.2738
Epoch 19 — loss: 0.2737
Epoch 20 — loss: 0.2713
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.989998
2    P(>V6)  84.070000
3    P(>V7)  87.389999
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.10%
Ordinal stacking meta epoch 1: loss=0.2495
Ordinal stacking meta epoch 2: loss=0.1600
Ordinal stacking meta epoch 3: loss=0.1519
Ordinal stacking meta epoch 4: loss=0.1478
Ordinal stacking meta epoch 5: loss=0.1453
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001493 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.629997
2    P(>V6)  85.269997
3    P(>V7)  88.930000
4    P(>V8)  93.019997
5    P(>V9)  97.059998
Overall accuracy: 49.33%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.580002
2    P(>V6)  84.889999
3    P(>V7)  88.550003
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  82.529999
1    P(>V5)  81.570000
2    P(>V6)  83.970001
3    P(>V7)  88.349998
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 47.79%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.290001
2    P(>V6)  84.940002
3    P(>V7)  88.879997
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 48.89%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  83.110001
2    P(>V6)  84.739998
3    P(>V7)  88.930000
4    P(>V8)  92.779999
5    P(>V9)  96.779999
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.290001
2    P(>V6)  84.839996
3    P(>V7)  88.550003
4    P(>V8)  92.639999
5    P(>V9)  96.339996
Overall accuracy: 47.98%
  threshold   accuracy
0    P(>V4)  82.959999
1    P(>V5)  81.760002
2    P(>V6)  84.410004
3    P(>V7)  88.349998
4    P(>V8)  92.690002
5    P(>V9)  96.199997
Overall accuracy: 47.02%
  threshold   accuracy
0    P(>V4)  82.629997
1    P(>V5)  81.949997
2    P(>V6)  84.019997
3    P(>V7)  88.309998
4    P(>V8)  92.440002
5    P(>V9)  96.250000
Overall accuracy: 46.58%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.629997
2    P(>V6)  85.269997
3    P(>V7)  88.930000
4    P(>V8)  93.019997
5    P(>V9)  97.059998
Overall accuracy: 49.33%
----------------- Ordinal iteration 14/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3703
Epoch 02 — loss: 0.3095
Epoch 03 — loss: 0.2972
Epoch 04 — loss: 0.2891
Epoch 05 — loss: 0.2790
Epoch 06 — loss: 0.2723
Epoch 07 — loss: 0.2682
Epoch 08 — loss: 0.2594
Epoch 09 — loss: 0.2520
Epoch 10 — loss: 0.2437
Epoch 11 — loss: 0.2400
Epoch 12 — loss: 0.2325
Epoch 13 — loss: 0.2251
Epoch 14 — loss: 0.2207
Epoch 15 — loss: 0.2168
Epoch 16 — loss: 0.2097
Epoch 17 — loss: 0.2051
Epoch 18 — loss: 0.2007
Epoch 19 — loss: 0.1943
Epoch 20 — loss: 0.1889
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  81.949997
2    P(>V6)  84.989998
3    P(>V7)  87.730003
4    P(>V8)  92.690002
5    P(>V9)  96.870003
Overall accuracy: 48.60%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3676
Epoch 02 — loss: 0.3101
Epoch 03 — loss: 0.2958
Epoch 04 — loss: 0.2879
Epoch 05 — loss: 0.2775
Epoch 06 — loss: 0.2687
Epoch 07 — loss: 0.2606
Epoch 08 — loss: 0.2536
Epoch 09 — loss: 0.2422
Epoch 10 — loss: 0.2361
Epoch 11 — loss: 0.2330
Epoch 12 — loss: 0.2267
Epoch 13 — loss: 0.2200
Epoch 14 — loss: 0.2138
Epoch 15 — loss: 0.2100
Epoch 16 — loss: 0.2046
Epoch 17 — loss: 0.2008
Epoch 18 — loss: 0.1924
Epoch 19 — loss: 0.1883
Epoch 20 — loss: 0.1843
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  80.900002
2    P(>V6)  85.370003
3    P(>V7)  87.970001
4    P(>V8)  92.059998
5    P(>V9)  96.629997
Overall accuracy: 48.12%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3639
Epoch 02 — loss: 0.3109
Epoch 03 — loss: 0.3009
Epoch 04 — loss: 0.2923
Epoch 05 — loss: 0.2845
Epoch 06 — loss: 0.2777
Epoch 07 — loss: 0.2710
Epoch 08 — loss: 0.2649
Epoch 09 — loss: 0.2573
Epoch 10 — loss: 0.2528
Epoch 11 — loss: 0.2473
Epoch 12 — loss: 0.2388
Epoch 13 — loss: 0.2354
Epoch 14 — loss: 0.2309
Epoch 15 — loss: 0.2255
Epoch 16 — loss: 0.2202
Epoch 17 — loss: 0.2165
Epoch 18 — loss: 0.2134
Epoch 19 — loss: 0.2070
Epoch 20 — loss: 0.2008
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  81.910004
2    P(>V6)  84.839996
3    P(>V7)  87.730003
4    P(>V8)  92.110001
5    P(>V9)  96.629997
Overall accuracy: 46.49%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4814
Epoch 02 — loss: 0.3611
Epoch 03 — loss: 0.3085
Epoch 04 — loss: 0.2936
Epoch 05 — loss: 0.2870
Epoch 06 — loss: 0.2826
Epoch 07 — loss: 0.2798
Epoch 08 — loss: 0.2775
Epoch 09 — loss: 0.2767
Epoch 10 — loss: 0.2753
Epoch 11 — loss: 0.2737
Epoch 12 — loss: 0.2713
Epoch 13 — loss: 0.2740
Epoch 14 — loss: 0.2725
Epoch 15 — loss: 0.2719
Epoch 16 — loss: 0.2705
Epoch 17 — loss: 0.2703
Epoch 18 — loss: 0.2699
Epoch 19 — loss: 0.2687
Epoch 20 — loss: 0.2688
  threshold   accuracy
0    P(>V4)  82.440002
1    P(>V5)  80.750000
2    P(>V6)  84.070000
3    P(>V7)  87.730003
4    P(>V8)  92.639999
5    P(>V9)  96.970001/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [11:34:58] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 46.01%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4677
Epoch 02 — loss: 0.3379
Epoch 03 — loss: 0.2993
Epoch 04 — loss: 0.2886
Epoch 05 — loss: 0.2867
Epoch 06 — loss: 0.2828
Epoch 07 — loss: 0.2804
Epoch 08 — loss: 0.2770
Epoch 09 — loss: 0.2764
Epoch 10 — loss: 0.2767
Epoch 11 — loss: 0.2762
Epoch 12 — loss: 0.2741
Epoch 13 — loss: 0.2746
Epoch 14 — loss: 0.2733
Epoch 15 — loss: 0.2725
Epoch 16 — loss: 0.2720
Epoch 17 — loss: 0.2696
Epoch 18 — loss: 0.2710
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2704
  threshold   accuracy
0    P(>V4)  82.000000
1    P(>V5)  80.849998
2    P(>V6)  84.410004
3    P(>V7)  87.580002
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 46.73%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4323
Epoch 02 — loss: 0.3347
Epoch 03 — loss: 0.3133
Epoch 04 — loss: 0.3025
Epoch 05 — loss: 0.2955
Epoch 06 — loss: 0.2911
Epoch 07 — loss: 0.2861
Epoch 08 — loss: 0.2848
Epoch 09 — loss: 0.2813
Epoch 10 — loss: 0.2826
Epoch 11 — loss: 0.2790
Epoch 12 — loss: 0.2782
Epoch 13 — loss: 0.2745
Epoch 14 — loss: 0.2745
Epoch 15 — loss: 0.2745
Epoch 16 — loss: 0.2746
Epoch 17 — loss: 0.2743
Epoch 18 — loss: 0.2744
Epoch 19 — loss: 0.2728
Epoch 20 — loss: 0.2731
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  80.940002
2    P(>V6)  83.930000
3    P(>V7)  87.440002
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 46.05%
Ordinal stacking meta epoch 1: loss=0.2379
Ordinal stacking meta epoch 2: loss=0.1585
Ordinal stacking meta epoch 3: loss=0.1525
Ordinal stacking meta epoch 4: loss=0.1500
Ordinal stacking meta epoch 5: loss=0.1484
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001548 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.290001
2    P(>V6)  85.660004
3    P(>V7)  88.930000
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.820000
2    P(>V6)  85.800003
3    P(>V7)  88.500000
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 48.80%
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  81.860001
2    P(>V6)  84.940002
3    P(>V7)  88.110001
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 48.08%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.239998
2    P(>V6)  85.660004
3    P(>V7)  88.589996
4    P(>V8)  92.930000
5    P(>V9)  97.059998
Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  82.680000
2    P(>V6)  86.620003
3    P(>V7)  89.559998
4    P(>V8)  92.489998
5    P(>V9)  96.820000
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.629997
2    P(>V6)  86.089996
3    P(>V7)  88.980003
4    P(>V8)  92.540001
5    P(>V9)  96.489998
Overall accuracy: 48.27%
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  82.050003
2    P(>V6)  86.239998
3    P(>V7)  88.879997
4    P(>V8)  92.059998
5    P(>V9)  96.440002
Overall accuracy: 47.74%
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  82.720001
2    P(>V6)  85.949997
3    P(>V7)  88.790001
4    P(>V8)  92.440002
5    P(>V9)  96.489998
Overall accuracy: 47.83%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.290001
2    P(>V6)  85.660004
3    P(>V7)  88.930000
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 49.04%
----------------- Ordinal iteration 15/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3816
Epoch 02 — loss: 0.3229
Epoch 03 — loss: 0.3040
Epoch 04 — loss: 0.2906
Epoch 05 — loss: 0.2820
Epoch 06 — loss: 0.2706
Epoch 07 — loss: 0.2629
Epoch 08 — loss: 0.2541
Epoch 09 — loss: 0.2489
Epoch 10 — loss: 0.2404
Epoch 11 — loss: 0.2354
Epoch 12 — loss: 0.2300
Epoch 13 — loss: 0.2218
Epoch 14 — loss: 0.2179
Epoch 15 — loss: 0.2109
Epoch 16 — loss: 0.2047
Epoch 17 — loss: 0.2003
Epoch 18 — loss: 0.1975
Epoch 19 — loss: 0.1895
Epoch 20 — loss: 0.1841
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  79.790001
2    P(>V6)  83.970001
3    P(>V7)  88.260002
4    P(>V8)  92.160004
5    P(>V9)  96.970001
Overall accuracy: 47.16%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3678
Epoch 02 — loss: 0.3100
Epoch 03 — loss: 0.2970
Epoch 04 — loss: 0.2844
Epoch 05 — loss: 0.2774
Epoch 06 — loss: 0.2687
Epoch 07 — loss: 0.2592
Epoch 08 — loss: 0.2503
Epoch 09 — loss: 0.2450
Epoch 10 — loss: 0.2380
Epoch 11 — loss: 0.2335
Epoch 12 — loss: 0.2249
Epoch 13 — loss: 0.2189
Epoch 14 — loss: 0.2154
Epoch 15 — loss: 0.2089
Epoch 16 — loss: 0.2027
Epoch 17 — loss: 0.2006
Epoch 18 — loss: 0.1922
Epoch 19 — loss: 0.1885
Epoch 20 — loss: 0.1827
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  82.389999
2    P(>V6)  85.849998
3    P(>V7)  87.629997
4    P(>V8)  93.120003
5    P(>V9)  96.730003
Overall accuracy: 47.79%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3595
Epoch 02 — loss: 0.3116
Epoch 03 — loss: 0.3040
Epoch 04 — loss: 0.2917
Epoch 05 — loss: 0.2884
Epoch 06 — loss: 0.2800
Epoch 07 — loss: 0.2718
Epoch 08 — loss: 0.2638
Epoch 09 — loss: 0.2567
Epoch 10 — loss: 0.2516
Epoch 11 — loss: 0.2431
Epoch 12 — loss: 0.2400
Epoch 13 — loss: 0.2324
Epoch 14 — loss: 0.2283
Epoch 15 — loss: 0.2234
Epoch 16 — loss: 0.2177
Epoch 17 — loss: 0.2134
Epoch 18 — loss: 0.2065
Epoch 19 — loss: 0.2035
Epoch 20 — loss: 0.1970
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.239998
2    P(>V6)  85.510002
3    P(>V7)  88.400002
4    P(>V8)  92.639999
5    P(>V9)  96.820000
Overall accuracy: 48.22%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4601
Epoch 02 — loss: 0.3282
Epoch 03 — loss: 0.2977
Epoch 04 — loss: 0.2895
Epoch 05 — loss: 0.2855
Epoch 06 — loss: 0.2796
Epoch 07 — loss: 0.2784
Epoch 08 — loss: 0.2777
Epoch 09 — loss: 0.2769
Epoch 10 — loss: 0.2753
Epoch 11 — loss: 0.2741
Epoch 12 — loss: 0.2714
Epoch 13 — loss: 0.2721
Epoch 14 — loss: 0.2721
Epoch 15 — loss: 0.2710
Epoch 16 — loss: 0.2700
Epoch 17 — loss: 0.2694
Epoch 18 — loss: 0.2702
Epoch 19 — loss: 0.2709
Epoch 20 — loss: 0.2699
  threshold   accuracy
0    P(>V4)  81.279999
1    P(>V5)  80.610001
2    P(>V6)  83.589996
3    P(>V7)  87.820000
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 44.51%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4596
Epoch 02 — loss: 0.3240
Epoch 03 — loss: 0.2952
Epoch 04 — loss: 0.2892
Epoch 05 — loss: 0.2848
Epoch 06 — loss: 0.2798
Epoch 07 — loss: 0.2795
Epoch 08 — loss: 0.2791
Epoch 09 — loss: 0.2742
Epoch 10 — loss: 0.2743
Epoch 11 — loss: 0.2750
Epoch 12 — loss: 0.2738
Epoch 13 — loss: 0.2725
Epoch 14 — loss: 0.2731
Epoch 15 — loss: 0.2707
Epoch 16 — loss: 0.2710
Epoch 17 — loss: 0.2705
Epoch 18 — loss: 0.2708
Epoch 19 — loss: 0.2705
Epoch 20 — loss: 0.2693
  threshold   accuracy
0    P(>V4)  81.910004
1    P(>V5)  80.989998
2    P(>V6)  83.879997
3    P(>V7)  87.680000
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.15%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4371
Epoch 02 — loss: 0.3286
Epoch 03 — loss: 0.3158
Epoch 04 — loss: 0.3045
Epoch 05 — loss: 0.2946
Epoch 06 — loss: 0.2908
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [11:58:14] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [12:20:46] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch 07 — loss: 0.2858
Epoch 08 — loss: 0.2850
Epoch 09 — loss: 0.2812
Epoch 10 — loss: 0.2806
Epoch 11 — loss: 0.2777
Epoch 12 — loss: 0.2780
Epoch 13 — loss: 0.2756
Epoch 14 — loss: 0.2747
Epoch 15 — loss: 0.2750
Epoch 16 — loss: 0.2750
Epoch 17 — loss: 0.2726
Epoch 18 — loss: 0.2725
Epoch 19 — loss: 0.2726
Epoch 20 — loss: 0.2723
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  79.309998
2    P(>V6)  83.540001
3    P(>V7)  87.580002
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.96%
Ordinal stacking meta epoch 1: loss=0.2104
Ordinal stacking meta epoch 2: loss=0.1511
Ordinal stacking meta epoch 3: loss=0.1444
Ordinal stacking meta epoch 4: loss=0.1419
Ordinal stacking meta epoch 5: loss=0.1403
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001536 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.629997
2    P(>V6)  86.330002
3    P(>V7)  88.839996
4    P(>V8)  93.309998
5    P(>V9)  97.059998
Overall accuracy: 49.81%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.389999
2    P(>V6)  86.190002
3    P(>V7)  88.839996
4    P(>V8)  93.260002
5    P(>V9)  97.059998
Overall accuracy: 49.13%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  81.570000
2    P(>V6)  85.370003
3    P(>V7)  88.550003
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.339996
2    P(>V6)  85.709999
3    P(>V7)  88.930000
4    P(>V8)  93.360001
5    P(>V9)  97.059998
Overall accuracy: 48.99%
  threshold   accuracy
0    P(>V4)  84.699997
1    P(>V5)  82.769997
2    P(>V6)  86.430000
3    P(>V7)  88.980003
4    P(>V8)  93.410004
5    P(>V9)  96.779999
Overall accuracy: 50.00%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.480003
2    P(>V6)  86.279999
3    P(>V7)  88.930000
4    P(>V8)  93.209999
5    P(>V9)  96.629997
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  81.949997
2    P(>V6)  86.620003
3    P(>V7)  88.500000
4    P(>V8)  92.830002
5    P(>V9)  96.730003
Overall accuracy: 49.37%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.190002
2    P(>V6)  86.279999
3    P(>V7)  88.839996
4    P(>V8)  92.930000
5    P(>V9)  96.629997
Overall accuracy: 49.62%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.629997
2    P(>V6)  86.330002
3    P(>V7)  88.839996
4    P(>V8)  93.309998
5    P(>V9)  97.059998
Overall accuracy: 49.81%
----------------- Ordinal iteration 16/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3842
Epoch 02 — loss: 0.3202
Epoch 03 — loss: 0.3055
Epoch 04 — loss: 0.2946
Epoch 05 — loss: 0.2827
Epoch 06 — loss: 0.2747
Epoch 07 — loss: 0.2665
Epoch 08 — loss: 0.2603
Epoch 09 — loss: 0.2524
Epoch 10 — loss: 0.2479
Epoch 11 — loss: 0.2400
Epoch 12 — loss: 0.2329
Epoch 13 — loss: 0.2285
Epoch 14 — loss: 0.2233
Epoch 15 — loss: 0.2169
Epoch 16 — loss: 0.2122
Epoch 17 — loss: 0.2079
Epoch 18 — loss: 0.2011
Epoch 19 — loss: 0.1956
Epoch 20 — loss: 0.1917
  threshold   accuracy
0    P(>V4)  82.389999
1    P(>V5)  81.330002
2    P(>V6)  84.599998
3    P(>V7)  87.250000
4    P(>V8)  92.970001
5    P(>V9)  96.580002
Overall accuracy: 47.74%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3626
Epoch 02 — loss: 0.3097
Epoch 03 — loss: 0.2968
Epoch 04 — loss: 0.2879
Epoch 05 — loss: 0.2798
Epoch 06 — loss: 0.2758
Epoch 07 — loss: 0.2658
Epoch 08 — loss: 0.2603
Epoch 09 — loss: 0.2516
Epoch 10 — loss: 0.2469
Epoch 11 — loss: 0.2367
Epoch 12 — loss: 0.2332
Epoch 13 — loss: 0.2248
Epoch 14 — loss: 0.2204
Epoch 15 — loss: 0.2124
Epoch 16 — loss: 0.2082
Epoch 17 — loss: 0.2034
Epoch 18 — loss: 0.1958
Epoch 19 — loss: 0.1917
Epoch 20 — loss: 0.1870
  threshold   accuracy
0    P(>V4)  82.629997
1    P(>V5)  81.519997
2    P(>V6)  85.760002
3    P(>V7)  88.260002
4    P(>V8)  92.489998
5    P(>V9)  96.970001
Overall accuracy: 47.88%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3613
Epoch 02 — loss: 0.3167
Epoch 03 — loss: 0.3033
Epoch 04 — loss: 0.2971
Epoch 05 — loss: 0.2905
Epoch 06 — loss: 0.2829
Epoch 07 — loss: 0.2780
Epoch 08 — loss: 0.2723
Epoch 09 — loss: 0.2652
Epoch 10 — loss: 0.2581
Epoch 11 — loss: 0.2547
Epoch 12 — loss: 0.2467
Epoch 13 — loss: 0.2438
Epoch 14 — loss: 0.2354
Epoch 15 — loss: 0.2339
Epoch 16 — loss: 0.2259
Epoch 17 — loss: 0.2222
Epoch 18 — loss: 0.2173
Epoch 19 — loss: 0.2144
Epoch 20 — loss: 0.2073
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  81.860001
2    P(>V6)  84.699997
3    P(>V7)  88.589996
4    P(>V8)  92.199997
5    P(>V9)  96.820000
Overall accuracy: 47.98%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4723
Epoch 02 — loss: 0.3573
Epoch 03 — loss: 0.3073
Epoch 04 — loss: 0.2964
Epoch 05 — loss: 0.2885
Epoch 06 — loss: 0.2848
Epoch 07 — loss: 0.2804
Epoch 08 — loss: 0.2804
Epoch 09 — loss: 0.2771
Epoch 10 — loss: 0.2755
Epoch 11 — loss: 0.2748
Epoch 12 — loss: 0.2736
Epoch 13 — loss: 0.2739
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2709
Epoch 16 — loss: 0.2705
Epoch 17 — loss: 0.2718
Epoch 18 — loss: 0.2703
Epoch 19 — loss: 0.2697
Epoch 20 — loss: 0.2706
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  79.879997
2    P(>V6)  83.930000
3    P(>V7)  87.540001
4    P(>V8)  92.440002
5    P(>V9)  97.019997
Overall accuracy: 43.74%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4681
Epoch 02 — loss: 0.3396
Epoch 03 — loss: 0.3013
Epoch 04 — loss: 0.2918
Epoch 05 — loss: 0.2864
Epoch 06 — loss: 0.2820
Epoch 07 — loss: 0.2787
Epoch 08 — loss: 0.2787
Epoch 09 — loss: 0.2763
Epoch 10 — loss: 0.2741
Epoch 11 — loss: 0.2736
Epoch 12 — loss: 0.2743
Epoch 13 — loss: 0.2726
Epoch 14 — loss: 0.2701
Epoch 15 — loss: 0.2708
Epoch 16 — loss: 0.2699
Epoch 17 — loss: 0.2718
Epoch 18 — loss: 0.2714
Epoch 19 — loss: 0.2685
Epoch 20 — loss: 0.2700
  threshold   accuracy
0    P(>V4)  81.040001
1    P(>V5)  80.849998
2    P(>V6)  83.489998
3    P(>V7)  87.489998
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 44.37%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4262
Epoch 02 — loss: 0.3227
Epoch 03 — loss: 0.3097
Epoch 04 — loss: 0.2996
Epoch 05 — loss: 0.2922
Epoch 06 — loss: 0.2871
Epoch 07 — loss: 0.2855
Epoch 08 — loss: 0.2815
Epoch 09 — loss: 0.2809
Epoch 10 — loss: 0.2784
Epoch 11 — loss: 0.2779
Epoch 12 — loss: 0.2745
Epoch 13 — loss: 0.2758
Epoch 14 — loss: 0.2757
Epoch 15 — loss: 0.2742
Epoch 16 — loss: 0.2739
Epoch 17 — loss: 0.2727
Epoch 18 — loss: 0.2714
Epoch 19 — loss: 0.2719
Epoch 20 — loss: 0.2714
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.900002
2    P(>V6)  83.489998
3    P(>V7)  87.540001
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 44.71%
Ordinal stacking meta epoch 1: loss=0.2354
Ordinal stacking meta epoch 2: loss=0.1585
Ordinal stacking meta epoch 3: loss=0.1528
Ordinal stacking meta epoch 4: loss=0.1505
Ordinal stacking meta epoch 5: loss=0.1489
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001612 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [12:43:50] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.190002
2    P(>V6)  86.089996
3    P(>V7)  88.589996
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.480003
2    P(>V6)  86.040001
3    P(>V7)  88.930000
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 49.62%
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  81.910004
2    P(>V6)  84.989998
3    P(>V7)  88.260002
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 47.69%
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  82.099998
2    P(>V6)  85.559998
3    P(>V7)  88.639999
4    P(>V8)  92.830002
5    P(>V9)  97.059998
Overall accuracy: 48.03%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  81.620003
2    P(>V6)  86.139999
3    P(>V7)  88.790001
4    P(>V8)  92.830002
5    P(>V9)  96.580002
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.769997
2    P(>V6)  86.529999
3    P(>V7)  89.080002
4    P(>V8)  92.300003
5    P(>V9)  96.199997
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  82.150002
2    P(>V6)  86.040001
3    P(>V7)  88.550003
4    P(>V8)  92.440002
5    P(>V9)  95.860001
Overall accuracy: 47.98%
  threshold   accuracy
0    P(>V4)  82.680000
1    P(>V5)  82.480003
2    P(>V6)  86.379997
3    P(>V7)  89.029999
4    P(>V8)  92.489998
5    P(>V9)  96.250000
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.190002
2    P(>V6)  86.089996
3    P(>V7)  88.589996
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 48.51%
----------------- Ordinal iteration 17/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3763
Epoch 02 — loss: 0.3146
Epoch 03 — loss: 0.3008
Epoch 04 — loss: 0.2906
Epoch 05 — loss: 0.2808
Epoch 06 — loss: 0.2737
Epoch 07 — loss: 0.2651
Epoch 08 — loss: 0.2588
Epoch 09 — loss: 0.2487
Epoch 10 — loss: 0.2435
Epoch 11 — loss: 0.2380
Epoch 12 — loss: 0.2316
Epoch 13 — loss: 0.2268
Epoch 14 — loss: 0.2207
Epoch 15 — loss: 0.2154
Epoch 16 — loss: 0.2095
Epoch 17 — loss: 0.2024
Epoch 18 — loss: 0.1988
Epoch 19 — loss: 0.1931
Epoch 20 — loss: 0.1880
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  81.519997
2    P(>V6)  85.180000
3    P(>V7)  87.010002
4    P(>V8)  92.730003
5    P(>V9)  96.779999
Overall accuracy: 49.33%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3712
Epoch 02 — loss: 0.3089
Epoch 03 — loss: 0.2957
Epoch 04 — loss: 0.2874
Epoch 05 — loss: 0.2786
Epoch 06 — loss: 0.2743
Epoch 07 — loss: 0.2642
Epoch 08 — loss: 0.2579
Epoch 09 — loss: 0.2480
Epoch 10 — loss: 0.2429
Epoch 11 — loss: 0.2380
Epoch 12 — loss: 0.2320
Epoch 13 — loss: 0.2246
Epoch 14 — loss: 0.2193
Epoch 15 — loss: 0.2147
Epoch 16 — loss: 0.2099
Epoch 17 — loss: 0.2051
Epoch 18 — loss: 0.1992
Epoch 19 — loss: 0.1934
Epoch 20 — loss: 0.1895
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  81.570000
2    P(>V6)  85.760002
3    P(>V7)  87.629997
4    P(>V8)  93.019997
5    P(>V9)  96.970001
Overall accuracy: 49.23%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3563
Epoch 02 — loss: 0.3108
Epoch 03 — loss: 0.3006
Epoch 04 — loss: 0.2945
Epoch 05 — loss: 0.2851
Epoch 06 — loss: 0.2775
Epoch 07 — loss: 0.2720
Epoch 08 — loss: 0.2671
Epoch 09 — loss: 0.2602
Epoch 10 — loss: 0.2537
Epoch 11 — loss: 0.2492
Epoch 12 — loss: 0.2456
Epoch 13 — loss: 0.2386
Epoch 14 — loss: 0.2332
Epoch 15 — loss: 0.2282
Epoch 16 — loss: 0.2243
Epoch 17 — loss: 0.2193
Epoch 18 — loss: 0.2130
Epoch 19 — loss: 0.2095
Epoch 20 — loss: 0.2024
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  81.760002
2    P(>V6)  85.029999
3    P(>V7)  88.309998
4    P(>V8)  92.690002
5    P(>V9)  96.730003
Overall accuracy: 49.23%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4779
Epoch 02 — loss: 0.3703
Epoch 03 — loss: 0.3072
Epoch 04 — loss: 0.2933
Epoch 05 — loss: 0.2866
Epoch 06 — loss: 0.2812
Epoch 07 — loss: 0.2804
Epoch 08 — loss: 0.2784
Epoch 09 — loss: 0.2773
Epoch 10 — loss: 0.2741
Epoch 11 — loss: 0.2730
Epoch 12 — loss: 0.2736
Epoch 13 — loss: 0.2717
Epoch 14 — loss: 0.2727
Epoch 15 — loss: 0.2704
Epoch 16 — loss: 0.2704
Epoch 17 — loss: 0.2702
Epoch 18 — loss: 0.2695
Epoch 19 — loss: 0.2697
Epoch 20 — loss: 0.2704
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.849998
2    P(>V6)  83.779999
3    P(>V7)  87.489998
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.33%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4721
Epoch 02 — loss: 0.3468
Epoch 03 — loss: 0.2982
Epoch 04 — loss: 0.2889
Epoch 05 — loss: 0.2832
Epoch 06 — loss: 0.2814
Epoch 07 — loss: 0.2773
Epoch 08 — loss: 0.2765
Epoch 09 — loss: 0.2744
Epoch 10 — loss: 0.2740
Epoch 11 — loss: 0.2722
Epoch 12 — loss: 0.2725
Epoch 13 — loss: 0.2716
Epoch 14 — loss: 0.2727
Epoch 15 — loss: 0.2701
Epoch 16 — loss: 0.2710
Epoch 17 — loss: 0.2698
Epoch 18 — loss: 0.2698
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2693
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.559998
2    P(>V6)  83.970001
3    P(>V7)  87.870003
4    P(>V8)  92.589996
5    P(>V9)  97.059998
Overall accuracy: 44.90%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4269
Epoch 02 — loss: 0.3234
Epoch 03 — loss: 0.3041
Epoch 04 — loss: 0.2950
Epoch 05 — loss: 0.2893
Epoch 06 — loss: 0.2855
Epoch 07 — loss: 0.2822
Epoch 08 — loss: 0.2823
Epoch 09 — loss: 0.2792
Epoch 10 — loss: 0.2785
Epoch 11 — loss: 0.2767
Epoch 12 — loss: 0.2774
Epoch 13 — loss: 0.2741
Epoch 14 — loss: 0.2763
Epoch 15 — loss: 0.2737
Epoch 16 — loss: 0.2728
Epoch 17 — loss: 0.2716
Epoch 18 — loss: 0.2711
Epoch 19 — loss: 0.2723
Epoch 20 — loss: 0.2712
  threshold   accuracy
0    P(>V4)  80.940002
1    P(>V5)  80.510002
2    P(>V6)  83.779999
3    P(>V7)  87.919998
4    P(>V8)  92.440002
5    P(>V9)  97.019997
Overall accuracy: 44.47%
Ordinal stacking meta epoch 1: loss=0.1937
Ordinal stacking meta epoch 2: loss=0.1534
Ordinal stacking meta epoch 3: loss=0.1493
Ordinal stacking meta epoch 4: loss=0.1474
Ordinal stacking meta epoch 5: loss=0.1461
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001563 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  82.480003
2    P(>V6)  86.190002
3    P(>V7)  88.400002
4    P(>V8)  93.410004
5    P(>V9)  97.019997
Overall accuracy: 50.14%
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  82.580002
2    P(>V6)  86.190002
3    P(>V7)  88.550003
4    P(>V8)  93.260002
5    P(>V9)  97.019997
Overall accuracy: 50.24%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.099998
2    P(>V6)  85.080002
3    P(>V7)  88.309998
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  82.680000
2    P(>V6)  85.849998
3    P(>V7)  88.309998
4    P(>V8)  93.260002
5    P(>V9)  97.019997/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [13:06:47] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 50.00%
  threshold   accuracy
0    P(>V4)  85.180000
1    P(>V5)  82.190002
2    P(>V6)  85.760002
3    P(>V7)  88.500000
4    P(>V8)  93.120003
5    P(>V9)  96.919998
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.480003
2    P(>V6)  86.620003
3    P(>V7)  88.260002
4    P(>V8)  92.300003
5    P(>V9)  97.019997
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.480003
2    P(>V6)  86.139999
3    P(>V7)  88.309998
4    P(>V8)  92.930000
5    P(>V9)  96.820000
Overall accuracy: 49.42%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.389999
2    P(>V6)  86.379997
3    P(>V7)  88.500000
4    P(>V8)  93.019997
5    P(>V9)  96.970001
Overall accuracy: 49.90%
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  82.480003
2    P(>V6)  86.190002
3    P(>V7)  88.400002
4    P(>V8)  93.410004
5    P(>V9)  97.019997
Overall accuracy: 50.14%
----------------- Ordinal iteration 18/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3699
Epoch 02 — loss: 0.3174
Epoch 03 — loss: 0.3055
Epoch 04 — loss: 0.2928
Epoch 05 — loss: 0.2873
Epoch 06 — loss: 0.2808
Epoch 07 — loss: 0.2723
Epoch 08 — loss: 0.2622
Epoch 09 — loss: 0.2558
Epoch 10 — loss: 0.2473
Epoch 11 — loss: 0.2413
Epoch 12 — loss: 0.2337
Epoch 13 — loss: 0.2290
Epoch 14 — loss: 0.2222
Epoch 15 — loss: 0.2161
Epoch 16 — loss: 0.2099
Epoch 17 — loss: 0.2032
Epoch 18 — loss: 0.1988
Epoch 19 — loss: 0.1933
Epoch 20 — loss: 0.1911
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  80.849998
2    P(>V6)  85.269997
3    P(>V7)  87.440002
4    P(>V8)  92.830002
5    P(>V9)  97.059998
Overall accuracy: 47.55%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3697
Epoch 02 — loss: 0.3097
Epoch 03 — loss: 0.2975
Epoch 04 — loss: 0.2871
Epoch 05 — loss: 0.2773
Epoch 06 — loss: 0.2707
Epoch 07 — loss: 0.2635
Epoch 08 — loss: 0.2562
Epoch 09 — loss: 0.2485
Epoch 10 — loss: 0.2429
Epoch 11 — loss: 0.2347
Epoch 12 — loss: 0.2318
Epoch 13 — loss: 0.2273
Epoch 14 — loss: 0.2197
Epoch 15 — loss: 0.2144
Epoch 16 — loss: 0.2090
Epoch 17 — loss: 0.2049
Epoch 18 — loss: 0.1967
Epoch 19 — loss: 0.1934
Epoch 20 — loss: 0.1897
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  81.709999
2    P(>V6)  85.510002
3    P(>V7)  87.300003
4    P(>V8)  92.970001
5    P(>V9)  96.820000
Overall accuracy: 48.60%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3690
Epoch 02 — loss: 0.3167
Epoch 03 — loss: 0.3028
Epoch 04 — loss: 0.2952
Epoch 05 — loss: 0.2869
Epoch 06 — loss: 0.2786
Epoch 07 — loss: 0.2720
Epoch 08 — loss: 0.2654
Epoch 09 — loss: 0.2593
Epoch 10 — loss: 0.2525
Epoch 11 — loss: 0.2450
Epoch 12 — loss: 0.2413
Epoch 13 — loss: 0.2346
Epoch 14 — loss: 0.2307
Epoch 15 — loss: 0.2288
Epoch 16 — loss: 0.2201
Epoch 17 — loss: 0.2200
Epoch 18 — loss: 0.2139
Epoch 19 — loss: 0.2071
Epoch 20 — loss: 0.2040
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  81.519997
2    P(>V6)  84.790001
3    P(>V7)  88.639999
4    P(>V8)  92.779999
5    P(>V9)  96.629997
Overall accuracy: 46.92%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4672
Epoch 02 — loss: 0.3419
Epoch 03 — loss: 0.3020
Epoch 04 — loss: 0.2920
Epoch 05 — loss: 0.2853
Epoch 06 — loss: 0.2831
Epoch 07 — loss: 0.2819
Epoch 08 — loss: 0.2775
Epoch 09 — loss: 0.2756
Epoch 10 — loss: 0.2748
Epoch 11 — loss: 0.2754
Epoch 12 — loss: 0.2729
Epoch 13 — loss: 0.2721
Epoch 14 — loss: 0.2717
Epoch 15 — loss: 0.2721
Epoch 16 — loss: 0.2712
Epoch 17 — loss: 0.2718
Epoch 18 — loss: 0.2704
Epoch 19 — loss: 0.2695
Epoch 20 — loss: 0.2703
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.699997
2    P(>V6)  83.970001
3    P(>V7)  87.489998
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.25%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4665
Epoch 02 — loss: 0.3356
Epoch 03 — loss: 0.2998
Epoch 04 — loss: 0.2892
Epoch 05 — loss: 0.2848
Epoch 06 — loss: 0.2814
Epoch 07 — loss: 0.2798
Epoch 08 — loss: 0.2776
Epoch 09 — loss: 0.2764
Epoch 10 — loss: 0.2739
Epoch 11 — loss: 0.2744
Epoch 12 — loss: 0.2729
Epoch 13 — loss: 0.2730
Epoch 14 — loss: 0.2718
Epoch 15 — loss: 0.2724
Epoch 16 — loss: 0.2707
Epoch 17 — loss: 0.2722
Epoch 18 — loss: 0.2710
Epoch 19 — loss: 0.2700
Epoch 20 — loss: 0.2695
  threshold   accuracy
0    P(>V4)  82.000000
1    P(>V5)  80.800003
2    P(>V6)  83.830002
3    P(>V7)  87.680000
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 46.58%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4323
Epoch 02 — loss: 0.3248
Epoch 03 — loss: 0.3062
Epoch 04 — loss: 0.2960
Epoch 05 — loss: 0.2891
Epoch 06 — loss: 0.2864
Epoch 07 — loss: 0.2815
Epoch 08 — loss: 0.2804
Epoch 09 — loss: 0.2765
Epoch 10 — loss: 0.2779
Epoch 11 — loss: 0.2741
Epoch 12 — loss: 0.2748
Epoch 13 — loss: 0.2736
Epoch 14 — loss: 0.2744
Epoch 15 — loss: 0.2727
Epoch 16 — loss: 0.2730
Epoch 17 — loss: 0.2716
Epoch 18 — loss: 0.2721
Epoch 19 — loss: 0.2712
Epoch 20 — loss: 0.2702
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.940002
2    P(>V6)  84.070000
3    P(>V7)  87.680000
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.72%
Ordinal stacking meta epoch 1: loss=0.2045
Ordinal stacking meta epoch 2: loss=0.1542
Ordinal stacking meta epoch 3: loss=0.1472
Ordinal stacking meta epoch 4: loss=0.1451
Ordinal stacking meta epoch 5: loss=0.1441
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001821 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  81.709999
2    P(>V6)  85.760002
3    P(>V7)  88.449997
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.339996
2    P(>V6)  85.370003
3    P(>V7)  88.309998
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  82.580002
1    P(>V5)  81.669998
2    P(>V6)  84.699997
3    P(>V7)  87.919998
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 47.74%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  81.760002
2    P(>V6)  85.610001
3    P(>V7)  88.500000
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 48.27%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.339996
2    P(>V6)  86.669998
3    P(>V7)  88.790001
4    P(>V8)  92.730003
5    P(>V9)  96.820000
Overall accuracy: 48.89%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  82.339996
2    P(>V6)  86.330002
3    P(>V7)  88.209999
4    P(>V8)  93.019997
5    P(>V9)  96.389999
Overall accuracy: 49.42%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  81.760002
2    P(>V6)  86.570000
3    P(>V7)  88.160004
4    P(>V8)  92.830002
5    P(>V9)  96.389999
Overall accuracy: 47.79%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  81.760002
2    P(>V6)  86.529999
3    P(>V7)  88.110001
4    P(>V8)  93.260002
5    P(>V9)  96.580002
Overall accuracy: 48.89%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  81.709999
2    P(>V6)  85.760002
3    P(>V7)  88.449997
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 48.85%
----------------- Ordinal iteration 19/25 -----------------/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [13:29:56] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3796
Epoch 02 — loss: 0.3136
Epoch 03 — loss: 0.3021
Epoch 04 — loss: 0.2880
Epoch 05 — loss: 0.2805
Epoch 06 — loss: 0.2729
Epoch 07 — loss: 0.2629
Epoch 08 — loss: 0.2567
Epoch 09 — loss: 0.2505
Epoch 10 — loss: 0.2443
Epoch 11 — loss: 0.2389
Epoch 12 — loss: 0.2326
Epoch 13 — loss: 0.2296
Epoch 14 — loss: 0.2223
Epoch 15 — loss: 0.2164
Epoch 16 — loss: 0.2106
Epoch 17 — loss: 0.2048
Epoch 18 — loss: 0.2024
Epoch 19 — loss: 0.1946
Epoch 20 — loss: 0.1893
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  81.809998
2    P(>V6)  84.739998
3    P(>V7)  86.860001
4    P(>V8)  92.779999
5    P(>V9)  96.540001
Overall accuracy: 47.74%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3709
Epoch 02 — loss: 0.3038
Epoch 03 — loss: 0.2909
Epoch 04 — loss: 0.2830
Epoch 05 — loss: 0.2730
Epoch 06 — loss: 0.2659
Epoch 07 — loss: 0.2602
Epoch 08 — loss: 0.2520
Epoch 09 — loss: 0.2472
Epoch 10 — loss: 0.2393
Epoch 11 — loss: 0.2306
Epoch 12 — loss: 0.2273
Epoch 13 — loss: 0.2239
Epoch 14 — loss: 0.2138
Epoch 15 — loss: 0.2091
Epoch 16 — loss: 0.2057
Epoch 17 — loss: 0.2007
Epoch 18 — loss: 0.1946
Epoch 19 — loss: 0.1886
Epoch 20 — loss: 0.1845
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.050003
2    P(>V6)  86.239998
3    P(>V7)  88.839996
4    P(>V8)  93.209999
5    P(>V9)  96.820000
Overall accuracy: 50.43%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3678
Epoch 02 — loss: 0.3219
Epoch 03 — loss: 0.3063
Epoch 04 — loss: 0.2966
Epoch 05 — loss: 0.2866
Epoch 06 — loss: 0.2800
Epoch 07 — loss: 0.2750
Epoch 08 — loss: 0.2695
Epoch 09 — loss: 0.2639
Epoch 10 — loss: 0.2581
Epoch 11 — loss: 0.2543
Epoch 12 — loss: 0.2465
Epoch 13 — loss: 0.2400
Epoch 14 — loss: 0.2355
Epoch 15 — loss: 0.2295
Epoch 16 — loss: 0.2233
Epoch 17 — loss: 0.2200
Epoch 18 — loss: 0.2141
Epoch 19 — loss: 0.2077
Epoch 20 — loss: 0.2023
  threshold   accuracy
0    P(>V4)  82.680000
1    P(>V5)  81.470001
2    P(>V6)  85.129997
3    P(>V7)  87.489998
4    P(>V8)  92.010002
5    P(>V9)  96.580002
Overall accuracy: 46.58%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4675
Epoch 02 — loss: 0.3568
Epoch 03 — loss: 0.3035
Epoch 04 — loss: 0.2955
Epoch 05 — loss: 0.2895
Epoch 06 — loss: 0.2859
Epoch 07 — loss: 0.2828
Epoch 08 — loss: 0.2822
Epoch 09 — loss: 0.2800
Epoch 10 — loss: 0.2766
Epoch 11 — loss: 0.2734
Epoch 12 — loss: 0.2767
Epoch 13 — loss: 0.2736
Epoch 14 — loss: 0.2739
Epoch 15 — loss: 0.2726
Epoch 16 — loss: 0.2723
Epoch 17 — loss: 0.2726
Epoch 18 — loss: 0.2716
Epoch 19 — loss: 0.2712
Epoch 20 — loss: 0.2717
  threshold   accuracy
0    P(>V4)  81.040001
1    P(>V5)  80.849998
2    P(>V6)  83.830002
3    P(>V7)  87.489998
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 44.66%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4743
Epoch 02 — loss: 0.3503
Epoch 03 — loss: 0.2969
Epoch 04 — loss: 0.2869
Epoch 05 — loss: 0.2828
Epoch 06 — loss: 0.2793
Epoch 07 — loss: 0.2781
Epoch 08 — loss: 0.2771
Epoch 09 — loss: 0.2747
Epoch 10 — loss: 0.2740
Epoch 11 — loss: 0.2742
Epoch 12 — loss: 0.2721
Epoch 13 — loss: 0.2717
Epoch 14 — loss: 0.2712
Epoch 15 — loss: 0.2717
Epoch 16 — loss: 0.2711
Epoch 17 — loss: 0.2719
Epoch 18 — loss: 0.2707
Epoch 19 — loss: 0.2696
Epoch 20 — loss: 0.2695
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.989998
2    P(>V6)  84.019997
3    P(>V7)  87.489998
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.48%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4277
Epoch 02 — loss: 0.3257
Epoch 03 — loss: 0.3061
Epoch 04 — loss: 0.2939
Epoch 05 — loss: 0.2881
Epoch 06 — loss: 0.2843
Epoch 07 — loss: 0.2830
Epoch 08 — loss: 0.2787
Epoch 09 — loss: 0.2774
Epoch 10 — loss: 0.2783
Epoch 11 — loss: 0.2757
Epoch 12 — loss: 0.2754
Epoch 13 — loss: 0.2745
Epoch 14 — loss: 0.2744
Epoch 15 — loss: 0.2732
Epoch 16 — loss: 0.2713
Epoch 17 — loss: 0.2725
Epoch 18 — loss: 0.2734
Epoch 19 — loss: 0.2717
Epoch 20 — loss: 0.2720
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  81.089996
2    P(>V6)  83.830002
3    P(>V7)  87.779999
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.63%
Ordinal stacking meta epoch 1: loss=0.2103
Ordinal stacking meta epoch 2: loss=0.1542
Ordinal stacking meta epoch 3: loss=0.1480
Ordinal stacking meta epoch 4: loss=0.1455
Ordinal stacking meta epoch 5: loss=0.1439
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001774 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  81.910004
2    P(>V6)  86.139999
3    P(>V7)  88.309998
4    P(>V8)  93.459999
5    P(>V9)  97.110001
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.190002
2    P(>V6)  86.139999
3    P(>V7)  88.639999
4    P(>V8)  93.120003
5    P(>V9)  97.059998
Overall accuracy: 49.66%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  81.570000
2    P(>V6)  84.839996
3    P(>V7)  87.919998
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 48.08%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.480003
2    P(>V6)  85.800003
3    P(>V7)  88.160004
4    P(>V8)  93.410004
5    P(>V9)  97.059998
Overall accuracy: 48.99%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.290001
2    P(>V6)  86.669998
3    P(>V7)  88.349998
4    P(>V8)  92.730003
5    P(>V9)  96.730003
Overall accuracy: 47.98%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.480003
2    P(>V6)  86.529999
3    P(>V7)  88.500000
4    P(>V8)  92.779999
5    P(>V9)  96.680000
Overall accuracy: 49.95%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.050003
2    P(>V6)  85.949997
3    P(>V7)  88.550003
4    P(>V8)  92.690002
5    P(>V9)  96.680000
Overall accuracy: 49.33%
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.820000
2    P(>V6)  86.239998
3    P(>V7)  88.500000
4    P(>V8)  92.690002
5    P(>V9)  96.680000
Overall accuracy: 49.71%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  81.910004
2    P(>V6)  86.139999
3    P(>V7)  88.309998
4    P(>V8)  93.459999
5    P(>V9)  97.110001
Overall accuracy: 49.04%
----------------- Ordinal iteration 20/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3852
Epoch 02 — loss: 0.3238
Epoch 03 — loss: 0.3057
Epoch 04 — loss: 0.2930
Epoch 05 — loss: 0.2804
Epoch 06 — loss: 0.2705
Epoch 07 — loss: 0.2602
Epoch 08 — loss: 0.2539
Epoch 09 — loss: 0.2460
Epoch 10 — loss: 0.2389
Epoch 11 — loss: 0.2320
Epoch 12 — loss: 0.2250
Epoch 13 — loss: 0.2181
Epoch 14 — loss: 0.2137
Epoch 15 — loss: 0.2073
Epoch 16 — loss: 0.2011
Epoch 17 — loss: 0.1951
Epoch 18 — loss: 0.1889
Epoch 19 — loss: 0.1861
Epoch 20 — loss: 0.1786
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  80.650002
2    P(>V6)  83.540001
3    P(>V7)  86.720001
4    P(>V8)  93.019997
5    P(>V9)  96.870003
Overall accuracy: 46.39%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3660
Epoch 02 — loss: 0.3125
Epoch 03 — loss: 0.2964
Epoch 04 — loss: 0.2843
Epoch 05 — loss: 0.2767
Epoch 06 — loss: 0.2680
Epoch 07 — loss: 0.2592/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [13:53:09] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 08 — loss: 0.2530
Epoch 09 — loss: 0.2462
Epoch 10 — loss: 0.2392
Epoch 11 — loss: 0.2353
Epoch 12 — loss: 0.2273
Epoch 13 — loss: 0.2221
Epoch 14 — loss: 0.2164
Epoch 15 — loss: 0.2102
Epoch 16 — loss: 0.2055
Epoch 17 — loss: 0.2016
Epoch 18 — loss: 0.1939
Epoch 19 — loss: 0.1883
Epoch 20 — loss: 0.1840
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.440002
2    P(>V6)  85.320000
3    P(>V7)  88.160004
4    P(>V8)  93.070000
5    P(>V9)  96.820000
Overall accuracy: 47.64%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3632
Epoch 02 — loss: 0.3200
Epoch 03 — loss: 0.3095
Epoch 04 — loss: 0.2980
Epoch 05 — loss: 0.2889
Epoch 06 — loss: 0.2841
Epoch 07 — loss: 0.2797
Epoch 08 — loss: 0.2707
Epoch 09 — loss: 0.2668
Epoch 10 — loss: 0.2596
Epoch 11 — loss: 0.2539
Epoch 12 — loss: 0.2467
Epoch 13 — loss: 0.2402
Epoch 14 — loss: 0.2379
Epoch 15 — loss: 0.2309
Epoch 16 — loss: 0.2288
Epoch 17 — loss: 0.2219
Epoch 18 — loss: 0.2145
Epoch 19 — loss: 0.2106
Epoch 20 — loss: 0.2061
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.680000
2    P(>V6)  84.739998
3    P(>V7)  88.349998
4    P(>V8)  92.540001
5    P(>V9)  96.680000
Overall accuracy: 48.03%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4726
Epoch 02 — loss: 0.3632
Epoch 03 — loss: 0.3073
Epoch 04 — loss: 0.2910
Epoch 05 — loss: 0.2844
Epoch 06 — loss: 0.2791
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2760
Epoch 09 — loss: 0.2747
Epoch 10 — loss: 0.2740
Epoch 11 — loss: 0.2735
Epoch 12 — loss: 0.2738
Epoch 13 — loss: 0.2731
Epoch 14 — loss: 0.2689
Epoch 15 — loss: 0.2702
Epoch 16 — loss: 0.2696
Epoch 17 — loss: 0.2698
Epoch 18 — loss: 0.2701
Epoch 19 — loss: 0.2694
Epoch 20 — loss: 0.2688
  threshold   accuracy
0    P(>V4)  81.089996
1    P(>V5)  80.940002
2    P(>V6)  83.830002
3    P(>V7)  87.680000
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.28%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4607
Epoch 02 — loss: 0.3280
Epoch 03 — loss: 0.3003
Epoch 04 — loss: 0.2905
Epoch 05 — loss: 0.2855
Epoch 06 — loss: 0.2812
Epoch 07 — loss: 0.2786
Epoch 08 — loss: 0.2760
Epoch 09 — loss: 0.2748
Epoch 10 — loss: 0.2742
Epoch 11 — loss: 0.2742
Epoch 12 — loss: 0.2721
Epoch 13 — loss: 0.2722
Epoch 14 — loss: 0.2697
Epoch 15 — loss: 0.2703
Epoch 16 — loss: 0.2714
Epoch 17 — loss: 0.2696
Epoch 18 — loss: 0.2675
Epoch 19 — loss: 0.2689
Epoch 20 — loss: 0.2695
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.320000
2    P(>V6)  83.930000
3    P(>V7)  87.300003
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 44.56%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4321
Epoch 02 — loss: 0.3252
Epoch 03 — loss: 0.3074
Epoch 04 — loss: 0.2985
Epoch 05 — loss: 0.2919
Epoch 06 — loss: 0.2892
Epoch 07 — loss: 0.2837
Epoch 08 — loss: 0.2811
Epoch 09 — loss: 0.2773
Epoch 10 — loss: 0.2780
Epoch 11 — loss: 0.2754
Epoch 12 — loss: 0.2737
Epoch 13 — loss: 0.2736
Epoch 14 — loss: 0.2733
Epoch 15 — loss: 0.2738
Epoch 16 — loss: 0.2737
Epoch 17 — loss: 0.2725
Epoch 18 — loss: 0.2712
Epoch 19 — loss: 0.2715
Epoch 20 — loss: 0.2704
  threshold   accuracy
0    P(>V4)  80.940002
1    P(>V5)  80.080002
2    P(>V6)  83.690002
3    P(>V7)  87.820000
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 43.31%
Ordinal stacking meta epoch 1: loss=0.1996
Ordinal stacking meta epoch 2: loss=0.1504
Ordinal stacking meta epoch 3: loss=0.1445
Ordinal stacking meta epoch 4: loss=0.1417
Ordinal stacking meta epoch 5: loss=0.1400
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001628 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.290001
2    P(>V6)  85.849998
3    P(>V7)  88.980003
4    P(>V8)  93.070000
5    P(>V9)  97.059998
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.339996
2    P(>V6)  86.279999
3    P(>V7)  88.879997
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.150002
2    P(>V6)  85.029999
3    P(>V7)  88.400002
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 47.45%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.050003
2    P(>V6)  85.419998
3    P(>V7)  88.790001
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  81.910004
2    P(>V6)  86.000000
3    P(>V7)  89.169998
4    P(>V8)  93.500000
5    P(>V9)  96.680000
Overall accuracy: 48.32%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.919998
2    P(>V6)  86.190002
3    P(>V7)  89.080002
4    P(>V8)  92.690002
5    P(>V9)  96.629997
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.480003
2    P(>V6)  85.370003
3    P(>V7)  89.120003
4    P(>V8)  92.779999
5    P(>V9)  96.680000
Overall accuracy: 49.33%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.629997
2    P(>V6)  85.660004
3    P(>V7)  88.879997
4    P(>V8)  92.879997
5    P(>V9)  96.820000
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.290001
2    P(>V6)  85.849998
3    P(>V7)  88.980003
4    P(>V8)  93.070000
5    P(>V9)  97.059998
Overall accuracy: 48.85%
----------------- Ordinal iteration 21/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3616
Epoch 02 — loss: 0.3114
Epoch 03 — loss: 0.2994
Epoch 04 — loss: 0.2911
Epoch 05 — loss: 0.2805
Epoch 06 — loss: 0.2727
Epoch 07 — loss: 0.2659
Epoch 08 — loss: 0.2573
Epoch 09 — loss: 0.2512
Epoch 10 — loss: 0.2441
Epoch 11 — loss: 0.2379
Epoch 12 — loss: 0.2323
Epoch 13 — loss: 0.2247
Epoch 14 — loss: 0.2216
Epoch 15 — loss: 0.2152
Epoch 16 — loss: 0.2085
Epoch 17 — loss: 0.2040
Epoch 18 — loss: 0.1978
Epoch 19 — loss: 0.1940
Epoch 20 — loss: 0.1895
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  82.190002
2    P(>V6)  84.889999
3    P(>V7)  87.489998
4    P(>V8)  92.440002
5    P(>V9)  96.820000
Overall accuracy: 50.19%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3738
Epoch 02 — loss: 0.3117
Epoch 03 — loss: 0.2987
Epoch 04 — loss: 0.2905
Epoch 05 — loss: 0.2794
Epoch 06 — loss: 0.2726
Epoch 07 — loss: 0.2653
Epoch 08 — loss: 0.2561
Epoch 09 — loss: 0.2491
Epoch 10 — loss: 0.2455
Epoch 11 — loss: 0.2349
Epoch 12 — loss: 0.2299
Epoch 13 — loss: 0.2254
Epoch 14 — loss: 0.2202
Epoch 15 — loss: 0.2131
Epoch 16 — loss: 0.2080
Epoch 17 — loss: 0.2040
Epoch 18 — loss: 0.1967
Epoch 19 — loss: 0.1907
Epoch 20 — loss: 0.1879
  threshold   accuracy
0    P(>V4)  82.720001
1    P(>V5)  82.050003
2    P(>V6)  85.470001
3    P(>V7)  87.730003
4    P(>V8)  92.540001
5    P(>V9)  96.680000
Overall accuracy: 48.60%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3703
Epoch 02 — loss: 0.3116
Epoch 03 — loss: 0.3029
Epoch 04 — loss: 0.2942
Epoch 05 — loss: 0.2909
Epoch 06 — loss: 0.2836
Epoch 07 — loss: 0.2792
Epoch 08 — loss: 0.2756
Epoch 09 — loss: 0.2691
Epoch 10 — loss: 0.2623
Epoch 11 — loss: 0.2581
Epoch 12 — loss: 0.2524
Epoch 13 — loss: 0.2476/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [14:16:23] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 14 — loss: 0.2406
Epoch 15 — loss: 0.2367
Epoch 16 — loss: 0.2329
Epoch 17 — loss: 0.2268
Epoch 18 — loss: 0.2226
Epoch 19 — loss: 0.2175
Epoch 20 — loss: 0.2137
  threshold   accuracy
0    P(>V4)  82.820000
1    P(>V5)  82.150002
2    P(>V6)  85.230003
3    P(>V7)  88.160004
4    P(>V8)  92.690002
5    P(>V9)  96.820000
Overall accuracy: 47.93%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4641
Epoch 02 — loss: 0.3355
Epoch 03 — loss: 0.2972
Epoch 04 — loss: 0.2893
Epoch 05 — loss: 0.2862
Epoch 06 — loss: 0.2827
Epoch 07 — loss: 0.2810
Epoch 08 — loss: 0.2778
Epoch 09 — loss: 0.2761
Epoch 10 — loss: 0.2759
Epoch 11 — loss: 0.2734
Epoch 12 — loss: 0.2749
Epoch 13 — loss: 0.2729
Epoch 14 — loss: 0.2713
Epoch 15 — loss: 0.2707
Epoch 16 — loss: 0.2710
Epoch 17 — loss: 0.2700
Epoch 18 — loss: 0.2700
Epoch 19 — loss: 0.2696
Epoch 20 — loss: 0.2688
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.750000
2    P(>V6)  83.879997
3    P(>V7)  87.730003
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 45.96%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4667
Epoch 02 — loss: 0.3352
Epoch 03 — loss: 0.2977
Epoch 04 — loss: 0.2892
Epoch 05 — loss: 0.2846
Epoch 06 — loss: 0.2806
Epoch 07 — loss: 0.2776
Epoch 08 — loss: 0.2767
Epoch 09 — loss: 0.2753
Epoch 10 — loss: 0.2727
Epoch 11 — loss: 0.2724
Epoch 12 — loss: 0.2712
Epoch 13 — loss: 0.2712
Epoch 14 — loss: 0.2704
Epoch 15 — loss: 0.2706
Epoch 16 — loss: 0.2705
Epoch 17 — loss: 0.2688
Epoch 18 — loss: 0.2692
Epoch 19 — loss: 0.2686
Epoch 20 — loss: 0.2685
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.559998
2    P(>V6)  83.779999
3    P(>V7)  87.970001
4    P(>V8)  92.830002
5    P(>V9)  97.059998
Overall accuracy: 46.29%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4190
Epoch 02 — loss: 0.3262
Epoch 03 — loss: 0.3125
Epoch 04 — loss: 0.3007
Epoch 05 — loss: 0.2924
Epoch 06 — loss: 0.2902
Epoch 07 — loss: 0.2853
Epoch 08 — loss: 0.2840
Epoch 09 — loss: 0.2799
Epoch 10 — loss: 0.2795
Epoch 11 — loss: 0.2786
Epoch 12 — loss: 0.2754
Epoch 13 — loss: 0.2761
Epoch 14 — loss: 0.2734
Epoch 15 — loss: 0.2764
Epoch 16 — loss: 0.2745
Epoch 17 — loss: 0.2736
Epoch 18 — loss: 0.2732
Epoch 19 — loss: 0.2721
Epoch 20 — loss: 0.2713
  threshold   accuracy
0    P(>V4)  81.809998
1    P(>V5)  81.040001
2    P(>V6)  83.540001
3    P(>V7)  87.440002
4    P(>V8)  92.970001
5    P(>V9)  96.970001
Overall accuracy: 45.43%
Ordinal stacking meta epoch 1: loss=0.2056
Ordinal stacking meta epoch 2: loss=0.1562
Ordinal stacking meta epoch 3: loss=0.1519
Ordinal stacking meta epoch 4: loss=0.1497
Ordinal stacking meta epoch 5: loss=0.1480
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001552 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.440002
2    P(>V6)  85.419998
3    P(>V7)  88.349998
4    P(>V8)  93.070000
5    P(>V9)  97.059998
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.440002
2    P(>V6)  85.419998
3    P(>V7)  88.070000
4    P(>V8)  93.070000
5    P(>V9)  97.059998
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  82.769997
1    P(>V5)  81.949997
2    P(>V6)  84.550003
3    P(>V7)  88.070000
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 47.59%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.050003
2    P(>V6)  85.419998
3    P(>V7)  88.400002
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 48.41%
  threshold   accuracy
0    P(>V4)  84.650002
1    P(>V5)  81.860001
2    P(>V6)  85.610001
3    P(>V7)  88.160004
4    P(>V8)  92.540001
5    P(>V9)  96.970001
Overall accuracy: 48.17%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.919998
2    P(>V6)  85.709999
3    P(>V7)  87.540001
4    P(>V8)  92.730003
5    P(>V9)  96.680000
Overall accuracy: 49.13%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  81.860001
2    P(>V6)  85.320000
3    P(>V7)  87.730003
4    P(>V8)  92.779999
5    P(>V9)  96.820000
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  81.910004
2    P(>V6)  85.320000
3    P(>V7)  88.110001
4    P(>V8)  92.930000
5    P(>V9)  96.779999
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.440002
2    P(>V6)  85.419998
3    P(>V7)  88.349998
4    P(>V8)  93.070000
5    P(>V9)  97.059998
Overall accuracy: 48.94%
----------------- Ordinal iteration 22/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3795
Epoch 02 — loss: 0.3265
Epoch 03 — loss: 0.3055
Epoch 04 — loss: 0.2892
Epoch 05 — loss: 0.2803
Epoch 06 — loss: 0.2694
Epoch 07 — loss: 0.2633
Epoch 08 — loss: 0.2547
Epoch 09 — loss: 0.2461
Epoch 10 — loss: 0.2418
Epoch 11 — loss: 0.2347
Epoch 12 — loss: 0.2266
Epoch 13 — loss: 0.2219
Epoch 14 — loss: 0.2155
Epoch 15 — loss: 0.2115
Epoch 16 — loss: 0.2041
Epoch 17 — loss: 0.2009
Epoch 18 — loss: 0.1948
Epoch 19 — loss: 0.1892
Epoch 20 — loss: 0.1836
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  80.559998
2    P(>V6)  83.879997
3    P(>V7)  87.870003
4    P(>V8)  92.930000
5    P(>V9)  96.779999
Overall accuracy: 47.06%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3693
Epoch 02 — loss: 0.3037
Epoch 03 — loss: 0.2943
Epoch 04 — loss: 0.2847
Epoch 05 — loss: 0.2769
Epoch 06 — loss: 0.2673
Epoch 07 — loss: 0.2617
Epoch 08 — loss: 0.2519
Epoch 09 — loss: 0.2491
Epoch 10 — loss: 0.2411
Epoch 11 — loss: 0.2360
Epoch 12 — loss: 0.2284
Epoch 13 — loss: 0.2241
Epoch 14 — loss: 0.2185
Epoch 15 — loss: 0.2116
Epoch 16 — loss: 0.2063
Epoch 17 — loss: 0.1997
Epoch 18 — loss: 0.1964
Epoch 19 — loss: 0.1904
Epoch 20 — loss: 0.1850
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.629997
2    P(>V6)  85.269997
3    P(>V7)  88.160004
4    P(>V8)  92.300003
5    P(>V9)  97.059998
Overall accuracy: 48.75%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3618
Epoch 02 — loss: 0.3150
Epoch 03 — loss: 0.3044
Epoch 04 — loss: 0.2922
Epoch 05 — loss: 0.2876
Epoch 06 — loss: 0.2816
Epoch 07 — loss: 0.2750
Epoch 08 — loss: 0.2686
Epoch 09 — loss: 0.2628
Epoch 10 — loss: 0.2592
Epoch 11 — loss: 0.2521
Epoch 12 — loss: 0.2483
Epoch 13 — loss: 0.2422
Epoch 14 — loss: 0.2349
Epoch 15 — loss: 0.2285
Epoch 16 — loss: 0.2250
Epoch 17 — loss: 0.2192
Epoch 18 — loss: 0.2142
Epoch 19 — loss: 0.2079
Epoch 20 — loss: 0.2037
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  82.629997
2    P(>V6)  85.610001
3    P(>V7)  87.730003
4    P(>V8)  92.400002
5    P(>V9)  96.730003
Overall accuracy: 48.89%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4697
Epoch 02 — loss: 0.3436
Epoch 03 — loss: 0.3002
Epoch 04 — loss: 0.2893
Epoch 05 — loss: 0.2839
Epoch 06 — loss: 0.2799
Epoch 07 — loss: 0.2779
Epoch 08 — loss: 0.2771
Epoch 09 — loss: 0.2753
Epoch 10 — loss: 0.2737
Epoch 11 — loss: 0.2724
Epoch 12 — loss: 0.2718
Epoch 13 — loss: 0.2716
Epoch 14 — loss: 0.2709
Epoch 15 — loss: 0.2711
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2700
Epoch 18 — loss: 0.2712
Epoch 19 — loss: 0.2707
Epoch 20 — loss: 0.2709
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  80.900002
2    P(>V6)  84.220001
3    P(>V7)  88.070000
4    P(>V8)  92.879997
5    P(>V9)  97.019997/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [14:39:46] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 45.86%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4739
Epoch 02 — loss: 0.3409
Epoch 03 — loss: 0.3028
Epoch 04 — loss: 0.2916
Epoch 05 — loss: 0.2853
Epoch 06 — loss: 0.2817
Epoch 07 — loss: 0.2791
Epoch 08 — loss: 0.2770
Epoch 09 — loss: 0.2750
Epoch 10 — loss: 0.2741
Epoch 11 — loss: 0.2751
Epoch 12 — loss: 0.2733
Epoch 13 — loss: 0.2712
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2711
Epoch 16 — loss: 0.2700
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2700
Epoch 19 — loss: 0.2695
Epoch 20 — loss: 0.2691
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  81.040001
2    P(>V6)  83.830002
3    P(>V7)  87.629997
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.72%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4292
Epoch 02 — loss: 0.3250
Epoch 03 — loss: 0.3090
Epoch 04 — loss: 0.3000
Epoch 05 — loss: 0.2928
Epoch 06 — loss: 0.2892
Epoch 07 — loss: 0.2853
Epoch 08 — loss: 0.2809
Epoch 09 — loss: 0.2819
Epoch 10 — loss: 0.2791
Epoch 11 — loss: 0.2784
Epoch 12 — loss: 0.2768
Epoch 13 — loss: 0.2743
Epoch 14 — loss: 0.2758
Epoch 15 — loss: 0.2742
Epoch 16 — loss: 0.2734
Epoch 17 — loss: 0.2737
Epoch 18 — loss: 0.2722
Epoch 19 — loss: 0.2726
Epoch 20 — loss: 0.2711
  threshold   accuracy
0    P(>V4)  81.089996
1    P(>V5)  81.089996
2    P(>V6)  83.879997
3    P(>V7)  87.199997
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 46.25%
Ordinal stacking meta epoch 1: loss=0.2136
Ordinal stacking meta epoch 2: loss=0.1518
Ordinal stacking meta epoch 3: loss=0.1457
Ordinal stacking meta epoch 4: loss=0.1433
Ordinal stacking meta epoch 5: loss=0.1417
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001596 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  82.290001
2    P(>V6)  85.419998
3    P(>V7)  89.269997
4    P(>V8)  93.360001
5    P(>V9)  97.059998
Overall accuracy: 50.10%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.870003
2    P(>V6)  85.610001
3    P(>V7)  89.169998
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 50.63%
  threshold   accuracy
0    P(>V4)  82.680000
1    P(>V5)  81.760002
2    P(>V6)  84.739998
3    P(>V7)  88.839996
4    P(>V8)  93.260002
5    P(>V9)  97.019997
Overall accuracy: 47.93%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.339996
2    P(>V6)  85.660004
3    P(>V7)  89.269997
4    P(>V8)  93.070000
5    P(>V9)  97.059998
Overall accuracy: 49.52%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  83.110001
2    P(>V6)  85.370003
3    P(>V7)  89.559998
4    P(>V8)  92.589996
5    P(>V9)  96.919998
Overall accuracy: 49.37%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.239998
2    P(>V6)  85.080002
3    P(>V7)  88.980003
4    P(>V8)  92.300003
5    P(>V9)  96.440002
Overall accuracy: 48.03%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  81.910004
2    P(>V6)  85.080002
3    P(>V7)  88.260002
4    P(>V8)  92.110001
5    P(>V9)  96.629997
Overall accuracy: 48.32%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.290001
2    P(>V6)  85.269997
3    P(>V7)  88.790001
4    P(>V8)  92.300003
5    P(>V9)  96.540001
Overall accuracy: 48.27%
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  82.290001
2    P(>V6)  85.419998
3    P(>V7)  89.269997
4    P(>V8)  93.360001
5    P(>V9)  97.059998
Overall accuracy: 50.10%
----------------- Ordinal iteration 23/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3822
Epoch 02 — loss: 0.3169
Epoch 03 — loss: 0.3041
Epoch 04 — loss: 0.2933
Epoch 05 — loss: 0.2815
Epoch 06 — loss: 0.2755
Epoch 07 — loss: 0.2671
Epoch 08 — loss: 0.2597
Epoch 09 — loss: 0.2538
Epoch 10 — loss: 0.2435
Epoch 11 — loss: 0.2382
Epoch 12 — loss: 0.2300
Epoch 13 — loss: 0.2237
Epoch 14 — loss: 0.2168
Epoch 15 — loss: 0.2098
Epoch 16 — loss: 0.2038
Epoch 17 — loss: 0.1980
Epoch 18 — loss: 0.1922
Epoch 19 — loss: 0.1864
Epoch 20 — loss: 0.1809
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  81.180000
2    P(>V6)  84.410004
3    P(>V7)  87.339996
4    P(>V8)  92.540001
5    P(>V9)  96.290001
Overall accuracy: 47.06%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3792
Epoch 02 — loss: 0.3156
Epoch 03 — loss: 0.3017
Epoch 04 — loss: 0.2910
Epoch 05 — loss: 0.2825
Epoch 06 — loss: 0.2746
Epoch 07 — loss: 0.2648
Epoch 08 — loss: 0.2591
Epoch 09 — loss: 0.2535
Epoch 10 — loss: 0.2453
Epoch 11 — loss: 0.2378
Epoch 12 — loss: 0.2310
Epoch 13 — loss: 0.2254
Epoch 14 — loss: 0.2194
Epoch 15 — loss: 0.2128
Epoch 16 — loss: 0.2056
Epoch 17 — loss: 0.2028
Epoch 18 — loss: 0.1990
Epoch 19 — loss: 0.1921
Epoch 20 — loss: 0.1842
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  81.570000
2    P(>V6)  84.599998
3    P(>V7)  87.680000
4    P(>V8)  92.160004
5    P(>V9)  96.629997
Overall accuracy: 48.51%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3557
Epoch 02 — loss: 0.3094
Epoch 03 — loss: 0.3054
Epoch 04 — loss: 0.2936
Epoch 05 — loss: 0.2912
Epoch 06 — loss: 0.2835
Epoch 07 — loss: 0.2791
Epoch 08 — loss: 0.2729
Epoch 09 — loss: 0.2690
Epoch 10 — loss: 0.2619
Epoch 11 — loss: 0.2556
Epoch 12 — loss: 0.2480
Epoch 13 — loss: 0.2456
Epoch 14 — loss: 0.2405
Epoch 15 — loss: 0.2344
Epoch 16 — loss: 0.2308
Epoch 17 — loss: 0.2244
Epoch 18 — loss: 0.2212
Epoch 19 — loss: 0.2168
Epoch 20 — loss: 0.2144
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  81.139999
2    P(>V6)  85.180000
3    P(>V7)  88.019997
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 48.08%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4692
Epoch 02 — loss: 0.3467
Epoch 03 — loss: 0.3052
Epoch 04 — loss: 0.2933
Epoch 05 — loss: 0.2885
Epoch 06 — loss: 0.2831
Epoch 07 — loss: 0.2816
Epoch 08 — loss: 0.2780
Epoch 09 — loss: 0.2760
Epoch 10 — loss: 0.2754
Epoch 11 — loss: 0.2745
Epoch 12 — loss: 0.2732
Epoch 13 — loss: 0.2717
Epoch 14 — loss: 0.2726
Epoch 15 — loss: 0.2718
Epoch 16 — loss: 0.2707
Epoch 17 — loss: 0.2716
Epoch 18 — loss: 0.2698
Epoch 19 — loss: 0.2687
Epoch 20 — loss: 0.2687
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  81.089996
2    P(>V6)  83.449997
3    P(>V7)  87.629997
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 45.52%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4761
Epoch 02 — loss: 0.3460
Epoch 03 — loss: 0.2994
Epoch 04 — loss: 0.2885
Epoch 05 — loss: 0.2831
Epoch 06 — loss: 0.2798
Epoch 07 — loss: 0.2773
Epoch 08 — loss: 0.2745
Epoch 09 — loss: 0.2742
Epoch 10 — loss: 0.2730
Epoch 11 — loss: 0.2727
Epoch 12 — loss: 0.2712
Epoch 13 — loss: 0.2718
Epoch 14 — loss: 0.2702
Epoch 15 — loss: 0.2724
Epoch 16 — loss: 0.2707
Epoch 17 — loss: 0.2697
Epoch 18 — loss: 0.2708
Epoch 19 — loss: 0.2694
Epoch 20 — loss: 0.2696
  threshold   accuracy
0    P(>V4)  81.949997
1    P(>V5)  80.940002
2    P(>V6)  83.879997
3    P(>V7)  88.019997
4    P(>V8)  92.540001
5    P(>V9)  97.059998
Overall accuracy: 45.38%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4222
Epoch 02 — loss: 0.3217
Epoch 03 — loss: 0.3051
Epoch 04 — loss: 0.2965
Epoch 05 — loss: 0.2888
Epoch 06 — loss: 0.2857
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [15:03:04] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [15:31:50] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
Epoch 07 — loss: 0.2838
Epoch 08 — loss: 0.2843
Epoch 09 — loss: 0.2807
Epoch 10 — loss: 0.2781
Epoch 11 — loss: 0.2749
Epoch 12 — loss: 0.2751
Epoch 13 — loss: 0.2755
Epoch 14 — loss: 0.2741
Epoch 15 — loss: 0.2738
Epoch 16 — loss: 0.2735
Epoch 17 — loss: 0.2727
Epoch 18 — loss: 0.2738
Epoch 19 — loss: 0.2732
Epoch 20 — loss: 0.2708
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  80.940002
2    P(>V6)  84.120003
3    P(>V7)  87.489998
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 44.95%
Ordinal stacking meta epoch 1: loss=0.2593
Ordinal stacking meta epoch 2: loss=0.1535
Ordinal stacking meta epoch 3: loss=0.1438
Ordinal stacking meta epoch 4: loss=0.1404
Ordinal stacking meta epoch 5: loss=0.1389
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001572 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  81.860001
2    P(>V6)  86.190002
3    P(>V7)  88.349998
4    P(>V8)  93.019997
5    P(>V9)  97.059998
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  84.650002
1    P(>V5)  82.150002
2    P(>V6)  85.510002
3    P(>V7)  88.589996
4    P(>V8)  93.169998
5    P(>V9)  97.110001
Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  81.760002
2    P(>V6)  84.550003
3    P(>V7)  87.970001
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 47.50%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  81.809998
2    P(>V6)  85.510002
3    P(>V7)  88.209999
4    P(>V8)  93.120003
5    P(>V9)  97.059998
Overall accuracy: 48.41%
  threshold   accuracy
0    P(>V4)  84.989998
1    P(>V5)  82.769997
2    P(>V6)  85.510002
3    P(>V7)  88.879997
4    P(>V8)  92.830002
5    P(>V9)  96.779999
Overall accuracy: 49.71%
  threshold   accuracy
0    P(>V4)  84.790001
1    P(>V5)  83.199997
2    P(>V6)  84.839996
3    P(>V7)  87.919998
4    P(>V8)  92.540001
5    P(>V9)  96.489998
Overall accuracy: 49.90%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  83.160004
2    P(>V6)  85.559998
3    P(>V7)  88.260002
4    P(>V8)  92.540001
5    P(>V9)  96.580002
Overall accuracy: 50.00%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  83.589996
2    P(>V6)  84.989998
3    P(>V7)  88.639999
4    P(>V8)  92.639999
5    P(>V9)  96.540001
Overall accuracy: 50.19%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  81.860001
2    P(>V6)  86.190002
3    P(>V7)  88.349998
4    P(>V8)  93.019997
5    P(>V9)  97.059998
Overall accuracy: 48.56%
----------------- Ordinal iteration 24/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3817
Epoch 02 — loss: 0.3173
Epoch 03 — loss: 0.3017
Epoch 04 — loss: 0.2923
Epoch 05 — loss: 0.2822
Epoch 06 — loss: 0.2716
Epoch 07 — loss: 0.2649
Epoch 08 — loss: 0.2582
Epoch 09 — loss: 0.2518
Epoch 10 — loss: 0.2437
Epoch 11 — loss: 0.2384
Epoch 12 — loss: 0.2312
Epoch 13 — loss: 0.2256
Epoch 14 — loss: 0.2206
Epoch 15 — loss: 0.2140
Epoch 16 — loss: 0.2081
Epoch 17 — loss: 0.2014
Epoch 18 — loss: 0.1992
Epoch 19 — loss: 0.1931
Epoch 20 — loss: 0.1895
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  80.610001
2    P(>V6)  84.599998
3    P(>V7)  87.300003
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 47.55%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3736
Epoch 02 — loss: 0.3125
Epoch 03 — loss: 0.2992
Epoch 04 — loss: 0.2908
Epoch 05 — loss: 0.2816
Epoch 06 — loss: 0.2746
Epoch 07 — loss: 0.2672
Epoch 08 — loss: 0.2604
Epoch 09 — loss: 0.2545
Epoch 10 — loss: 0.2461
Epoch 11 — loss: 0.2391
Epoch 12 — loss: 0.2342
Epoch 13 — loss: 0.2295
Epoch 14 — loss: 0.2219
Epoch 15 — loss: 0.2162
Epoch 16 — loss: 0.2113
Epoch 17 — loss: 0.2037
Epoch 18 — loss: 0.2005
Epoch 19 — loss: 0.1946
Epoch 20 — loss: 0.1894
  threshold   accuracy
0    P(>V4)  82.389999
1    P(>V5)  81.809998
2    P(>V6)  84.309998
3    P(>V7)  87.540001
4    P(>V8)  92.440002
5    P(>V9)  96.050003
Overall accuracy: 46.73%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3619
Epoch 02 — loss: 0.3180
Epoch 03 — loss: 0.3012
Epoch 04 — loss: 0.2939
Epoch 05 — loss: 0.2917
Epoch 06 — loss: 0.2831
Epoch 07 — loss: 0.2768
Epoch 08 — loss: 0.2711
Epoch 09 — loss: 0.2644
Epoch 10 — loss: 0.2599
Epoch 11 — loss: 0.2539
Epoch 12 — loss: 0.2490
Epoch 13 — loss: 0.2456
Epoch 14 — loss: 0.2384
Epoch 15 — loss: 0.2333
Epoch 16 — loss: 0.2278
Epoch 17 — loss: 0.2253
Epoch 18 — loss: 0.2185
Epoch 19 — loss: 0.2140
Epoch 20 — loss: 0.2126
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.339996
2    P(>V6)  85.849998
3    P(>V7)  88.260002
4    P(>V8)  92.779999
5    P(>V9)  97.059998
Overall accuracy: 47.98%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4700
Epoch 02 — loss: 0.3452
Epoch 03 — loss: 0.3070
Epoch 04 — loss: 0.2947
Epoch 05 — loss: 0.2879
Epoch 06 — loss: 0.2821
Epoch 07 — loss: 0.2799
Epoch 08 — loss: 0.2777
Epoch 09 — loss: 0.2755
Epoch 10 — loss: 0.2751
Epoch 11 — loss: 0.2740
Epoch 12 — loss: 0.2732
Epoch 13 — loss: 0.2718
Epoch 14 — loss: 0.2713
Epoch 15 — loss: 0.2712
Epoch 16 — loss: 0.2710
Epoch 17 — loss: 0.2708
Epoch 18 — loss: 0.2714
Epoch 19 — loss: 0.2697
Epoch 20 — loss: 0.2714
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.750000
2    P(>V6)  83.690002
3    P(>V7)  87.199997
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.77%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4708
Epoch 02 — loss: 0.3353
Epoch 03 — loss: 0.2984
Epoch 04 — loss: 0.2904
Epoch 05 — loss: 0.2865
Epoch 06 — loss: 0.2828
Epoch 07 — loss: 0.2807
Epoch 08 — loss: 0.2781
Epoch 09 — loss: 0.2774
Epoch 10 — loss: 0.2763
Epoch 11 — loss: 0.2753
Epoch 12 — loss: 0.2731
Epoch 13 — loss: 0.2740
Epoch 14 — loss: 0.2725
Epoch 15 — loss: 0.2718
Epoch 16 — loss: 0.2708
Epoch 17 — loss: 0.2712
Epoch 18 — loss: 0.2700
Epoch 19 — loss: 0.2702
Epoch 20 — loss: 0.2699
  threshold   accuracy
0    P(>V4)  81.860001
1    P(>V5)  80.559998
2    P(>V6)  83.690002
3    P(>V7)  87.680000
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.86%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4305
Epoch 02 — loss: 0.3257
Epoch 03 — loss: 0.3090
Epoch 04 — loss: 0.3006
Epoch 05 — loss: 0.2951
Epoch 06 — loss: 0.2900
Epoch 07 — loss: 0.2869
Epoch 08 — loss: 0.2837
Epoch 09 — loss: 0.2810
Epoch 10 — loss: 0.2795
Epoch 11 — loss: 0.2770
Epoch 12 — loss: 0.2755
Epoch 13 — loss: 0.2751
Epoch 14 — loss: 0.2759
Epoch 15 — loss: 0.2725
Epoch 16 — loss: 0.2728
Epoch 17 — loss: 0.2723
Epoch 18 — loss: 0.2700
Epoch 19 — loss: 0.2730
Epoch 20 — loss: 0.2723
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.940002
2    P(>V6)  83.830002
3    P(>V7)  87.339996
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.81%
Ordinal stacking meta epoch 1: loss=0.2362
Ordinal stacking meta epoch 2: loss=0.1601
Ordinal stacking meta epoch 3: loss=0.1526
Ordinal stacking meta epoch 4: loss=0.1495
Ordinal stacking meta epoch 5: loss=0.1476
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005079 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [15:55:09] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.820000
2    P(>V6)  85.510002
3    P(>V7)  88.839996
4    P(>V8)  93.070000
5    P(>V9)  97.059998
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.339996
2    P(>V6)  85.559998
3    P(>V7)  88.449997
4    P(>V8)  92.970001
5    P(>V9)  97.059998
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  82.820000
1    P(>V5)  81.860001
2    P(>V6)  85.129997
3    P(>V7)  88.160004
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 47.64%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.529999
2    P(>V6)  85.559998
3    P(>V7)  88.449997
4    P(>V8)  92.930000
5    P(>V9)  97.059998
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.769997
2    P(>V6)  85.129997
3    P(>V7)  89.120003
4    P(>V8)  93.019997
5    P(>V9)  96.970001
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  82.720001
2    P(>V6)  85.370003
3    P(>V7)  88.879997
4    P(>V8)  93.169998
5    P(>V9)  96.489998
Overall accuracy: 49.86%
  threshold   accuracy
0    P(>V4)  82.720001
1    P(>V5)  82.480003
2    P(>V6)  85.419998
3    P(>V7)  88.400002
4    P(>V8)  93.019997
5    P(>V9)  96.680000
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  82.720001
2    P(>V6)  85.709999
3    P(>V7)  88.500000
4    P(>V8)  93.459999
5    P(>V9)  96.730003
Overall accuracy: 49.42%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.820000
2    P(>V6)  85.510002
3    P(>V7)  88.839996
4    P(>V8)  93.070000
5    P(>V9)  97.059998
Overall accuracy: 49.04%
----------------- Ordinal iteration 25/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3794
Epoch 02 — loss: 0.3187
Epoch 03 — loss: 0.3006
Epoch 04 — loss: 0.2899
Epoch 05 — loss: 0.2768
Epoch 06 — loss: 0.2675
Epoch 07 — loss: 0.2585
Epoch 08 — loss: 0.2517
Epoch 09 — loss: 0.2444
Epoch 10 — loss: 0.2366
Epoch 11 — loss: 0.2321
Epoch 12 — loss: 0.2267
Epoch 13 — loss: 0.2207
Epoch 14 — loss: 0.2142
Epoch 15 — loss: 0.2073
Epoch 16 — loss: 0.2026
Epoch 17 — loss: 0.1971
Epoch 18 — loss: 0.1921
Epoch 19 — loss: 0.1856
Epoch 20 — loss: 0.1804
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  81.620003
2    P(>V6)  84.500000
3    P(>V7)  87.820000
4    P(>V8)  92.540001
5    P(>V9)  96.290001
Overall accuracy: 47.55%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3651
Epoch 02 — loss: 0.3029
Epoch 03 — loss: 0.2921
Epoch 04 — loss: 0.2801
Epoch 05 — loss: 0.2738
Epoch 06 — loss: 0.2656
Epoch 07 — loss: 0.2567
Epoch 08 — loss: 0.2504
Epoch 09 — loss: 0.2437
Epoch 10 — loss: 0.2373
Epoch 11 — loss: 0.2296
Epoch 12 — loss: 0.2256
Epoch 13 — loss: 0.2194
Epoch 14 — loss: 0.2131
Epoch 15 — loss: 0.2089
Epoch 16 — loss: 0.2040
Epoch 17 — loss: 0.2003
Epoch 18 — loss: 0.1920
Epoch 19 — loss: 0.1863
Epoch 20 — loss: 0.1838
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  81.669998
2    P(>V6)  84.070000
3    P(>V7)  87.580002
4    P(>V8)  92.639999
5    P(>V9)  96.629997
Overall accuracy: 47.11%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3665
Epoch 02 — loss: 0.3173
Epoch 03 — loss: 0.3033
Epoch 04 — loss: 0.2962
Epoch 05 — loss: 0.2875
Epoch 06 — loss: 0.2812
Epoch 07 — loss: 0.2781
Epoch 08 — loss: 0.2699
Epoch 09 — loss: 0.2632
Epoch 10 — loss: 0.2575
Epoch 11 — loss: 0.2520
Epoch 12 — loss: 0.2468
Epoch 13 — loss: 0.2407
Epoch 14 — loss: 0.2358
Epoch 15 — loss: 0.2298
Epoch 16 — loss: 0.2259
Epoch 17 — loss: 0.2223
Epoch 18 — loss: 0.2150
Epoch 19 — loss: 0.2112
Epoch 20 — loss: 0.2070
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  81.379997
2    P(>V6)  85.470001
3    P(>V7)  88.209999
4    P(>V8)  92.639999
5    P(>V9)  97.059998
Overall accuracy: 49.42%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4733
Epoch 02 — loss: 0.3540
Epoch 03 — loss: 0.3045
Epoch 04 — loss: 0.2926
Epoch 05 — loss: 0.2881
Epoch 06 — loss: 0.2829
Epoch 07 — loss: 0.2821
Epoch 08 — loss: 0.2803
Epoch 09 — loss: 0.2777
Epoch 10 — loss: 0.2756
Epoch 11 — loss: 0.2743
Epoch 12 — loss: 0.2753
Epoch 13 — loss: 0.2745
Epoch 14 — loss: 0.2731
Epoch 15 — loss: 0.2721
Epoch 16 — loss: 0.2711
Epoch 17 — loss: 0.2712
Epoch 18 — loss: 0.2701
Epoch 19 — loss: 0.2699
Epoch 20 — loss: 0.2692
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  81.089996
2    P(>V6)  83.639999
3    P(>V7)  87.489998
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 44.90%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4660
Epoch 02 — loss: 0.3357
Epoch 03 — loss: 0.3023
Epoch 04 — loss: 0.2928
Epoch 05 — loss: 0.2878
Epoch 06 — loss: 0.2830
Epoch 07 — loss: 0.2816
Epoch 08 — loss: 0.2791
Epoch 09 — loss: 0.2770
Epoch 10 — loss: 0.2775
Epoch 11 — loss: 0.2743
Epoch 12 — loss: 0.2727
Epoch 13 — loss: 0.2715
Epoch 14 — loss: 0.2728
Epoch 15 — loss: 0.2714
Epoch 16 — loss: 0.2704
Epoch 17 — loss: 0.2702
Epoch 18 — loss: 0.2703
Epoch 19 — loss: 0.2687
Epoch 20 — loss: 0.2707
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  80.510002
2    P(>V6)  83.589996
3    P(>V7)  87.339996
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.49%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4332
Epoch 02 — loss: 0.3215
Epoch 03 — loss: 0.3027
Epoch 04 — loss: 0.2970
Epoch 05 — loss: 0.2916
Epoch 06 — loss: 0.2882
Epoch 07 — loss: 0.2862
Epoch 08 — loss: 0.2816
Epoch 09 — loss: 0.2802
Epoch 10 — loss: 0.2797
Epoch 11 — loss: 0.2768
Epoch 12 — loss: 0.2759
Epoch 13 — loss: 0.2752
Epoch 14 — loss: 0.2776
Epoch 15 — loss: 0.2742
Epoch 16 — loss: 0.2729
Epoch 17 — loss: 0.2729
Epoch 18 — loss: 0.2732
Epoch 19 — loss: 0.2736
Epoch 20 — loss: 0.2705
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.940002
2    P(>V6)  84.220001
3    P(>V7)  87.580002
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 45.86%
Ordinal stacking meta epoch 1: loss=0.1876
Ordinal stacking meta epoch 2: loss=0.1475
Ordinal stacking meta epoch 3: loss=0.1442
Ordinal stacking meta epoch 4: loss=0.1427
Ordinal stacking meta epoch 5: loss=0.1416
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001736 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 9180
[LightGBM] [Info] Number of data points in the train set: 8310, number of used features: 36
[LightGBM] [Info] Start training from score -1.163103
[LightGBM] [Info] Start training from score -1.207992
[LightGBM] [Info] Start training from score -2.028533
[LightGBM] [Info] Start training from score -2.531461
[LightGBM] [Info] Start training from score -2.291813
[LightGBM] [Info] Start training from score -3.064210
[LightGBM] [Info] Start training from score -3.511786
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.440002
2    P(>V6)  85.610001
3    P(>V7)  88.589996
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.389999
2    P(>V6)  85.320000
3    P(>V7)  88.690002
4    P(>V8)  93.260002
5    P(>V9)  97.019997
Overall accuracy: 50.00%
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  81.860001
2    P(>V6)  84.940002
3    P(>V7)  88.500000
4    P(>V8)  93.169998
5    P(>V9)  97.019997
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.290001
2    P(>V6)  85.510002
3    P(>V7)  88.839996
4    P(>V8)  93.169998
5    P(>V9)  97.019997
Overall accuracy: 49.62%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.150002
2    P(>V6)  85.370003
3    P(>V7)  89.269997
4    P(>V8)  93.209999
5    P(>V9)  96.389999
Overall accuracy: 48.36%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.190002
2    P(>V6)  85.029999
3    P(>V7)  88.980003
4    P(>V8)  92.830002
5    P(>V9)  96.290001
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  81.809998
2    P(>V6)  84.650002
3    P(>V7)  88.500000
4    P(>V8)  92.639999
5    P(>V9)  96.540001
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  81.910004
2    P(>V6)  84.699997
3    P(>V7)  88.449997
4    P(>V8)  93.120003
5    P(>V9)  96.489998
Overall accuracy: 48.99%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.440002
2    P(>V6)  85.610001
3    P(>V7)  88.589996
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 49.57%
Saved aggregated ordinal results to ./result/ordinal_result.xlsx
                                  model  ...  overall_accuracy_std
0                       deepset_ordinal  ...              0.670808
1                    deepset_ordinal_xy  ...              0.680043
2           deepset_ordinal_xy_additive  ...              0.889403
3             ordinal_adaboost_ensemble  ...              0.469145
4                  ordinal_gbm_ensemble  ...              0.859845
5       ordinal_geometric_mean_ensemble  ...              0.492827
6             ordinal_lightgbm_ensemble  ...              0.936846
7               ordinal_median_ensemble  ...              0.422252
8          ordinal_soft_voting_ensemble  ...              0.469145
9             ordinal_stacking_ensemble  ...              0.743172
10        ordinal_trimmed_mean_ensemble  ...              0.531342
11             ordinal_xgboost_ensemble  ...              0.867220
12              set_transformer_ordinal  ...              0.959001
13           set_transformer_ordinal_xy  ...              1.174203
14  set_transformer_ordinal_xy_additive  ...              1.185818

[15 rows x 3 columns]
