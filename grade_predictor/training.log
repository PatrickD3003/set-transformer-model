{'A1': 0, 'B1': 1, 'C1': 2, 'D1': 3, 'E1': 4, 'F1': 5, 'G1': 6, 'H1': 7, 'I1': 8, 'J1': 9, 'K1': 10, 'A2': 11, 'B2': 12, 'C2': 13, 'D2': 14, 'E2': 15, 'F2': 16, 'G2': 17, 'H2': 18, 'I2': 19, 'J2': 20, 'K2': 21, 'A3': 22, 'B3': 23, 'C3': 24, 'D3': 25, 'E3': 26, 'F3': 27, 'G3': 28, 'H3': 29, 'I3': 30, 'J3': 31, 'K3': 32, 'A4': 33, 'B4': 34, 'C4': 35, 'D4': 36, 'E4': 37, 'F4': 38, 'G4': 39, 'H4': 40, 'I4': 41, 'J4': 42, 'K4': 43, 'A5': 44, 'B5': 45, 'C5': 46, 'D5': 47, 'E5': 48, 'F5': 49, 'G5': 50, 'H5': 51, 'I5': 52, 'J5': 53, 'K5': 54, 'A6': 55, 'B6': 56, 'C6': 57, 'D6': 58, 'E6': 59, 'F6': 60, 'G6': 61, 'H6': 62, 'I6': 63, 'J6': 64, 'K6': 65, 'A7': 66, 'B7': 67, 'C7': 68, 'D7': 69, 'E7': 70, 'F7': 71, 'G7': 72, 'H7': 73, 'I7': 74, 'J7': 75, 'K7': 76, 'A8': 77, 'B8': 78, 'C8': 79, 'D8': 80, 'E8': 81, 'F8': 82, 'G8': 83, 'H8': 84, 'I8': 85, 'J8': 86, 'K8': 87, 'A9': 88, 'B9': 89, 'C9': 90, 'D9': 91, 'E9': 92, 'F9': 93, 'G9': 94, 'H9': 95, 'I9': 96, 'J9': 97, 'K9': 98, 'A10': 99, 'B10': 100, 'C10': 101, 'D10': 102, 'E10': 103, 'F10': 104, 'G10': 105, 'H10': 106, 'I10': 107, 'J10': 108, 'K10': 109, 'A11': 110, 'B11': 111, 'C11': 112, 'D11': 113, 'E11': 114, 'F11': 115, 'G11': 116, 'H11': 117, 'I11': 118, 'J11': 119, 'K11': 120, 'A12': 121, 'B12': 122, 'C12': 123, 'D12': 124, 'E12': 125, 'F12': 126, 'G12': 127, 'H12': 128, 'I12': 129, 'J12': 130, 'K12': 131, 'A13': 132, 'B13': 133, 'C13': 134, 'D13': 135, 'E13': 136, 'F13': 137, 'G13': 138, 'H13': 139, 'I13': 140, 'J13': 141, 'K13': 142, 'A14': 143, 'B14': 144, 'C14': 145, 'D14': 146, 'E14': 147, 'F14': 148, 'G14': 149, 'H14': 150, 'I14': 151, 'J14': 152, 'K14': 153, 'A15': 154, 'B15': 155, 'C15': 156, 'D15': 157, 'E15': 158, 'F15': 159, 'G15': 160, 'H15': 161, 'I15': 162, 'J15': 163, 'K15': 164, 'A16': 165, 'B16': 166, 'C16': 167, 'D16': 168, 'E16': 169, 'F16': 170, 'G16': 171, 'H16': 172, 'I16': 173, 'J16': 174, 'K16': 175, 'A17': 176, 'B17': 177, 'C17': 178, 'D17': 179, 'E17': 180, 'F17': 181, 'G17': 182, 'H17': 183, 'I17': 184, 'J17': 185, 'K17': 186, 'A18': 187, 'B18': 188, 'C18': 189, 'D18': 190, 'E18': 191, 'F18': 192, 'G18': 193, 'H18': 194, 'I18': 195, 'J18': 196, 'K18': 197}
successfully parsed hold difficulty file
successfully prepare type vocabulary
successfully created (x,y) position to each hold:
{'A1': (0, 0), 'A2': (0, 1), 'A3': (0, 2), 'A4': (0, 3), 'A5': (0, 4), 'A6': (0, 5), 'A7': (0, 6), 'A8': (0, 7), 'A9': (0, 8), 'A10': (0, 9), 'A11': (0, 10), 'A12': (0, 11), 'A13': (0, 12), 'A14': (0, 13), 'A15': (0, 14), 'A16': (0, 15), 'A17': (0, 16), 'A18': (0, 17), 'B1': (1, 0), 'B2': (1, 1), 'B3': (1, 2), 'B4': (1, 3), 'B5': (1, 4), 'B6': (1, 5), 'B7': (1, 6), 'B8': (1, 7), 'B9': (1, 8), 'B10': (1, 9), 'B11': (1, 10), 'B12': (1, 11), 'B13': (1, 12), 'B14': (1, 13), 'B15': (1, 14), 'B16': (1, 15), 'B17': (1, 16), 'B18': (1, 17), 'C1': (2, 0), 'C2': (2, 1), 'C3': (2, 2), 'C4': (2, 3), 'C5': (2, 4), 'C6': (2, 5), 'C7': (2, 6), 'C8': (2, 7), 'C9': (2, 8), 'C10': (2, 9), 'C11': (2, 10), 'C12': (2, 11), 'C13': (2, 12), 'C14': (2, 13), 'C15': (2, 14), 'C16': (2, 15), 'C17': (2, 16), 'C18': (2, 17), 'D1': (3, 0), 'D2': (3, 1), 'D3': (3, 2), 'D4': (3, 3), 'D5': (3, 4), 'D6': (3, 5), 'D7': (3, 6), 'D8': (3, 7), 'D9': (3, 8), 'D10': (3, 9), 'D11': (3, 10), 'D12': (3, 11), 'D13': (3, 12), 'D14': (3, 13), 'D15': (3, 14), 'D16': (3, 15), 'D17': (3, 16), 'D18': (3, 17), 'E1': (4, 0), 'E2': (4, 1), 'E3': (4, 2), 'E4': (4, 3), 'E5': (4, 4), 'E6': (4, 5), 'E7': (4, 6), 'E8': (4, 7), 'E9': (4, 8), 'E10': (4, 9), 'E11': (4, 10), 'E12': (4, 11), 'E13': (4, 12), 'E14': (4, 13), 'E15': (4, 14), 'E16': (4, 15), 'E17': (4, 16), 'E18': (4, 17), 'F1': (5, 0), 'F2': (5, 1), 'F3': (5, 2), 'F4': (5, 3), 'F5': (5, 4), 'F6': (5, 5), 'F7': (5, 6), 'F8': (5, 7), 'F9': (5, 8), 'F10': (5, 9), 'F11': (5, 10), 'F12': (5, 11), 'F13': (5, 12), 'F14': (5, 13), 'F15': (5, 14), 'F16': (5, 15), 'F17': (5, 16), 'F18': (5, 17), 'G1': (6, 0), 'G2': (6, 1), 'G3': (6, 2), 'G4': (6, 3), 'G5': (6, 4), 'G6': (6, 5), 'G7': (6, 6), 'G8': (6, 7), 'G9': (6, 8), 'G10': (6, 9), 'G11': (6, 10), 'G12': (6, 11), 'G13': (6, 12), 'G14': (6, 13), 'G15': (6, 14), 'G16': (6, 15), 'G17': (6, 16), 'G18': (6, 17), 'H1': (7, 0), 'H2': (7, 1), 'H3': (7, 2), 'H4': (7, 3), 'H5': (7, 4), 'H6': (7, 5), 'H7': (7, 6), 'H8': (7, 7), 'H9': (7, 8), 'H10': (7, 9), 'H11': (7, 10), 'H12': (7, 11), 'H13': (7, 12), 'H14': (7, 13), 'H15': (7, 14), 'H16': (7, 15), 'H17': (7, 16), 'H18': (7, 17), 'I1': (8, 0), 'I2': (8, 1), 'I3': (8, 2), 'I4': (8, 3), 'I5': (8, 4), 'I6': (8, 5), 'I7': (8, 6), 'I8': (8, 7), 'I9': (8, 8), 'I10': (8, 9), 'I11': (8, 10), 'I12': (8, 11), 'I13': (8, 12), 'I14': (8, 13), 'I15': (8, 14), 'I16': (8, 15), 'I17': (8, 16), 'I18': (8, 17), 'J1': (9, 0), 'J2': (9, 1), 'J3': (9, 2), 'J4': (9, 3), 'J5': (9, 4), 'J6': (9, 5), 'J7': (9, 6), 'J8': (9, 7), 'J9': (9, 8), 'J10': (9, 9), 'J11': (9, 10), 'J12': (9, 11), 'J13': (9, 12), 'J14': (9, 13), 'J15': (9, 14), 'J16': (9, 15), 'J17': (9, 16), 'J18': (9, 17), 'K1': (10, 0), 'K2': (10, 1), 'K3': (10, 2), 'K4': (10, 3), 'K5': (10, 4), 'K6': (10, 5), 'K7': (10, 6), 'K8': (10, 7), 'K9': (10, 8), 'K10': (10, 9), 'K11': (10, 10), 'K12': (10, 11), 'K13': (10, 12), 'K14': (10, 13), 'K15': (10, 14), 'K16': (10, 15), 'K17': (10, 16), 'K18': (10, 17)}
Using device: cuda
------------------------------------iteration no 1------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7004
Epoch 02 — loss: 1.5746
Epoch 03 — loss: 1.5189
Epoch 04 — loss: 1.4761
Epoch 05 — loss: 1.4300
Epoch 06 — loss: 1.3901
Epoch 07 — loss: 1.3535
Epoch 08 — loss: 1.3139
Epoch 09 — loss: 1.2742
Epoch 10 — loss: 1.2288
Epoch 11 — loss: 1.1796
Epoch 12 — loss: 1.1434
Epoch 13 — loss: 1.0976
Epoch 14 — loss: 1.0587
Epoch 15 — loss: 1.0146
Epoch 16 — loss: 0.9849
Epoch 17 — loss: 0.9312
Epoch 18 — loss: 0.9040
Epoch 19 — loss: 0.8704
Epoch 20 — loss: 0.8331
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8688
Epoch 02 — loss: 1.5811
Epoch 03 — loss: 1.5282
Epoch 04 — loss: 1.4984
Epoch 05 — loss: 1.4922
Epoch 06 — loss: 1.4761
Epoch 07 — loss: 1.4752
Epoch 08 — loss: 1.4645
Epoch 09 — loss: 1.4734
Epoch 10 — loss: 1.4618
Epoch 11 — loss: 1.4580
Epoch 12 — loss: 1.4598
Epoch 13 — loss: 1.4472
Epoch 14 — loss: 1.4508
Epoch 15 — loss: 1.4452
Epoch 16 — loss: 1.4519
Epoch 17 — loss: 1.4430
Epoch 18 — loss: 1.4460
Epoch 19 — loss: 1.4377
Epoch 20 — loss: 1.4419
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.6886
Epoch 02 — loss: 1.5530
Epoch 03 — loss: 1.4980
Epoch 04 — loss: 1.4612
Epoch 05 — loss: 1.4301
Epoch 06 — loss: 1.3895
Epoch 07 — loss: 1.3505
Epoch 08 — loss: 1.3166
Epoch 09 — loss: 1.2857
Epoch 10 — loss: 1.2406
Epoch 11 — loss: 1.2056
Epoch 12 — loss: 1.1538
Epoch 13 — loss: 1.1120
Epoch 14 — loss: 1.0762
Epoch 15 — loss: 1.0251
Epoch 16 — loss: 0.9692
Epoch 17 — loss: 0.9326
Epoch 18 — loss: 0.8973
Epoch 19 — loss: 0.8607
Epoch 20 — loss: 0.8029
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8792
Epoch 02 — loss: 1.5805
Epoch 03 — loss: 1.5200
Epoch 04 — loss: 1.4947
Epoch 05 — loss: 1.4888
Epoch 06 — loss: 1.4770
Epoch 07 — loss: 1.4734
Epoch 08 — loss: 1.4684
Epoch 09 — loss: 1.4660
Epoch 10 — loss: 1.4584
Epoch 11 — loss: 1.4511
Epoch 12 — loss: 1.4516
Epoch 13 — loss: 1.4544
Epoch 14 — loss: 1.4447
Epoch 15 — loss: 1.4419
Epoch 16 — loss: 1.4476/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 17 — loss: 1.4411
Epoch 18 — loss: 1.4407
Epoch 19 — loss: 1.4371
Epoch 20 — loss: 1.4357
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7043
Epoch 02 — loss: 1.6091
Epoch 03 — loss: 1.5684
Epoch 04 — loss: 1.5326
Epoch 05 — loss: 1.5248
Epoch 06 — loss: 1.4941
Epoch 07 — loss: 1.4730
Epoch 08 — loss: 1.4472
Epoch 09 — loss: 1.4366
Epoch 10 — loss: 1.4201
Epoch 11 — loss: 1.3903
Epoch 12 — loss: 1.3684
Epoch 13 — loss: 1.3452
Epoch 14 — loss: 1.3170
Epoch 15 — loss: 1.3053
Epoch 16 — loss: 1.2719
Epoch 17 — loss: 1.2346
Epoch 18 — loss: 1.2220
Epoch 19 — loss: 1.1859
Epoch 20 — loss: 1.1629
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8229
Epoch 02 — loss: 1.6255
Epoch 03 — loss: 1.5706
Epoch 04 — loss: 1.5303
Epoch 05 — loss: 1.5131
Epoch 06 — loss: 1.5037
Epoch 07 — loss: 1.4864
Epoch 08 — loss: 1.4837
Epoch 09 — loss: 1.4765
Epoch 10 — loss: 1.4708
Epoch 11 — loss: 1.4629
Epoch 12 — loss: 1.4598
Epoch 13 — loss: 1.4551
Epoch 14 — loss: 1.4534
Epoch 15 — loss: 1.4526
Epoch 16 — loss: 1.4500
Epoch 17 — loss: 1.4523
Epoch 18 — loss: 1.4487
Epoch 19 — loss: 1.4527
Epoch 20 — loss: 1.4502
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4186
Epoch 02 — loss: 1.3145
Epoch 03 — loss: 1.2790
Stage 1: Error=0.5011, Alpha=1.7874
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7957
Epoch 02 — loss: 1.6296
Epoch 03 — loss: 1.5300
Stage 2: Error=0.6619, Alpha=1.1198
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7056
Epoch 02 — loss: 1.5678
Epoch 03 — loss: 1.4906
Stage 3: Error=0.5727, Alpha=1.4990
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8633
Epoch 02 — loss: 1.7650
Epoch 03 — loss: 1.6928
Stage 4: Error=0.7476, Alpha=0.7058
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8162
Epoch 02 — loss: 1.7445
Epoch 03 — loss: 1.6761
Stage 5: Error=0.6824, Alpha=1.0269
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4314
Epoch 02 — loss: 1.3316
Epoch 03 — loss: 1.2648
Stage 1: Error=0.5088, Alpha=1.7566
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6042
Epoch 02 — loss: 1.4938
Epoch 03 — loss: 1.4397
Stage 2: Error=0.6005, Alpha=1.3842
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6927
Epoch 02 — loss: 1.6044
Epoch 03 — loss: 1.5645
Stage 3: Error=0.6171, Alpha=1.3145
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7812
Epoch 02 — loss: 1.6550
Epoch 03 — loss: 1.5687
Stage 4: Error=0.6007, Alpha=1.3835
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7810
Epoch 02 — loss: 1.6475
Epoch 03 — loss: 1.5267
Stage 5: Error=0.5732, Alpha=1.4970
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6210
Epoch 02 — loss: 1.3759
Epoch 03 — loss: 1.3075
Stage 1: Error=0.5259, Alpha=1.6882
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7812
Epoch 02 — loss: 1.6025
Epoch 03 — loss: 1.5275
Stage 2: Error=0.6324, Alpha=1.2491
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7848
Epoch 02 — loss: 1.6522
Epoch 03 — loss: 1.6220
Stage 3: Error=0.6583, Alpha=1.1362
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8690
Epoch 02 — loss: 1.7969
Epoch 03 — loss: 1.7217
Stage 4: Error=0.7756, Alpha=0.5513
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8698
Epoch 02 — loss: 1.8115
Epoch 03 — loss: 1.7200
Stage 5: Error=0.7427, Alpha=0.7316
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8848
Stacking meta epoch 2: loss=0.6864
Stacking meta epoch 3: loss=0.6350
Stacking meta epoch 4: loss=0.6086
Stacking meta epoch 5: loss=0.5917
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8764
Stacking meta epoch 2: loss=0.6902
Stacking meta epoch 3: loss=0.6398
Stacking meta epoch 4: loss=0.6123
Stacking meta epoch 5: loss=0.5943
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2563
Stacking meta epoch 2: loss=1.2412
Stacking meta epoch 3: loss=1.2384
Stacking meta epoch 4: loss=1.2366
Stacking meta epoch 5: loss=1.2352
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      78.97
1                             deepset  ...                      78.92
2                  set_transformer_xy  ...                      79.55
3                          deepset_xy  ...                      79.11
4            set_transformer_additive  ...                      80.56
5                 deepset_xy_additive  ...                      80.41
6                        adaboost_all  ...                      80.94
7            adaboost_set_transformer  ...                      81.14
8                    adaboost_deepset  ...                      80.80
9               stacking_ensemble_all  ...                      81.28
10  stacking_ensemble_set_transformer  ...                      82.10
11          stacking_ensemble_deepset  ...                      82.58

[12 rows x 5 columns]
------------------------------------iteration no 2------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7337
Epoch 02 — loss: 1.6171
Epoch 03 — loss: 1.5629
Epoch 04 — loss: 1.5114
Epoch 05 — loss: 1.4699
Epoch 06 — loss: 1.4292
Epoch 07 — loss: 1.3810
Epoch 08 — loss: 1.3473
Epoch 09 — loss: 1.2964
Epoch 10 — loss: 1.2558
Epoch 11 — loss: 1.2185
Epoch 12 — loss: 1.1648
Epoch 13 — loss: 1.1244
Epoch 14 — loss: 1.0804
Epoch 15 — loss: 1.0393
Epoch 16 — loss: 1.0004
Epoch 17 — loss: 0.9440
Epoch 18 — loss: 0.9151
Epoch 19 — loss: 0.8735
Epoch 20 — loss: 0.8328
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8875
Epoch 02 — loss: 1.5809
Epoch 03 — loss: 1.5160
Epoch 04 — loss: 1.4934
Epoch 05 — loss: 1.4810
Epoch 06 — loss: 1.4817
Epoch 07 — loss: 1.4725
Epoch 08 — loss: 1.4590
Epoch 09 — loss: 1.4726
Epoch 10 — loss: 1.4528
Epoch 11 — loss: 1.4503
Epoch 12 — loss: 1.4473
Epoch 13 — loss: 1.4515
Epoch 14 — loss: 1.4415
Epoch 15 — loss: 1.4407
Epoch 16 — loss: 1.4473
Epoch 17 — loss: 1.4351
Epoch 18 — loss: 1.4403
Epoch 19 — loss: 1.4400
Epoch 20 — loss: 1.4329
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.6928
Epoch 02 — loss: 1.5633
Epoch 03 — loss: 1.5276
Epoch 04 — loss: 1.4845
Epoch 05 — loss: 1.4449
Epoch 06 — loss: 1.4040
Epoch 07 — loss: 1.3716
Epoch 08 — loss: 1.3210
Epoch 09 — loss: 1.2860
Epoch 10 — loss: 1.2401
Epoch 11 — loss: 1.2125
Epoch 12 — loss: 1.1630
Epoch 13 — loss: 1.1158
Epoch 14 — loss: 1.0694
Epoch 15 — loss: 1.0388
Epoch 16 — loss: 0.9864
Epoch 17 — loss: 0.9433
Epoch 18 — loss: 0.9118
Epoch 19 — loss: 0.8536
Epoch 20 — loss: 0.8379
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8343
Epoch 02 — loss: 1.5766
Epoch 03 — loss: 1.5260
Epoch 04 — loss: 1.5051
Epoch 05 — loss: 1.4860
Epoch 06 — loss: 1.4819
Epoch 07 — loss: 1.4729
Epoch 08 — loss: 1.4646
Epoch 09 — loss: 1.4630
Epoch 10 — loss: 1.4659
Epoch 11 — loss: 1.4612
Epoch 12 — loss: 1.4557
Epoch 13 — loss: 1.4480
Epoch 14 — loss: 1.4503
Epoch 15 — loss: 1.4464
Epoch 16 — loss: 1.4456
Epoch 17 — loss: 1.4398
Epoch 18 — loss: 1.4422
Epoch 19 — loss: 1.4373
Epoch 20 — loss: 1.4387
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.6939
Epoch 02 — loss: 1.5910
Epoch 03 — loss: 1.5473
Epoch 04 — loss: 1.5216
Epoch 05 — loss: 1.5033
Epoch 06 — loss: 1.4898
Epoch 07 — loss: 1.4585
Epoch 08 — loss: 1.4493
Epoch 09 — loss: 1.4234
Epoch 10 — loss: 1.4114
Epoch 11 — loss: 1.3775
Epoch 12 — loss: 1.3584
Epoch 13 — loss: 1.3341
Epoch 14 — loss: 1.3043
Epoch 15 — loss: 1.2768
Epoch 16 — loss: 1.2611
Epoch 17 — loss: 1.2269
Epoch 18 — loss: 1.1974
Epoch 19 — loss: 1.1714
Epoch 20 — loss: 1.1362
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8176
Epoch 02 — loss: 1.6288
Epoch 03 — loss: 1.5739
Epoch 04 — loss: 1.5439
Epoch 05 — loss: 1.5232
Epoch 06 — loss: 1.5018
Epoch 07 — loss: 1.4980
Epoch 08 — loss: 1.4896
Epoch 09 — loss: 1.4865
Epoch 10 — loss: 1.4806
Epoch 11 — loss: 1.4736
Epoch 12 — loss: 1.4685
Epoch 13 — loss: 1.4648
Epoch 14 — loss: 1.4630
Epoch 15 — loss: 1.4672
Epoch 16 — loss: 1.4557
Epoch 17 — loss: 1.4497
Epoch 18 — loss: 1.4443
Epoch 19 — loss: 1.4481
Epoch 20 — loss: 1.4524
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4217
Epoch 02 — loss: 1.3160
Epoch 03 — loss: 1.2500
Stage 1: Error=0.4986, Alpha=1.7975
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7902
Epoch 02 — loss: 1.5995
Epoch 03 — loss: 1.5311
Stage 2: Error=0.6773, Alpha=1.0502
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6812
Epoch 02 — loss: 1.5144
Epoch 03 — loss: 1.4624
Stage 3: Error=0.5890, Alpha=1.4320
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8718
Epoch 02 — loss: 1.7656
Epoch 03 — loss: 1.6831
Stage 4: Error=0.7137, Alpha=0.8785
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8234
Epoch 02 — loss: 1.7619
Epoch 03 — loss: 1.7141
Stage 5: Error=0.7098, Alpha=0.8972
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4225
Epoch 02 — loss: 1.3239
Epoch 03 — loss: 1.2685
Stage 1: Error=0.4968, Alpha=1.8048
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6293
Epoch 02 — loss: 1.5055
Epoch 03 — loss: 1.4479
Stage 2: Error=0.5917, Alpha=1.4207
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7281
Epoch 02 — loss: 1.6470
Epoch 03 — loss: 1.6159
Stage 3: Error=0.6787, Alpha=1.0442
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7739
Epoch 02 — loss: 1.6571
Epoch 03 — loss: 1.5473
Stage 4: Error=0.6044, Alpha=1.3679
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7607
Epoch 02 — loss: 1.6170
Epoch 03 — loss: 1.5096
Stage 5: Error=0.5583, Alpha=1.5576
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6197
Epoch 02 — loss: 1.3847
Epoch 03 — loss: 1.3248
Stage 1: Error=0.5274, Alpha=1.6819
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7751
Epoch 02 — loss: 1.5861
Epoch 03 — loss: 1.5170
Stage 2: Error=0.6348, Alpha=1.2387
--- Boosting Stage 3/5 (deepset_xy_additive) ---/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7704
Epoch 02 — loss: 1.6466
Epoch 03 — loss: 1.6146
Stage 3: Error=0.6691, Alpha=1.0878
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8810
Epoch 02 — loss: 1.8149
Epoch 03 — loss: 1.7415
Stage 4: Error=0.7440, Alpha=0.7247
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8751
Epoch 02 — loss: 1.8009
Epoch 03 — loss: 1.7570
Stage 5: Error=0.7249, Alpha=0.8227
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8964
Stacking meta epoch 2: loss=0.6949
Stacking meta epoch 3: loss=0.6439
Stacking meta epoch 4: loss=0.6181
Stacking meta epoch 5: loss=0.6015
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8803
Stacking meta epoch 2: loss=0.6924
Stacking meta epoch 3: loss=0.6438
Stacking meta epoch 4: loss=0.6176
Stacking meta epoch 5: loss=0.6005
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2607
Stacking meta epoch 2: loss=1.2424
Stacking meta epoch 3: loss=1.2391
Stacking meta epoch 4: loss=1.2370
Stacking meta epoch 5: loss=1.2354
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      78.83
1                             deepset  ...                      76.28
2                  set_transformer_xy  ...                      80.03
3                          deepset_xy  ...                      80.32
4            set_transformer_additive  ...                      82.24
5                 deepset_xy_additive  ...                      75.70
6                        adaboost_all  ...                      80.94
7            adaboost_set_transformer  ...                      82.72
8                    adaboost_deepset  ...                      82.10
9               stacking_ensemble_all  ...                      82.92
10  stacking_ensemble_set_transformer  ...                      83.16
11          stacking_ensemble_deepset  ...                      82.29

[12 rows x 5 columns]
------------------------------------iteration no 3------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7392
Epoch 02 — loss: 1.6021
Epoch 03 — loss: 1.5544
Epoch 04 — loss: 1.5005
Epoch 05 — loss: 1.4488
Epoch 06 — loss: 1.4024
Epoch 07 — loss: 1.3598
Epoch 08 — loss: 1.3165
Epoch 09 — loss: 1.2614
Epoch 10 — loss: 1.2280
Epoch 11 — loss: 1.1902
Epoch 12 — loss: 1.1378
Epoch 13 — loss: 1.0927
Epoch 14 — loss: 1.0459
Epoch 15 — loss: 1.0075
Epoch 16 — loss: 0.9661
Epoch 17 — loss: 0.9226
Epoch 18 — loss: 0.8961
Epoch 19 — loss: 0.8387
Epoch 20 — loss: 0.8194
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8748
Epoch 02 — loss: 1.5902
Epoch 03 — loss: 1.5320
Epoch 04 — loss: 1.5074
Epoch 05 — loss: 1.4928
Epoch 06 — loss: 1.4868
Epoch 07 — loss: 1.4738
Epoch 08 — loss: 1.4685
Epoch 09 — loss: 1.4668
Epoch 10 — loss: 1.4580
Epoch 11 — loss: 1.4587
Epoch 12 — loss: 1.4569
Epoch 13 — loss: 1.4571
Epoch 14 — loss: 1.4504
Epoch 15 — loss: 1.4490
Epoch 16 — loss: 1.4502
Epoch 17 — loss: 1.4355
Epoch 18 — loss: 1.4402
Epoch 19 — loss: 1.4405
Epoch 20 — loss: 1.4394
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7284
Epoch 02 — loss: 1.5989
Epoch 03 — loss: 1.5410
Epoch 04 — loss: 1.5004
Epoch 05 — loss: 1.4589
Epoch 06 — loss: 1.4188
Epoch 07 — loss: 1.3744
Epoch 08 — loss: 1.3390
Epoch 09 — loss: 1.2931
Epoch 10 — loss: 1.2650
Epoch 11 — loss: 1.2177
Epoch 12 — loss: 1.1789
Epoch 13 — loss: 1.1357
Epoch 14 — loss: 1.0847
Epoch 15 — loss: 1.0435
Epoch 16 — loss: 0.9907
Epoch 17 — loss: 0.9626
Epoch 18 — loss: 0.9200
Epoch 19 — loss: 0.8789
Epoch 20 — loss: 0.8544
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8766
Epoch 02 — loss: 1.5964
Epoch 03 — loss: 1.5387
Epoch 04 — loss: 1.5110
Epoch 05 — loss: 1.4874
Epoch 06 — loss: 1.4831
Epoch 07 — loss: 1.4749
Epoch 08 — loss: 1.4593
Epoch 09 — loss: 1.4627
Epoch 10 — loss: 1.4546
Epoch 11 — loss: 1.4493
Epoch 12 — loss: 1.4540
Epoch 13 — loss: 1.4453
Epoch 14 — loss: 1.4474
Epoch 15 — loss: 1.4451
Epoch 16 — loss: 1.4467
Epoch 17 — loss: 1.4438
Epoch 18 — loss: 1.4430
Epoch 19 — loss: 1.4407
Epoch 20 — loss: 1.4372
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7170
Epoch 02 — loss: 1.6215
Epoch 03 — loss: 1.5916
Epoch 04 — loss: 1.5655
Epoch 05 — loss: 1.5417
Epoch 06 — loss: 1.5216
Epoch 07 — loss: 1.5009
Epoch 08 — loss: 1.4832
Epoch 09 — loss: 1.4525
Epoch 10 — loss: 1.4286
Epoch 11 — loss: 1.4078
Epoch 12 — loss: 1.3861
Epoch 13 — loss: 1.3574
Epoch 14 — loss: 1.3369
Epoch 15 — loss: 1.3115
Epoch 16 — loss: 1.2825
Epoch 17 — loss: 1.2573
Epoch 18 — loss: 1.2312
Epoch 19 — loss: 1.2019
Epoch 20 — loss: 1.1731
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7979
Epoch 02 — loss: 1.6259
Epoch 03 — loss: 1.5789
Epoch 04 — loss: 1.5363
Epoch 05 — loss: 1.5171
Epoch 06 — loss: 1.4998
Epoch 07 — loss: 1.4874
Epoch 08 — loss: 1.4889
Epoch 09 — loss: 1.4743
Epoch 10 — loss: 1.4687
Epoch 11 — loss: 1.4677
Epoch 12 — loss: 1.4617
Epoch 13 — loss: 1.4643
Epoch 14 — loss: 1.4610
Epoch 15 — loss: 1.4518
Epoch 16 — loss: 1.4545
Epoch 17 — loss: 1.4551
Epoch 18 — loss: 1.4490
Epoch 19 — loss: 1.4460
Epoch 20 — loss: 1.4520
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4629
Epoch 02 — loss: 1.3340
Epoch 03 — loss: 1.2747/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Stage 1: Error=0.5141, Alpha=1.7354
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7793
Epoch 02 — loss: 1.5941
Epoch 03 — loss: 1.5131
Stage 2: Error=0.6412, Alpha=1.2113
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6928
Epoch 02 — loss: 1.5529
Epoch 03 — loss: 1.4466
Stage 3: Error=0.5801, Alpha=1.4686
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8278
Epoch 02 — loss: 1.7494
Epoch 03 — loss: 1.6819
Stage 4: Error=0.7204, Alpha=0.8451
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7846
Epoch 02 — loss: 1.7307
Epoch 03 — loss: 1.6681
Stage 5: Error=0.6790, Alpha=1.0425
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4086
Epoch 02 — loss: 1.3098
Epoch 03 — loss: 1.2415
Stage 1: Error=0.4946, Alpha=1.8134
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6117
Epoch 02 — loss: 1.5049
Epoch 03 — loss: 1.4451
Stage 2: Error=0.5706, Alpha=1.5076
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7306
Epoch 02 — loss: 1.6623
Epoch 03 — loss: 1.6003
Stage 3: Error=0.6716, Alpha=1.0765
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7705
Epoch 02 — loss: 1.6616
Epoch 03 — loss: 1.5562
Stage 4: Error=0.6008, Alpha=1.3830
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7589
Epoch 02 — loss: 1.6045
Epoch 03 — loss: 1.4839
Stage 5: Error=0.5474, Alpha=1.6015
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6393
Epoch 02 — loss: 1.4008
Epoch 03 — loss: 1.3174
Stage 1: Error=0.5344, Alpha=1.6539
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7737
Epoch 02 — loss: 1.5746
Epoch 03 — loss: 1.5231
Stage 2: Error=0.6345, Alpha=1.2403
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7897
Epoch 02 — loss: 1.6700
Epoch 03 — loss: 1.6461
Stage 3: Error=0.7035, Alpha=0.9275
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8343
Epoch 02 — loss: 1.7716
Epoch 03 — loss: 1.7057
Stage 4: Error=0.7704, Alpha=0.5810
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8547
Epoch 02 — loss: 1.7940
Epoch 03 — loss: 1.7316
Stage 5: Error=0.7576, Alpha=0.6520
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8828
Stacking meta epoch 2: loss=0.6929
Stacking meta epoch 3: loss=0.6425
Stacking meta epoch 4: loss=0.6165
Stacking meta epoch 5: loss=0.6000
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8720
Stacking meta epoch 2: loss=0.6906
Stacking meta epoch 3: loss=0.6424
Stacking meta epoch 4: loss=0.6163
Stacking meta epoch 5: loss=0.5991
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2601
Stacking meta epoch 2: loss=1.2440
Stacking meta epoch 3: loss=1.2410
Stacking meta epoch 4: loss=1.2388
Stacking meta epoch 5: loss=1.2372
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      80.70
1                             deepset  ...                      81.67
2                  set_transformer_xy  ...                      78.10
3                          deepset_xy  ...                      81.95
4            set_transformer_additive  ...                      80.32
5                 deepset_xy_additive  ...                      80.94
6                        adaboost_all  ...                      81.95
7            adaboost_set_transformer  ...                      80.61
8                    adaboost_deepset  ...                      81.47
9               stacking_ensemble_all  ...                      84.22
10  stacking_ensemble_set_transformer  ...                      84.07
11          stacking_ensemble_deepset  ...                      82.48

[12 rows x 5 columns]
------------------------------------iteration no 4------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7355
Epoch 02 — loss: 1.5960
Epoch 03 — loss: 1.5525
Epoch 04 — loss: 1.5011
Epoch 05 — loss: 1.4564
Epoch 06 — loss: 1.4058
Epoch 07 — loss: 1.3611
Epoch 08 — loss: 1.3271
Epoch 09 — loss: 1.2840
Epoch 10 — loss: 1.2390
Epoch 11 — loss: 1.2050
Epoch 12 — loss: 1.1518
Epoch 13 — loss: 1.1008
Epoch 14 — loss: 1.0601
Epoch 15 — loss: 1.0264
Epoch 16 — loss: 0.9818
Epoch 17 — loss: 0.9331
Epoch 18 — loss: 0.8953
Epoch 19 — loss: 0.8566
Epoch 20 — loss: 0.8174
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8943
Epoch 02 — loss: 1.6036
Epoch 03 — loss: 1.5255
Epoch 04 — loss: 1.4987
Epoch 05 — loss: 1.4908
Epoch 06 — loss: 1.4785
Epoch 07 — loss: 1.4794
Epoch 08 — loss: 1.4679
Epoch 09 — loss: 1.4681
Epoch 10 — loss: 1.4668
Epoch 11 — loss: 1.4625
Epoch 12 — loss: 1.4549
Epoch 13 — loss: 1.4538
Epoch 14 — loss: 1.4517
Epoch 15 — loss: 1.4572
Epoch 16 — loss: 1.4470
Epoch 17 — loss: 1.4473
Epoch 18 — loss: 1.4421
Epoch 19 — loss: 1.4429
Epoch 20 — loss: 1.4433
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.6997
Epoch 02 — loss: 1.5831
Epoch 03 — loss: 1.5283
Epoch 04 — loss: 1.4835
Epoch 05 — loss: 1.4370
Epoch 06 — loss: 1.3962
Epoch 07 — loss: 1.3611
Epoch 08 — loss: 1.3250
Epoch 09 — loss: 1.2717
Epoch 10 — loss: 1.2478
Epoch 11 — loss: 1.1919
Epoch 12 — loss: 1.1616
Epoch 13 — loss: 1.1192
Epoch 14 — loss: 1.0765
Epoch 15 — loss: 1.0267
Epoch 16 — loss: 0.9736/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 17 — loss: 0.9539
Epoch 18 — loss: 0.9101
Epoch 19 — loss: 0.8676
Epoch 20 — loss: 0.8421
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8646
Epoch 02 — loss: 1.5917
Epoch 03 — loss: 1.5328
Epoch 04 — loss: 1.5130
Epoch 05 — loss: 1.4933
Epoch 06 — loss: 1.4908
Epoch 07 — loss: 1.4793
Epoch 08 — loss: 1.4679
Epoch 09 — loss: 1.4621
Epoch 10 — loss: 1.4651
Epoch 11 — loss: 1.4594
Epoch 12 — loss: 1.4589
Epoch 13 — loss: 1.4546
Epoch 14 — loss: 1.4587
Epoch 15 — loss: 1.4527
Epoch 16 — loss: 1.4469
Epoch 17 — loss: 1.4508
Epoch 18 — loss: 1.4453
Epoch 19 — loss: 1.4414
Epoch 20 — loss: 1.4400
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7281
Epoch 02 — loss: 1.6071
Epoch 03 — loss: 1.5675
Epoch 04 — loss: 1.5480
Epoch 05 — loss: 1.5223
Epoch 06 — loss: 1.5074
Epoch 07 — loss: 1.4914
Epoch 08 — loss: 1.4724
Epoch 09 — loss: 1.4501
Epoch 10 — loss: 1.4431
Epoch 11 — loss: 1.4325
Epoch 12 — loss: 1.4054
Epoch 13 — loss: 1.3881
Epoch 14 — loss: 1.3781
Epoch 15 — loss: 1.3498
Epoch 16 — loss: 1.3307
Epoch 17 — loss: 1.3047
Epoch 18 — loss: 1.2845
Epoch 19 — loss: 1.2529
Epoch 20 — loss: 1.2371
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8197
Epoch 02 — loss: 1.6453
Epoch 03 — loss: 1.5849
Epoch 04 — loss: 1.5476
Epoch 05 — loss: 1.5358
Epoch 06 — loss: 1.5070
Epoch 07 — loss: 1.4910
Epoch 08 — loss: 1.4938
Epoch 09 — loss: 1.4811
Epoch 10 — loss: 1.4745
Epoch 11 — loss: 1.4637
Epoch 12 — loss: 1.4648
Epoch 13 — loss: 1.4563
Epoch 14 — loss: 1.4564
Epoch 15 — loss: 1.4532
Epoch 16 — loss: 1.4523
Epoch 17 — loss: 1.4558
Epoch 18 — loss: 1.4422
Epoch 19 — loss: 1.4529
Epoch 20 — loss: 1.4434
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4334
Epoch 02 — loss: 1.3093
Epoch 03 — loss: 1.2754
Stage 1: Error=0.5166, Alpha=1.7253
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7736
Epoch 02 — loss: 1.5783
Epoch 03 — loss: 1.5001
Stage 2: Error=0.6203, Alpha=1.3007
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7103
Epoch 02 — loss: 1.5723
Epoch 03 — loss: 1.4951
Stage 3: Error=0.5878, Alpha=1.4369
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8703
Epoch 02 — loss: 1.7711
Epoch 03 — loss: 1.7054
Stage 4: Error=0.7625, Alpha=0.6253
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8162
Epoch 02 — loss: 1.7748
Epoch 03 — loss: 1.7306
Stage 5: Error=0.7374, Alpha=0.7590
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4167
Epoch 02 — loss: 1.3130
Epoch 03 — loss: 1.2719
Stage 1: Error=0.4998, Alpha=1.7927
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6441
Epoch 02 — loss: 1.5246
Epoch 03 — loss: 1.4718
Stage 2: Error=0.6003, Alpha=1.3852
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7326
Epoch 02 — loss: 1.6557
Epoch 03 — loss: 1.6038
Stage 3: Error=0.6800, Alpha=1.0378
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7724
Epoch 02 — loss: 1.6589
Epoch 03 — loss: 1.5683
Stage 4: Error=0.6160, Alpha=1.3190
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7928
Epoch 02 — loss: 1.6368
Epoch 03 — loss: 1.5278
Stage 5: Error=0.5718, Alpha=1.5026
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6544
Epoch 02 — loss: 1.3791
Epoch 03 — loss: 1.2991
Stage 1: Error=0.5232, Alpha=1.6988
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7724
Epoch 02 — loss: 1.5765
Epoch 03 — loss: 1.5169
Stage 2: Error=0.6321, Alpha=1.2507
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7700
Epoch 02 — loss: 1.6449
Epoch 03 — loss: 1.6055
Stage 3: Error=0.6526, Alpha=1.1611
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8808
Epoch 02 — loss: 1.7912
Epoch 03 — loss: 1.7054
Stage 4: Error=0.7327, Alpha=0.7832
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8749
Epoch 02 — loss: 1.8228
Epoch 03 — loss: 1.7514
Stage 5: Error=0.7511, Alpha=0.6874
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.9047
Stacking meta epoch 2: loss=0.7098
Stacking meta epoch 3: loss=0.6574
Stacking meta epoch 4: loss=0.6303
Stacking meta epoch 5: loss=0.6130
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8857
Stacking meta epoch 2: loss=0.7061
Stacking meta epoch 3: loss=0.6563
Stacking meta epoch 4: loss=0.6295
Stacking meta epoch 5: loss=0.6122
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2595
Stacking meta epoch 2: loss=1.2431
Stacking meta epoch 3: loss=1.2401
Stacking meta epoch 4: loss=1.2381
Stacking meta epoch 5: loss=1.2365
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      77.19
1                             deepset  ...                      79.88
2                  set_transformer_xy  ...                      78.44
3                          deepset_xy  ...                      80.41
4            set_transformer_additive  ...                      80.65
5                 deepset_xy_additive  ...                      79.79
6                        adaboost_all  ...                      81.91
7            adaboost_set_transformer  ...                      82.92
8                    adaboost_deepset  ...                      82.34
9               stacking_ensemble_all  ...                      82.96
10  stacking_ensemble_set_transformer  ...                      83.06
11          stacking_ensemble_deepset  ...                      82.63

[12 rows x 5 columns]
------------------------------------iteration no 5------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7578
Epoch 02 — loss: 1.6219
Epoch 03 — loss: 1.5577
Epoch 04 — loss: 1.5096
Epoch 05 — loss: 1.4618
Epoch 06 — loss: 1.4139
Epoch 07 — loss: 1.3734
Epoch 08 — loss: 1.3322
Epoch 09 — loss: 1.2824
Epoch 10 — loss: 1.2421
Epoch 11 — loss: 1.1921
Epoch 12 — loss: 1.1573
Epoch 13 — loss: 1.1188
Epoch 14 — loss: 1.0613
Epoch 15 — loss: 1.0219
Epoch 16 — loss: 0.9822
Epoch 17 — loss: 0.9298
Epoch 18 — loss: 0.9014
Epoch 19 — loss: 0.8719
Epoch 20 — loss: 0.8382
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8837
Epoch 02 — loss: 1.6140
Epoch 03 — loss: 1.5360
Epoch 04 — loss: 1.5093
Epoch 05 — loss: 1.4916
Epoch 06 — loss: 1.4748
Epoch 07 — loss: 1.4694
Epoch 08 — loss: 1.4792
Epoch 09 — loss: 1.4590
Epoch 10 — loss: 1.4593
Epoch 11 — loss: 1.4532
Epoch 12 — loss: 1.4534
Epoch 13 — loss: 1.4611
Epoch 14 — loss: 1.4508
Epoch 15 — loss: 1.4651
Epoch 16 — loss: 1.4475
Epoch 17 — loss: 1.4424
Epoch 18 — loss: 1.4468
Epoch 19 — loss: 1.4431
Epoch 20 — loss: 1.4465
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7045
Epoch 02 — loss: 1.5798
Epoch 03 — loss: 1.5275
Epoch 04 — loss: 1.4920
Epoch 05 — loss: 1.4527
Epoch 06 — loss: 1.4113
Epoch 07 — loss: 1.3698
Epoch 08 — loss: 1.3316
Epoch 09 — loss: 1.2948
Epoch 10 — loss: 1.2514
Epoch 11 — loss: 1.2150
Epoch 12 — loss: 1.1814
Epoch 13 — loss: 1.1381
Epoch 14 — loss: 1.0900
Epoch 15 — loss: 1.0555
Epoch 16 — loss: 1.0112
Epoch 17 — loss: 0.9782
Epoch 18 — loss: 0.9229
Epoch 19 — loss: 0.8896
Epoch 20 — loss: 0.8547
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8638
Epoch 02 — loss: 1.5963
Epoch 03 — loss: 1.5373
Epoch 04 — loss: 1.5116
Epoch 05 — loss: 1.4978
Epoch 06 — loss: 1.4820
Epoch 07 — loss: 1.4722
Epoch 08 — loss: 1.4667
Epoch 09 — loss: 1.4653
Epoch 10 — loss: 1.4606
Epoch 11 — loss: 1.4571
Epoch 12 — loss: 1.4588
Epoch 13 — loss: 1.4526
Epoch 14 — loss: 1.4508
Epoch 15 — loss: 1.4441
Epoch 16 — loss: 1.4478
Epoch 17 — loss: 1.4427
Epoch 18 — loss: 1.4403
Epoch 19 — loss: 1.4424
Epoch 20 — loss: 1.4496
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7220
Epoch 02 — loss: 1.6326
Epoch 03 — loss: 1.5902
Epoch 04 — loss: 1.5706
Epoch 05 — loss: 1.5358
Epoch 06 — loss: 1.5343
Epoch 07 — loss: 1.4915
Epoch 08 — loss: 1.4701
Epoch 09 — loss: 1.4435
Epoch 10 — loss: 1.4081
Epoch 11 — loss: 1.3955
Epoch 12 — loss: 1.3617
Epoch 13 — loss: 1.3471
Epoch 14 — loss: 1.3069
Epoch 15 — loss: 1.2844
Epoch 16 — loss: 1.2549
Epoch 17 — loss: 1.2271
Epoch 18 — loss: 1.2017
Epoch 19 — loss: 1.1584
Epoch 20 — loss: 1.1492
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7914
Epoch 02 — loss: 1.5968
Epoch 03 — loss: 1.5574
Epoch 04 — loss: 1.5404
Epoch 05 — loss: 1.5234
Epoch 06 — loss: 1.5046
Epoch 07 — loss: 1.5002
Epoch 08 — loss: 1.4847
Epoch 09 — loss: 1.4786
Epoch 10 — loss: 1.4769
Epoch 11 — loss: 1.4734
Epoch 12 — loss: 1.4665
Epoch 13 — loss: 1.4647
Epoch 14 — loss: 1.4584
Epoch 15 — loss: 1.4670
Epoch 16 — loss: 1.4572
Epoch 17 — loss: 1.4505
Epoch 18 — loss: 1.4492
Epoch 19 — loss: 1.4443
Epoch 20 — loss: 1.4388
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4279
Epoch 02 — loss: 1.3099
Epoch 03 — loss: 1.2929
Stage 1: Error=0.5073, Alpha=1.7624
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7724
Epoch 02 — loss: 1.5883
Epoch 03 — loss: 1.5440
Stage 2: Error=0.6533, Alpha=1.1584
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6966
Epoch 02 — loss: 1.5639
Epoch 03 — loss: 1.4715
Stage 3: Error=0.5644, Alpha=1.5327
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8649
Epoch 02 — loss: 1.7989
Epoch 03 — loss: 1.7093
Stage 4: Error=0.7271, Alpha=0.8120
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8103
Epoch 02 — loss: 1.7422
Epoch 03 — loss: 1.6714
Stage 5: Error=0.6798, Alpha=1.0388
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4513
Epoch 02 — loss: 1.3344
Epoch 03 — loss: 1.2701
Stage 1: Error=0.5101, Alpha=1.7513
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6173
Epoch 02 — loss: 1.4960
Epoch 03 — loss: 1.4276
Stage 2: Error=0.5987, Alpha=1.3916
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7253
Epoch 02 — loss: 1.6453
Epoch 03 — loss: 1.6066
Stage 3: Error=0.6830, Alpha=1.0242
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7867
Epoch 02 — loss: 1.6594
Epoch 03 — loss: 1.5652
Stage 4: Error=0.6138, Alpha=1.3285
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7844
Epoch 02 — loss: 1.6242
Epoch 03 — loss: 1.5141
Stage 5: Error=0.5634, Alpha=1.5369
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6195
Epoch 02 — loss: 1.3809
Epoch 03 — loss: 1.3076
Stage 1: Error=0.5238, Alpha=1.6964
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7988
Epoch 02 — loss: 1.5919
Epoch 03 — loss: 1.5031
Stage 2: Error=0.6585, Alpha=1.1350
--- Boosting Stage 3/5 (deepset_xy_additive) ---/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7528
Epoch 02 — loss: 1.6175
Epoch 03 — loss: 1.5845
Stage 3: Error=0.6333, Alpha=1.2454
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8552
Epoch 02 — loss: 1.7498
Epoch 03 — loss: 1.6831
Stage 4: Error=0.7330, Alpha=0.7818
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8738
Epoch 02 — loss: 1.7970
Epoch 03 — loss: 1.7250
Stage 5: Error=0.7553, Alpha=0.6647
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8852
Stacking meta epoch 2: loss=0.6930
Stacking meta epoch 3: loss=0.6415
Stacking meta epoch 4: loss=0.6149
Stacking meta epoch 5: loss=0.5977
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8704
Stacking meta epoch 2: loss=0.6907
Stacking meta epoch 3: loss=0.6421
Stacking meta epoch 4: loss=0.6156
Stacking meta epoch 5: loss=0.5982
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2573
Stacking meta epoch 2: loss=1.2431
Stacking meta epoch 3: loss=1.2397
Stacking meta epoch 4: loss=1.2375
Stacking meta epoch 5: loss=1.2359
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      77.00
1                             deepset  ...                      81.09
2                  set_transformer_xy  ...                      79.40
3                          deepset_xy  ...                      81.33
4            set_transformer_additive  ...                      78.78
5                 deepset_xy_additive  ...                      81.23
6                        adaboost_all  ...                      81.23
7            adaboost_set_transformer  ...                      80.70
8                    adaboost_deepset  ...                      81.86
9               stacking_ensemble_all  ...                      83.59
10  stacking_ensemble_set_transformer  ...                      83.45
11          stacking_ensemble_deepset  ...                      82.92

[12 rows x 5 columns]
------------------------------------iteration no 6------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7195
Epoch 02 — loss: 1.5961
Epoch 03 — loss: 1.5510
Epoch 04 — loss: 1.5071
Epoch 05 — loss: 1.4682
Epoch 06 — loss: 1.4261
Epoch 07 — loss: 1.3890
Epoch 08 — loss: 1.3501
Epoch 09 — loss: 1.3127
Epoch 10 — loss: 1.2711
Epoch 11 — loss: 1.2252
Epoch 12 — loss: 1.1879
Epoch 13 — loss: 1.1420
Epoch 14 — loss: 1.1010
Epoch 15 — loss: 1.0552
Epoch 16 — loss: 1.0157
Epoch 17 — loss: 0.9622
Epoch 18 — loss: 0.9270
Epoch 19 — loss: 0.8907
Epoch 20 — loss: 0.8503
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8694
Epoch 02 — loss: 1.5889
Epoch 03 — loss: 1.5299
Epoch 04 — loss: 1.4977
Epoch 05 — loss: 1.4859
Epoch 06 — loss: 1.4821
Epoch 07 — loss: 1.4618
Epoch 08 — loss: 1.4711
Epoch 09 — loss: 1.4582
Epoch 10 — loss: 1.4611
Epoch 11 — loss: 1.4607
Epoch 12 — loss: 1.4526
Epoch 13 — loss: 1.4535
Epoch 14 — loss: 1.4508
Epoch 15 — loss: 1.4385
Epoch 16 — loss: 1.4529
Epoch 17 — loss: 1.4476
Epoch 18 — loss: 1.4499
Epoch 19 — loss: 1.4490
Epoch 20 — loss: 1.4478
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.6945
Epoch 02 — loss: 1.5733
Epoch 03 — loss: 1.5192
Epoch 04 — loss: 1.4850
Epoch 05 — loss: 1.4370
Epoch 06 — loss: 1.3966
Epoch 07 — loss: 1.3570
Epoch 08 — loss: 1.3174
Epoch 09 — loss: 1.2749
Epoch 10 — loss: 1.2415
Epoch 11 — loss: 1.1988
Epoch 12 — loss: 1.1516
Epoch 13 — loss: 1.1158
Epoch 14 — loss: 1.0681
Epoch 15 — loss: 1.0314
Epoch 16 — loss: 0.9796
Epoch 17 — loss: 0.9457
Epoch 18 — loss: 0.8958
Epoch 19 — loss: 0.8670
Epoch 20 — loss: 0.8250
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8840
Epoch 02 — loss: 1.5765
Epoch 03 — loss: 1.5255
Epoch 04 — loss: 1.5040
Epoch 05 — loss: 1.4914
Epoch 06 — loss: 1.4796
Epoch 07 — loss: 1.4752
Epoch 08 — loss: 1.4634
Epoch 09 — loss: 1.4643
Epoch 10 — loss: 1.4593
Epoch 11 — loss: 1.4604
Epoch 12 — loss: 1.4493
Epoch 13 — loss: 1.4477
Epoch 14 — loss: 1.4468
Epoch 15 — loss: 1.4473
Epoch 16 — loss: 1.4470
Epoch 17 — loss: 1.4437
Epoch 18 — loss: 1.4415
Epoch 19 — loss: 1.4391
Epoch 20 — loss: 1.4390
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.6911
Epoch 02 — loss: 1.6029
Epoch 03 — loss: 1.5667
Epoch 04 — loss: 1.5413
Epoch 05 — loss: 1.5139
Epoch 06 — loss: 1.4937
Epoch 07 — loss: 1.4758
Epoch 08 — loss: 1.4460
Epoch 09 — loss: 1.4211
Epoch 10 — loss: 1.4049
Epoch 11 — loss: 1.3783
Epoch 12 — loss: 1.3484
Epoch 13 — loss: 1.3331
Epoch 14 — loss: 1.3098
Epoch 15 — loss: 1.2785
Epoch 16 — loss: 1.2584
Epoch 17 — loss: 1.2309
Epoch 18 — loss: 1.2033
Epoch 19 — loss: 1.1781
Epoch 20 — loss: 1.1421
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7926
Epoch 02 — loss: 1.6171
Epoch 03 — loss: 1.5652
Epoch 04 — loss: 1.5367
Epoch 05 — loss: 1.5176
Epoch 06 — loss: 1.5001
Epoch 07 — loss: 1.4924
Epoch 08 — loss: 1.4852
Epoch 09 — loss: 1.4792
Epoch 10 — loss: 1.4637
Epoch 11 — loss: 1.4755
Epoch 12 — loss: 1.4578
Epoch 13 — loss: 1.4624
Epoch 14 — loss: 1.4597
Epoch 15 — loss: 1.4543
Epoch 16 — loss: 1.4513
Epoch 17 — loss: 1.4545
Epoch 18 — loss: 1.4499
Epoch 19 — loss: 1.4482
Epoch 20 — loss: 1.4473
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4373
Epoch 02 — loss: 1.3237
Epoch 03 — loss: 1.2542/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Stage 1: Error=0.5071, Alpha=1.7634
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7856
Epoch 02 — loss: 1.5805
Epoch 03 — loss: 1.5145
Stage 2: Error=0.6462, Alpha=1.1896
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6587
Epoch 02 — loss: 1.5566
Epoch 03 — loss: 1.4866
Stage 3: Error=0.5825, Alpha=1.4585
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8764
Epoch 02 — loss: 1.7509
Epoch 03 — loss: 1.6958
Stage 4: Error=0.7364, Alpha=0.7644
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8289
Epoch 02 — loss: 1.7663
Epoch 03 — loss: 1.6971
Stage 5: Error=0.6829, Alpha=1.0245
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4152
Epoch 02 — loss: 1.3003
Epoch 03 — loss: 1.2536
Stage 1: Error=0.5140, Alpha=1.7359
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6126
Epoch 02 — loss: 1.4810
Epoch 03 — loss: 1.4031
Stage 2: Error=0.5603, Alpha=1.5493
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7267
Epoch 02 — loss: 1.6397
Epoch 03 — loss: 1.6251
Stage 3: Error=0.6508, Alpha=1.1692
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7732
Epoch 02 — loss: 1.6539
Epoch 03 — loss: 1.5604
Stage 4: Error=0.6077, Alpha=1.3541
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7798
Epoch 02 — loss: 1.6491
Epoch 03 — loss: 1.5303
Stage 5: Error=0.5914, Alpha=1.4220
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6261
Epoch 02 — loss: 1.3683
Epoch 03 — loss: 1.3151
Stage 1: Error=0.5201, Alpha=1.7113
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7992
Epoch 02 — loss: 1.6078
Epoch 03 — loss: 1.5319
Stage 2: Error=0.6490, Alpha=1.1771
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7858
Epoch 02 — loss: 1.6598
Epoch 03 — loss: 1.6077
Stage 3: Error=0.6610, Alpha=1.1241
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8596
Epoch 02 — loss: 1.7828
Epoch 03 — loss: 1.7069
Stage 4: Error=0.7387, Alpha=0.7526
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8764
Epoch 02 — loss: 1.8165
Epoch 03 — loss: 1.7464
Stage 5: Error=0.7632, Alpha=0.6215
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8841
Stacking meta epoch 2: loss=0.6826
Stacking meta epoch 3: loss=0.6312
Stacking meta epoch 4: loss=0.6054
Stacking meta epoch 5: loss=0.5890
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8707
Stacking meta epoch 2: loss=0.6781
Stacking meta epoch 3: loss=0.6295
Stacking meta epoch 4: loss=0.6038
Stacking meta epoch 5: loss=0.5871
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2562
Stacking meta epoch 2: loss=1.2399
Stacking meta epoch 3: loss=1.2368
Stacking meta epoch 4: loss=1.2348
Stacking meta epoch 5: loss=1.2334
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      79.74
1                             deepset  ...                      80.51
2                  set_transformer_xy  ...                      79.26
3                          deepset_xy  ...                      81.42
4            set_transformer_additive  ...                      78.59
5                 deepset_xy_additive  ...                      77.24
6                        adaboost_all  ...                      82.92
7            adaboost_set_transformer  ...                      80.85
8                    adaboost_deepset  ...                      82.58
9               stacking_ensemble_all  ...                      84.50
10  stacking_ensemble_set_transformer  ...                      83.83
11          stacking_ensemble_deepset  ...                      82.92

[12 rows x 5 columns]
------------------------------------iteration no 7------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7354
Epoch 02 — loss: 1.5907
Epoch 03 — loss: 1.5318
Epoch 04 — loss: 1.4816
Epoch 05 — loss: 1.4385
Epoch 06 — loss: 1.4020
Epoch 07 — loss: 1.3648
Epoch 08 — loss: 1.3190
Epoch 09 — loss: 1.2856
Epoch 10 — loss: 1.2487
Epoch 11 — loss: 1.1992
Epoch 12 — loss: 1.1648
Epoch 13 — loss: 1.1146
Epoch 14 — loss: 1.0664
Epoch 15 — loss: 1.0310
Epoch 16 — loss: 0.9818
Epoch 17 — loss: 0.9362
Epoch 18 — loss: 0.9037
Epoch 19 — loss: 0.8525
Epoch 20 — loss: 0.8281
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8876
Epoch 02 — loss: 1.6096
Epoch 03 — loss: 1.5326
Epoch 04 — loss: 1.5049
Epoch 05 — loss: 1.4944
Epoch 06 — loss: 1.4850
Epoch 07 — loss: 1.4716
Epoch 08 — loss: 1.4699
Epoch 09 — loss: 1.4567
Epoch 10 — loss: 1.4560
Epoch 11 — loss: 1.4587
Epoch 12 — loss: 1.4493
Epoch 13 — loss: 1.4519
Epoch 14 — loss: 1.4548
Epoch 15 — loss: 1.4501
Epoch 16 — loss: 1.4525
Epoch 17 — loss: 1.4466
Epoch 18 — loss: 1.4472
Epoch 19 — loss: 1.4433
Epoch 20 — loss: 1.4386
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7201
Epoch 02 — loss: 1.5752
Epoch 03 — loss: 1.5209
Epoch 04 — loss: 1.4821
Epoch 05 — loss: 1.4384
Epoch 06 — loss: 1.4071
Epoch 07 — loss: 1.3599
Epoch 08 — loss: 1.3162
Epoch 09 — loss: 1.2735
Epoch 10 — loss: 1.2404
Epoch 11 — loss: 1.2030
Epoch 12 — loss: 1.1557
Epoch 13 — loss: 1.1104
Epoch 14 — loss: 1.0744
Epoch 15 — loss: 1.0268
Epoch 16 — loss: 0.9929/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 17 — loss: 0.9369
Epoch 18 — loss: 0.9160
Epoch 19 — loss: 0.8683
Epoch 20 — loss: 0.8333
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8915
Epoch 02 — loss: 1.6001
Epoch 03 — loss: 1.5306
Epoch 04 — loss: 1.5023
Epoch 05 — loss: 1.4853
Epoch 06 — loss: 1.4830
Epoch 07 — loss: 1.4655
Epoch 08 — loss: 1.4636
Epoch 09 — loss: 1.4591
Epoch 10 — loss: 1.4664
Epoch 11 — loss: 1.4552
Epoch 12 — loss: 1.4500
Epoch 13 — loss: 1.4523
Epoch 14 — loss: 1.4482
Epoch 15 — loss: 1.4452
Epoch 16 — loss: 1.4453
Epoch 17 — loss: 1.4433
Epoch 18 — loss: 1.4382
Epoch 19 — loss: 1.4447
Epoch 20 — loss: 1.4412
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7298
Epoch 02 — loss: 1.6246
Epoch 03 — loss: 1.5935
Epoch 04 — loss: 1.5612
Epoch 05 — loss: 1.5434
Epoch 06 — loss: 1.5202
Epoch 07 — loss: 1.4936
Epoch 08 — loss: 1.4684
Epoch 09 — loss: 1.4525
Epoch 10 — loss: 1.4239
Epoch 11 — loss: 1.4113
Epoch 12 — loss: 1.3860
Epoch 13 — loss: 1.3610
Epoch 14 — loss: 1.3406
Epoch 15 — loss: 1.3165
Epoch 16 — loss: 1.2874
Epoch 17 — loss: 1.2722
Epoch 18 — loss: 1.2272
Epoch 19 — loss: 1.2065
Epoch 20 — loss: 1.1806
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7932
Epoch 02 — loss: 1.6251
Epoch 03 — loss: 1.5860
Epoch 04 — loss: 1.5472
Epoch 05 — loss: 1.5343
Epoch 06 — loss: 1.5075
Epoch 07 — loss: 1.5069
Epoch 08 — loss: 1.4968
Epoch 09 — loss: 1.4842
Epoch 10 — loss: 1.4849
Epoch 11 — loss: 1.4780
Epoch 12 — loss: 1.4653
Epoch 13 — loss: 1.4621
Epoch 14 — loss: 1.4676
Epoch 15 — loss: 1.4538
Epoch 16 — loss: 1.4494
Epoch 17 — loss: 1.4609
Epoch 18 — loss: 1.4431
Epoch 19 — loss: 1.4491
Epoch 20 — loss: 1.4507
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4161
Epoch 02 — loss: 1.2949
Epoch 03 — loss: 1.2575
Stage 1: Error=0.5132, Alpha=1.7388
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7908
Epoch 02 — loss: 1.5911
Epoch 03 — loss: 1.5351
Stage 2: Error=0.6756, Alpha=1.0582
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6609
Epoch 02 — loss: 1.5307
Epoch 03 — loss: 1.4441
Stage 3: Error=0.5852, Alpha=1.4478
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8274
Epoch 02 — loss: 1.7568
Epoch 03 — loss: 1.6793
Stage 4: Error=0.7509, Alpha=0.6882
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7691
Epoch 02 — loss: 1.7192
Epoch 03 — loss: 1.6781
Stage 5: Error=0.6855, Alpha=1.0124
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4230
Epoch 02 — loss: 1.3175
Epoch 03 — loss: 1.2799
Stage 1: Error=0.5267, Alpha=1.6848
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.5869
Epoch 02 — loss: 1.4819
Epoch 03 — loss: 1.4293
Stage 2: Error=0.5789, Alpha=1.4736
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7029
Epoch 02 — loss: 1.6427
Epoch 03 — loss: 1.5971
Stage 3: Error=0.6656, Alpha=1.1034
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7590
Epoch 02 — loss: 1.6596
Epoch 03 — loss: 1.5586
Stage 4: Error=0.6201, Alpha=1.3016
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7678
Epoch 02 — loss: 1.6499
Epoch 03 — loss: 1.5255
Stage 5: Error=0.5823, Alpha=1.4595
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6445
Epoch 02 — loss: 1.3923
Epoch 03 — loss: 1.2979
Stage 1: Error=0.5303, Alpha=1.6703
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7887
Epoch 02 — loss: 1.5782
Epoch 03 — loss: 1.5130
Stage 2: Error=0.6287, Alpha=1.2653
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7828
Epoch 02 — loss: 1.6640
Epoch 03 — loss: 1.6278
Stage 3: Error=0.6479, Alpha=1.1821
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8661
Epoch 02 — loss: 1.7944
Epoch 03 — loss: 1.7217
Stage 4: Error=0.7440, Alpha=0.7247
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8960
Epoch 02 — loss: 1.8498
Epoch 03 — loss: 1.7655
Stage 5: Error=0.7826, Alpha=0.5109
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8868
Stacking meta epoch 2: loss=0.6924
Stacking meta epoch 3: loss=0.6408
Stacking meta epoch 4: loss=0.6143
Stacking meta epoch 5: loss=0.5971
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8716
Stacking meta epoch 2: loss=0.6917
Stacking meta epoch 3: loss=0.6424
Stacking meta epoch 4: loss=0.6158
Stacking meta epoch 5: loss=0.5983
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2566
Stacking meta epoch 2: loss=1.2436
Stacking meta epoch 3: loss=1.2407
Stacking meta epoch 4: loss=1.2389
Stacking meta epoch 5: loss=1.2375
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      78.39
1                             deepset  ...                      80.85
2                  set_transformer_xy  ...                      80.46
3                          deepset_xy  ...                      81.47
4            set_transformer_additive  ...                      79.84
5                 deepset_xy_additive  ...                      79.36
6                        adaboost_all  ...                      80.75
7            adaboost_set_transformer  ...                      81.38
8                    adaboost_deepset  ...                      81.23
9               stacking_ensemble_all  ...                      83.16
10  stacking_ensemble_set_transformer  ...                      83.01
11          stacking_ensemble_deepset  ...                      82.72

[12 rows x 5 columns]
------------------------------------iteration no 8------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7005
Epoch 02 — loss: 1.5722
Epoch 03 — loss: 1.5272
Epoch 04 — loss: 1.4914
Epoch 05 — loss: 1.4472
Epoch 06 — loss: 1.4063
Epoch 07 — loss: 1.3715
Epoch 08 — loss: 1.3261
Epoch 09 — loss: 1.2963
Epoch 10 — loss: 1.2460
Epoch 11 — loss: 1.2085
Epoch 12 — loss: 1.1601
Epoch 13 — loss: 1.1176
Epoch 14 — loss: 1.0610
Epoch 15 — loss: 1.0342
Epoch 16 — loss: 0.9948
Epoch 17 — loss: 0.9426
Epoch 18 — loss: 0.9061
Epoch 19 — loss: 0.8680
Epoch 20 — loss: 0.8242
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.9027
Epoch 02 — loss: 1.6084
Epoch 03 — loss: 1.5268
Epoch 04 — loss: 1.5016
Epoch 05 — loss: 1.4898
Epoch 06 — loss: 1.4785
Epoch 07 — loss: 1.4771
Epoch 08 — loss: 1.4700
Epoch 09 — loss: 1.4720
Epoch 10 — loss: 1.4623
Epoch 11 — loss: 1.4642
Epoch 12 — loss: 1.4644
Epoch 13 — loss: 1.4517
Epoch 14 — loss: 1.4540
Epoch 15 — loss: 1.4437
Epoch 16 — loss: 1.4466
Epoch 17 — loss: 1.4480
Epoch 18 — loss: 1.4479
Epoch 19 — loss: 1.4464
Epoch 20 — loss: 1.4357
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7195
Epoch 02 — loss: 1.5789
Epoch 03 — loss: 1.5235
Epoch 04 — loss: 1.4782
Epoch 05 — loss: 1.4360
Epoch 06 — loss: 1.3954
Epoch 07 — loss: 1.3513
Epoch 08 — loss: 1.3115
Epoch 09 — loss: 1.2698
Epoch 10 — loss: 1.2324
Epoch 11 — loss: 1.2014
Epoch 12 — loss: 1.1449
Epoch 13 — loss: 1.0998
Epoch 14 — loss: 1.0659
Epoch 15 — loss: 1.0236
Epoch 16 — loss: 0.9759
Epoch 17 — loss: 0.9407
Epoch 18 — loss: 0.9059
Epoch 19 — loss: 0.8673
Epoch 20 — loss: 0.8321
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8531
Epoch 02 — loss: 1.5888
Epoch 03 — loss: 1.5410
Epoch 04 — loss: 1.5137
Epoch 05 — loss: 1.4967
Epoch 06 — loss: 1.4779
Epoch 07 — loss: 1.4786
Epoch 08 — loss: 1.4654
Epoch 09 — loss: 1.4665
Epoch 10 — loss: 1.4611
Epoch 11 — loss: 1.4651
Epoch 12 — loss: 1.4473
Epoch 13 — loss: 1.4509
Epoch 14 — loss: 1.4398
Epoch 15 — loss: 1.4473
Epoch 16 — loss: 1.4471
Epoch 17 — loss: 1.4390
Epoch 18 — loss: 1.4408
Epoch 19 — loss: 1.4363
Epoch 20 — loss: 1.4391
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.6900
Epoch 02 — loss: 1.5959
Epoch 03 — loss: 1.5737
Epoch 04 — loss: 1.5429
Epoch 05 — loss: 1.5252
Epoch 06 — loss: 1.5039
Epoch 07 — loss: 1.4953
Epoch 08 — loss: 1.4669
Epoch 09 — loss: 1.4594
Epoch 10 — loss: 1.4335
Epoch 11 — loss: 1.4028
Epoch 12 — loss: 1.3860
Epoch 13 — loss: 1.3485
Epoch 14 — loss: 1.3402
Epoch 15 — loss: 1.3025
Epoch 16 — loss: 1.2692
Epoch 17 — loss: 1.2490
Epoch 18 — loss: 1.2117
Epoch 19 — loss: 1.1752
Epoch 20 — loss: 1.1524
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7887
Epoch 02 — loss: 1.6218
Epoch 03 — loss: 1.5746
Epoch 04 — loss: 1.5472
Epoch 05 — loss: 1.5305
Epoch 06 — loss: 1.5138
Epoch 07 — loss: 1.5073
Epoch 08 — loss: 1.4975
Epoch 09 — loss: 1.4834
Epoch 10 — loss: 1.4817
Epoch 11 — loss: 1.4755
Epoch 12 — loss: 1.4733
Epoch 13 — loss: 1.4678
Epoch 14 — loss: 1.4659
Epoch 15 — loss: 1.4603
Epoch 16 — loss: 1.4561
Epoch 17 — loss: 1.4543
Epoch 18 — loss: 1.4436
Epoch 19 — loss: 1.4483
Epoch 20 — loss: 1.4464
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.3880
Epoch 02 — loss: 1.3140
Epoch 03 — loss: 1.2759
Stage 1: Error=0.5138, Alpha=1.7364
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7775
Epoch 02 — loss: 1.5852
Epoch 03 — loss: 1.5241
Stage 2: Error=0.6360, Alpha=1.2339
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6976
Epoch 02 — loss: 1.5729
Epoch 03 — loss: 1.4771
Stage 3: Error=0.5739, Alpha=1.4941
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8563
Epoch 02 — loss: 1.7554
Epoch 03 — loss: 1.7011
Stage 4: Error=0.7283, Alpha=0.8060
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8180
Epoch 02 — loss: 1.7279
Epoch 03 — loss: 1.6776
Stage 5: Error=0.6673, Alpha=1.0956
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4584
Epoch 02 — loss: 1.3321
Epoch 03 — loss: 1.2842
Stage 1: Error=0.4963, Alpha=1.8067
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6295
Epoch 02 — loss: 1.5056
Epoch 03 — loss: 1.4372
Stage 2: Error=0.5744, Alpha=1.4919
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7342
Epoch 02 — loss: 1.6750
Epoch 03 — loss: 1.6328
Stage 3: Error=0.6814, Alpha=1.0314
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8170
Epoch 02 — loss: 1.6856
Epoch 03 — loss: 1.5752
Stage 4: Error=0.6195, Alpha=1.3042
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7548
Epoch 02 — loss: 1.6145
Epoch 03 — loss: 1.4830
Stage 5: Error=0.5587, Alpha=1.5558
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6096
Epoch 02 — loss: 1.3456
Epoch 03 — loss: 1.2974
Stage 1: Error=0.5165, Alpha=1.7258
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7898
Epoch 02 — loss: 1.5922
Epoch 03 — loss: 1.5144
Stage 2: Error=0.6501, Alpha=1.1725
--- Boosting Stage 3/5 (deepset_xy_additive) ---/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7772
Epoch 02 — loss: 1.6393
Epoch 03 — loss: 1.6037
Stage 3: Error=0.6502, Alpha=1.1717
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8849
Epoch 02 — loss: 1.7834
Epoch 03 — loss: 1.6853
Stage 4: Error=0.7182, Alpha=0.8561
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8817
Epoch 02 — loss: 1.8020
Epoch 03 — loss: 1.7299
Stage 5: Error=0.7583, Alpha=0.6482
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8812
Stacking meta epoch 2: loss=0.6897
Stacking meta epoch 3: loss=0.6403
Stacking meta epoch 4: loss=0.6150
Stacking meta epoch 5: loss=0.5988
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8708
Stacking meta epoch 2: loss=0.6897
Stacking meta epoch 3: loss=0.6416
Stacking meta epoch 4: loss=0.6160
Stacking meta epoch 5: loss=0.5992
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2633
Stacking meta epoch 2: loss=1.2433
Stacking meta epoch 3: loss=1.2406
Stacking meta epoch 4: loss=1.2387
Stacking meta epoch 5: loss=1.2373
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      78.59
1                             deepset  ...                      80.75
2                  set_transformer_xy  ...                      79.79
3                          deepset_xy  ...                      82.05
4            set_transformer_additive  ...                      79.11
5                 deepset_xy_additive  ...                      82.48
6                        adaboost_all  ...                      81.33
7            adaboost_set_transformer  ...                      81.67
8                    adaboost_deepset  ...                      82.39
9               stacking_ensemble_all  ...                      83.21
10  stacking_ensemble_set_transformer  ...                      83.01
11          stacking_ensemble_deepset  ...                      83.01

[12 rows x 5 columns]
------------------------------------iteration no 9------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7522
Epoch 02 — loss: 1.6177
Epoch 03 — loss: 1.5673
Epoch 04 — loss: 1.5164
Epoch 05 — loss: 1.4648
Epoch 06 — loss: 1.4197
Epoch 07 — loss: 1.3755
Epoch 08 — loss: 1.3361
Epoch 09 — loss: 1.2898
Epoch 10 — loss: 1.2527
Epoch 11 — loss: 1.2041
Epoch 12 — loss: 1.1672
Epoch 13 — loss: 1.1264
Epoch 14 — loss: 1.0886
Epoch 15 — loss: 1.0435
Epoch 16 — loss: 0.9953
Epoch 17 — loss: 0.9496
Epoch 18 — loss: 0.9181
Epoch 19 — loss: 0.8801
Epoch 20 — loss: 0.8456
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8913
Epoch 02 — loss: 1.6156
Epoch 03 — loss: 1.5315
Epoch 04 — loss: 1.5103
Epoch 05 — loss: 1.4916
Epoch 06 — loss: 1.4810
Epoch 07 — loss: 1.4771
Epoch 08 — loss: 1.4769
Epoch 09 — loss: 1.4661
Epoch 10 — loss: 1.4575
Epoch 11 — loss: 1.4572
Epoch 12 — loss: 1.4524
Epoch 13 — loss: 1.4469
Epoch 14 — loss: 1.4485
Epoch 15 — loss: 1.4510
Epoch 16 — loss: 1.4458
Epoch 17 — loss: 1.4442
Epoch 18 — loss: 1.4444
Epoch 19 — loss: 1.4379
Epoch 20 — loss: 1.4402
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.6932
Epoch 02 — loss: 1.5632
Epoch 03 — loss: 1.5125
Epoch 04 — loss: 1.4685
Epoch 05 — loss: 1.4286
Epoch 06 — loss: 1.3853
Epoch 07 — loss: 1.3535
Epoch 08 — loss: 1.3096
Epoch 09 — loss: 1.2611
Epoch 10 — loss: 1.2207
Epoch 11 — loss: 1.1759
Epoch 12 — loss: 1.1404
Epoch 13 — loss: 1.0824
Epoch 14 — loss: 1.0505
Epoch 15 — loss: 1.0088
Epoch 16 — loss: 0.9707
Epoch 17 — loss: 0.9229
Epoch 18 — loss: 0.8947
Epoch 19 — loss: 0.8684
Epoch 20 — loss: 0.8171
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8835
Epoch 02 — loss: 1.5869
Epoch 03 — loss: 1.5244
Epoch 04 — loss: 1.4943
Epoch 05 — loss: 1.4867
Epoch 06 — loss: 1.4732
Epoch 07 — loss: 1.4768
Epoch 08 — loss: 1.4628
Epoch 09 — loss: 1.4582
Epoch 10 — loss: 1.4606
Epoch 11 — loss: 1.4577
Epoch 12 — loss: 1.4451
Epoch 13 — loss: 1.4510
Epoch 14 — loss: 1.4438
Epoch 15 — loss: 1.4425
Epoch 16 — loss: 1.4419
Epoch 17 — loss: 1.4394
Epoch 18 — loss: 1.4434
Epoch 19 — loss: 1.4354
Epoch 20 — loss: 1.4363
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7145
Epoch 02 — loss: 1.6185
Epoch 03 — loss: 1.5837
Epoch 04 — loss: 1.5630
Epoch 05 — loss: 1.5435
Epoch 06 — loss: 1.5159
Epoch 07 — loss: 1.4962
Epoch 08 — loss: 1.4730
Epoch 09 — loss: 1.4566
Epoch 10 — loss: 1.4286
Epoch 11 — loss: 1.4003
Epoch 12 — loss: 1.3759
Epoch 13 — loss: 1.3515
Epoch 14 — loss: 1.3205
Epoch 15 — loss: 1.2879
Epoch 16 — loss: 1.2652
Epoch 17 — loss: 1.2334
Epoch 18 — loss: 1.2045
Epoch 19 — loss: 1.1760
Epoch 20 — loss: 1.1486
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8018
Epoch 02 — loss: 1.6277
Epoch 03 — loss: 1.5749
Epoch 04 — loss: 1.5478
Epoch 05 — loss: 1.5291
Epoch 06 — loss: 1.5092
Epoch 07 — loss: 1.4958
Epoch 08 — loss: 1.4919
Epoch 09 — loss: 1.4802
Epoch 10 — loss: 1.4752
Epoch 11 — loss: 1.4734
Epoch 12 — loss: 1.4678
Epoch 13 — loss: 1.4694
Epoch 14 — loss: 1.4624
Epoch 15 — loss: 1.4582
Epoch 16 — loss: 1.4546
Epoch 17 — loss: 1.4484
Epoch 18 — loss: 1.4512
Epoch 19 — loss: 1.4417
Epoch 20 — loss: 1.4382
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4452
Epoch 02 — loss: 1.3187
Epoch 03 — loss: 1.2726/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Stage 1: Error=0.5059, Alpha=1.7682
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7659
Epoch 02 — loss: 1.6045
Epoch 03 — loss: 1.5489
Stage 2: Error=0.6433, Alpha=1.2021
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6493
Epoch 02 — loss: 1.5362
Epoch 03 — loss: 1.4528
Stage 3: Error=0.5607, Alpha=1.5476
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8710
Epoch 02 — loss: 1.7713
Epoch 03 — loss: 1.6957
Stage 4: Error=0.7341, Alpha=0.7761
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8183
Epoch 02 — loss: 1.7489
Epoch 03 — loss: 1.7162
Stage 5: Error=0.7165, Alpha=0.8646
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4386
Epoch 02 — loss: 1.3214
Epoch 03 — loss: 1.2784
Stage 1: Error=0.4986, Alpha=1.7975
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6428
Epoch 02 — loss: 1.5171
Epoch 03 — loss: 1.4509
Stage 2: Error=0.5923, Alpha=1.4182
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7210
Epoch 02 — loss: 1.6257
Epoch 03 — loss: 1.6080
Stage 3: Error=0.6710, Alpha=1.0790
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7731
Epoch 02 — loss: 1.6461
Epoch 03 — loss: 1.5672
Stage 4: Error=0.6201, Alpha=1.3020
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7846
Epoch 02 — loss: 1.6457
Epoch 03 — loss: 1.5113
Stage 5: Error=0.6085, Alpha=1.3506
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6189
Epoch 02 — loss: 1.3471
Epoch 03 — loss: 1.2925
Stage 1: Error=0.5315, Alpha=1.6655
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7926
Epoch 02 — loss: 1.5783
Epoch 03 — loss: 1.5160
Stage 2: Error=0.6336, Alpha=1.2440
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7937
Epoch 02 — loss: 1.6763
Epoch 03 — loss: 1.6339
Stage 3: Error=0.6722, Alpha=1.0737
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8717
Epoch 02 — loss: 1.8010
Epoch 03 — loss: 1.7299
Stage 4: Error=0.7842, Alpha=0.5012
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8717
Epoch 02 — loss: 1.8093
Epoch 03 — loss: 1.7417
Stage 5: Error=0.7792, Alpha=0.5306
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8859
Stacking meta epoch 2: loss=0.7077
Stacking meta epoch 3: loss=0.6612
Stacking meta epoch 4: loss=0.6367
Stacking meta epoch 5: loss=0.6206
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8663
Stacking meta epoch 2: loss=0.7036
Stacking meta epoch 3: loss=0.6587
Stacking meta epoch 4: loss=0.6334
Stacking meta epoch 5: loss=0.6164
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2594
Stacking meta epoch 2: loss=1.2430
Stacking meta epoch 3: loss=1.2401
Stacking meta epoch 4: loss=1.2382
Stacking meta epoch 5: loss=1.2367
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      80.90
1                             deepset  ...                      83.01
2                  set_transformer_xy  ...                      79.93
3                          deepset_xy  ...                      80.75
4            set_transformer_additive  ...                      81.23
5                 deepset_xy_additive  ...                      81.18
6                        adaboost_all  ...                      80.94
7            adaboost_set_transformer  ...                      82.10
8                    adaboost_deepset  ...                      81.38
9               stacking_ensemble_all  ...                      82.96
10  stacking_ensemble_set_transformer  ...                      84.02
11          stacking_ensemble_deepset  ...                      82.63

[12 rows x 5 columns]
------------------------------------iteration no 10------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7196
Epoch 02 — loss: 1.5787
Epoch 03 — loss: 1.5351
Epoch 04 — loss: 1.4864
Epoch 05 — loss: 1.4462
Epoch 06 — loss: 1.4066
Epoch 07 — loss: 1.3591
Epoch 08 — loss: 1.3275
Epoch 09 — loss: 1.2899
Epoch 10 — loss: 1.2371
Epoch 11 — loss: 1.2021
Epoch 12 — loss: 1.1584
Epoch 13 — loss: 1.1069
Epoch 14 — loss: 1.0743
Epoch 15 — loss: 1.0340
Epoch 16 — loss: 0.9867
Epoch 17 — loss: 0.9634
Epoch 18 — loss: 0.9045
Epoch 19 — loss: 0.8770
Epoch 20 — loss: 0.8555
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8759
Epoch 02 — loss: 1.5940
Epoch 03 — loss: 1.5267
Epoch 04 — loss: 1.5041
Epoch 05 — loss: 1.4923
Epoch 06 — loss: 1.4837
Epoch 07 — loss: 1.4768
Epoch 08 — loss: 1.4744
Epoch 09 — loss: 1.4643
Epoch 10 — loss: 1.4590
Epoch 11 — loss: 1.4580
Epoch 12 — loss: 1.4543
Epoch 13 — loss: 1.4533
Epoch 14 — loss: 1.4406
Epoch 15 — loss: 1.4450
Epoch 16 — loss: 1.4429
Epoch 17 — loss: 1.4487
Epoch 18 — loss: 1.4399
Epoch 19 — loss: 1.4457
Epoch 20 — loss: 1.4381
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7236
Epoch 02 — loss: 1.5696
Epoch 03 — loss: 1.5134
Epoch 04 — loss: 1.4721
Epoch 05 — loss: 1.4256
Epoch 06 — loss: 1.3875
Epoch 07 — loss: 1.3442
Epoch 08 — loss: 1.3012
Epoch 09 — loss: 1.2718
Epoch 10 — loss: 1.2232
Epoch 11 — loss: 1.1856
Epoch 12 — loss: 1.1433
Epoch 13 — loss: 1.1023
Epoch 14 — loss: 1.0496
Epoch 15 — loss: 1.0250
Epoch 16 — loss: 0.9688/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 17 — loss: 0.9179
Epoch 18 — loss: 0.8926
Epoch 19 — loss: 0.8527
Epoch 20 — loss: 0.8053
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8524
Epoch 02 — loss: 1.5801
Epoch 03 — loss: 1.5322
Epoch 04 — loss: 1.5043
Epoch 05 — loss: 1.4882
Epoch 06 — loss: 1.4746
Epoch 07 — loss: 1.4736
Epoch 08 — loss: 1.4770
Epoch 09 — loss: 1.4594
Epoch 10 — loss: 1.4602
Epoch 11 — loss: 1.4507
Epoch 12 — loss: 1.4453
Epoch 13 — loss: 1.4588
Epoch 14 — loss: 1.4474
Epoch 15 — loss: 1.4417
Epoch 16 — loss: 1.4450
Epoch 17 — loss: 1.4372
Epoch 18 — loss: 1.4447
Epoch 19 — loss: 1.4364
Epoch 20 — loss: 1.4401
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7106
Epoch 02 — loss: 1.6009
Epoch 03 — loss: 1.5607
Epoch 04 — loss: 1.5347
Epoch 05 — loss: 1.5119
Epoch 06 — loss: 1.4854
Epoch 07 — loss: 1.4569
Epoch 08 — loss: 1.4339
Epoch 09 — loss: 1.4119
Epoch 10 — loss: 1.3799
Epoch 11 — loss: 1.3575
Epoch 12 — loss: 1.3309
Epoch 13 — loss: 1.3026
Epoch 14 — loss: 1.2674
Epoch 15 — loss: 1.2558
Epoch 16 — loss: 1.2210
Epoch 17 — loss: 1.1902
Epoch 18 — loss: 1.1610
Epoch 19 — loss: 1.1154
Epoch 20 — loss: 1.1052
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8017
Epoch 02 — loss: 1.6267
Epoch 03 — loss: 1.5877
Epoch 04 — loss: 1.5558
Epoch 05 — loss: 1.5346
Epoch 06 — loss: 1.5171
Epoch 07 — loss: 1.5116
Epoch 08 — loss: 1.5022
Epoch 09 — loss: 1.4942
Epoch 10 — loss: 1.4854
Epoch 11 — loss: 1.4795
Epoch 12 — loss: 1.4832
Epoch 13 — loss: 1.4729
Epoch 14 — loss: 1.4721
Epoch 15 — loss: 1.4668
Epoch 16 — loss: 1.4656
Epoch 17 — loss: 1.4601
Epoch 18 — loss: 1.4597
Epoch 19 — loss: 1.4563
Epoch 20 — loss: 1.4544
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4045
Epoch 02 — loss: 1.3141
Epoch 03 — loss: 1.2497
Stage 1: Error=0.5045, Alpha=1.7739
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7740
Epoch 02 — loss: 1.6108
Epoch 03 — loss: 1.5278
Stage 2: Error=0.6746, Alpha=1.0627
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6814
Epoch 02 — loss: 1.5613
Epoch 03 — loss: 1.4739
Stage 3: Error=0.5828, Alpha=1.4573
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8569
Epoch 02 — loss: 1.7549
Epoch 03 — loss: 1.6942
Stage 4: Error=0.7165, Alpha=0.8648
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8306
Epoch 02 — loss: 1.7770
Epoch 03 — loss: 1.7277
Stage 5: Error=0.7042, Alpha=0.9245
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4387
Epoch 02 — loss: 1.3234
Epoch 03 — loss: 1.2811
Stage 1: Error=0.4999, Alpha=1.7922
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6498
Epoch 02 — loss: 1.5237
Epoch 03 — loss: 1.4412
Stage 2: Error=0.5867, Alpha=1.4415
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7114
Epoch 02 — loss: 1.6351
Epoch 03 — loss: 1.6074
Stage 3: Error=0.6780, Alpha=1.0472
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7320
Epoch 02 — loss: 1.6268
Epoch 03 — loss: 1.5395
Stage 4: Error=0.6086, Alpha=1.3505
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7603
Epoch 02 — loss: 1.6324
Epoch 03 — loss: 1.5000
Stage 5: Error=0.5718, Alpha=1.5026
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6110
Epoch 02 — loss: 1.3504
Epoch 03 — loss: 1.2802
Stage 1: Error=0.5295, Alpha=1.6737
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7914
Epoch 02 — loss: 1.5937
Epoch 03 — loss: 1.5173
Stage 2: Error=0.6055, Alpha=1.3632
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8036
Epoch 02 — loss: 1.6790
Epoch 03 — loss: 1.6386
Stage 3: Error=0.6981, Alpha=0.9537
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8693
Epoch 02 — loss: 1.7635
Epoch 03 — loss: 1.6992
Stage 4: Error=0.7198, Alpha=0.8481
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8722
Epoch 02 — loss: 1.7735
Epoch 03 — loss: 1.7265
Stage 5: Error=0.7230, Alpha=0.8322
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8879
Stacking meta epoch 2: loss=0.6879
Stacking meta epoch 3: loss=0.6361
Stacking meta epoch 4: loss=0.6095
Stacking meta epoch 5: loss=0.5924
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8829
Stacking meta epoch 2: loss=0.6862
Stacking meta epoch 3: loss=0.6345
Stacking meta epoch 4: loss=0.6069
Stacking meta epoch 5: loss=0.5889
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2591
Stacking meta epoch 2: loss=1.2444
Stacking meta epoch 3: loss=1.2415
Stacking meta epoch 4: loss=1.2397
Stacking meta epoch 5: loss=1.2383
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      78.73
1                             deepset  ...                      81.52
2                  set_transformer_xy  ...                      77.33
3                          deepset_xy  ...                      80.65
4            set_transformer_additive  ...                      79.98
5                 deepset_xy_additive  ...                      80.56
6                        adaboost_all  ...                      81.95
7            adaboost_set_transformer  ...                      81.42
8                    adaboost_deepset  ...                      81.52
9               stacking_ensemble_all  ...                      83.06
10  stacking_ensemble_set_transformer  ...                      83.25
11          stacking_ensemble_deepset  ...                      82.96

[12 rows x 5 columns]
------------------------------------iteration no 11------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7285
Epoch 02 — loss: 1.6083
Epoch 03 — loss: 1.5538
Epoch 04 — loss: 1.5161
Epoch 05 — loss: 1.4638
Epoch 06 — loss: 1.4291
Epoch 07 — loss: 1.3738
Epoch 08 — loss: 1.3468
Epoch 09 — loss: 1.2989
Epoch 10 — loss: 1.2470
Epoch 11 — loss: 1.2073
Epoch 12 — loss: 1.1607
Epoch 13 — loss: 1.1075
Epoch 14 — loss: 1.0751
Epoch 15 — loss: 1.0264
Epoch 16 — loss: 0.9819
Epoch 17 — loss: 0.9414
Epoch 18 — loss: 0.8952
Epoch 19 — loss: 0.8723
Epoch 20 — loss: 0.8169
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8662
Epoch 02 — loss: 1.5901
Epoch 03 — loss: 1.5250
Epoch 04 — loss: 1.5059
Epoch 05 — loss: 1.4907
Epoch 06 — loss: 1.4784
Epoch 07 — loss: 1.4693
Epoch 08 — loss: 1.4645
Epoch 09 — loss: 1.4631
Epoch 10 — loss: 1.4604
Epoch 11 — loss: 1.4610
Epoch 12 — loss: 1.4483
Epoch 13 — loss: 1.4510
Epoch 14 — loss: 1.4446
Epoch 15 — loss: 1.4505
Epoch 16 — loss: 1.4488
Epoch 17 — loss: 1.4448
Epoch 18 — loss: 1.4399
Epoch 19 — loss: 1.4398
Epoch 20 — loss: 1.4413
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7347
Epoch 02 — loss: 1.5835
Epoch 03 — loss: 1.5237
Epoch 04 — loss: 1.4844
Epoch 05 — loss: 1.4408
Epoch 06 — loss: 1.4070
Epoch 07 — loss: 1.3667
Epoch 08 — loss: 1.3322
Epoch 09 — loss: 1.2936
Epoch 10 — loss: 1.2501
Epoch 11 — loss: 1.2158
Epoch 12 — loss: 1.1847
Epoch 13 — loss: 1.1488
Epoch 14 — loss: 1.1025
Epoch 15 — loss: 1.0726
Epoch 16 — loss: 1.0275
Epoch 17 — loss: 1.0003
Epoch 18 — loss: 0.9449
Epoch 19 — loss: 0.9129
Epoch 20 — loss: 0.8780
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8956
Epoch 02 — loss: 1.6049
Epoch 03 — loss: 1.5347
Epoch 04 — loss: 1.5125
Epoch 05 — loss: 1.4982
Epoch 06 — loss: 1.4792
Epoch 07 — loss: 1.4701
Epoch 08 — loss: 1.4657
Epoch 09 — loss: 1.4629
Epoch 10 — loss: 1.4593
Epoch 11 — loss: 1.4572
Epoch 12 — loss: 1.4506
Epoch 13 — loss: 1.4496
Epoch 14 — loss: 1.4529
Epoch 15 — loss: 1.4466
Epoch 16 — loss: 1.4510
Epoch 17 — loss: 1.4450
Epoch 18 — loss: 1.4393
Epoch 19 — loss: 1.4410
Epoch 20 — loss: 1.4375
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7118
Epoch 02 — loss: 1.6090
Epoch 03 — loss: 1.5769
Epoch 04 — loss: 1.5507
Epoch 05 — loss: 1.5348
Epoch 06 — loss: 1.5030
Epoch 07 — loss: 1.4848
Epoch 08 — loss: 1.4669
Epoch 09 — loss: 1.4438
Epoch 10 — loss: 1.4202
Epoch 11 — loss: 1.4011
Epoch 12 — loss: 1.3683
Epoch 13 — loss: 1.3485
Epoch 14 — loss: 1.3223
Epoch 15 — loss: 1.2962
Epoch 16 — loss: 1.2770
Epoch 17 — loss: 1.2561
Epoch 18 — loss: 1.2227
Epoch 19 — loss: 1.1925
Epoch 20 — loss: 1.1567
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7975
Epoch 02 — loss: 1.6415
Epoch 03 — loss: 1.5909
Epoch 04 — loss: 1.5433
Epoch 05 — loss: 1.5210
Epoch 06 — loss: 1.5031
Epoch 07 — loss: 1.4921
Epoch 08 — loss: 1.4861
Epoch 09 — loss: 1.4821
Epoch 10 — loss: 1.4769
Epoch 11 — loss: 1.4653
Epoch 12 — loss: 1.4661
Epoch 13 — loss: 1.4650
Epoch 14 — loss: 1.4517
Epoch 15 — loss: 1.4595
Epoch 16 — loss: 1.4513
Epoch 17 — loss: 1.4504
Epoch 18 — loss: 1.4549
Epoch 19 — loss: 1.4507
Epoch 20 — loss: 1.4487
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4390
Epoch 02 — loss: 1.3321
Epoch 03 — loss: 1.2845
Stage 1: Error=0.5057, Alpha=1.7691
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7973
Epoch 02 — loss: 1.6004
Epoch 03 — loss: 1.5340
Stage 2: Error=0.6712, Alpha=1.0783
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6711
Epoch 02 — loss: 1.5431
Epoch 03 — loss: 1.4714
Stage 3: Error=0.5700, Alpha=1.5101
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8511
Epoch 02 — loss: 1.7501
Epoch 03 — loss: 1.6909
Stage 4: Error=0.7444, Alpha=0.7228
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8070
Epoch 02 — loss: 1.7349
Epoch 03 — loss: 1.6817
Stage 5: Error=0.6859, Alpha=1.0108
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4450
Epoch 02 — loss: 1.3390
Epoch 03 — loss: 1.2771
Stage 1: Error=0.5070, Alpha=1.7638
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6198
Epoch 02 — loss: 1.5092
Epoch 03 — loss: 1.4405
Stage 2: Error=0.5838, Alpha=1.4533
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7234
Epoch 02 — loss: 1.6630
Epoch 03 — loss: 1.6325
Stage 3: Error=0.6795, Alpha=1.0402
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7171
Epoch 02 — loss: 1.6157
Epoch 03 — loss: 1.5154
Stage 4: Error=0.5858, Alpha=1.4451
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7902
Epoch 02 — loss: 1.6336
Epoch 03 — loss: 1.5331
Stage 5: Error=0.5761, Alpha=1.4849
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6325
Epoch 02 — loss: 1.3551
Epoch 03 — loss: 1.3089
Stage 1: Error=0.5258, Alpha=1.6887
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8026
Epoch 02 — loss: 1.5759
Epoch 03 — loss: 1.5072
Stage 2: Error=0.6585, Alpha=1.1351
--- Boosting Stage 3/5 (deepset_xy_additive) ---/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7676
Epoch 02 — loss: 1.6481
Epoch 03 — loss: 1.6248
Stage 3: Error=0.6757, Alpha=1.0577
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8476
Epoch 02 — loss: 1.7530
Epoch 03 — loss: 1.6774
Stage 4: Error=0.6964, Alpha=0.9615
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8692
Epoch 02 — loss: 1.8086
Epoch 03 — loss: 1.7367
Stage 5: Error=0.7537, Alpha=0.6731
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.9058
Stacking meta epoch 2: loss=0.7168
Stacking meta epoch 3: loss=0.6652
Stacking meta epoch 4: loss=0.6379
Stacking meta epoch 5: loss=0.6201
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8877
Stacking meta epoch 2: loss=0.7133
Stacking meta epoch 3: loss=0.6638
Stacking meta epoch 4: loss=0.6364
Stacking meta epoch 5: loss=0.6182
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2562
Stacking meta epoch 2: loss=1.2435
Stacking meta epoch 3: loss=1.2403
Stacking meta epoch 4: loss=1.2382
Stacking meta epoch 5: loss=1.2367
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      78.20
1                             deepset  ...                      82.05
2                  set_transformer_xy  ...                      80.56
3                          deepset_xy  ...                      79.69
4            set_transformer_additive  ...                      81.18
5                 deepset_xy_additive  ...                      81.09
6                        adaboost_all  ...                      81.42
7            adaboost_set_transformer  ...                      81.91
8                    adaboost_deepset  ...                      80.99
9               stacking_ensemble_all  ...                      82.24
10  stacking_ensemble_set_transformer  ...                      82.34
11          stacking_ensemble_deepset  ...                      82.96

[12 rows x 5 columns]
------------------------------------iteration no 12------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7479
Epoch 02 — loss: 1.6043
Epoch 03 — loss: 1.5547
Epoch 04 — loss: 1.5046
Epoch 05 — loss: 1.4672
Epoch 06 — loss: 1.4224
Epoch 07 — loss: 1.3887
Epoch 08 — loss: 1.3434
Epoch 09 — loss: 1.3100
Epoch 10 — loss: 1.2672
Epoch 11 — loss: 1.2287
Epoch 12 — loss: 1.1792
Epoch 13 — loss: 1.1441
Epoch 14 — loss: 1.1014
Epoch 15 — loss: 1.0473
Epoch 16 — loss: 1.0143
Epoch 17 — loss: 0.9671
Epoch 18 — loss: 0.9257
Epoch 19 — loss: 0.8964
Epoch 20 — loss: 0.8422
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8779
Epoch 02 — loss: 1.5955
Epoch 03 — loss: 1.5295
Epoch 04 — loss: 1.5007
Epoch 05 — loss: 1.4883
Epoch 06 — loss: 1.4789
Epoch 07 — loss: 1.4735
Epoch 08 — loss: 1.4643
Epoch 09 — loss: 1.4602
Epoch 10 — loss: 1.4609
Epoch 11 — loss: 1.4530
Epoch 12 — loss: 1.4562
Epoch 13 — loss: 1.4533
Epoch 14 — loss: 1.4484
Epoch 15 — loss: 1.4486
Epoch 16 — loss: 1.4404
Epoch 17 — loss: 1.4498
Epoch 18 — loss: 1.4409
Epoch 19 — loss: 1.4387
Epoch 20 — loss: 1.4367
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7005
Epoch 02 — loss: 1.5733
Epoch 03 — loss: 1.5241
Epoch 04 — loss: 1.4767
Epoch 05 — loss: 1.4404
Epoch 06 — loss: 1.3882
Epoch 07 — loss: 1.3539
Epoch 08 — loss: 1.3091
Epoch 09 — loss: 1.2736
Epoch 10 — loss: 1.2218
Epoch 11 — loss: 1.1769
Epoch 12 — loss: 1.1473
Epoch 13 — loss: 1.1010
Epoch 14 — loss: 1.0519
Epoch 15 — loss: 1.0125
Epoch 16 — loss: 0.9649
Epoch 17 — loss: 0.9380
Epoch 18 — loss: 0.9022
Epoch 19 — loss: 0.8591
Epoch 20 — loss: 0.8354
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8440
Epoch 02 — loss: 1.5844
Epoch 03 — loss: 1.5289
Epoch 04 — loss: 1.5033
Epoch 05 — loss: 1.4899
Epoch 06 — loss: 1.4791
Epoch 07 — loss: 1.4710
Epoch 08 — loss: 1.4660
Epoch 09 — loss: 1.4584
Epoch 10 — loss: 1.4609
Epoch 11 — loss: 1.4516
Epoch 12 — loss: 1.4556
Epoch 13 — loss: 1.4493
Epoch 14 — loss: 1.4482
Epoch 15 — loss: 1.4526
Epoch 16 — loss: 1.4482
Epoch 17 — loss: 1.4406
Epoch 18 — loss: 1.4390
Epoch 19 — loss: 1.4324
Epoch 20 — loss: 1.4424
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7167
Epoch 02 — loss: 1.6359
Epoch 03 — loss: 1.5746
Epoch 04 — loss: 1.5639
Epoch 05 — loss: 1.5381
Epoch 06 — loss: 1.5210
Epoch 07 — loss: 1.4939
Epoch 08 — loss: 1.4803
Epoch 09 — loss: 1.4615
Epoch 10 — loss: 1.4385
Epoch 11 — loss: 1.4218
Epoch 12 — loss: 1.3919
Epoch 13 — loss: 1.3648
Epoch 14 — loss: 1.3366
Epoch 15 — loss: 1.3181
Epoch 16 — loss: 1.2855
Epoch 17 — loss: 1.2611
Epoch 18 — loss: 1.2386
Epoch 19 — loss: 1.2061
Epoch 20 — loss: 1.1874
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8045
Epoch 02 — loss: 1.6260
Epoch 03 — loss: 1.5700
Epoch 04 — loss: 1.5348
Epoch 05 — loss: 1.5263
Epoch 06 — loss: 1.5015
Epoch 07 — loss: 1.5025
Epoch 08 — loss: 1.4838
Epoch 09 — loss: 1.4806
Epoch 10 — loss: 1.4725
Epoch 11 — loss: 1.4712
Epoch 12 — loss: 1.4659
Epoch 13 — loss: 1.4573
Epoch 14 — loss: 1.4631
Epoch 15 — loss: 1.4553
Epoch 16 — loss: 1.4566
Epoch 17 — loss: 1.4433
Epoch 18 — loss: 1.4495
Epoch 19 — loss: 1.4442
Epoch 20 — loss: 1.4466
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4335
Epoch 02 — loss: 1.2945
Epoch 03 — loss: 1.2462/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Stage 1: Error=0.5031, Alpha=1.7792
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8009
Epoch 02 — loss: 1.6330
Epoch 03 — loss: 1.5422
Stage 2: Error=0.6829, Alpha=1.0247
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6794
Epoch 02 — loss: 1.5558
Epoch 03 — loss: 1.4790
Stage 3: Error=0.5718, Alpha=1.5026
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8546
Epoch 02 — loss: 1.7745
Epoch 03 — loss: 1.7107
Stage 4: Error=0.7554, Alpha=0.6643
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8081
Epoch 02 — loss: 1.7534
Epoch 03 — loss: 1.6926
Stage 5: Error=0.7328, Alpha=0.7826
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4246
Epoch 02 — loss: 1.3061
Epoch 03 — loss: 1.2387
Stage 1: Error=0.4931, Alpha=1.8192
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6454
Epoch 02 — loss: 1.5133
Epoch 03 — loss: 1.4289
Stage 2: Error=0.5984, Alpha=1.3928
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7144
Epoch 02 — loss: 1.6351
Epoch 03 — loss: 1.5870
Stage 3: Error=0.6439, Alpha=1.1994
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7606
Epoch 02 — loss: 1.6390
Epoch 03 — loss: 1.5538
Stage 4: Error=0.6041, Alpha=1.3692
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7975
Epoch 02 — loss: 1.6679
Epoch 03 — loss: 1.5449
Stage 5: Error=0.5973, Alpha=1.3975
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6266
Epoch 02 — loss: 1.3871
Epoch 03 — loss: 1.3142
Stage 1: Error=0.5276, Alpha=1.6814
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7869
Epoch 02 — loss: 1.6058
Epoch 03 — loss: 1.5109
Stage 2: Error=0.6302, Alpha=1.2588
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7767
Epoch 02 — loss: 1.6709
Epoch 03 — loss: 1.6276
Stage 3: Error=0.6568, Alpha=1.1426
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8738
Epoch 02 — loss: 1.7794
Epoch 03 — loss: 1.7011
Stage 4: Error=0.7753, Alpha=0.5532
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8734
Epoch 02 — loss: 1.8171
Epoch 03 — loss: 1.7529
Stage 5: Error=0.7747, Alpha=0.5567
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8976
Stacking meta epoch 2: loss=0.7038
Stacking meta epoch 3: loss=0.6530
Stacking meta epoch 4: loss=0.6268
Stacking meta epoch 5: loss=0.6101
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8760
Stacking meta epoch 2: loss=0.6977
Stacking meta epoch 3: loss=0.6494
Stacking meta epoch 4: loss=0.6236
Stacking meta epoch 5: loss=0.6069
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2584
Stacking meta epoch 2: loss=1.2440
Stacking meta epoch 3: loss=1.2412
Stacking meta epoch 4: loss=1.2394
Stacking meta epoch 5: loss=1.2380
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      78.97
1                             deepset  ...                      82.15
2                  set_transformer_xy  ...                      80.65
3                          deepset_xy  ...                      81.47
4            set_transformer_additive  ...                      81.38
5                 deepset_xy_additive  ...                      80.41
6                        adaboost_all  ...                      80.65
7            adaboost_set_transformer  ...                      81.23
8                    adaboost_deepset  ...                      83.73
9               stacking_ensemble_all  ...                      83.88
10  stacking_ensemble_set_transformer  ...                      83.88
11          stacking_ensemble_deepset  ...                      82.29

[12 rows x 5 columns]
------------------------------------iteration no 13------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7247
Epoch 02 — loss: 1.5892
Epoch 03 — loss: 1.5443
Epoch 04 — loss: 1.4928
Epoch 05 — loss: 1.4519
Epoch 06 — loss: 1.4122
Epoch 07 — loss: 1.3684
Epoch 08 — loss: 1.3391
Epoch 09 — loss: 1.2790
Epoch 10 — loss: 1.2476
Epoch 11 — loss: 1.2041
Epoch 12 — loss: 1.1696
Epoch 13 — loss: 1.1246
Epoch 14 — loss: 1.0811
Epoch 15 — loss: 1.0424
Epoch 16 — loss: 0.9894
Epoch 17 — loss: 0.9552
Epoch 18 — loss: 0.9195
Epoch 19 — loss: 0.8707
Epoch 20 — loss: 0.8252
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8794
Epoch 02 — loss: 1.5957
Epoch 03 — loss: 1.5245
Epoch 04 — loss: 1.5072
Epoch 05 — loss: 1.4898
Epoch 06 — loss: 1.4776
Epoch 07 — loss: 1.4762
Epoch 08 — loss: 1.4755
Epoch 09 — loss: 1.4640
Epoch 10 — loss: 1.4668
Epoch 11 — loss: 1.4533
Epoch 12 — loss: 1.4527
Epoch 13 — loss: 1.4517
Epoch 14 — loss: 1.4473
Epoch 15 — loss: 1.4476
Epoch 16 — loss: 1.4459
Epoch 17 — loss: 1.4449
Epoch 18 — loss: 1.4399
Epoch 19 — loss: 1.4414
Epoch 20 — loss: 1.4380
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7116
Epoch 02 — loss: 1.5623
Epoch 03 — loss: 1.5124
Epoch 04 — loss: 1.4766
Epoch 05 — loss: 1.4285
Epoch 06 — loss: 1.3872
Epoch 07 — loss: 1.3435
Epoch 08 — loss: 1.3133
Epoch 09 — loss: 1.2692
Epoch 10 — loss: 1.2388
Epoch 11 — loss: 1.1896
Epoch 12 — loss: 1.1392
Epoch 13 — loss: 1.1112
Epoch 14 — loss: 1.0507
Epoch 15 — loss: 1.0148
Epoch 16 — loss: 0.9777/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 17 — loss: 0.9268
Epoch 18 — loss: 0.8947
Epoch 19 — loss: 0.8635
Epoch 20 — loss: 0.8287
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8652
Epoch 02 — loss: 1.5781
Epoch 03 — loss: 1.5245
Epoch 04 — loss: 1.4960
Epoch 05 — loss: 1.4850
Epoch 06 — loss: 1.4745
Epoch 07 — loss: 1.4688
Epoch 08 — loss: 1.4615
Epoch 09 — loss: 1.4636
Epoch 10 — loss: 1.4583
Epoch 11 — loss: 1.4535
Epoch 12 — loss: 1.4557
Epoch 13 — loss: 1.4526
Epoch 14 — loss: 1.4459
Epoch 15 — loss: 1.4561
Epoch 16 — loss: 1.4418
Epoch 17 — loss: 1.4422
Epoch 18 — loss: 1.4448
Epoch 19 — loss: 1.4413
Epoch 20 — loss: 1.4464
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7134
Epoch 02 — loss: 1.6000
Epoch 03 — loss: 1.5709
Epoch 04 — loss: 1.5499
Epoch 05 — loss: 1.5291
Epoch 06 — loss: 1.4962
Epoch 07 — loss: 1.4820
Epoch 08 — loss: 1.4520
Epoch 09 — loss: 1.4296
Epoch 10 — loss: 1.4099
Epoch 11 — loss: 1.3833
Epoch 12 — loss: 1.3587
Epoch 13 — loss: 1.3349
Epoch 14 — loss: 1.3118
Epoch 15 — loss: 1.2876
Epoch 16 — loss: 1.2559
Epoch 17 — loss: 1.2361
Epoch 18 — loss: 1.1962
Epoch 19 — loss: 1.1709
Epoch 20 — loss: 1.1429
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8114
Epoch 02 — loss: 1.6169
Epoch 03 — loss: 1.5782
Epoch 04 — loss: 1.5442
Epoch 05 — loss: 1.5170
Epoch 06 — loss: 1.4985
Epoch 07 — loss: 1.4993
Epoch 08 — loss: 1.4925
Epoch 09 — loss: 1.4782
Epoch 10 — loss: 1.4736
Epoch 11 — loss: 1.4670
Epoch 12 — loss: 1.4674
Epoch 13 — loss: 1.4645
Epoch 14 — loss: 1.4560
Epoch 15 — loss: 1.4539
Epoch 16 — loss: 1.4446
Epoch 17 — loss: 1.4540
Epoch 18 — loss: 1.4443
Epoch 19 — loss: 1.4454
Epoch 20 — loss: 1.4418
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4400
Epoch 02 — loss: 1.3267
Epoch 03 — loss: 1.2786
Stage 1: Error=0.5110, Alpha=1.7479
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7720
Epoch 02 — loss: 1.6115
Epoch 03 — loss: 1.5340
Stage 2: Error=0.6786, Alpha=1.0442
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6724
Epoch 02 — loss: 1.5448
Epoch 03 — loss: 1.4800
Stage 3: Error=0.5772, Alpha=1.4803
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8433
Epoch 02 — loss: 1.7254
Epoch 03 — loss: 1.6600
Stage 4: Error=0.7281, Alpha=0.8066
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7976
Epoch 02 — loss: 1.7371
Epoch 03 — loss: 1.6939
Stage 5: Error=0.7020, Alpha=0.9350
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4064
Epoch 02 — loss: 1.3091
Epoch 03 — loss: 1.2468
Stage 1: Error=0.5119, Alpha=1.7441
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.5995
Epoch 02 — loss: 1.4788
Epoch 03 — loss: 1.4202
Stage 2: Error=0.5612, Alpha=1.5456
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7395
Epoch 02 — loss: 1.6632
Epoch 03 — loss: 1.6389
Stage 3: Error=0.6957, Alpha=0.9649
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7589
Epoch 02 — loss: 1.6275
Epoch 03 — loss: 1.5399
Stage 4: Error=0.5987, Alpha=1.3917
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7755
Epoch 02 — loss: 1.6405
Epoch 03 — loss: 1.5037
Stage 5: Error=0.5780, Alpha=1.4771
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6132
Epoch 02 — loss: 1.3772
Epoch 03 — loss: 1.2970
Stage 1: Error=0.5258, Alpha=1.6887
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7764
Epoch 02 — loss: 1.5876
Epoch 03 — loss: 1.5250
Stage 2: Error=0.6127, Alpha=1.3329
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7840
Epoch 02 — loss: 1.6655
Epoch 03 — loss: 1.6328
Stage 3: Error=0.6590, Alpha=1.1328
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8527
Epoch 02 — loss: 1.8007
Epoch 03 — loss: 1.7203
Stage 4: Error=0.7435, Alpha=0.7275
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8709
Epoch 02 — loss: 1.8074
Epoch 03 — loss: 1.7480
Stage 5: Error=0.7777, Alpha=0.5392
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8758
Stacking meta epoch 2: loss=0.6835
Stacking meta epoch 3: loss=0.6353
Stacking meta epoch 4: loss=0.6104
Stacking meta epoch 5: loss=0.5945
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8668
Stacking meta epoch 2: loss=0.6829
Stacking meta epoch 3: loss=0.6367
Stacking meta epoch 4: loss=0.6120
Stacking meta epoch 5: loss=0.5957
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2595
Stacking meta epoch 2: loss=1.2449
Stacking meta epoch 3: loss=1.2418
Stacking meta epoch 4: loss=1.2398
Stacking meta epoch 5: loss=1.2383
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      80.41
1                             deepset  ...                      80.13
2                  set_transformer_xy  ...                      79.93
3                          deepset_xy  ...                      80.32
4            set_transformer_additive  ...                      77.96
5                 deepset_xy_additive  ...                      81.52
6                        adaboost_all  ...                      80.94
7            adaboost_set_transformer  ...                      81.81
8                    adaboost_deepset  ...                      82.44
9               stacking_ensemble_all  ...                      83.21
10  stacking_ensemble_set_transformer  ...                      83.35
11          stacking_ensemble_deepset  ...                      82.77

[12 rows x 5 columns]
------------------------------------iteration no 14------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7218
Epoch 02 — loss: 1.5887
Epoch 03 — loss: 1.5264
Epoch 04 — loss: 1.4879
Epoch 05 — loss: 1.4430
Epoch 06 — loss: 1.3993
Epoch 07 — loss: 1.3679
Epoch 08 — loss: 1.3195
Epoch 09 — loss: 1.2839
Epoch 10 — loss: 1.2416
Epoch 11 — loss: 1.2035
Epoch 12 — loss: 1.1470
Epoch 13 — loss: 1.1076
Epoch 14 — loss: 1.0715
Epoch 15 — loss: 1.0375
Epoch 16 — loss: 0.9826
Epoch 17 — loss: 0.9441
Epoch 18 — loss: 0.9152
Epoch 19 — loss: 0.8617
Epoch 20 — loss: 0.8307
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8874
Epoch 02 — loss: 1.5853
Epoch 03 — loss: 1.5241
Epoch 04 — loss: 1.5003
Epoch 05 — loss: 1.4888
Epoch 06 — loss: 1.4822
Epoch 07 — loss: 1.4687
Epoch 08 — loss: 1.4623
Epoch 09 — loss: 1.4615
Epoch 10 — loss: 1.4656
Epoch 11 — loss: 1.4545
Epoch 12 — loss: 1.4556
Epoch 13 — loss: 1.4479
Epoch 14 — loss: 1.4447
Epoch 15 — loss: 1.4433
Epoch 16 — loss: 1.4409
Epoch 17 — loss: 1.4391
Epoch 18 — loss: 1.4394
Epoch 19 — loss: 1.4380
Epoch 20 — loss: 1.4360
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7255
Epoch 02 — loss: 1.5757
Epoch 03 — loss: 1.5131
Epoch 04 — loss: 1.4752
Epoch 05 — loss: 1.4416
Epoch 06 — loss: 1.4057
Epoch 07 — loss: 1.3583
Epoch 08 — loss: 1.3208
Epoch 09 — loss: 1.2703
Epoch 10 — loss: 1.2414
Epoch 11 — loss: 1.1954
Epoch 12 — loss: 1.1470
Epoch 13 — loss: 1.1136
Epoch 14 — loss: 1.0702
Epoch 15 — loss: 1.0386
Epoch 16 — loss: 0.9900
Epoch 17 — loss: 0.9339
Epoch 18 — loss: 0.9087
Epoch 19 — loss: 0.8695
Epoch 20 — loss: 0.8349
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8775
Epoch 02 — loss: 1.5740
Epoch 03 — loss: 1.5214
Epoch 04 — loss: 1.5047
Epoch 05 — loss: 1.4889
Epoch 06 — loss: 1.4758
Epoch 07 — loss: 1.4792
Epoch 08 — loss: 1.4647
Epoch 09 — loss: 1.4652
Epoch 10 — loss: 1.4582
Epoch 11 — loss: 1.4585
Epoch 12 — loss: 1.4554
Epoch 13 — loss: 1.4531
Epoch 14 — loss: 1.4500
Epoch 15 — loss: 1.4545
Epoch 16 — loss: 1.4504
Epoch 17 — loss: 1.4465
Epoch 18 — loss: 1.4370
Epoch 19 — loss: 1.4394
Epoch 20 — loss: 1.4412
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7055
Epoch 02 — loss: 1.6290
Epoch 03 — loss: 1.5829
Epoch 04 — loss: 1.5593
Epoch 05 — loss: 1.5390
Epoch 06 — loss: 1.5139
Epoch 07 — loss: 1.4868
Epoch 08 — loss: 1.4680
Epoch 09 — loss: 1.4438
Epoch 10 — loss: 1.4136
Epoch 11 — loss: 1.3883
Epoch 12 — loss: 1.3561
Epoch 13 — loss: 1.3323
Epoch 14 — loss: 1.3238
Epoch 15 — loss: 1.2843
Epoch 16 — loss: 1.2618
Epoch 17 — loss: 1.2238
Epoch 18 — loss: 1.2008
Epoch 19 — loss: 1.1654
Epoch 20 — loss: 1.1305
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8031
Epoch 02 — loss: 1.6202
Epoch 03 — loss: 1.5711
Epoch 04 — loss: 1.5340
Epoch 05 — loss: 1.5178
Epoch 06 — loss: 1.5003
Epoch 07 — loss: 1.4906
Epoch 08 — loss: 1.4763
Epoch 09 — loss: 1.4822
Epoch 10 — loss: 1.4724
Epoch 11 — loss: 1.4696
Epoch 12 — loss: 1.4639
Epoch 13 — loss: 1.4621
Epoch 14 — loss: 1.4549
Epoch 15 — loss: 1.4484
Epoch 16 — loss: 1.4624
Epoch 17 — loss: 1.4480
Epoch 18 — loss: 1.4472
Epoch 19 — loss: 1.4416
Epoch 20 — loss: 1.4461
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4455
Epoch 02 — loss: 1.3271
Epoch 03 — loss: 1.2869
Stage 1: Error=0.5034, Alpha=1.7783
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7808
Epoch 02 — loss: 1.5889
Epoch 03 — loss: 1.5305
Stage 2: Error=0.6953, Alpha=0.9666
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6988
Epoch 02 — loss: 1.5592
Epoch 03 — loss: 1.4867
Stage 3: Error=0.5772, Alpha=1.4806
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8561
Epoch 02 — loss: 1.7525
Epoch 03 — loss: 1.6871
Stage 4: Error=0.7286, Alpha=0.8040
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7961
Epoch 02 — loss: 1.7417
Epoch 03 — loss: 1.6923
Stage 5: Error=0.6905, Alpha=0.9892
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4434
Epoch 02 — loss: 1.3453
Epoch 03 — loss: 1.2882
Stage 1: Error=0.5155, Alpha=1.7296
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6515
Epoch 02 — loss: 1.4936
Epoch 03 — loss: 1.4267
Stage 2: Error=0.5856, Alpha=1.4460
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7264
Epoch 02 — loss: 1.6561
Epoch 03 — loss: 1.6155
Stage 3: Error=0.6637, Alpha=1.1120
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7593
Epoch 02 — loss: 1.6178
Epoch 03 — loss: 1.5048
Stage 4: Error=0.5680, Alpha=1.5180
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7939
Epoch 02 — loss: 1.6336
Epoch 03 — loss: 1.4944
Stage 5: Error=0.5396, Alpha=1.6330
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6371
Epoch 02 — loss: 1.3981
Epoch 03 — loss: 1.2894
Stage 1: Error=0.5237, Alpha=1.6969
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7775
Epoch 02 — loss: 1.6035
Epoch 03 — loss: 1.5229
Stage 2: Error=0.6204, Alpha=1.3004
--- Boosting Stage 3/5 (deepset_xy_additive) ---/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7900
Epoch 02 — loss: 1.6596
Epoch 03 — loss: 1.6273
Stage 3: Error=0.6881, Alpha=1.0003
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8678
Epoch 02 — loss: 1.7892
Epoch 03 — loss: 1.7055
Stage 4: Error=0.7397, Alpha=0.7475
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8763
Epoch 02 — loss: 1.7977
Epoch 03 — loss: 1.7296
Stage 5: Error=0.7537, Alpha=0.6736
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8979
Stacking meta epoch 2: loss=0.7009
Stacking meta epoch 3: loss=0.6477
Stacking meta epoch 4: loss=0.6201
Stacking meta epoch 5: loss=0.6024
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8846
Stacking meta epoch 2: loss=0.6980
Stacking meta epoch 3: loss=0.6464
Stacking meta epoch 4: loss=0.6186
Stacking meta epoch 5: loss=0.6005
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2569
Stacking meta epoch 2: loss=1.2408
Stacking meta epoch 3: loss=1.2378
Stacking meta epoch 4: loss=1.2359
Stacking meta epoch 5: loss=1.2345
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      80.37
1                             deepset  ...                      80.17
2                  set_transformer_xy  ...                      80.80
3                          deepset_xy  ...                      80.03
4            set_transformer_additive  ...                      82.05
5                 deepset_xy_additive  ...                      80.08
6                        adaboost_all  ...                      80.70
7            adaboost_set_transformer  ...                      81.57
8                    adaboost_deepset  ...                      82.77
9               stacking_ensemble_all  ...                      84.84
10  stacking_ensemble_set_transformer  ...                      84.41
11          stacking_ensemble_deepset  ...                      82.53

[12 rows x 5 columns]
------------------------------------iteration no 15------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7312
Epoch 02 — loss: 1.6087
Epoch 03 — loss: 1.5525
Epoch 04 — loss: 1.5050
Epoch 05 — loss: 1.4683
Epoch 06 — loss: 1.4206
Epoch 07 — loss: 1.3797
Epoch 08 — loss: 1.3466
Epoch 09 — loss: 1.3018
Epoch 10 — loss: 1.2727
Epoch 11 — loss: 1.2138
Epoch 12 — loss: 1.1741
Epoch 13 — loss: 1.1365
Epoch 14 — loss: 1.0933
Epoch 15 — loss: 1.0641
Epoch 16 — loss: 1.0011
Epoch 17 — loss: 0.9716
Epoch 18 — loss: 0.9243
Epoch 19 — loss: 0.8789
Epoch 20 — loss: 0.8578
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8571
Epoch 02 — loss: 1.5899
Epoch 03 — loss: 1.5310
Epoch 04 — loss: 1.5086
Epoch 05 — loss: 1.4885
Epoch 06 — loss: 1.4788
Epoch 07 — loss: 1.4686
Epoch 08 — loss: 1.4673
Epoch 09 — loss: 1.4631
Epoch 10 — loss: 1.4624
Epoch 11 — loss: 1.4596
Epoch 12 — loss: 1.4616
Epoch 13 — loss: 1.4532
Epoch 14 — loss: 1.4547
Epoch 15 — loss: 1.4374
Epoch 16 — loss: 1.4540
Epoch 17 — loss: 1.4469
Epoch 18 — loss: 1.4462
Epoch 19 — loss: 1.4395
Epoch 20 — loss: 1.4411
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.6958
Epoch 02 — loss: 1.5698
Epoch 03 — loss: 1.5157
Epoch 04 — loss: 1.4755
Epoch 05 — loss: 1.4327
Epoch 06 — loss: 1.4016
Epoch 07 — loss: 1.3433
Epoch 08 — loss: 1.3128
Epoch 09 — loss: 1.2749
Epoch 10 — loss: 1.2336
Epoch 11 — loss: 1.1893
Epoch 12 — loss: 1.1499
Epoch 13 — loss: 1.1176
Epoch 14 — loss: 1.0716
Epoch 15 — loss: 1.0296
Epoch 16 — loss: 0.9910
Epoch 17 — loss: 0.9594
Epoch 18 — loss: 0.9211
Epoch 19 — loss: 0.8793
Epoch 20 — loss: 0.8473
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8561
Epoch 02 — loss: 1.5982
Epoch 03 — loss: 1.5380
Epoch 04 — loss: 1.5116
Epoch 05 — loss: 1.4999
Epoch 06 — loss: 1.4829
Epoch 07 — loss: 1.4713
Epoch 08 — loss: 1.4762
Epoch 09 — loss: 1.4668
Epoch 10 — loss: 1.4652
Epoch 11 — loss: 1.4547
Epoch 12 — loss: 1.4511
Epoch 13 — loss: 1.4506
Epoch 14 — loss: 1.4437
Epoch 15 — loss: 1.4479
Epoch 16 — loss: 1.4382
Epoch 17 — loss: 1.4363
Epoch 18 — loss: 1.4414
Epoch 19 — loss: 1.4415
Epoch 20 — loss: 1.4422
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.6847
Epoch 02 — loss: 1.5794
Epoch 03 — loss: 1.5566
Epoch 04 — loss: 1.5408
Epoch 05 — loss: 1.5243
Epoch 06 — loss: 1.5026
Epoch 07 — loss: 1.4908
Epoch 08 — loss: 1.4728
Epoch 09 — loss: 1.4586
Epoch 10 — loss: 1.4387
Epoch 11 — loss: 1.4271
Epoch 12 — loss: 1.3986
Epoch 13 — loss: 1.3871
Epoch 14 — loss: 1.3622
Epoch 15 — loss: 1.3421
Epoch 16 — loss: 1.3080
Epoch 17 — loss: 1.2852
Epoch 18 — loss: 1.2506
Epoch 19 — loss: 1.2417
Epoch 20 — loss: 1.2068
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8168
Epoch 02 — loss: 1.6228
Epoch 03 — loss: 1.5740
Epoch 04 — loss: 1.5358
Epoch 05 — loss: 1.5150
Epoch 06 — loss: 1.5007
Epoch 07 — loss: 1.4808
Epoch 08 — loss: 1.4889
Epoch 09 — loss: 1.4731
Epoch 10 — loss: 1.4732
Epoch 11 — loss: 1.4619
Epoch 12 — loss: 1.4579
Epoch 13 — loss: 1.4562
Epoch 14 — loss: 1.4560
Epoch 15 — loss: 1.4518
Epoch 16 — loss: 1.4542
Epoch 17 — loss: 1.4426
Epoch 18 — loss: 1.4481
Epoch 19 — loss: 1.4420
Epoch 20 — loss: 1.4450
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4414
Epoch 02 — loss: 1.3071
Epoch 03 — loss: 1.2532/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Stage 1: Error=0.5059, Alpha=1.7682
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7955
Epoch 02 — loss: 1.6232
Epoch 03 — loss: 1.5509
Stage 2: Error=0.6652, Alpha=1.1050
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6839
Epoch 02 — loss: 1.5597
Epoch 03 — loss: 1.4637
Stage 3: Error=0.5722, Alpha=1.5008
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8760
Epoch 02 — loss: 1.7590
Epoch 03 — loss: 1.7086
Stage 4: Error=0.7620, Alpha=0.6283
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8131
Epoch 02 — loss: 1.7377
Epoch 03 — loss: 1.6719
Stage 5: Error=0.6736, Alpha=1.0674
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4341
Epoch 02 — loss: 1.3013
Epoch 03 — loss: 1.2776
Stage 1: Error=0.5128, Alpha=1.7407
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6400
Epoch 02 — loss: 1.5281
Epoch 03 — loss: 1.4465
Stage 2: Error=0.5916, Alpha=1.4210
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7183
Epoch 02 — loss: 1.6570
Epoch 03 — loss: 1.6182
Stage 3: Error=0.6751, Alpha=1.0606
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7371
Epoch 02 — loss: 1.6383
Epoch 03 — loss: 1.5472
Stage 4: Error=0.6042, Alpha=1.3687
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7889
Epoch 02 — loss: 1.6373
Epoch 03 — loss: 1.5365
Stage 5: Error=0.5689, Alpha=1.5144
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5979
Epoch 02 — loss: 1.3408
Epoch 03 — loss: 1.2889
Stage 1: Error=0.5211, Alpha=1.7075
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7907
Epoch 02 — loss: 1.5861
Epoch 03 — loss: 1.5217
Stage 2: Error=0.6522, Alpha=1.1629
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7977
Epoch 02 — loss: 1.6779
Epoch 03 — loss: 1.6255
Stage 3: Error=0.6906, Alpha=0.9890
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8877
Epoch 02 — loss: 1.7817
Epoch 03 — loss: 1.6759
Stage 4: Error=0.7169, Alpha=0.8627
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8754
Epoch 02 — loss: 1.7980
Epoch 03 — loss: 1.7173
Stage 5: Error=0.7230, Alpha=0.8325
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8991
Stacking meta epoch 2: loss=0.7130
Stacking meta epoch 3: loss=0.6624
Stacking meta epoch 4: loss=0.6357
Stacking meta epoch 5: loss=0.6184
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8801
Stacking meta epoch 2: loss=0.7078
Stacking meta epoch 3: loss=0.6603
Stacking meta epoch 4: loss=0.6345
Stacking meta epoch 5: loss=0.6177
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2567
Stacking meta epoch 2: loss=1.2419
Stacking meta epoch 3: loss=1.2391
Stacking meta epoch 4: loss=1.2373
Stacking meta epoch 5: loss=1.2359
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      77.82
1                             deepset  ...                      80.41
2                  set_transformer_xy  ...                      78.15
3                          deepset_xy  ...                      80.32
4            set_transformer_additive  ...                      78.06
5                 deepset_xy_additive  ...                      82.87
6                        adaboost_all  ...                      80.90
7            adaboost_set_transformer  ...                      81.71
8                    adaboost_deepset  ...                      81.62
9               stacking_ensemble_all  ...                      83.40
10  stacking_ensemble_set_transformer  ...                      82.96
11          stacking_ensemble_deepset  ...                      83.21

[12 rows x 5 columns]
------------------------------------iteration no 16------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7169
Epoch 02 — loss: 1.5890
Epoch 03 — loss: 1.5405
Epoch 04 — loss: 1.4904
Epoch 05 — loss: 1.4488
Epoch 06 — loss: 1.4032
Epoch 07 — loss: 1.3652
Epoch 08 — loss: 1.3238
Epoch 09 — loss: 1.2831
Epoch 10 — loss: 1.2429
Epoch 11 — loss: 1.1950
Epoch 12 — loss: 1.1492
Epoch 13 — loss: 1.1065
Epoch 14 — loss: 1.0675
Epoch 15 — loss: 1.0265
Epoch 16 — loss: 0.9877
Epoch 17 — loss: 0.9345
Epoch 18 — loss: 0.9000
Epoch 19 — loss: 0.8676
Epoch 20 — loss: 0.8223
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8613
Epoch 02 — loss: 1.5992
Epoch 03 — loss: 1.5357
Epoch 04 — loss: 1.5054
Epoch 05 — loss: 1.4967
Epoch 06 — loss: 1.4871
Epoch 07 — loss: 1.4740
Epoch 08 — loss: 1.4688
Epoch 09 — loss: 1.4679
Epoch 10 — loss: 1.4570
Epoch 11 — loss: 1.4519
Epoch 12 — loss: 1.4593
Epoch 13 — loss: 1.4567
Epoch 14 — loss: 1.4548
Epoch 15 — loss: 1.4547
Epoch 16 — loss: 1.4443
Epoch 17 — loss: 1.4486
Epoch 18 — loss: 1.4459
Epoch 19 — loss: 1.4418
Epoch 20 — loss: 1.4413
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.6825
Epoch 02 — loss: 1.5478
Epoch 03 — loss: 1.4986
Epoch 04 — loss: 1.4610
Epoch 05 — loss: 1.4256
Epoch 06 — loss: 1.3868
Epoch 07 — loss: 1.3541
Epoch 08 — loss: 1.3078
Epoch 09 — loss: 1.2704
Epoch 10 — loss: 1.2433
Epoch 11 — loss: 1.1928
Epoch 12 — loss: 1.1539
Epoch 13 — loss: 1.1187
Epoch 14 — loss: 1.0693
Epoch 15 — loss: 1.0273
Epoch 16 — loss: 0.9943/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 17 — loss: 0.9412
Epoch 18 — loss: 0.8925
Epoch 19 — loss: 0.8592
Epoch 20 — loss: 0.8281
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8429
Epoch 02 — loss: 1.5918
Epoch 03 — loss: 1.5339
Epoch 04 — loss: 1.5046
Epoch 05 — loss: 1.4921
Epoch 06 — loss: 1.4865
Epoch 07 — loss: 1.4779
Epoch 08 — loss: 1.4722
Epoch 09 — loss: 1.4607
Epoch 10 — loss: 1.4618
Epoch 11 — loss: 1.4570
Epoch 12 — loss: 1.4525
Epoch 13 — loss: 1.4624
Epoch 14 — loss: 1.4538
Epoch 15 — loss: 1.4471
Epoch 16 — loss: 1.4441
Epoch 17 — loss: 1.4460
Epoch 18 — loss: 1.4427
Epoch 19 — loss: 1.4422
Epoch 20 — loss: 1.4396
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7251
Epoch 02 — loss: 1.6051
Epoch 03 — loss: 1.5812
Epoch 04 — loss: 1.5422
Epoch 05 — loss: 1.5196
Epoch 06 — loss: 1.5069
Epoch 07 — loss: 1.4797
Epoch 08 — loss: 1.4709
Epoch 09 — loss: 1.4482
Epoch 10 — loss: 1.4205
Epoch 11 — loss: 1.4055
Epoch 12 — loss: 1.3812
Epoch 13 — loss: 1.3527
Epoch 14 — loss: 1.3245
Epoch 15 — loss: 1.2998
Epoch 16 — loss: 1.2726
Epoch 17 — loss: 1.2505
Epoch 18 — loss: 1.2125
Epoch 19 — loss: 1.1840
Epoch 20 — loss: 1.1605
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8174
Epoch 02 — loss: 1.6371
Epoch 03 — loss: 1.5869
Epoch 04 — loss: 1.5534
Epoch 05 — loss: 1.5305
Epoch 06 — loss: 1.5183
Epoch 07 — loss: 1.5043
Epoch 08 — loss: 1.4915
Epoch 09 — loss: 1.4834
Epoch 10 — loss: 1.4737
Epoch 11 — loss: 1.4722
Epoch 12 — loss: 1.4671
Epoch 13 — loss: 1.4640
Epoch 14 — loss: 1.4688
Epoch 15 — loss: 1.4552
Epoch 16 — loss: 1.4624
Epoch 17 — loss: 1.4536
Epoch 18 — loss: 1.4567
Epoch 19 — loss: 1.4470
Epoch 20 — loss: 1.4501
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4366
Epoch 02 — loss: 1.3188
Epoch 03 — loss: 1.2799
Stage 1: Error=0.5137, Alpha=1.7369
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7804
Epoch 02 — loss: 1.5912
Epoch 03 — loss: 1.5156
Stage 2: Error=0.6430, Alpha=1.2035
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6841
Epoch 02 — loss: 1.5508
Epoch 03 — loss: 1.4568
Stage 3: Error=0.5736, Alpha=1.4951
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8649
Epoch 02 — loss: 1.7802
Epoch 03 — loss: 1.7049
Stage 4: Error=0.7263, Alpha=0.8157
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8132
Epoch 02 — loss: 1.7380
Epoch 03 — loss: 1.6974
Stage 5: Error=0.7006, Alpha=0.9414
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4325
Epoch 02 — loss: 1.3419
Epoch 03 — loss: 1.2706
Stage 1: Error=0.5238, Alpha=1.6964
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6146
Epoch 02 — loss: 1.4845
Epoch 03 — loss: 1.4461
Stage 2: Error=0.5646, Alpha=1.5321
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7316
Epoch 02 — loss: 1.6443
Epoch 03 — loss: 1.6255
Stage 3: Error=0.6763, Alpha=1.0551
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7620
Epoch 02 — loss: 1.6361
Epoch 03 — loss: 1.5262
Stage 4: Error=0.6022, Alpha=1.3772
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7772
Epoch 02 — loss: 1.6324
Epoch 03 — loss: 1.5196
Stage 5: Error=0.5694, Alpha=1.5124
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6321
Epoch 02 — loss: 1.3812
Epoch 03 — loss: 1.2944
Stage 1: Error=0.5230, Alpha=1.6998
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7917
Epoch 02 — loss: 1.5986
Epoch 03 — loss: 1.5190
Stage 2: Error=0.6650, Alpha=1.1061
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7551
Epoch 02 — loss: 1.6218
Epoch 03 — loss: 1.5693
Stage 3: Error=0.6348, Alpha=1.2391
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8689
Epoch 02 — loss: 1.7903
Epoch 03 — loss: 1.6925
Stage 4: Error=0.7104, Alpha=0.8946
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8705
Epoch 02 — loss: 1.8034
Epoch 03 — loss: 1.7393
Stage 5: Error=0.7454, Alpha=0.7177
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8686
Stacking meta epoch 2: loss=0.6780
Stacking meta epoch 3: loss=0.6292
Stacking meta epoch 4: loss=0.6050
Stacking meta epoch 5: loss=0.5898
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8645
Stacking meta epoch 2: loss=0.6826
Stacking meta epoch 3: loss=0.6355
Stacking meta epoch 4: loss=0.6106
Stacking meta epoch 5: loss=0.5947
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2605
Stacking meta epoch 2: loss=1.2451
Stacking meta epoch 3: loss=1.2418
Stacking meta epoch 4: loss=1.2396
Stacking meta epoch 5: loss=1.2379
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      79.74
1                             deepset  ...                      80.41
2                  set_transformer_xy  ...                      79.21
3                          deepset_xy  ...                      79.40
4            set_transformer_additive  ...                      80.08
5                 deepset_xy_additive  ...                      79.02
6                        adaboost_all  ...                      80.17
7            adaboost_set_transformer  ...                      82.34
8                    adaboost_deepset  ...                      82.19
9               stacking_ensemble_all  ...                      82.29
10  stacking_ensemble_set_transformer  ...                      81.62
11          stacking_ensemble_deepset  ...                      82.87

[12 rows x 5 columns]
------------------------------------iteration no 17------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7465
Epoch 02 — loss: 1.6152
Epoch 03 — loss: 1.5675
Epoch 04 — loss: 1.5185
Epoch 05 — loss: 1.4726
Epoch 06 — loss: 1.4402
Epoch 07 — loss: 1.3860
Epoch 08 — loss: 1.3529
Epoch 09 — loss: 1.3191
Epoch 10 — loss: 1.2642
Epoch 11 — loss: 1.2338
Epoch 12 — loss: 1.1883
Epoch 13 — loss: 1.1501
Epoch 14 — loss: 1.1051
Epoch 15 — loss: 1.0551
Epoch 16 — loss: 1.0093
Epoch 17 — loss: 0.9810
Epoch 18 — loss: 0.9408
Epoch 19 — loss: 0.8988
Epoch 20 — loss: 0.8616
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8793
Epoch 02 — loss: 1.6090
Epoch 03 — loss: 1.5336
Epoch 04 — loss: 1.5072
Epoch 05 — loss: 1.4880
Epoch 06 — loss: 1.4727
Epoch 07 — loss: 1.4697
Epoch 08 — loss: 1.4649
Epoch 09 — loss: 1.4548
Epoch 10 — loss: 1.4552
Epoch 11 — loss: 1.4479
Epoch 12 — loss: 1.4550
Epoch 13 — loss: 1.4465
Epoch 14 — loss: 1.4519
Epoch 15 — loss: 1.4451
Epoch 16 — loss: 1.4407
Epoch 17 — loss: 1.4397
Epoch 18 — loss: 1.4376
Epoch 19 — loss: 1.4412
Epoch 20 — loss: 1.4441
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7368
Epoch 02 — loss: 1.5837
Epoch 03 — loss: 1.5182
Epoch 04 — loss: 1.4608
Epoch 05 — loss: 1.4218
Epoch 06 — loss: 1.3769
Epoch 07 — loss: 1.3456
Epoch 08 — loss: 1.2956
Epoch 09 — loss: 1.2595
Epoch 10 — loss: 1.2076
Epoch 11 — loss: 1.1667
Epoch 12 — loss: 1.1224
Epoch 13 — loss: 1.0767
Epoch 14 — loss: 1.0278
Epoch 15 — loss: 0.9949
Epoch 16 — loss: 0.9437
Epoch 17 — loss: 0.9047
Epoch 18 — loss: 0.8713
Epoch 19 — loss: 0.8285
Epoch 20 — loss: 0.7938
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8407
Epoch 02 — loss: 1.5832
Epoch 03 — loss: 1.5411
Epoch 04 — loss: 1.5042
Epoch 05 — loss: 1.4935
Epoch 06 — loss: 1.4844
Epoch 07 — loss: 1.4646
Epoch 08 — loss: 1.4669
Epoch 09 — loss: 1.4648
Epoch 10 — loss: 1.4620
Epoch 11 — loss: 1.4607
Epoch 12 — loss: 1.4511
Epoch 13 — loss: 1.4453
Epoch 14 — loss: 1.4514
Epoch 15 — loss: 1.4499
Epoch 16 — loss: 1.4442
Epoch 17 — loss: 1.4438
Epoch 18 — loss: 1.4389
Epoch 19 — loss: 1.4345
Epoch 20 — loss: 1.4347
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7024
Epoch 02 — loss: 1.6061
Epoch 03 — loss: 1.5718
Epoch 04 — loss: 1.5504
Epoch 05 — loss: 1.5341
Epoch 06 — loss: 1.5010
Epoch 07 — loss: 1.4827
Epoch 08 — loss: 1.4659
Epoch 09 — loss: 1.4350
Epoch 10 — loss: 1.4156
Epoch 11 — loss: 1.3967
Epoch 12 — loss: 1.3741
Epoch 13 — loss: 1.3432
Epoch 14 — loss: 1.3181
Epoch 15 — loss: 1.2971
Epoch 16 — loss: 1.2618
Epoch 17 — loss: 1.2250
Epoch 18 — loss: 1.1965
Epoch 19 — loss: 1.1666
Epoch 20 — loss: 1.1371
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8021
Epoch 02 — loss: 1.6218
Epoch 03 — loss: 1.5765
Epoch 04 — loss: 1.5490
Epoch 05 — loss: 1.5241
Epoch 06 — loss: 1.5093
Epoch 07 — loss: 1.4979
Epoch 08 — loss: 1.4873
Epoch 09 — loss: 1.4803
Epoch 10 — loss: 1.4734
Epoch 11 — loss: 1.4694
Epoch 12 — loss: 1.4693
Epoch 13 — loss: 1.4606
Epoch 14 — loss: 1.4570
Epoch 15 — loss: 1.4534
Epoch 16 — loss: 1.4601
Epoch 17 — loss: 1.4552
Epoch 18 — loss: 1.4468
Epoch 19 — loss: 1.4478
Epoch 20 — loss: 1.4495
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4376
Epoch 02 — loss: 1.3417
Epoch 03 — loss: 1.2969
Stage 1: Error=0.5142, Alpha=1.7349
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7787
Epoch 02 — loss: 1.6271
Epoch 03 — loss: 1.5419
Stage 2: Error=0.6550, Alpha=1.1506
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6896
Epoch 02 — loss: 1.5484
Epoch 03 — loss: 1.4869
Stage 3: Error=0.5943, Alpha=1.4100
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8452
Epoch 02 — loss: 1.7413
Epoch 03 — loss: 1.6879
Stage 4: Error=0.7497, Alpha=0.6950
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8054
Epoch 02 — loss: 1.7451
Epoch 03 — loss: 1.7071
Stage 5: Error=0.7130, Alpha=0.8817
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4441
Epoch 02 — loss: 1.3215
Epoch 03 — loss: 1.2660
Stage 1: Error=0.5128, Alpha=1.7407
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6116
Epoch 02 — loss: 1.5094
Epoch 03 — loss: 1.4290
Stage 2: Error=0.5932, Alpha=1.4145
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7036
Epoch 02 — loss: 1.6493
Epoch 03 — loss: 1.6172
Stage 3: Error=0.6534, Alpha=1.1576
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7642
Epoch 02 — loss: 1.6274
Epoch 03 — loss: 1.5267
Stage 4: Error=0.5746, Alpha=1.4911
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7919
Epoch 02 — loss: 1.6424
Epoch 03 — loss: 1.5191
Stage 5: Error=0.5508, Alpha=1.5880
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6336
Epoch 02 — loss: 1.3635
Epoch 03 — loss: 1.3016
Stage 1: Error=0.5242, Alpha=1.6949
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7988
Epoch 02 — loss: 1.6069
Epoch 03 — loss: 1.5139
Stage 2: Error=0.6387, Alpha=1.2221
--- Boosting Stage 3/5 (deepset_xy_additive) ---/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7710
Epoch 02 — loss: 1.6608
Epoch 03 — loss: 1.5967
Stage 3: Error=0.6343, Alpha=1.2409
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8511
Epoch 02 — loss: 1.7908
Epoch 03 — loss: 1.7175
Stage 4: Error=0.7572, Alpha=0.6541
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8762
Epoch 02 — loss: 1.8236
Epoch 03 — loss: 1.7468
Stage 5: Error=0.7682, Alpha=0.5933
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8841
Stacking meta epoch 2: loss=0.6892
Stacking meta epoch 3: loss=0.6403
Stacking meta epoch 4: loss=0.6159
Stacking meta epoch 5: loss=0.6004
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8708
Stacking meta epoch 2: loss=0.6886
Stacking meta epoch 3: loss=0.6409
Stacking meta epoch 4: loss=0.6158
Stacking meta epoch 5: loss=0.5997
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2592
Stacking meta epoch 2: loss=1.2428
Stacking meta epoch 3: loss=1.2403
Stacking meta epoch 4: loss=1.2385
Stacking meta epoch 5: loss=1.2372
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      77.05
1                             deepset  ...                      81.52
2                  set_transformer_xy  ...                      77.62
3                          deepset_xy  ...                      81.09
4            set_transformer_additive  ...                      80.65
5                 deepset_xy_additive  ...                      77.29
6                        adaboost_all  ...                      81.57
7            adaboost_set_transformer  ...                      80.37
8                    adaboost_deepset  ...                      80.56
9               stacking_ensemble_all  ...                      83.64
10  stacking_ensemble_set_transformer  ...                      82.63
11          stacking_ensemble_deepset  ...                      82.72

[12 rows x 5 columns]
------------------------------------iteration no 18------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7423
Epoch 02 — loss: 1.6078
Epoch 03 — loss: 1.5456
Epoch 04 — loss: 1.4943
Epoch 05 — loss: 1.4434
Epoch 06 — loss: 1.4048
Epoch 07 — loss: 1.3522
Epoch 08 — loss: 1.3173
Epoch 09 — loss: 1.2794
Epoch 10 — loss: 1.2403
Epoch 11 — loss: 1.2067
Epoch 12 — loss: 1.1559
Epoch 13 — loss: 1.1194
Epoch 14 — loss: 1.0705
Epoch 15 — loss: 1.0343
Epoch 16 — loss: 0.9989
Epoch 17 — loss: 0.9459
Epoch 18 — loss: 0.9079
Epoch 19 — loss: 0.8653
Epoch 20 — loss: 0.8479
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8769
Epoch 02 — loss: 1.6064
Epoch 03 — loss: 1.5432
Epoch 04 — loss: 1.5196
Epoch 05 — loss: 1.4957
Epoch 06 — loss: 1.4819
Epoch 07 — loss: 1.4675
Epoch 08 — loss: 1.4731
Epoch 09 — loss: 1.4667
Epoch 10 — loss: 1.4644
Epoch 11 — loss: 1.4546
Epoch 12 — loss: 1.4566
Epoch 13 — loss: 1.4550
Epoch 14 — loss: 1.4457
Epoch 15 — loss: 1.4489
Epoch 16 — loss: 1.4444
Epoch 17 — loss: 1.4481
Epoch 18 — loss: 1.4415
Epoch 19 — loss: 1.4442
Epoch 20 — loss: 1.4387
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7262
Epoch 02 — loss: 1.5952
Epoch 03 — loss: 1.5395
Epoch 04 — loss: 1.4900
Epoch 05 — loss: 1.4600
Epoch 06 — loss: 1.4069
Epoch 07 — loss: 1.3772
Epoch 08 — loss: 1.3338
Epoch 09 — loss: 1.2889
Epoch 10 — loss: 1.2488
Epoch 11 — loss: 1.2060
Epoch 12 — loss: 1.1647
Epoch 13 — loss: 1.1245
Epoch 14 — loss: 1.0790
Epoch 15 — loss: 1.0365
Epoch 16 — loss: 0.9881
Epoch 17 — loss: 0.9594
Epoch 18 — loss: 0.9110
Epoch 19 — loss: 0.8816
Epoch 20 — loss: 0.8380
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8342
Epoch 02 — loss: 1.5712
Epoch 03 — loss: 1.5234
Epoch 04 — loss: 1.5025
Epoch 05 — loss: 1.4895
Epoch 06 — loss: 1.4717
Epoch 07 — loss: 1.4694
Epoch 08 — loss: 1.4605
Epoch 09 — loss: 1.4590
Epoch 10 — loss: 1.4625
Epoch 11 — loss: 1.4513
Epoch 12 — loss: 1.4553
Epoch 13 — loss: 1.4549
Epoch 14 — loss: 1.4487
Epoch 15 — loss: 1.4500
Epoch 16 — loss: 1.4501
Epoch 17 — loss: 1.4413
Epoch 18 — loss: 1.4417
Epoch 19 — loss: 1.4409
Epoch 20 — loss: 1.4363
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7125
Epoch 02 — loss: 1.6284
Epoch 03 — loss: 1.5845
Epoch 04 — loss: 1.5662
Epoch 05 — loss: 1.5383
Epoch 06 — loss: 1.5231
Epoch 07 — loss: 1.4982
Epoch 08 — loss: 1.4844
Epoch 09 — loss: 1.4569
Epoch 10 — loss: 1.4374
Epoch 11 — loss: 1.4239
Epoch 12 — loss: 1.3778
Epoch 13 — loss: 1.3597
Epoch 14 — loss: 1.3434
Epoch 15 — loss: 1.3119
Epoch 16 — loss: 1.2836
Epoch 17 — loss: 1.2550
Epoch 18 — loss: 1.2447
Epoch 19 — loss: 1.2049
Epoch 20 — loss: 1.1668
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8228
Epoch 02 — loss: 1.6231
Epoch 03 — loss: 1.5793
Epoch 04 — loss: 1.5473
Epoch 05 — loss: 1.5220
Epoch 06 — loss: 1.5084
Epoch 07 — loss: 1.4981
Epoch 08 — loss: 1.4853
Epoch 09 — loss: 1.4811
Epoch 10 — loss: 1.4740
Epoch 11 — loss: 1.4708
Epoch 12 — loss: 1.4583
Epoch 13 — loss: 1.4636
Epoch 14 — loss: 1.4615
Epoch 15 — loss: 1.4643
Epoch 16 — loss: 1.4587
Epoch 17 — loss: 1.4462
Epoch 18 — loss: 1.4455
Epoch 19 — loss: 1.4506
Epoch 20 — loss: 1.4478
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4378
Epoch 02 — loss: 1.3154
Epoch 03 — loss: 1.2594/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Stage 1: Error=0.5136, Alpha=1.7374
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7950
Epoch 02 — loss: 1.6149
Epoch 03 — loss: 1.5329
Stage 2: Error=0.6552, Alpha=1.1498
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6834
Epoch 02 — loss: 1.5660
Epoch 03 — loss: 1.4857
Stage 3: Error=0.5808, Alpha=1.4657
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8463
Epoch 02 — loss: 1.7594
Epoch 03 — loss: 1.6937
Stage 4: Error=0.7368, Alpha=0.7625
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8052
Epoch 02 — loss: 1.7253
Epoch 03 — loss: 1.6603
Stage 5: Error=0.6813, Alpha=1.0318
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4523
Epoch 02 — loss: 1.3651
Epoch 03 — loss: 1.2973
Stage 1: Error=0.5072, Alpha=1.7629
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6315
Epoch 02 — loss: 1.5043
Epoch 03 — loss: 1.4278
Stage 2: Error=0.5888, Alpha=1.4329
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7258
Epoch 02 — loss: 1.6491
Epoch 03 — loss: 1.6058
Stage 3: Error=0.6804, Alpha=1.0360
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7585
Epoch 02 — loss: 1.6351
Epoch 03 — loss: 1.5482
Stage 4: Error=0.5951, Alpha=1.4066
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7764
Epoch 02 — loss: 1.6166
Epoch 03 — loss: 1.4992
Stage 5: Error=0.5673, Alpha=1.5210
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6319
Epoch 02 — loss: 1.3617
Epoch 03 — loss: 1.3147
Stage 1: Error=0.5255, Alpha=1.6896
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7863
Epoch 02 — loss: 1.6139
Epoch 03 — loss: 1.5300
Stage 2: Error=0.6940, Alpha=0.9731
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7771
Epoch 02 — loss: 1.6629
Epoch 03 — loss: 1.6296
Stage 3: Error=0.6725, Alpha=1.0723
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8638
Epoch 02 — loss: 1.8064
Epoch 03 — loss: 1.6991
Stage 4: Error=0.7656, Alpha=0.6082
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8629
Epoch 02 — loss: 1.7806
Epoch 03 — loss: 1.7094
Stage 5: Error=0.7542, Alpha=0.6704
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8950
Stacking meta epoch 2: loss=0.6960
Stacking meta epoch 3: loss=0.6433
Stacking meta epoch 4: loss=0.6164
Stacking meta epoch 5: loss=0.5992
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8964
Stacking meta epoch 2: loss=0.6986
Stacking meta epoch 3: loss=0.6464
Stacking meta epoch 4: loss=0.6182
Stacking meta epoch 5: loss=0.5998
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2618
Stacking meta epoch 2: loss=1.2439
Stacking meta epoch 3: loss=1.2405
Stacking meta epoch 4: loss=1.2384
Stacking meta epoch 5: loss=1.2368
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      79.45
1                             deepset  ...                      80.90
2                  set_transformer_xy  ...                      78.78
3                          deepset_xy  ...                      78.25
4            set_transformer_additive  ...                      79.07
5                 deepset_xy_additive  ...                      78.97
6                        adaboost_all  ...                      80.99
7            adaboost_set_transformer  ...                      81.23
8                    adaboost_deepset  ...                      81.76
9               stacking_ensemble_all  ...                      83.21
10  stacking_ensemble_set_transformer  ...                      82.34
11          stacking_ensemble_deepset  ...                      82.68

[12 rows x 5 columns]
------------------------------------iteration no 19------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7405
Epoch 02 — loss: 1.6099
Epoch 03 — loss: 1.5524
Epoch 04 — loss: 1.5010
Epoch 05 — loss: 1.4528
Epoch 06 — loss: 1.4072
Epoch 07 — loss: 1.3799
Epoch 08 — loss: 1.3289
Epoch 09 — loss: 1.2885
Epoch 10 — loss: 1.2318
Epoch 11 — loss: 1.1992
Epoch 12 — loss: 1.1587
Epoch 13 — loss: 1.1150
Epoch 14 — loss: 1.0761
Epoch 15 — loss: 1.0214
Epoch 16 — loss: 0.9933
Epoch 17 — loss: 0.9366
Epoch 18 — loss: 0.9031
Epoch 19 — loss: 0.8785
Epoch 20 — loss: 0.8296
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8731
Epoch 02 — loss: 1.5987
Epoch 03 — loss: 1.5288
Epoch 04 — loss: 1.5001
Epoch 05 — loss: 1.4914
Epoch 06 — loss: 1.4802
Epoch 07 — loss: 1.4743
Epoch 08 — loss: 1.4695
Epoch 09 — loss: 1.4714
Epoch 10 — loss: 1.4634
Epoch 11 — loss: 1.4566
Epoch 12 — loss: 1.4594
Epoch 13 — loss: 1.4564
Epoch 14 — loss: 1.4520
Epoch 15 — loss: 1.4568
Epoch 16 — loss: 1.4472
Epoch 17 — loss: 1.4442
Epoch 18 — loss: 1.4484
Epoch 19 — loss: 1.4457
Epoch 20 — loss: 1.4397
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7314
Epoch 02 — loss: 1.5722
Epoch 03 — loss: 1.5196
Epoch 04 — loss: 1.4872
Epoch 05 — loss: 1.4395
Epoch 06 — loss: 1.3994
Epoch 07 — loss: 1.3673
Epoch 08 — loss: 1.3310
Epoch 09 — loss: 1.2862
Epoch 10 — loss: 1.2408
Epoch 11 — loss: 1.1988
Epoch 12 — loss: 1.1600
Epoch 13 — loss: 1.1093
Epoch 14 — loss: 1.0686
Epoch 15 — loss: 1.0304
Epoch 16 — loss: 0.9837/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 17 — loss: 0.9445
Epoch 18 — loss: 0.9011
Epoch 19 — loss: 0.8518
Epoch 20 — loss: 0.8356
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8386
Epoch 02 — loss: 1.5845
Epoch 03 — loss: 1.5330
Epoch 04 — loss: 1.5049
Epoch 05 — loss: 1.4999
Epoch 06 — loss: 1.4878
Epoch 07 — loss: 1.4801
Epoch 08 — loss: 1.4690
Epoch 09 — loss: 1.4689
Epoch 10 — loss: 1.4617
Epoch 11 — loss: 1.4654
Epoch 12 — loss: 1.4587
Epoch 13 — loss: 1.4620
Epoch 14 — loss: 1.4598
Epoch 15 — loss: 1.4469
Epoch 16 — loss: 1.4555
Epoch 17 — loss: 1.4425
Epoch 18 — loss: 1.4416
Epoch 19 — loss: 1.4421
Epoch 20 — loss: 1.4436
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7199
Epoch 02 — loss: 1.6335
Epoch 03 — loss: 1.5856
Epoch 04 — loss: 1.5514
Epoch 05 — loss: 1.5213
Epoch 06 — loss: 1.4953
Epoch 07 — loss: 1.4672
Epoch 08 — loss: 1.4565
Epoch 09 — loss: 1.4285
Epoch 10 — loss: 1.3966
Epoch 11 — loss: 1.3731
Epoch 12 — loss: 1.3406
Epoch 13 — loss: 1.3326
Epoch 14 — loss: 1.2896
Epoch 15 — loss: 1.2637
Epoch 16 — loss: 1.2345
Epoch 17 — loss: 1.2161
Epoch 18 — loss: 1.1802
Epoch 19 — loss: 1.1400
Epoch 20 — loss: 1.1173
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8140
Epoch 02 — loss: 1.6153
Epoch 03 — loss: 1.5672
Epoch 04 — loss: 1.5337
Epoch 05 — loss: 1.5134
Epoch 06 — loss: 1.5072
Epoch 07 — loss: 1.4919
Epoch 08 — loss: 1.4900
Epoch 09 — loss: 1.4798
Epoch 10 — loss: 1.4654
Epoch 11 — loss: 1.4626
Epoch 12 — loss: 1.4674
Epoch 13 — loss: 1.4676
Epoch 14 — loss: 1.4550
Epoch 15 — loss: 1.4496
Epoch 16 — loss: 1.4529
Epoch 17 — loss: 1.4463
Epoch 18 — loss: 1.4501
Epoch 19 — loss: 1.4388
Epoch 20 — loss: 1.4469
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4395
Epoch 02 — loss: 1.3282
Epoch 03 — loss: 1.2776
Stage 1: Error=0.5075, Alpha=1.7619
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7712
Epoch 02 — loss: 1.6053
Epoch 03 — loss: 1.5247
Stage 2: Error=0.6567, Alpha=1.1430
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6731
Epoch 02 — loss: 1.5676
Epoch 03 — loss: 1.4954
Stage 3: Error=0.5776, Alpha=1.4786
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8533
Epoch 02 — loss: 1.7609
Epoch 03 — loss: 1.7043
Stage 4: Error=0.7797, Alpha=0.5277
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8055
Epoch 02 — loss: 1.7303
Epoch 03 — loss: 1.6852
Stage 5: Error=0.7058, Alpha=0.9165
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4490
Epoch 02 — loss: 1.3280
Epoch 03 — loss: 1.2545
Stage 1: Error=0.5113, Alpha=1.7465
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6468
Epoch 02 — loss: 1.5199
Epoch 03 — loss: 1.4560
Stage 2: Error=0.5930, Alpha=1.4155
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6958
Epoch 02 — loss: 1.6316
Epoch 03 — loss: 1.5956
Stage 3: Error=0.6463, Alpha=1.1889
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7699
Epoch 02 — loss: 1.6527
Epoch 03 — loss: 1.5479
Stage 4: Error=0.5919, Alpha=1.4198
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7999
Epoch 02 — loss: 1.6619
Epoch 03 — loss: 1.5168
Stage 5: Error=0.5701, Alpha=1.5095
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6247
Epoch 02 — loss: 1.3828
Epoch 03 — loss: 1.3169
Stage 1: Error=0.5292, Alpha=1.6747
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7998
Epoch 02 — loss: 1.5946
Epoch 03 — loss: 1.5144
Stage 2: Error=0.6183, Alpha=1.3093
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7840
Epoch 02 — loss: 1.6654
Epoch 03 — loss: 1.6195
Stage 3: Error=0.6425, Alpha=1.2055
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8761
Epoch 02 — loss: 1.8132
Epoch 03 — loss: 1.7022
Stage 4: Error=0.7140, Alpha=0.8769
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8846
Epoch 02 — loss: 1.8332
Epoch 03 — loss: 1.7679
Stage 5: Error=0.7525, Alpha=0.6796
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8829
Stacking meta epoch 2: loss=0.6821
Stacking meta epoch 3: loss=0.6308
Stacking meta epoch 4: loss=0.6043
Stacking meta epoch 5: loss=0.5873
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8753
Stacking meta epoch 2: loss=0.6828
Stacking meta epoch 3: loss=0.6333
Stacking meta epoch 4: loss=0.6060
Stacking meta epoch 5: loss=0.5881
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2573
Stacking meta epoch 2: loss=1.2440
Stacking meta epoch 3: loss=1.2410
Stacking meta epoch 4: loss=1.2390
Stacking meta epoch 5: loss=1.2375
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      79.40
1                             deepset  ...                      81.23
2                  set_transformer_xy  ...                      76.85
3                          deepset_xy  ...                      78.54
4            set_transformer_additive  ...                      80.37
5                 deepset_xy_additive  ...                      78.63
6                        adaboost_all  ...                      80.41
7            adaboost_set_transformer  ...                      80.99
8                    adaboost_deepset  ...                      81.38
9               stacking_ensemble_all  ...                      82.92
10  stacking_ensemble_set_transformer  ...                      82.87
11          stacking_ensemble_deepset  ...                      82.63

[12 rows x 5 columns]
------------------------------------iteration no 20------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7312
Epoch 02 — loss: 1.6034
Epoch 03 — loss: 1.5604
Epoch 04 — loss: 1.5137
Epoch 05 — loss: 1.4815
Epoch 06 — loss: 1.4361
Epoch 07 — loss: 1.3972
Epoch 08 — loss: 1.3429
Epoch 09 — loss: 1.3083
Epoch 10 — loss: 1.2642
Epoch 11 — loss: 1.2213
Epoch 12 — loss: 1.1605
Epoch 13 — loss: 1.1325
Epoch 14 — loss: 1.0832
Epoch 15 — loss: 1.0385
Epoch 16 — loss: 0.9901
Epoch 17 — loss: 0.9497
Epoch 18 — loss: 0.9112
Epoch 19 — loss: 0.8566
Epoch 20 — loss: 0.8253
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8704
Epoch 02 — loss: 1.5888
Epoch 03 — loss: 1.5339
Epoch 04 — loss: 1.5028
Epoch 05 — loss: 1.4927
Epoch 06 — loss: 1.4857
Epoch 07 — loss: 1.4732
Epoch 08 — loss: 1.4760
Epoch 09 — loss: 1.4648
Epoch 10 — loss: 1.4626
Epoch 11 — loss: 1.4606
Epoch 12 — loss: 1.4548
Epoch 13 — loss: 1.4506
Epoch 14 — loss: 1.4563
Epoch 15 — loss: 1.4449
Epoch 16 — loss: 1.4412
Epoch 17 — loss: 1.4474
Epoch 18 — loss: 1.4388
Epoch 19 — loss: 1.4377
Epoch 20 — loss: 1.4458
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7017
Epoch 02 — loss: 1.5636
Epoch 03 — loss: 1.5210
Epoch 04 — loss: 1.4724
Epoch 05 — loss: 1.4366
Epoch 06 — loss: 1.3987
Epoch 07 — loss: 1.3574
Epoch 08 — loss: 1.3207
Epoch 09 — loss: 1.2788
Epoch 10 — loss: 1.2317
Epoch 11 — loss: 1.2074
Epoch 12 — loss: 1.1550
Epoch 13 — loss: 1.1173
Epoch 14 — loss: 1.0777
Epoch 15 — loss: 1.0332
Epoch 16 — loss: 0.9899
Epoch 17 — loss: 0.9512
Epoch 18 — loss: 0.9109
Epoch 19 — loss: 0.8764
Epoch 20 — loss: 0.8403
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8602
Epoch 02 — loss: 1.5894
Epoch 03 — loss: 1.5354
Epoch 04 — loss: 1.5110
Epoch 05 — loss: 1.4954
Epoch 06 — loss: 1.4844
Epoch 07 — loss: 1.4765
Epoch 08 — loss: 1.4724
Epoch 09 — loss: 1.4633
Epoch 10 — loss: 1.4579
Epoch 11 — loss: 1.4596
Epoch 12 — loss: 1.4586
Epoch 13 — loss: 1.4609
Epoch 14 — loss: 1.4477
Epoch 15 — loss: 1.4485
Epoch 16 — loss: 1.4481
Epoch 17 — loss: 1.4418
Epoch 18 — loss: 1.4370
Epoch 19 — loss: 1.4373
Epoch 20 — loss: 1.4370
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.6989
Epoch 02 — loss: 1.5867
Epoch 03 — loss: 1.5608
Epoch 04 — loss: 1.5286
Epoch 05 — loss: 1.5078
Epoch 06 — loss: 1.4951
Epoch 07 — loss: 1.4727
Epoch 08 — loss: 1.4494
Epoch 09 — loss: 1.4383
Epoch 10 — loss: 1.3991
Epoch 11 — loss: 1.3871
Epoch 12 — loss: 1.3566
Epoch 13 — loss: 1.3405
Epoch 14 — loss: 1.3059
Epoch 15 — loss: 1.2862
Epoch 16 — loss: 1.2537
Epoch 17 — loss: 1.2292
Epoch 18 — loss: 1.1930
Epoch 19 — loss: 1.1595
Epoch 20 — loss: 1.1332
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8078
Epoch 02 — loss: 1.6118
Epoch 03 — loss: 1.5614
Epoch 04 — loss: 1.5324
Epoch 05 — loss: 1.5138
Epoch 06 — loss: 1.4953
Epoch 07 — loss: 1.4952
Epoch 08 — loss: 1.4819
Epoch 09 — loss: 1.4803
Epoch 10 — loss: 1.4724
Epoch 11 — loss: 1.4641
Epoch 12 — loss: 1.4600
Epoch 13 — loss: 1.4567
Epoch 14 — loss: 1.4599
Epoch 15 — loss: 1.4507
Epoch 16 — loss: 1.4454
Epoch 17 — loss: 1.4479
Epoch 18 — loss: 1.4498
Epoch 19 — loss: 1.4501
Epoch 20 — loss: 1.4419
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4516
Epoch 02 — loss: 1.3409
Epoch 03 — loss: 1.2628
Stage 1: Error=0.5093, Alpha=1.7547
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7720
Epoch 02 — loss: 1.5968
Epoch 03 — loss: 1.5316
Stage 2: Error=0.6529, Alpha=1.1601
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6770
Epoch 02 — loss: 1.5686
Epoch 03 — loss: 1.4751
Stage 3: Error=0.5827, Alpha=1.4580
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8606
Epoch 02 — loss: 1.7853
Epoch 03 — loss: 1.6995
Stage 4: Error=0.7277, Alpha=0.8087
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8147
Epoch 02 — loss: 1.7471
Epoch 03 — loss: 1.6897
Stage 5: Error=0.6787, Alpha=1.0440
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4284
Epoch 02 — loss: 1.2981
Epoch 03 — loss: 1.2452
Stage 1: Error=0.4993, Alpha=1.7946
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6283
Epoch 02 — loss: 1.5080
Epoch 03 — loss: 1.4449
Stage 2: Error=0.6005, Alpha=1.3844
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6946
Epoch 02 — loss: 1.6295
Epoch 03 — loss: 1.6028
Stage 3: Error=0.6732, Alpha=1.0689
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7671
Epoch 02 — loss: 1.6145
Epoch 03 — loss: 1.5307
Stage 4: Error=0.5708, Alpha=1.5065
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7777
Epoch 02 — loss: 1.6381
Epoch 03 — loss: 1.5240
Stage 5: Error=0.5668, Alpha=1.5230
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6141
Epoch 02 — loss: 1.3627
Epoch 03 — loss: 1.3008
Stage 1: Error=0.5219, Alpha=1.7041
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7963
Epoch 02 — loss: 1.6110
Epoch 03 — loss: 1.5195
Stage 2: Error=0.6302, Alpha=1.2589
--- Boosting Stage 3/5 (deepset_xy_additive) ---/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7920
Epoch 02 — loss: 1.6667
Epoch 03 — loss: 1.6237
Stage 3: Error=0.6477, Alpha=1.1829
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8707
Epoch 02 — loss: 1.7856
Epoch 03 — loss: 1.7111
Stage 4: Error=0.7420, Alpha=0.7351
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8915
Epoch 02 — loss: 1.8220
Epoch 03 — loss: 1.7554
Stage 5: Error=0.7954, Alpha=0.4339
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8864
Stacking meta epoch 2: loss=0.6899
Stacking meta epoch 3: loss=0.6361
Stacking meta epoch 4: loss=0.6085
Stacking meta epoch 5: loss=0.5908
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8824
Stacking meta epoch 2: loss=0.6904
Stacking meta epoch 3: loss=0.6370
Stacking meta epoch 4: loss=0.6083
Stacking meta epoch 5: loss=0.5899
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2594
Stacking meta epoch 2: loss=1.2428
Stacking meta epoch 3: loss=1.2394
Stacking meta epoch 4: loss=1.2371
Stacking meta epoch 5: loss=1.2355
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      78.30
1                             deepset  ...                      81.18
2                  set_transformer_xy  ...                      81.81
3                          deepset_xy  ...                      80.32
4            set_transformer_additive  ...                      82.10
5                 deepset_xy_additive  ...                      79.64
6                        adaboost_all  ...                      79.55
7            adaboost_set_transformer  ...                      82.53
8                    adaboost_deepset  ...                      80.99
9               stacking_ensemble_all  ...                      83.97
10  stacking_ensemble_set_transformer  ...                      83.83
11          stacking_ensemble_deepset  ...                      82.68

[12 rows x 5 columns]
------------------------------------iteration no 21------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7228
Epoch 02 — loss: 1.6055
Epoch 03 — loss: 1.5603
Epoch 04 — loss: 1.5063
Epoch 05 — loss: 1.4670
Epoch 06 — loss: 1.4217
Epoch 07 — loss: 1.3803
Epoch 08 — loss: 1.3354
Epoch 09 — loss: 1.2954
Epoch 10 — loss: 1.2581
Epoch 11 — loss: 1.2088
Epoch 12 — loss: 1.1729
Epoch 13 — loss: 1.1319
Epoch 14 — loss: 1.0891
Epoch 15 — loss: 1.0410
Epoch 16 — loss: 0.9982
Epoch 17 — loss: 0.9493
Epoch 18 — loss: 0.9115
Epoch 19 — loss: 0.8759
Epoch 20 — loss: 0.8353
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8783
Epoch 02 — loss: 1.5923
Epoch 03 — loss: 1.5313
Epoch 04 — loss: 1.5057
Epoch 05 — loss: 1.4890
Epoch 06 — loss: 1.4845
Epoch 07 — loss: 1.4701
Epoch 08 — loss: 1.4753
Epoch 09 — loss: 1.4635
Epoch 10 — loss: 1.4637
Epoch 11 — loss: 1.4617
Epoch 12 — loss: 1.4546
Epoch 13 — loss: 1.4491
Epoch 14 — loss: 1.4509
Epoch 15 — loss: 1.4485
Epoch 16 — loss: 1.4482
Epoch 17 — loss: 1.4420
Epoch 18 — loss: 1.4422
Epoch 19 — loss: 1.4476
Epoch 20 — loss: 1.4402
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.6944
Epoch 02 — loss: 1.5442
Epoch 03 — loss: 1.5024
Epoch 04 — loss: 1.4554
Epoch 05 — loss: 1.4112
Epoch 06 — loss: 1.3747
Epoch 07 — loss: 1.3461
Epoch 08 — loss: 1.3066
Epoch 09 — loss: 1.2720
Epoch 10 — loss: 1.2403
Epoch 11 — loss: 1.1904
Epoch 12 — loss: 1.1529
Epoch 13 — loss: 1.1331
Epoch 14 — loss: 1.0782
Epoch 15 — loss: 1.0321
Epoch 16 — loss: 0.9978
Epoch 17 — loss: 0.9534
Epoch 18 — loss: 0.9255
Epoch 19 — loss: 0.8818
Epoch 20 — loss: 0.8340
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8731
Epoch 02 — loss: 1.5859
Epoch 03 — loss: 1.5294
Epoch 04 — loss: 1.5081
Epoch 05 — loss: 1.4881
Epoch 06 — loss: 1.4778
Epoch 07 — loss: 1.4772
Epoch 08 — loss: 1.4606
Epoch 09 — loss: 1.4636
Epoch 10 — loss: 1.4569
Epoch 11 — loss: 1.4508
Epoch 12 — loss: 1.4472
Epoch 13 — loss: 1.4415
Epoch 14 — loss: 1.4524
Epoch 15 — loss: 1.4491
Epoch 16 — loss: 1.4453
Epoch 17 — loss: 1.4395
Epoch 18 — loss: 1.4400
Epoch 19 — loss: 1.4369
Epoch 20 — loss: 1.4458
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7125
Epoch 02 — loss: 1.6265
Epoch 03 — loss: 1.5805
Epoch 04 — loss: 1.5487
Epoch 05 — loss: 1.5261
Epoch 06 — loss: 1.5081
Epoch 07 — loss: 1.4833
Epoch 08 — loss: 1.4567
Epoch 09 — loss: 1.4272
Epoch 10 — loss: 1.4150
Epoch 11 — loss: 1.3940
Epoch 12 — loss: 1.3701
Epoch 13 — loss: 1.3505
Epoch 14 — loss: 1.3237
Epoch 15 — loss: 1.3118
Epoch 16 — loss: 1.2777
Epoch 17 — loss: 1.2495
Epoch 18 — loss: 1.2213
Epoch 19 — loss: 1.1994
Epoch 20 — loss: 1.1632
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7998
Epoch 02 — loss: 1.6091
Epoch 03 — loss: 1.5660
Epoch 04 — loss: 1.5398
Epoch 05 — loss: 1.5300
Epoch 06 — loss: 1.5133
Epoch 07 — loss: 1.4997
Epoch 08 — loss: 1.4891
Epoch 09 — loss: 1.4877
Epoch 10 — loss: 1.4801
Epoch 11 — loss: 1.4767
Epoch 12 — loss: 1.4717
Epoch 13 — loss: 1.4693
Epoch 14 — loss: 1.4577
Epoch 15 — loss: 1.4579
Epoch 16 — loss: 1.4425
Epoch 17 — loss: 1.4540
Epoch 18 — loss: 1.4541
Epoch 19 — loss: 1.4468
Epoch 20 — loss: 1.4516
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4379
Epoch 02 — loss: 1.3035
Epoch 03 — loss: 1.2716/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Stage 1: Error=0.4951, Alpha=1.8115
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8145
Epoch 02 — loss: 1.6266
Epoch 03 — loss: 1.5460
Stage 2: Error=0.6785, Alpha=1.0449
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6829
Epoch 02 — loss: 1.5476
Epoch 03 — loss: 1.4751
Stage 3: Error=0.5731, Alpha=1.4971
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8656
Epoch 02 — loss: 1.7608
Epoch 03 — loss: 1.6916
Stage 4: Error=0.7542, Alpha=0.6707
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8020
Epoch 02 — loss: 1.7295
Epoch 03 — loss: 1.6829
Stage 5: Error=0.6888, Alpha=0.9972
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4504
Epoch 02 — loss: 1.3338
Epoch 03 — loss: 1.2757
Stage 1: Error=0.4996, Alpha=1.7932
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6202
Epoch 02 — loss: 1.5004
Epoch 03 — loss: 1.4311
Stage 2: Error=0.5923, Alpha=1.4183
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7340
Epoch 02 — loss: 1.6528
Epoch 03 — loss: 1.6181
Stage 3: Error=0.6610, Alpha=1.1238
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7687
Epoch 02 — loss: 1.6511
Epoch 03 — loss: 1.5479
Stage 4: Error=0.6052, Alpha=1.3646
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7938
Epoch 02 — loss: 1.6464
Epoch 03 — loss: 1.5155
Stage 5: Error=0.5756, Alpha=1.4869
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6239
Epoch 02 — loss: 1.3716
Epoch 03 — loss: 1.3018
Stage 1: Error=0.5266, Alpha=1.6853
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7717
Epoch 02 — loss: 1.5736
Epoch 03 — loss: 1.5114
Stage 2: Error=0.6891, Alpha=0.9958
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7493
Epoch 02 — loss: 1.6213
Epoch 03 — loss: 1.5707
Stage 3: Error=0.6377, Alpha=1.2262
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8621
Epoch 02 — loss: 1.7733
Epoch 03 — loss: 1.7048
Stage 4: Error=0.7522, Alpha=0.6816
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8755
Epoch 02 — loss: 1.8059
Epoch 03 — loss: 1.7433
Stage 5: Error=0.7818, Alpha=0.5155
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8918
Stacking meta epoch 2: loss=0.7032
Stacking meta epoch 3: loss=0.6534
Stacking meta epoch 4: loss=0.6275
Stacking meta epoch 5: loss=0.6108
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8800
Stacking meta epoch 2: loss=0.7012
Stacking meta epoch 3: loss=0.6529
Stacking meta epoch 4: loss=0.6265
Stacking meta epoch 5: loss=0.6090
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2616
Stacking meta epoch 2: loss=1.2439
Stacking meta epoch 3: loss=1.2409
Stacking meta epoch 4: loss=1.2390
Stacking meta epoch 5: loss=1.2375
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      78.73
1                             deepset  ...                      83.11
2                  set_transformer_xy  ...                      80.27
3                          deepset_xy  ...                      81.47
4            set_transformer_additive  ...                      76.80
5                 deepset_xy_additive  ...                      78.97
6                        adaboost_all  ...                      82.00
7            adaboost_set_transformer  ...                      81.76
8                    adaboost_deepset  ...                      82.29
9               stacking_ensemble_all  ...                      83.83
10  stacking_ensemble_set_transformer  ...                      83.45
11          stacking_ensemble_deepset  ...                      82.53

[12 rows x 5 columns]
------------------------------------iteration no 22------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7229
Epoch 02 — loss: 1.6178
Epoch 03 — loss: 1.5751
Epoch 04 — loss: 1.5261
Epoch 05 — loss: 1.4725
Epoch 06 — loss: 1.4325
Epoch 07 — loss: 1.3783
Epoch 08 — loss: 1.3388
Epoch 09 — loss: 1.2840
Epoch 10 — loss: 1.2431
Epoch 11 — loss: 1.1924
Epoch 12 — loss: 1.1617
Epoch 13 — loss: 1.1110
Epoch 14 — loss: 1.0717
Epoch 15 — loss: 1.0277
Epoch 16 — loss: 0.9878
Epoch 17 — loss: 0.9458
Epoch 18 — loss: 0.9015
Epoch 19 — loss: 0.8901
Epoch 20 — loss: 0.8367
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8517
Epoch 02 — loss: 1.5896
Epoch 03 — loss: 1.5269
Epoch 04 — loss: 1.5009
Epoch 05 — loss: 1.4984
Epoch 06 — loss: 1.4796
Epoch 07 — loss: 1.4723
Epoch 08 — loss: 1.4690
Epoch 09 — loss: 1.4589
Epoch 10 — loss: 1.4658
Epoch 11 — loss: 1.4568
Epoch 12 — loss: 1.4520
Epoch 13 — loss: 1.4536
Epoch 14 — loss: 1.4510
Epoch 15 — loss: 1.4482
Epoch 16 — loss: 1.4414
Epoch 17 — loss: 1.4484
Epoch 18 — loss: 1.4399
Epoch 19 — loss: 1.4476
Epoch 20 — loss: 1.4400
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.6933
Epoch 02 — loss: 1.5602
Epoch 03 — loss: 1.5098
Epoch 04 — loss: 1.4662
Epoch 05 — loss: 1.4233
Epoch 06 — loss: 1.3814
Epoch 07 — loss: 1.3477
Epoch 08 — loss: 1.3061
Epoch 09 — loss: 1.2570
Epoch 10 — loss: 1.2311
Epoch 11 — loss: 1.1729
Epoch 12 — loss: 1.1326
Epoch 13 — loss: 1.0874
Epoch 14 — loss: 1.0475
Epoch 15 — loss: 0.9893
Epoch 16 — loss: 0.9573/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 17 — loss: 0.9068
Epoch 18 — loss: 0.8673
Epoch 19 — loss: 0.8252
Epoch 20 — loss: 0.7946
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8543
Epoch 02 — loss: 1.5733
Epoch 03 — loss: 1.5290
Epoch 04 — loss: 1.4987
Epoch 05 — loss: 1.4830
Epoch 06 — loss: 1.4758
Epoch 07 — loss: 1.4695
Epoch 08 — loss: 1.4699
Epoch 09 — loss: 1.4585
Epoch 10 — loss: 1.4515
Epoch 11 — loss: 1.4601
Epoch 12 — loss: 1.4478
Epoch 13 — loss: 1.4515
Epoch 14 — loss: 1.4530
Epoch 15 — loss: 1.4483
Epoch 16 — loss: 1.4485
Epoch 17 — loss: 1.4424
Epoch 18 — loss: 1.4433
Epoch 19 — loss: 1.4426
Epoch 20 — loss: 1.4351
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7333
Epoch 02 — loss: 1.6302
Epoch 03 — loss: 1.6009
Epoch 04 — loss: 1.5627
Epoch 05 — loss: 1.5428
Epoch 06 — loss: 1.5075
Epoch 07 — loss: 1.4892
Epoch 08 — loss: 1.4772
Epoch 09 — loss: 1.4342
Epoch 10 — loss: 1.4159
Epoch 11 — loss: 1.3889
Epoch 12 — loss: 1.3669
Epoch 13 — loss: 1.3398
Epoch 14 — loss: 1.3099
Epoch 15 — loss: 1.2860
Epoch 16 — loss: 1.2573
Epoch 17 — loss: 1.2294
Epoch 18 — loss: 1.2094
Epoch 19 — loss: 1.1752
Epoch 20 — loss: 1.1516
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.7812
Epoch 02 — loss: 1.6110
Epoch 03 — loss: 1.5619
Epoch 04 — loss: 1.5240
Epoch 05 — loss: 1.5163
Epoch 06 — loss: 1.5071
Epoch 07 — loss: 1.4956
Epoch 08 — loss: 1.4973
Epoch 09 — loss: 1.4823
Epoch 10 — loss: 1.4808
Epoch 11 — loss: 1.4740
Epoch 12 — loss: 1.4616
Epoch 13 — loss: 1.4580
Epoch 14 — loss: 1.4620
Epoch 15 — loss: 1.4577
Epoch 16 — loss: 1.4591
Epoch 17 — loss: 1.4564
Epoch 18 — loss: 1.4472
Epoch 19 — loss: 1.4513
Epoch 20 — loss: 1.4463
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4131
Epoch 02 — loss: 1.3219
Epoch 03 — loss: 1.2725
Stage 1: Error=0.5022, Alpha=1.7831
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8067
Epoch 02 — loss: 1.6055
Epoch 03 — loss: 1.5372
Stage 2: Error=0.6686, Alpha=1.0897
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6637
Epoch 02 — loss: 1.5300
Epoch 03 — loss: 1.4473
Stage 3: Error=0.5521, Alpha=1.5824
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8534
Epoch 02 — loss: 1.7552
Epoch 03 — loss: 1.7142
Stage 4: Error=0.7246, Alpha=0.8245
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8141
Epoch 02 — loss: 1.7664
Epoch 03 — loss: 1.7154
Stage 5: Error=0.7163, Alpha=0.8653
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4537
Epoch 02 — loss: 1.3359
Epoch 03 — loss: 1.2821
Stage 1: Error=0.5001, Alpha=1.7913
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6335
Epoch 02 — loss: 1.5083
Epoch 03 — loss: 1.4250
Stage 2: Error=0.5861, Alpha=1.4440
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7144
Epoch 02 — loss: 1.6327
Epoch 03 — loss: 1.5997
Stage 3: Error=0.6756, Alpha=1.0581
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7542
Epoch 02 — loss: 1.6583
Epoch 03 — loss: 1.5639
Stage 4: Error=0.6076, Alpha=1.3545
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7805
Epoch 02 — loss: 1.6263
Epoch 03 — loss: 1.4998
Stage 5: Error=0.5683, Alpha=1.5169
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6217
Epoch 02 — loss: 1.3522
Epoch 03 — loss: 1.3005
Stage 1: Error=0.5250, Alpha=1.6916
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7918
Epoch 02 — loss: 1.5897
Epoch 03 — loss: 1.5075
Stage 2: Error=0.6278, Alpha=1.2691
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7783
Epoch 02 — loss: 1.6634
Epoch 03 — loss: 1.6241
Stage 3: Error=0.6598, Alpha=1.1292
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8551
Epoch 02 — loss: 1.7591
Epoch 03 — loss: 1.6987
Stage 4: Error=0.7091, Alpha=0.9007
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8607
Epoch 02 — loss: 1.7973
Epoch 03 — loss: 1.7437
Stage 5: Error=0.7576, Alpha=0.6524
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8973
Stacking meta epoch 2: loss=0.6893
Stacking meta epoch 3: loss=0.6364
Stacking meta epoch 4: loss=0.6097
Stacking meta epoch 5: loss=0.5929
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8668
Stacking meta epoch 2: loss=0.6855
Stacking meta epoch 3: loss=0.6356
Stacking meta epoch 4: loss=0.6088
Stacking meta epoch 5: loss=0.5915
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2569
Stacking meta epoch 2: loss=1.2435
Stacking meta epoch 3: loss=1.2403
Stacking meta epoch 4: loss=1.2382
Stacking meta epoch 5: loss=1.2366
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      76.80
1                             deepset  ...                      81.86
2                  set_transformer_xy  ...                      78.59
3                          deepset_xy  ...                      77.91
4            set_transformer_additive  ...                      76.85
5                 deepset_xy_additive  ...                      81.23
6                        adaboost_all  ...                      80.56
7            adaboost_set_transformer  ...                      82.44
8                    adaboost_deepset  ...                      82.63
9               stacking_ensemble_all  ...                      83.30
10  stacking_ensemble_set_transformer  ...                      83.40
11          stacking_ensemble_deepset  ...                      83.06

[12 rows x 5 columns]
------------------------------------iteration no 23------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7405
Epoch 02 — loss: 1.6146
Epoch 03 — loss: 1.5678
Epoch 04 — loss: 1.5064
Epoch 05 — loss: 1.4667
Epoch 06 — loss: 1.4254
Epoch 07 — loss: 1.3945
Epoch 08 — loss: 1.3420
Epoch 09 — loss: 1.3085
Epoch 10 — loss: 1.2693
Epoch 11 — loss: 1.2237
Epoch 12 — loss: 1.1803
Epoch 13 — loss: 1.1304
Epoch 14 — loss: 1.0901
Epoch 15 — loss: 1.0499
Epoch 16 — loss: 0.9948
Epoch 17 — loss: 0.9579
Epoch 18 — loss: 0.9176
Epoch 19 — loss: 0.8769
Epoch 20 — loss: 0.8519
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.9030
Epoch 02 — loss: 1.6174
Epoch 03 — loss: 1.5299
Epoch 04 — loss: 1.5021
Epoch 05 — loss: 1.4858
Epoch 06 — loss: 1.4846
Epoch 07 — loss: 1.4771
Epoch 08 — loss: 1.4729
Epoch 09 — loss: 1.4645
Epoch 10 — loss: 1.4628
Epoch 11 — loss: 1.4574
Epoch 12 — loss: 1.4588
Epoch 13 — loss: 1.4528
Epoch 14 — loss: 1.4492
Epoch 15 — loss: 1.4503
Epoch 16 — loss: 1.4422
Epoch 17 — loss: 1.4386
Epoch 18 — loss: 1.4442
Epoch 19 — loss: 1.4404
Epoch 20 — loss: 1.4376
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7236
Epoch 02 — loss: 1.5642
Epoch 03 — loss: 1.5171
Epoch 04 — loss: 1.4720
Epoch 05 — loss: 1.4317
Epoch 06 — loss: 1.3873
Epoch 07 — loss: 1.3495
Epoch 08 — loss: 1.3037
Epoch 09 — loss: 1.2668
Epoch 10 — loss: 1.2209
Epoch 11 — loss: 1.1785
Epoch 12 — loss: 1.1450
Epoch 13 — loss: 1.0966
Epoch 14 — loss: 1.0444
Epoch 15 — loss: 1.0111
Epoch 16 — loss: 0.9586
Epoch 17 — loss: 0.9233
Epoch 18 — loss: 0.8873
Epoch 19 — loss: 0.8440
Epoch 20 — loss: 0.8101
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8730
Epoch 02 — loss: 1.5850
Epoch 03 — loss: 1.5174
Epoch 04 — loss: 1.4903
Epoch 05 — loss: 1.4836
Epoch 06 — loss: 1.4753
Epoch 07 — loss: 1.4603
Epoch 08 — loss: 1.4563
Epoch 09 — loss: 1.4605
Epoch 10 — loss: 1.4636
Epoch 11 — loss: 1.4484
Epoch 12 — loss: 1.4498
Epoch 13 — loss: 1.4512
Epoch 14 — loss: 1.4497
Epoch 15 — loss: 1.4443
Epoch 16 — loss: 1.4352
Epoch 17 — loss: 1.4456
Epoch 18 — loss: 1.4393
Epoch 19 — loss: 1.4354
Epoch 20 — loss: 1.4348
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7373
Epoch 02 — loss: 1.6158
Epoch 03 — loss: 1.5714
Epoch 04 — loss: 1.5392
Epoch 05 — loss: 1.5157
Epoch 06 — loss: 1.4866
Epoch 07 — loss: 1.4647
Epoch 08 — loss: 1.4360
Epoch 09 — loss: 1.4188
Epoch 10 — loss: 1.3877
Epoch 11 — loss: 1.3539
Epoch 12 — loss: 1.3325
Epoch 13 — loss: 1.3146
Epoch 14 — loss: 1.2794
Epoch 15 — loss: 1.2654
Epoch 16 — loss: 1.2371
Epoch 17 — loss: 1.1986
Epoch 18 — loss: 1.1796
Epoch 19 — loss: 1.1539
Epoch 20 — loss: 1.1194
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8002
Epoch 02 — loss: 1.6159
Epoch 03 — loss: 1.5677
Epoch 04 — loss: 1.5359
Epoch 05 — loss: 1.5192
Epoch 06 — loss: 1.5008
Epoch 07 — loss: 1.4925
Epoch 08 — loss: 1.4889
Epoch 09 — loss: 1.4769
Epoch 10 — loss: 1.4747
Epoch 11 — loss: 1.4625
Epoch 12 — loss: 1.4683
Epoch 13 — loss: 1.4659
Epoch 14 — loss: 1.4562
Epoch 15 — loss: 1.4542
Epoch 16 — loss: 1.4577
Epoch 17 — loss: 1.4500
Epoch 18 — loss: 1.4415
Epoch 19 — loss: 1.4464
Epoch 20 — loss: 1.4446
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4434
Epoch 02 — loss: 1.3278
Epoch 03 — loss: 1.2729
Stage 1: Error=0.5064, Alpha=1.7662
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8053
Epoch 02 — loss: 1.6131
Epoch 03 — loss: 1.5496
Stage 2: Error=0.6449, Alpha=1.1951
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7053
Epoch 02 — loss: 1.5719
Epoch 03 — loss: 1.4905
Stage 3: Error=0.5830, Alpha=1.4567
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8560
Epoch 02 — loss: 1.7518
Epoch 03 — loss: 1.6972
Stage 4: Error=0.7297, Alpha=0.7984
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8195
Epoch 02 — loss: 1.7478
Epoch 03 — loss: 1.6812
Stage 5: Error=0.6998, Alpha=0.9456
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4577
Epoch 02 — loss: 1.3511
Epoch 03 — loss: 1.3011
Stage 1: Error=0.5122, Alpha=1.7431
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6222
Epoch 02 — loss: 1.5132
Epoch 03 — loss: 1.4390
Stage 2: Error=0.5888, Alpha=1.4327
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7103
Epoch 02 — loss: 1.6220
Epoch 03 — loss: 1.5912
Stage 3: Error=0.6941, Alpha=0.9726
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7515
Epoch 02 — loss: 1.6145
Epoch 03 — loss: 1.5212
Stage 4: Error=0.5931, Alpha=1.4149
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7896
Epoch 02 — loss: 1.6417
Epoch 03 — loss: 1.5431
Stage 5: Error=0.5871, Alpha=1.4397
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6331
Epoch 02 — loss: 1.3591
Epoch 03 — loss: 1.3020
Stage 1: Error=0.5290, Alpha=1.6756
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7837
Epoch 02 — loss: 1.6235
Epoch 03 — loss: 1.5099
Stage 2: Error=0.6213, Alpha=1.2967
--- Boosting Stage 3/5 (deepset_xy_additive) ---/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7996
Epoch 02 — loss: 1.6773
Epoch 03 — loss: 1.6344
Stage 3: Error=0.6795, Alpha=1.0402
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8663
Epoch 02 — loss: 1.7921
Epoch 03 — loss: 1.7136
Stage 4: Error=0.7490, Alpha=0.6987
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8819
Epoch 02 — loss: 1.8229
Epoch 03 — loss: 1.7559
Stage 5: Error=0.7689, Alpha=0.5897
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8799
Stacking meta epoch 2: loss=0.6812
Stacking meta epoch 3: loss=0.6308
Stacking meta epoch 4: loss=0.6053
Stacking meta epoch 5: loss=0.5892
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8586
Stacking meta epoch 2: loss=0.6753
Stacking meta epoch 3: loss=0.6268
Stacking meta epoch 4: loss=0.6008
Stacking meta epoch 5: loss=0.5840
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2545
Stacking meta epoch 2: loss=1.2398
Stacking meta epoch 3: loss=1.2372
Stacking meta epoch 4: loss=1.2356
Stacking meta epoch 5: loss=1.2343
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      77.62
1                             deepset  ...                      80.17
2                  set_transformer_xy  ...                      77.77
3                          deepset_xy  ...                      78.06
4            set_transformer_additive  ...                      75.51
5                 deepset_xy_additive  ...                      81.86
6                        adaboost_all  ...                      80.37
7            adaboost_set_transformer  ...                      80.94
8                    adaboost_deepset  ...                      82.34
9               stacking_ensemble_all  ...                      82.77
10  stacking_ensemble_set_transformer  ...                      82.24
11          stacking_ensemble_deepset  ...                      82.96

[12 rows x 5 columns]
------------------------------------iteration no 24------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7506
Epoch 02 — loss: 1.6144
Epoch 03 — loss: 1.5542
Epoch 04 — loss: 1.4997
Epoch 05 — loss: 1.4512
Epoch 06 — loss: 1.4087
Epoch 07 — loss: 1.3623
Epoch 08 — loss: 1.3219
Epoch 09 — loss: 1.2809
Epoch 10 — loss: 1.2402
Epoch 11 — loss: 1.1958
Epoch 12 — loss: 1.1474
Epoch 13 — loss: 1.0998
Epoch 14 — loss: 1.0606
Epoch 15 — loss: 1.0168
Epoch 16 — loss: 0.9862
Epoch 17 — loss: 0.9331
Epoch 18 — loss: 0.8973
Epoch 19 — loss: 0.8543
Epoch 20 — loss: 0.8232
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8926
Epoch 02 — loss: 1.5998
Epoch 03 — loss: 1.5305
Epoch 04 — loss: 1.4991
Epoch 05 — loss: 1.4911
Epoch 06 — loss: 1.4799
Epoch 07 — loss: 1.4638
Epoch 08 — loss: 1.4608
Epoch 09 — loss: 1.4620
Epoch 10 — loss: 1.4533
Epoch 11 — loss: 1.4569
Epoch 12 — loss: 1.4514
Epoch 13 — loss: 1.4500
Epoch 14 — loss: 1.4477
Epoch 15 — loss: 1.4398
Epoch 16 — loss: 1.4362
Epoch 17 — loss: 1.4428
Epoch 18 — loss: 1.4338
Epoch 19 — loss: 1.4354
Epoch 20 — loss: 1.4354
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.7190
Epoch 02 — loss: 1.5764
Epoch 03 — loss: 1.5173
Epoch 04 — loss: 1.4640
Epoch 05 — loss: 1.4266
Epoch 06 — loss: 1.3864
Epoch 07 — loss: 1.3423
Epoch 08 — loss: 1.3058
Epoch 09 — loss: 1.2644
Epoch 10 — loss: 1.2344
Epoch 11 — loss: 1.1812
Epoch 12 — loss: 1.1410
Epoch 13 — loss: 1.0983
Epoch 14 — loss: 1.0629
Epoch 15 — loss: 1.0159
Epoch 16 — loss: 0.9822
Epoch 17 — loss: 0.9266
Epoch 18 — loss: 0.8951
Epoch 19 — loss: 0.8612
Epoch 20 — loss: 0.8237
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8700
Epoch 02 — loss: 1.5903
Epoch 03 — loss: 1.5294
Epoch 04 — loss: 1.5042
Epoch 05 — loss: 1.4901
Epoch 06 — loss: 1.4775
Epoch 07 — loss: 1.4685
Epoch 08 — loss: 1.4614
Epoch 09 — loss: 1.4606
Epoch 10 — loss: 1.4588
Epoch 11 — loss: 1.4657
Epoch 12 — loss: 1.4495
Epoch 13 — loss: 1.4542
Epoch 14 — loss: 1.4470
Epoch 15 — loss: 1.4472
Epoch 16 — loss: 1.4449
Epoch 17 — loss: 1.4410
Epoch 18 — loss: 1.4427
Epoch 19 — loss: 1.4412
Epoch 20 — loss: 1.4429
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7022
Epoch 02 — loss: 1.6008
Epoch 03 — loss: 1.5811
Epoch 04 — loss: 1.5461
Epoch 05 — loss: 1.5322
Epoch 06 — loss: 1.5102
Epoch 07 — loss: 1.4858
Epoch 08 — loss: 1.4834
Epoch 09 — loss: 1.4569
Epoch 10 — loss: 1.4362
Epoch 11 — loss: 1.4145
Epoch 12 — loss: 1.3915
Epoch 13 — loss: 1.3634
Epoch 14 — loss: 1.3395
Epoch 15 — loss: 1.3119
Epoch 16 — loss: 1.2859
Epoch 17 — loss: 1.2454
Epoch 18 — loss: 1.2237
Epoch 19 — loss: 1.1907
Epoch 20 — loss: 1.1659
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8075
Epoch 02 — loss: 1.6187
Epoch 03 — loss: 1.5664
Epoch 04 — loss: 1.5294
Epoch 05 — loss: 1.5183
Epoch 06 — loss: 1.5053
Epoch 07 — loss: 1.4886
Epoch 08 — loss: 1.4853
Epoch 09 — loss: 1.4788
Epoch 10 — loss: 1.4750
Epoch 11 — loss: 1.4701
Epoch 12 — loss: 1.4659
Epoch 13 — loss: 1.4636
Epoch 14 — loss: 1.4596
Epoch 15 — loss: 1.4619
Epoch 16 — loss: 1.4516
Epoch 17 — loss: 1.4515
Epoch 18 — loss: 1.4476
Epoch 19 — loss: 1.4486
Epoch 20 — loss: 1.4485
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4232
Epoch 02 — loss: 1.3070
Epoch 03 — loss: 1.2684/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Stage 1: Error=0.5191, Alpha=1.7152
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7988
Epoch 02 — loss: 1.6041
Epoch 03 — loss: 1.5296
Stage 2: Error=0.6317, Alpha=1.2521
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6894
Epoch 02 — loss: 1.5681
Epoch 03 — loss: 1.4976
Stage 3: Error=0.5835, Alpha=1.4548
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8732
Epoch 02 — loss: 1.7974
Epoch 03 — loss: 1.7004
Stage 4: Error=0.7294, Alpha=0.8000
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8325
Epoch 02 — loss: 1.7508
Epoch 03 — loss: 1.7053
Stage 5: Error=0.7076, Alpha=0.9082
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4382
Epoch 02 — loss: 1.3377
Epoch 03 — loss: 1.2842
Stage 1: Error=0.5124, Alpha=1.7422
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6487
Epoch 02 — loss: 1.5113
Epoch 03 — loss: 1.4559
Stage 2: Error=0.5852, Alpha=1.4478
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7168
Epoch 02 — loss: 1.6553
Epoch 03 — loss: 1.6251
Stage 3: Error=0.6913, Alpha=0.9853
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7784
Epoch 02 — loss: 1.6452
Epoch 03 — loss: 1.5546
Stage 4: Error=0.5883, Alpha=1.4347
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7995
Epoch 02 — loss: 1.6431
Epoch 03 — loss: 1.5165
Stage 5: Error=0.5644, Alpha=1.5325
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6315
Epoch 02 — loss: 1.3787
Epoch 03 — loss: 1.2957
Stage 1: Error=0.5243, Alpha=1.6945
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7748
Epoch 02 — loss: 1.6030
Epoch 03 — loss: 1.5219
Stage 2: Error=0.6340, Alpha=1.2425
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7641
Epoch 02 — loss: 1.6649
Epoch 03 — loss: 1.6423
Stage 3: Error=0.6559, Alpha=1.1467
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8733
Epoch 02 — loss: 1.7799
Epoch 03 — loss: 1.7015
Stage 4: Error=0.7560, Alpha=0.6610
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8694
Epoch 02 — loss: 1.8027
Epoch 03 — loss: 1.7364
Stage 5: Error=0.7190, Alpha=0.8523
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8910
Stacking meta epoch 2: loss=0.6965
Stacking meta epoch 3: loss=0.6459
Stacking meta epoch 4: loss=0.6196
Stacking meta epoch 5: loss=0.6025
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8772
Stacking meta epoch 2: loss=0.6933
Stacking meta epoch 3: loss=0.6442
Stacking meta epoch 4: loss=0.6175
Stacking meta epoch 5: loss=0.6000
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2570
Stacking meta epoch 2: loss=1.2427
Stacking meta epoch 3: loss=1.2404
Stacking meta epoch 4: loss=1.2388
Stacking meta epoch 5: loss=1.2375
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      79.88
1                             deepset  ...                      79.21
2                  set_transformer_xy  ...                      80.85
3                          deepset_xy  ...                      81.62
4            set_transformer_additive  ...                      80.08
5                 deepset_xy_additive  ...                      82.48
6                        adaboost_all  ...                      79.84
7            adaboost_set_transformer  ...                      81.71
8                    adaboost_deepset  ...                      81.76
9               stacking_ensemble_all  ...                      83.54
10  stacking_ensemble_set_transformer  ...                      83.06
11          stacking_ensemble_deepset  ...                      82.24

[12 rows x 5 columns]
------------------------------------iteration no 25------------------------------------
===== Processing set_transformer =====
===== Training set_transformer =====
Epoch 01 — loss: 1.7204
Epoch 02 — loss: 1.5969
Epoch 03 — loss: 1.5536
Epoch 04 — loss: 1.5046
Epoch 05 — loss: 1.4636
Epoch 06 — loss: 1.4233
Epoch 07 — loss: 1.3790
Epoch 08 — loss: 1.3293
Epoch 09 — loss: 1.2947
Epoch 10 — loss: 1.2507
Epoch 11 — loss: 1.2059
Epoch 12 — loss: 1.1510
Epoch 13 — loss: 1.1052
Epoch 14 — loss: 1.0732
Epoch 15 — loss: 1.0192
Epoch 16 — loss: 0.9812
Epoch 17 — loss: 0.9521
Epoch 18 — loss: 0.9079
Epoch 19 — loss: 0.8528
Epoch 20 — loss: 0.8339
Predictions for set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset =====
===== Training deepset =====
Epoch 01 — loss: 1.8930
Epoch 02 — loss: 1.6064
Epoch 03 — loss: 1.5355
Epoch 04 — loss: 1.5152
Epoch 05 — loss: 1.4989
Epoch 06 — loss: 1.4849
Epoch 07 — loss: 1.4734
Epoch 08 — loss: 1.4649
Epoch 09 — loss: 1.4601
Epoch 10 — loss: 1.4669
Epoch 11 — loss: 1.4526
Epoch 12 — loss: 1.4551
Epoch 13 — loss: 1.4489
Epoch 14 — loss: 1.4496
Epoch 15 — loss: 1.4427
Epoch 16 — loss: 1.4493
Epoch 17 — loss: 1.4404
Epoch 18 — loss: 1.4368
Epoch 19 — loss: 1.4387
Epoch 20 — loss: 1.4357
Predictions for deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_xy =====
===== Training set_transformer_xy =====
Epoch 01 — loss: 1.6980
Epoch 02 — loss: 1.5656
Epoch 03 — loss: 1.5140
Epoch 04 — loss: 1.4606
Epoch 05 — loss: 1.4186
Epoch 06 — loss: 1.3682
Epoch 07 — loss: 1.3366
Epoch 08 — loss: 1.2904
Epoch 09 — loss: 1.2487
Epoch 10 — loss: 1.1991
Epoch 11 — loss: 1.1521
Epoch 12 — loss: 1.1030
Epoch 13 — loss: 1.0693
Epoch 14 — loss: 1.0288
Epoch 15 — loss: 0.9781
Epoch 16 — loss: 0.9459/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")
/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

Epoch 17 — loss: 0.9033
Epoch 18 — loss: 0.8541
Epoch 19 — loss: 0.8218
Epoch 20 — loss: 0.7830
Predictions for set_transformer_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy =====
===== Training deepset_xy =====
Epoch 01 — loss: 1.8838
Epoch 02 — loss: 1.5979
Epoch 03 — loss: 1.5368
Epoch 04 — loss: 1.5099
Epoch 05 — loss: 1.5012
Epoch 06 — loss: 1.4839
Epoch 07 — loss: 1.4738
Epoch 08 — loss: 1.4744
Epoch 09 — loss: 1.4711
Epoch 10 — loss: 1.4642
Epoch 11 — loss: 1.4586
Epoch 12 — loss: 1.4551
Epoch 13 — loss: 1.4603
Epoch 14 — loss: 1.4472
Epoch 15 — loss: 1.4511
Epoch 16 — loss: 1.4418
Epoch 17 — loss: 1.4407
Epoch 18 — loss: 1.4416
Epoch 19 — loss: 1.4378
Epoch 20 — loss: 1.4389
Predictions for deepset_xy_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing set_transformer_additive =====
===== Training set_transformer_additive =====
Epoch 01 — loss: 1.7073
Epoch 02 — loss: 1.5958
Epoch 03 — loss: 1.5742
Epoch 04 — loss: 1.5371
Epoch 05 — loss: 1.5190
Epoch 06 — loss: 1.5038
Epoch 07 — loss: 1.4763
Epoch 08 — loss: 1.4529
Epoch 09 — loss: 1.4384
Epoch 10 — loss: 1.4146
Epoch 11 — loss: 1.3900
Epoch 12 — loss: 1.3650
Epoch 13 — loss: 1.3393
Epoch 14 — loss: 1.3180
Epoch 15 — loss: 1.2876
Epoch 16 — loss: 1.2720
Epoch 17 — loss: 1.2336
Epoch 18 — loss: 1.2140
Epoch 19 — loss: 1.1797
Epoch 20 — loss: 1.1453
Predictions for set_transformer_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing deepset_xy_additive =====
===== Training deepset_xy_additive =====
Epoch 01 — loss: 1.8005
Epoch 02 — loss: 1.6328
Epoch 03 — loss: 1.5797
Epoch 04 — loss: 1.5439
Epoch 05 — loss: 1.5118
Epoch 06 — loss: 1.4973
Epoch 07 — loss: 1.4832
Epoch 08 — loss: 1.4764
Epoch 09 — loss: 1.4804
Epoch 10 — loss: 1.4724
Epoch 11 — loss: 1.4663
Epoch 12 — loss: 1.4578
Epoch 13 — loss: 1.4626
Epoch 14 — loss: 1.4544
Epoch 15 — loss: 1.4500
Epoch 16 — loss: 1.4500
Epoch 17 — loss: 1.4524
Epoch 18 — loss: 1.4492
Epoch 19 — loss: 1.4428
Epoch 20 — loss: 1.4437
Predictions for deepset_xy_additive_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4604
Epoch 02 — loss: 1.3063
Epoch 03 — loss: 1.2720
Stage 1: Error=0.5089, Alpha=1.7561
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7905
Epoch 02 — loss: 1.6076
Epoch 03 — loss: 1.5346
Stage 2: Error=0.6345, Alpha=1.2401
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6914
Epoch 02 — loss: 1.5611
Epoch 03 — loss: 1.4971
Stage 3: Error=0.6000, Alpha=1.3864
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8565
Epoch 02 — loss: 1.7479
Epoch 03 — loss: 1.6942
Stage 4: Error=0.7156, Alpha=0.8688
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8159
Epoch 02 — loss: 1.7669
Epoch 03 — loss: 1.7308
Stage 5: Error=0.7146, Alpha=0.8737
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4451
Epoch 02 — loss: 1.3323
Epoch 03 — loss: 1.2787
Stage 1: Error=0.5047, Alpha=1.7730
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6289
Epoch 02 — loss: 1.4965
Epoch 03 — loss: 1.4154
Stage 2: Error=0.5800, Alpha=1.4692
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7183
Epoch 02 — loss: 1.6452
Epoch 03 — loss: 1.6058
Stage 3: Error=0.6660, Alpha=1.1016
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7471
Epoch 02 — loss: 1.6337
Epoch 03 — loss: 1.5562
Stage 4: Error=0.5967, Alpha=1.4000
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7882
Epoch 02 — loss: 1.6597
Epoch 03 — loss: 1.5274
Stage 5: Error=0.5804, Alpha=1.4675
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6401
Epoch 02 — loss: 1.3955
Epoch 03 — loss: 1.3168
Stage 1: Error=0.5211, Alpha=1.7075
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7849
Epoch 02 — loss: 1.5848
Epoch 03 — loss: 1.4902
Stage 2: Error=0.6443, Alpha=1.1977
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7594
Epoch 02 — loss: 1.6490
Epoch 03 — loss: 1.6062
Stage 3: Error=0.6419, Alpha=1.2082
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8684
Epoch 02 — loss: 1.7925
Epoch 03 — loss: 1.7157
Stage 4: Error=0.7407, Alpha=0.7420
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8711
Epoch 02 — loss: 1.8247
Epoch 03 — loss: 1.7511
Stage 5: Error=0.7722, Alpha=0.5708
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Evaluating ensembles =====
--- Evaluating all ensembles (6 models) ---
Stacking meta epoch 1: loss=0.8841
Stacking meta epoch 2: loss=0.6873
Stacking meta epoch 3: loss=0.6359
Stacking meta epoch 4: loss=0.6094
Stacking meta epoch 5: loss=0.5923
Predictions for stacking_ensemble_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating set_transformer ensembles (3 models) ---
Stacking meta epoch 1: loss=0.8590
Stacking meta epoch 2: loss=0.6841
Stacking meta epoch 3: loss=0.6364
Stacking meta epoch 4: loss=0.6101
Stacking meta epoch 5: loss=0.5930
Predictions for stacking_ensemble_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
--- Evaluating deepset ensembles (3 models) ---
Stacking meta epoch 1: loss=1.2570
Stacking meta epoch 2: loss=1.2415
Stacking meta epoch 3: loss=1.2384
Stacking meta epoch 4: loss=1.2363
Stacking meta epoch 5: loss=1.2349
Predictions for stacking_ensemble_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                           Model Type  ...  Val ±1 Grade Accuracy (%)
0                     set_transformer  ...                      77.09
1                             deepset  ...                      80.94
2                  set_transformer_xy  ...                      79.16
3                          deepset_xy  ...                      81.38
4            set_transformer_additive  ...                      78.83
5                 deepset_xy_additive  ...                      79.11
6                        adaboost_all  ...                      80.90
7            adaboost_set_transformer  ...                      82.05
8                    adaboost_deepset  ...                      83.49
9               stacking_ensemble_all  ...                      82.68
10  stacking_ensemble_set_transformer  ...                      82.10
11          stacking_ensemble_deepset  ...                      82.34

[12 rows x 5 columns]/home/patr/anaconda3/lib/python3.12/site-packages/openpyxl/workbook/child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file
  warnings.warn("Title is more than 31 characters. Some applications may not be able to read the file")

=== Accuracy Stability Summary ===
                           Model Type  ...  Val ±1 Grade Std (%)
6                        adaboost_all  ...                 0.746
8                    adaboost_deepset  ...                 0.788
7            adaboost_set_transformer  ...                 0.677
1                             deepset  ...                 1.384
3                          deepset_xy  ...                 1.236
5                 deepset_xy_additive  ...                 1.746
0                     set_transformer  ...                 1.222
4            set_transformer_additive  ...                 1.735
2                  set_transformer_xy  ...                 1.251
9               stacking_ensemble_all  ...                 0.749
11          stacking_ensemble_deepset  ...                 0.261
10  stacking_ensemble_set_transformer  ...                 0.707

[12 rows x 9 columns]
----------------- Ordinal iteration 1/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3619
Epoch 02 — loss: 0.3096
Epoch 03 — loss: 0.2955
Epoch 04 — loss: 0.2842
Epoch 05 — loss: 0.2744
Epoch 06 — loss: 0.2668
Epoch 07 — loss: 0.2567
Epoch 08 — loss: 0.2492
Epoch 09 — loss: 0.2406
Epoch 10 — loss: 0.2338
Epoch 11 — loss: 0.2258
Epoch 12 — loss: 0.2190
Epoch 13 — loss: 0.2125
Epoch 14 — loss: 0.2058
Epoch 15 — loss: 0.2003
Epoch 16 — loss: 0.1937
Epoch 17 — loss: 0.1868
Epoch 18 — loss: 0.1828
Epoch 19 — loss: 0.1764
Epoch 20 — loss: 0.1691
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  81.180000
2    P(>V6)  83.830002
3    P(>V7)  87.339996
4    P(>V8)  92.059998
5    P(>V9)  96.389999
Overall accuracy: 46.92%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3542
Epoch 02 — loss: 0.2955
Epoch 03 — loss: 0.2817
Epoch 04 — loss: 0.2723
Epoch 05 — loss: 0.2625
Epoch 06 — loss: 0.2532
Epoch 07 — loss: 0.2468
Epoch 08 — loss: 0.2379
Epoch 09 — loss: 0.2323
Epoch 10 — loss: 0.2250
Epoch 11 — loss: 0.2193
Epoch 12 — loss: 0.2137
Epoch 13 — loss: 0.2080
Epoch 14 — loss: 0.2018
Epoch 15 — loss: 0.1966
Epoch 16 — loss: 0.1897
Epoch 17 — loss: 0.1837
Epoch 18 — loss: 0.1788
Epoch 19 — loss: 0.1731
Epoch 20 — loss: 0.1689
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  81.330002
2    P(>V6)  83.730003
3    P(>V7)  87.629997
4    P(>V8)  92.300003
5    P(>V9)  96.339996
Overall accuracy: 46.78%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3655
Epoch 02 — loss: 0.3149
Epoch 03 — loss: 0.3026
Epoch 04 — loss: 0.2964
Epoch 05 — loss: 0.2843
Epoch 06 — loss: 0.2781
Epoch 07 — loss: 0.2735
Epoch 08 — loss: 0.2652
Epoch 09 — loss: 0.2616
Epoch 10 — loss: 0.2543
Epoch 11 — loss: 0.2493
Epoch 12 — loss: 0.2428
Epoch 13 — loss: 0.2376
Epoch 14 — loss: 0.2319
Epoch 15 — loss: 0.2281
Epoch 16 — loss: 0.2203
Epoch 17 — loss: 0.2173
Epoch 18 — loss: 0.2121
Epoch 19 — loss: 0.2071
Epoch 20 — loss: 0.2027
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  80.750000
2    P(>V6)  84.599998
3    P(>V7)  87.489998
4    P(>V8)  92.440002
5    P(>V9)  96.820000
Overall accuracy: 46.73%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4684
Epoch 02 — loss: 0.3477
Epoch 03 — loss: 0.3003
Epoch 04 — loss: 0.2914
Epoch 05 — loss: 0.2857
Epoch 06 — loss: 0.2816
Epoch 07 — loss: 0.2789
Epoch 08 — loss: 0.2790
Epoch 09 — loss: 0.2764
Epoch 10 — loss: 0.2740
Epoch 11 — loss: 0.2733
Epoch 12 — loss: 0.2712
Epoch 13 — loss: 0.2716
Epoch 14 — loss: 0.2712
Epoch 15 — loss: 0.2706
Epoch 16 — loss: 0.2706
Epoch 17 — loss: 0.2688
Epoch 18 — loss: 0.2685
Epoch 19 — loss: 0.2688
Epoch 20 — loss: 0.2698
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.559998
2    P(>V6)  83.970001
3    P(>V7)  87.970001
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 46.54%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4730
Epoch 02 — loss: 0.3490
Epoch 03 — loss: 0.2936
Epoch 04 — loss: 0.2846
Epoch 05 — loss: 0.2820
Epoch 06 — loss: 0.2792
Epoch 07 — loss: 0.2767
Epoch 08 — loss: 0.2764
Epoch 09 — loss: 0.2744
Epoch 10 — loss: 0.2737
Epoch 11 — loss: 0.2729
Epoch 12 — loss: 0.2721
Epoch 13 — loss: 0.2700
Epoch 14 — loss: 0.2706
Epoch 15 — loss: 0.2720
Epoch 16 — loss: 0.2692
Epoch 17 — loss: 0.2691
Epoch 18 — loss: 0.2698
Epoch 19 — loss: 0.2681
Epoch 20 — loss: 0.2681
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  80.699997
2    P(>V6)  83.779999
3    P(>V7)  87.730003
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 45.81%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4319
Epoch 02 — loss: 0.3234
Epoch 03 — loss: 0.3016
Epoch 04 — loss: 0.2916
Epoch 05 — loss: 0.2877
Epoch 06 — loss: 0.2844
Epoch 07 — loss: 0.2803
Epoch 08 — loss: 0.2787
Epoch 09 — loss: 0.2791
Epoch 10 — loss: 0.2778
Epoch 11 — loss: 0.2753
Epoch 12 — loss: 0.2768
Epoch 13 — loss: 0.2738
Epoch 14 — loss: 0.2776
Epoch 15 — loss: 0.2740
Epoch 16 — loss: 0.2731
Epoch 17 — loss: 0.2716
Epoch 18 — loss: 0.2711
Epoch 19 — loss: 0.2715
Epoch 20 — loss: 0.2719
  threshold   accuracy
0    P(>V4)  80.510002
1    P(>V5)  80.459999
2    P(>V6)  83.690002
3    P(>V7)  87.050003
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 46.01%
Ordinal stacking meta epoch 1: loss=0.2136
Ordinal stacking meta epoch 2: loss=0.1371
Ordinal stacking meta epoch 3: loss=0.1312
Ordinal stacking meta epoch 4: loss=0.1283
Ordinal stacking meta epoch 5: loss=0.1263
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.190002
2    P(>V6)  84.989998
3    P(>V7)  87.919998
4    P(>V8)  93.209999
5    P(>V9)  96.870003
Overall accuracy: 47.45%
Traceback (most recent call last):
  File "/home/patr/docker-dir/set-transformer-model/grade_predictor/main.py", line 1879, in <module>
    run_ordinal_iterations(
  File "/home/patr/docker-dir/set-transformer-model/grade_predictor/main.py", line 1792, in run_ordinal_iterations
    ensembles = build_ordinal_ensemble_models(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/patr/docker-dir/set-transformer-model/grade_predictor/main.py", line 1641, in build_ordinal_ensemble_models
    inferred_dim = infer_ordinal_stacking_feature_dim(ensemble_model, train_loader, device)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/patr/docker-dir/set-transformer-model/grade_predictor/main.py", line 1548, in infer_ordinal_stacking_feature_dim
    member_feats = stacking_model._member_features(inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/patr/docker-dir/set-transformer-model/grade_predictor/models/ordinal.py", line 494, in _member_features
    raise RuntimeError(
RuntimeError: Model 'set_transformer_ordinal' failed to cache encoder features for stacking.
