{'A1': 0, 'B1': 1, 'C1': 2, 'D1': 3, 'E1': 4, 'F1': 5, 'G1': 6, 'H1': 7, 'I1': 8, 'J1': 9, 'K1': 10, 'A2': 11, 'B2': 12, 'C2': 13, 'D2': 14, 'E2': 15, 'F2': 16, 'G2': 17, 'H2': 18, 'I2': 19, 'J2': 20, 'K2': 21, 'A3': 22, 'B3': 23, 'C3': 24, 'D3': 25, 'E3': 26, 'F3': 27, 'G3': 28, 'H3': 29, 'I3': 30, 'J3': 31, 'K3': 32, 'A4': 33, 'B4': 34, 'C4': 35, 'D4': 36, 'E4': 37, 'F4': 38, 'G4': 39, 'H4': 40, 'I4': 41, 'J4': 42, 'K4': 43, 'A5': 44, 'B5': 45, 'C5': 46, 'D5': 47, 'E5': 48, 'F5': 49, 'G5': 50, 'H5': 51, 'I5': 52, 'J5': 53, 'K5': 54, 'A6': 55, 'B6': 56, 'C6': 57, 'D6': 58, 'E6': 59, 'F6': 60, 'G6': 61, 'H6': 62, 'I6': 63, 'J6': 64, 'K6': 65, 'A7': 66, 'B7': 67, 'C7': 68, 'D7': 69, 'E7': 70, 'F7': 71, 'G7': 72, 'H7': 73, 'I7': 74, 'J7': 75, 'K7': 76, 'A8': 77, 'B8': 78, 'C8': 79, 'D8': 80, 'E8': 81, 'F8': 82, 'G8': 83, 'H8': 84, 'I8': 85, 'J8': 86, 'K8': 87, 'A9': 88, 'B9': 89, 'C9': 90, 'D9': 91, 'E9': 92, 'F9': 93, 'G9': 94, 'H9': 95, 'I9': 96, 'J9': 97, 'K9': 98, 'A10': 99, 'B10': 100, 'C10': 101, 'D10': 102, 'E10': 103, 'F10': 104, 'G10': 105, 'H10': 106, 'I10': 107, 'J10': 108, 'K10': 109, 'A11': 110, 'B11': 111, 'C11': 112, 'D11': 113, 'E11': 114, 'F11': 115, 'G11': 116, 'H11': 117, 'I11': 118, 'J11': 119, 'K11': 120, 'A12': 121, 'B12': 122, 'C12': 123, 'D12': 124, 'E12': 125, 'F12': 126, 'G12': 127, 'H12': 128, 'I12': 129, 'J12': 130, 'K12': 131, 'A13': 132, 'B13': 133, 'C13': 134, 'D13': 135, 'E13': 136, 'F13': 137, 'G13': 138, 'H13': 139, 'I13': 140, 'J13': 141, 'K13': 142, 'A14': 143, 'B14': 144, 'C14': 145, 'D14': 146, 'E14': 147, 'F14': 148, 'G14': 149, 'H14': 150, 'I14': 151, 'J14': 152, 'K14': 153, 'A15': 154, 'B15': 155, 'C15': 156, 'D15': 157, 'E15': 158, 'F15': 159, 'G15': 160, 'H15': 161, 'I15': 162, 'J15': 163, 'K15': 164, 'A16': 165, 'B16': 166, 'C16': 167, 'D16': 168, 'E16': 169, 'F16': 170, 'G16': 171, 'H16': 172, 'I16': 173, 'J16': 174, 'K16': 175, 'A17': 176, 'B17': 177, 'C17': 178, 'D17': 179, 'E17': 180, 'F17': 181, 'G17': 182, 'H17': 183, 'I17': 184, 'J17': 185, 'K17': 186, 'A18': 187, 'B18': 188, 'C18': 189, 'D18': 190, 'E18': 191, 'F18': 192, 'G18': 193, 'H18': 194, 'I18': 195, 'J18': 196, 'K18': 197}
successfully parsed hold difficulty file
successfully prepare type vocabulary
successfully created (x,y) position to each hold:
{'A1': (0, 0), 'A2': (0, 1), 'A3': (0, 2), 'A4': (0, 3), 'A5': (0, 4), 'A6': (0, 5), 'A7': (0, 6), 'A8': (0, 7), 'A9': (0, 8), 'A10': (0, 9), 'A11': (0, 10), 'A12': (0, 11), 'A13': (0, 12), 'A14': (0, 13), 'A15': (0, 14), 'A16': (0, 15), 'A17': (0, 16), 'A18': (0, 17), 'B1': (1, 0), 'B2': (1, 1), 'B3': (1, 2), 'B4': (1, 3), 'B5': (1, 4), 'B6': (1, 5), 'B7': (1, 6), 'B8': (1, 7), 'B9': (1, 8), 'B10': (1, 9), 'B11': (1, 10), 'B12': (1, 11), 'B13': (1, 12), 'B14': (1, 13), 'B15': (1, 14), 'B16': (1, 15), 'B17': (1, 16), 'B18': (1, 17), 'C1': (2, 0), 'C2': (2, 1), 'C3': (2, 2), 'C4': (2, 3), 'C5': (2, 4), 'C6': (2, 5), 'C7': (2, 6), 'C8': (2, 7), 'C9': (2, 8), 'C10': (2, 9), 'C11': (2, 10), 'C12': (2, 11), 'C13': (2, 12), 'C14': (2, 13), 'C15': (2, 14), 'C16': (2, 15), 'C17': (2, 16), 'C18': (2, 17), 'D1': (3, 0), 'D2': (3, 1), 'D3': (3, 2), 'D4': (3, 3), 'D5': (3, 4), 'D6': (3, 5), 'D7': (3, 6), 'D8': (3, 7), 'D9': (3, 8), 'D10': (3, 9), 'D11': (3, 10), 'D12': (3, 11), 'D13': (3, 12), 'D14': (3, 13), 'D15': (3, 14), 'D16': (3, 15), 'D17': (3, 16), 'D18': (3, 17), 'E1': (4, 0), 'E2': (4, 1), 'E3': (4, 2), 'E4': (4, 3), 'E5': (4, 4), 'E6': (4, 5), 'E7': (4, 6), 'E8': (4, 7), 'E9': (4, 8), 'E10': (4, 9), 'E11': (4, 10), 'E12': (4, 11), 'E13': (4, 12), 'E14': (4, 13), 'E15': (4, 14), 'E16': (4, 15), 'E17': (4, 16), 'E18': (4, 17), 'F1': (5, 0), 'F2': (5, 1), 'F3': (5, 2), 'F4': (5, 3), 'F5': (5, 4), 'F6': (5, 5), 'F7': (5, 6), 'F8': (5, 7), 'F9': (5, 8), 'F10': (5, 9), 'F11': (5, 10), 'F12': (5, 11), 'F13': (5, 12), 'F14': (5, 13), 'F15': (5, 14), 'F16': (5, 15), 'F17': (5, 16), 'F18': (5, 17), 'G1': (6, 0), 'G2': (6, 1), 'G3': (6, 2), 'G4': (6, 3), 'G5': (6, 4), 'G6': (6, 5), 'G7': (6, 6), 'G8': (6, 7), 'G9': (6, 8), 'G10': (6, 9), 'G11': (6, 10), 'G12': (6, 11), 'G13': (6, 12), 'G14': (6, 13), 'G15': (6, 14), 'G16': (6, 15), 'G17': (6, 16), 'G18': (6, 17), 'H1': (7, 0), 'H2': (7, 1), 'H3': (7, 2), 'H4': (7, 3), 'H5': (7, 4), 'H6': (7, 5), 'H7': (7, 6), 'H8': (7, 7), 'H9': (7, 8), 'H10': (7, 9), 'H11': (7, 10), 'H12': (7, 11), 'H13': (7, 12), 'H14': (7, 13), 'H15': (7, 14), 'H16': (7, 15), 'H17': (7, 16), 'H18': (7, 17), 'I1': (8, 0), 'I2': (8, 1), 'I3': (8, 2), 'I4': (8, 3), 'I5': (8, 4), 'I6': (8, 5), 'I7': (8, 6), 'I8': (8, 7), 'I9': (8, 8), 'I10': (8, 9), 'I11': (8, 10), 'I12': (8, 11), 'I13': (8, 12), 'I14': (8, 13), 'I15': (8, 14), 'I16': (8, 15), 'I17': (8, 16), 'I18': (8, 17), 'J1': (9, 0), 'J2': (9, 1), 'J3': (9, 2), 'J4': (9, 3), 'J5': (9, 4), 'J6': (9, 5), 'J7': (9, 6), 'J8': (9, 7), 'J9': (9, 8), 'J10': (9, 9), 'J11': (9, 10), 'J12': (9, 11), 'J13': (9, 12), 'J14': (9, 13), 'J15': (9, 14), 'J16': (9, 15), 'J17': (9, 16), 'J18': (9, 17), 'K1': (10, 0), 'K2': (10, 1), 'K3': (10, 2), 'K4': (10, 3), 'K5': (10, 4), 'K6': (10, 5), 'K7': (10, 6), 'K8': (10, 7), 'K9': (10, 8), 'K10': (10, 9), 'K11': (10, 10), 'K12': (10, 11), 'K13': (10, 12), 'K14': (10, 13), 'K15': (10, 14), 'K16': (10, 15), 'K17': (10, 16), 'K18': (10, 17)}
Using device: cuda
------------------------------------iteration no 1------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4765
Epoch 02 — loss: 1.3511
Epoch 03 — loss: 1.3026
Stage 1: Error=0.5221, Alpha=1.7031
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7743
Epoch 02 — loss: 1.5848
Epoch 03 — loss: 1.5262
Stage 2: Error=0.6495, Alpha=1.1751
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7555
Epoch 02 — loss: 1.6101
Epoch 03 — loss: 1.5387
Stage 3: Error=0.6263, Alpha=1.2755
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8662
Epoch 02 — loss: 1.7744
Epoch 03 — loss: 1.7074
Stage 4: Error=0.7266, Alpha=0.8143
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7917
Epoch 02 — loss: 1.7354
Epoch 03 — loss: 1.7010
Stage 5: Error=0.7171, Alpha=0.8618
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4609
Epoch 02 — loss: 1.3367
Epoch 03 — loss: 1.2665
Stage 1: Error=0.5130, Alpha=1.7398
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6730
Epoch 02 — loss: 1.5417
Epoch 03 — loss: 1.4776
Stage 2: Error=0.6303, Alpha=1.2583
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6741
Epoch 02 — loss: 1.6136
Epoch 03 — loss: 1.5832
Stage 3: Error=0.6356, Alpha=1.2354
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7972
Epoch 02 — loss: 1.6997
Epoch 03 — loss: 1.6352
Stage 4: Error=0.6696, Alpha=1.0854
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8077
Epoch 02 — loss: 1.7034
Epoch 03 — loss: 1.6188
Stage 5: Error=0.6643, Alpha=1.1092
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6149
Epoch 02 — loss: 1.3841
Epoch 03 — loss: 1.2963
Stage 1: Error=0.5315, Alpha=1.6655
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7910
Epoch 02 — loss: 1.5805
Epoch 03 — loss: 1.5271
Stage 2: Error=0.6351, Alpha=1.2376
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7741
Epoch 02 — loss: 1.6663
Epoch 03 — loss: 1.6126
Stage 3: Error=0.6571, Alpha=1.1415
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8663
Epoch 02 — loss: 1.7947
Epoch 03 — loss: 1.7401
Stage 4: Error=0.7566, Alpha=0.6577
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8830
Epoch 02 — loss: 1.7889
Epoch 03 — loss: 1.7237
Stage 5: Error=0.7604, Alpha=0.6370
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      80.17
1  adaboost_set_transformer  ...                      82.29
2          adaboost_deepset  ...                      81.09

[3 rows x 5 columns]
------------------------------------iteration no 2------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4460
Epoch 02 — loss: 1.3344
Epoch 03 — loss: 1.2898
Stage 1: Error=0.5171, Alpha=1.7234
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7819
Epoch 02 — loss: 1.5930
Epoch 03 — loss: 1.5182
Stage 2: Error=0.6322, Alpha=1.2499
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7205
Epoch 02 — loss: 1.6265
Epoch 03 — loss: 1.5694
Stage 3: Error=0.6350, Alpha=1.2382
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8558
Epoch 02 — loss: 1.7805
Epoch 03 — loss: 1.6997
Stage 4: Error=0.7510, Alpha=0.6877
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7975
Epoch 02 — loss: 1.7372
Epoch 03 — loss: 1.7048
Stage 5: Error=0.6881, Alpha=1.0005
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4544
Epoch 02 — loss: 1.3406
Epoch 03 — loss: 1.3023
Stage 1: Error=0.5259, Alpha=1.6882
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6258
Epoch 02 — loss: 1.5217
Epoch 03 — loss: 1.4593
Stage 2: Error=0.6145, Alpha=1.3255
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6863
Epoch 02 — loss: 1.6260
Epoch 03 — loss: 1.5557
Stage 3: Error=0.6348, Alpha=1.2390
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7830
Epoch 02 — loss: 1.7118
Epoch 03 — loss: 1.6330
Stage 4: Error=0.6831, Alpha=1.0239
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8021
Epoch 02 — loss: 1.7052
Epoch 03 — loss: 1.6303
Stage 5: Error=0.6605, Alpha=1.1261
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6398
Epoch 02 — loss: 1.3656
Epoch 03 — loss: 1.3112
Stage 1: Error=0.5290, Alpha=1.6756
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7990
Epoch 02 — loss: 1.6085
Epoch 03 — loss: 1.5323
Stage 2: Error=0.6443, Alpha=1.1978
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7488
Epoch 02 — loss: 1.6408
Epoch 03 — loss: 1.6113
Stage 3: Error=0.6297, Alpha=1.2609
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8650
Epoch 02 — loss: 1.8197
Epoch 03 — loss: 1.7321
Stage 4: Error=0.7366, Alpha=0.7634
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8678
Epoch 02 — loss: 1.8048
Epoch 03 — loss: 1.7594
Stage 5: Error=0.7889, Alpha=0.4732
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.47
1  adaboost_set_transformer  ...                      80.85
2          adaboost_deepset  ...                      80.56

[3 rows x 5 columns]
------------------------------------iteration no 3------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4554
Epoch 02 — loss: 1.3379
Epoch 03 — loss: 1.2817
Stage 1: Error=0.5189, Alpha=1.7162
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7921
Epoch 02 — loss: 1.5939
Epoch 03 — loss: 1.5311
Stage 2: Error=0.6519, Alpha=1.1642
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6808
Epoch 02 — loss: 1.5679
Epoch 03 — loss: 1.5090
Stage 3: Error=0.5939, Alpha=1.4115
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8575
Epoch 02 — loss: 1.7434
Epoch 03 — loss: 1.6924
Stage 4: Error=0.7491, Alpha=0.6980
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8003
Epoch 02 — loss: 1.7291
Epoch 03 — loss: 1.6801
Stage 5: Error=0.7286, Alpha=0.8041
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4317
Epoch 02 — loss: 1.3329
Epoch 03 — loss: 1.2907
Stage 1: Error=0.5280, Alpha=1.6795
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6089
Epoch 02 — loss: 1.5078
Epoch 03 — loss: 1.4823
Stage 2: Error=0.5960, Alpha=1.4029
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7307
Epoch 02 — loss: 1.6649
Epoch 03 — loss: 1.6409
Stage 3: Error=0.6897, Alpha=0.9930
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7937
Epoch 02 — loss: 1.7049
Epoch 03 — loss: 1.6564
Stage 4: Error=0.6659, Alpha=1.1021
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7989
Epoch 02 — loss: 1.7221
Epoch 03 — loss: 1.6315
Stage 5: Error=0.6725, Alpha=1.0721
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6100
Epoch 02 — loss: 1.3550
Epoch 03 — loss: 1.3177
Stage 1: Error=0.5338, Alpha=1.6563
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7883
Epoch 02 — loss: 1.5958
Epoch 03 — loss: 1.4985
Stage 2: Error=0.6229, Alpha=1.2897
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7738
Epoch 02 — loss: 1.6522
Epoch 03 — loss: 1.6090
Stage 3: Error=0.6527, Alpha=1.1609
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8622
Epoch 02 — loss: 1.7899
Epoch 03 — loss: 1.7184
Stage 4: Error=0.7434, Alpha=0.7282
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8844
Epoch 02 — loss: 1.8134
Epoch 03 — loss: 1.7546
Stage 5: Error=0.7262, Alpha=0.8162
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      80.70
1  adaboost_set_transformer  ...                      81.09
2          adaboost_deepset  ...                      82.44

[3 rows x 5 columns]
------------------------------------iteration no 4------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4297
Epoch 02 — loss: 1.3524
Epoch 03 — loss: 1.3098
Stage 1: Error=0.5218, Alpha=1.7046
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8002
Epoch 02 — loss: 1.6034
Epoch 03 — loss: 1.5241
Stage 2: Error=0.6630, Alpha=1.1149
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6655
Epoch 02 — loss: 1.5705
Epoch 03 — loss: 1.5179
Stage 3: Error=0.5994, Alpha=1.3889
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8575
Epoch 02 — loss: 1.7553
Epoch 03 — loss: 1.7051
Stage 4: Error=0.7673, Alpha=0.5988
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7899
Epoch 02 — loss: 1.7521
Epoch 03 — loss: 1.7222
Stage 5: Error=0.7481, Alpha=0.7034
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4681
Epoch 02 — loss: 1.3210
Epoch 03 — loss: 1.2880
Stage 1: Error=0.5131, Alpha=1.7393
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6328
Epoch 02 — loss: 1.5418
Epoch 03 — loss: 1.4717
Stage 2: Error=0.6146, Alpha=1.3252
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7250
Epoch 02 — loss: 1.6439
Epoch 03 — loss: 1.6242
Stage 3: Error=0.6835, Alpha=1.0216
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7778
Epoch 02 — loss: 1.6696
Epoch 03 — loss: 1.6002
Stage 4: Error=0.6631, Alpha=1.1144
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7880
Epoch 02 — loss: 1.6854
Epoch 03 — loss: 1.6106
Stage 5: Error=0.6144, Alpha=1.3258
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6208
Epoch 02 — loss: 1.3717
Epoch 03 — loss: 1.3030
Stage 1: Error=0.5326, Alpha=1.6611
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7889
Epoch 02 — loss: 1.5686
Epoch 03 — loss: 1.5177
Stage 2: Error=0.6144, Alpha=1.3261
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7977
Epoch 02 — loss: 1.6815
Epoch 03 — loss: 1.6429
Stage 3: Error=0.6751, Alpha=1.0605
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8797
Epoch 02 — loss: 1.7849
Epoch 03 — loss: 1.7125
Stage 4: Error=0.7330, Alpha=0.7817
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8915
Epoch 02 — loss: 1.8018
Epoch 03 — loss: 1.7376
Stage 5: Error=0.7594, Alpha=0.6426
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.57
1  adaboost_set_transformer  ...                      81.52
2          adaboost_deepset  ...                      81.67

[3 rows x 5 columns]
------------------------------------iteration no 5------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4598
Epoch 02 — loss: 1.3519
Epoch 03 — loss: 1.2978
Stage 1: Error=0.5188, Alpha=1.7166
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7911
Epoch 02 — loss: 1.5984
Epoch 03 — loss: 1.5320
Stage 2: Error=0.6434, Alpha=1.2015
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7358
Epoch 02 — loss: 1.6180
Epoch 03 — loss: 1.5468
Stage 3: Error=0.6225, Alpha=1.2917
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8809
Epoch 02 — loss: 1.7900
Epoch 03 — loss: 1.7073
Stage 4: Error=0.7392, Alpha=0.7500
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8241
Epoch 02 — loss: 1.7607
Epoch 03 — loss: 1.7063
Stage 5: Error=0.7135, Alpha=0.8794
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4605
Epoch 02 — loss: 1.3469
Epoch 03 — loss: 1.2931
Stage 1: Error=0.5291, Alpha=1.6751
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6430
Epoch 02 — loss: 1.5466
Epoch 03 — loss: 1.5081
Stage 2: Error=0.6133, Alpha=1.3304
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7139
Epoch 02 — loss: 1.6393
Epoch 03 — loss: 1.6107
Stage 3: Error=0.6668, Alpha=1.0981
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7695
Epoch 02 — loss: 1.6887
Epoch 03 — loss: 1.6268
Stage 4: Error=0.6688, Alpha=1.0891
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8179
Epoch 02 — loss: 1.6973
Epoch 03 — loss: 1.6224
Stage 5: Error=0.6673, Alpha=1.0959
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6051
Epoch 02 — loss: 1.3747
Epoch 03 — loss: 1.3073
Stage 1: Error=0.5318, Alpha=1.6645
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8104
Epoch 02 — loss: 1.5882
Epoch 03 — loss: 1.5186
Stage 2: Error=0.6540, Alpha=1.1550
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7853
Epoch 02 — loss: 1.6689
Epoch 03 — loss: 1.6162
Stage 3: Error=0.6811, Alpha=1.0330
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8191
Epoch 02 — loss: 1.7156
Epoch 03 — loss: 1.6551
Stage 4: Error=0.6991, Alpha=0.9488
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8761
Epoch 02 — loss: 1.7948
Epoch 03 — loss: 1.7235
Stage 5: Error=0.7588, Alpha=0.6457
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      79.88
1  adaboost_set_transformer  ...                      82.63
2          adaboost_deepset  ...                      81.91

[3 rows x 5 columns]
------------------------------------iteration no 6------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4781
Epoch 02 — loss: 1.3643
Epoch 03 — loss: 1.3087
Stage 1: Error=0.5195, Alpha=1.7137
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8023
Epoch 02 — loss: 1.6203
Epoch 03 — loss: 1.5337
Stage 2: Error=0.6423, Alpha=1.2065
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7265
Epoch 02 — loss: 1.6014
Epoch 03 — loss: 1.5450
Stage 3: Error=0.6177, Alpha=1.3119
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8603
Epoch 02 — loss: 1.7539
Epoch 03 — loss: 1.6935
Stage 4: Error=0.7468, Alpha=0.7099
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7892
Epoch 02 — loss: 1.7405
Epoch 03 — loss: 1.7004
Stage 5: Error=0.6982, Alpha=0.9532
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4545
Epoch 02 — loss: 1.3393
Epoch 03 — loss: 1.2942
Stage 1: Error=0.5337, Alpha=1.6568
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6110
Epoch 02 — loss: 1.5003
Epoch 03 — loss: 1.4384
Stage 2: Error=0.5724, Alpha=1.5000
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7082
Epoch 02 — loss: 1.6353
Epoch 03 — loss: 1.6132
Stage 3: Error=0.6788, Alpha=1.0436
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7888
Epoch 02 — loss: 1.7128
Epoch 03 — loss: 1.6395
Stage 4: Error=0.6721, Alpha=1.0741
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8076
Epoch 02 — loss: 1.7161
Epoch 03 — loss: 1.6382
Stage 5: Error=0.6613, Alpha=1.1225
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6416
Epoch 02 — loss: 1.3847
Epoch 03 — loss: 1.2937
Stage 1: Error=0.5335, Alpha=1.6577
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7793
Epoch 02 — loss: 1.5859
Epoch 03 — loss: 1.5037
Stage 2: Error=0.6231, Alpha=1.2891
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7992
Epoch 02 — loss: 1.6613
Epoch 03 — loss: 1.6380
Stage 3: Error=0.6840, Alpha=1.0198
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8693
Epoch 02 — loss: 1.7910
Epoch 03 — loss: 1.7140
Stage 4: Error=0.7637, Alpha=0.6188
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8777
Epoch 02 — loss: 1.8205
Epoch 03 — loss: 1.7608
Stage 5: Error=0.7932, Alpha=0.4473
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      82.05
1  adaboost_set_transformer  ...                      81.04
2          adaboost_deepset  ...                      83.21

[3 rows x 5 columns]
------------------------------------iteration no 7------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4593
Epoch 02 — loss: 1.3267
Epoch 03 — loss: 1.2661
Stage 1: Error=0.5105, Alpha=1.7499
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8198
Epoch 02 — loss: 1.6242
Epoch 03 — loss: 1.5277
Stage 2: Error=0.6654, Alpha=1.1043
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6739
Epoch 02 — loss: 1.5662
Epoch 03 — loss: 1.4949
Stage 3: Error=0.5959, Alpha=1.4035
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8786
Epoch 02 — loss: 1.7731
Epoch 03 — loss: 1.7048
Stage 4: Error=0.7392, Alpha=0.7501
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8274
Epoch 02 — loss: 1.7713
Epoch 03 — loss: 1.7358
Stage 5: Error=0.7380, Alpha=0.7561
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4356
Epoch 02 — loss: 1.3495
Epoch 03 — loss: 1.2977
Stage 1: Error=0.5191, Alpha=1.7152
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6736
Epoch 02 — loss: 1.5281
Epoch 03 — loss: 1.4943
Stage 2: Error=0.6291, Alpha=1.2633
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6957
Epoch 02 — loss: 1.6150
Epoch 03 — loss: 1.5785
Stage 3: Error=0.6797, Alpha=1.0394
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7342
Epoch 02 — loss: 1.6271
Epoch 03 — loss: 1.5849
Stage 4: Error=0.6339, Alpha=1.2427
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7761
Epoch 02 — loss: 1.6933
Epoch 03 — loss: 1.6224
Stage 5: Error=0.6888, Alpha=0.9975
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6222
Epoch 02 — loss: 1.3754
Epoch 03 — loss: 1.3310
Stage 1: Error=0.5451, Alpha=1.6108
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7648
Epoch 02 — loss: 1.5637
Epoch 03 — loss: 1.5081
Stage 2: Error=0.6093, Alpha=1.3473
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8063
Epoch 02 — loss: 1.6826
Epoch 03 — loss: 1.6503
Stage 3: Error=0.6852, Alpha=1.0140
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8668
Epoch 02 — loss: 1.7931
Epoch 03 — loss: 1.7328
Stage 4: Error=0.7495, Alpha=0.6958
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8975
Epoch 02 — loss: 1.8142
Epoch 03 — loss: 1.7365
Stage 5: Error=0.7715, Alpha=0.5749
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      78.54
1  adaboost_set_transformer  ...                      80.75
2          adaboost_deepset  ...                      81.04

[3 rows x 5 columns]
------------------------------------iteration no 8------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4509
Epoch 02 — loss: 1.3228
Epoch 03 — loss: 1.3114
Stage 1: Error=0.5208, Alpha=1.7084
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7887
Epoch 02 — loss: 1.6041
Epoch 03 — loss: 1.5262
Stage 2: Error=0.6385, Alpha=1.2231
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6980
Epoch 02 — loss: 1.5880
Epoch 03 — loss: 1.5182
Stage 3: Error=0.6214, Alpha=1.2964
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8995
Epoch 02 — loss: 1.7779
Epoch 03 — loss: 1.7066
Stage 4: Error=0.7055, Alpha=0.9181
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8112
Epoch 02 — loss: 1.7475
Epoch 03 — loss: 1.6969
Stage 5: Error=0.7060, Alpha=0.9157
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4976
Epoch 02 — loss: 1.3537
Epoch 03 — loss: 1.2882
Stage 1: Error=0.5242, Alpha=1.6949
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6417
Epoch 02 — loss: 1.5150
Epoch 03 — loss: 1.4648
Stage 2: Error=0.6077, Alpha=1.3540
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6924
Epoch 02 — loss: 1.6231
Epoch 03 — loss: 1.5842
Stage 3: Error=0.6650, Alpha=1.1063
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7619
Epoch 02 — loss: 1.6784
Epoch 03 — loss: 1.6110
Stage 4: Error=0.6791, Alpha=1.0421
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7691
Epoch 02 — loss: 1.6962
Epoch 03 — loss: 1.6248
Stage 5: Error=0.6562, Alpha=1.1452
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6212
Epoch 02 — loss: 1.3696
Epoch 03 — loss: 1.2915
Stage 1: Error=0.5229, Alpha=1.7002
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7831
Epoch 02 — loss: 1.5764
Epoch 03 — loss: 1.5153
Stage 2: Error=0.6390, Alpha=1.2209
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7938
Epoch 02 — loss: 1.6616
Epoch 03 — loss: 1.6211
Stage 3: Error=0.6909, Alpha=0.9875
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8429
Epoch 02 — loss: 1.7457
Epoch 03 — loss: 1.6757
Stage 4: Error=0.7018, Alpha=0.9358
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8771
Epoch 02 — loss: 1.7921
Epoch 03 — loss: 1.7411
Stage 5: Error=0.7569, Alpha=0.6558
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      80.90
1  adaboost_set_transformer  ...                      82.19
2          adaboost_deepset  ...                      83.30

[3 rows x 5 columns]
------------------------------------iteration no 9------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4777
Epoch 02 — loss: 1.3625
Epoch 03 — loss: 1.3118
Stage 1: Error=0.5582, Alpha=1.5577
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7911
Epoch 02 — loss: 1.5655
Epoch 03 — loss: 1.4970
Stage 2: Error=0.5958, Alpha=1.4036
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7268
Epoch 02 — loss: 1.6146
Epoch 03 — loss: 1.5603
Stage 3: Error=0.6321, Alpha=1.2504
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8796
Epoch 02 — loss: 1.7833
Epoch 03 — loss: 1.7084
Stage 4: Error=0.7521, Alpha=0.6820
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8312
Epoch 02 — loss: 1.7749
Epoch 03 — loss: 1.7234
Stage 5: Error=0.7242, Alpha=0.8264
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4537
Epoch 02 — loss: 1.3374
Epoch 03 — loss: 1.2937
Stage 1: Error=0.5479, Alpha=1.5996
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6343
Epoch 02 — loss: 1.5041
Epoch 03 — loss: 1.4567
Stage 2: Error=0.5827, Alpha=1.4580
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7152
Epoch 02 — loss: 1.6369
Epoch 03 — loss: 1.6133
Stage 3: Error=0.6805, Alpha=1.0355
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8008
Epoch 02 — loss: 1.6850
Epoch 03 — loss: 1.6140
Stage 4: Error=0.6539, Alpha=1.1555
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8210
Epoch 02 — loss: 1.7282
Epoch 03 — loss: 1.6466
Stage 5: Error=0.6724, Alpha=1.0728
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6374
Epoch 02 — loss: 1.3618
Epoch 03 — loss: 1.2912
Stage 1: Error=0.5337, Alpha=1.6568
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7923
Epoch 02 — loss: 1.5733
Epoch 03 — loss: 1.5060
Stage 2: Error=0.6038, Alpha=1.3704
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8116
Epoch 02 — loss: 1.6819
Epoch 03 — loss: 1.6286
Stage 3: Error=0.6673, Alpha=1.0958
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8730
Epoch 02 — loss: 1.7630
Epoch 03 — loss: 1.7092
Stage 4: Error=0.7224, Alpha=0.8355
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8935
Epoch 02 — loss: 1.8235
Epoch 03 — loss: 1.7501
Stage 5: Error=0.7795, Alpha=0.5288
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      79.55
1  adaboost_set_transformer  ...                      81.23
2          adaboost_deepset  ...                      80.75

[3 rows x 5 columns]
------------------------------------iteration no 10------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4472
Epoch 02 — loss: 1.3571
Epoch 03 — loss: 1.3120
Stage 1: Error=0.5252, Alpha=1.6911
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7885
Epoch 02 — loss: 1.6110
Epoch 03 — loss: 1.5360
Stage 2: Error=0.6695, Alpha=1.0861
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6802
Epoch 02 — loss: 1.5718
Epoch 03 — loss: 1.5273
Stage 3: Error=0.6297, Alpha=1.2609
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8426
Epoch 02 — loss: 1.7325
Epoch 03 — loss: 1.6641
Stage 4: Error=0.7224, Alpha=0.8356
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7932
Epoch 02 — loss: 1.7342
Epoch 03 — loss: 1.7061
Stage 5: Error=0.7194, Alpha=0.8505
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4230
Epoch 02 — loss: 1.3129
Epoch 03 — loss: 1.2906
Stage 1: Error=0.5166, Alpha=1.7253
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6501
Epoch 02 — loss: 1.5312
Epoch 03 — loss: 1.4629
Stage 2: Error=0.5954, Alpha=1.4055
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7302
Epoch 02 — loss: 1.6531
Epoch 03 — loss: 1.6104
Stage 3: Error=0.6761, Alpha=1.0557
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7886
Epoch 02 — loss: 1.6910
Epoch 03 — loss: 1.6284
Stage 4: Error=0.6600, Alpha=1.1286
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7987
Epoch 02 — loss: 1.6992
Epoch 03 — loss: 1.6231
Stage 5: Error=0.6457, Alpha=1.1917
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6333
Epoch 02 — loss: 1.3912
Epoch 03 — loss: 1.3241
Stage 1: Error=0.5267, Alpha=1.6848
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8039
Epoch 02 — loss: 1.5872
Epoch 03 — loss: 1.5144
Stage 2: Error=0.6629, Alpha=1.1157
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7686
Epoch 02 — loss: 1.6305
Epoch 03 — loss: 1.5979
Stage 3: Error=0.6690, Alpha=1.0879
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8526
Epoch 02 — loss: 1.7486
Epoch 03 — loss: 1.6746
Stage 4: Error=0.6975, Alpha=0.9565
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8727
Epoch 02 — loss: 1.8086
Epoch 03 — loss: 1.7401
Stage 5: Error=0.7360, Alpha=0.7665
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      79.26
1  adaboost_set_transformer  ...                      82.19
2          adaboost_deepset  ...                      82.29

[3 rows x 5 columns]
------------------------------------iteration no 11------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4710
Epoch 02 — loss: 1.3394
Epoch 03 — loss: 1.3067
Stage 1: Error=0.5400, Alpha=1.6316
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7517
Epoch 02 — loss: 1.5772
Epoch 03 — loss: 1.4991
Stage 2: Error=0.6180, Alpha=1.3106
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7066
Epoch 02 — loss: 1.6101
Epoch 03 — loss: 1.5519
Stage 3: Error=0.6646, Alpha=1.1079
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8566
Epoch 02 — loss: 1.7528
Epoch 03 — loss: 1.6777
Stage 4: Error=0.7207, Alpha=0.8437
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8110
Epoch 02 — loss: 1.7545
Epoch 03 — loss: 1.7211
Stage 5: Error=0.7415, Alpha=0.7379
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4567
Epoch 02 — loss: 1.3714
Epoch 03 — loss: 1.3098
Stage 1: Error=0.5248, Alpha=1.6925
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6136
Epoch 02 — loss: 1.5206
Epoch 03 — loss: 1.4629
Stage 2: Error=0.6057, Alpha=1.3625
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6968
Epoch 02 — loss: 1.6489
Epoch 03 — loss: 1.6228
Stage 3: Error=0.6802, Alpha=1.0371
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7770
Epoch 02 — loss: 1.6874
Epoch 03 — loss: 1.6098
Stage 4: Error=0.6558, Alpha=1.1470
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8131
Epoch 02 — loss: 1.7098
Epoch 03 — loss: 1.6334
Stage 5: Error=0.6758, Alpha=1.0570
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6448
Epoch 02 — loss: 1.3979
Epoch 03 — loss: 1.2988
Stage 1: Error=0.5309, Alpha=1.6679
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7879
Epoch 02 — loss: 1.5826
Epoch 03 — loss: 1.5259
Stage 2: Error=0.6681, Alpha=1.0923
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7646
Epoch 02 — loss: 1.6506
Epoch 03 — loss: 1.5976
Stage 3: Error=0.6707, Alpha=1.0803
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8702
Epoch 02 — loss: 1.8017
Epoch 03 — loss: 1.7154
Stage 4: Error=0.7259, Alpha=0.8180
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8784
Epoch 02 — loss: 1.8256
Epoch 03 — loss: 1.7507
Stage 5: Error=0.7605, Alpha=0.6362
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      80.08
1  adaboost_set_transformer  ...                      81.09
2          adaboost_deepset  ...                      82.05

[3 rows x 5 columns]
------------------------------------iteration no 12------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4509
Epoch 02 — loss: 1.3457
Epoch 03 — loss: 1.2984
Stage 1: Error=0.5297, Alpha=1.6727
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8077
Epoch 02 — loss: 1.6023
Epoch 03 — loss: 1.5254
Stage 2: Error=0.6427, Alpha=1.2048
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7265
Epoch 02 — loss: 1.6285
Epoch 03 — loss: 1.5496
Stage 3: Error=0.6191, Alpha=1.3061
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8758
Epoch 02 — loss: 1.7366
Epoch 03 — loss: 1.6896
Stage 4: Error=0.7334, Alpha=0.7797
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7957
Epoch 02 — loss: 1.7568
Epoch 03 — loss: 1.7224
Stage 5: Error=0.7164, Alpha=0.8651
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4473
Epoch 02 — loss: 1.3677
Epoch 03 — loss: 1.3064
Stage 1: Error=0.5247, Alpha=1.6930
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6487
Epoch 02 — loss: 1.5363
Epoch 03 — loss: 1.5079
Stage 2: Error=0.6325, Alpha=1.2489
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7260
Epoch 02 — loss: 1.6503
Epoch 03 — loss: 1.6191
Stage 3: Error=0.6736, Alpha=1.0675
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7665
Epoch 02 — loss: 1.6810
Epoch 03 — loss: 1.6042
Stage 4: Error=0.6411, Alpha=1.2118
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7868
Epoch 02 — loss: 1.7112
Epoch 03 — loss: 1.6507
Stage 5: Error=0.6677, Alpha=1.0939
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6553
Epoch 02 — loss: 1.3842
Epoch 03 — loss: 1.3009
Stage 1: Error=0.5306, Alpha=1.6693
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8013
Epoch 02 — loss: 1.6150
Epoch 03 — loss: 1.5201
Stage 2: Error=0.6348, Alpha=1.2390
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7721
Epoch 02 — loss: 1.6565
Epoch 03 — loss: 1.6201
Stage 3: Error=0.6554, Alpha=1.1491
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8554
Epoch 02 — loss: 1.7725
Epoch 03 — loss: 1.7077
Stage 4: Error=0.7264, Alpha=0.8153
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8847
Epoch 02 — loss: 1.8429
Epoch 03 — loss: 1.7643
Stage 5: Error=0.7574, Alpha=0.6533
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      79.74
1  adaboost_set_transformer  ...                      81.62
2          adaboost_deepset  ...                      81.23

[3 rows x 5 columns]
------------------------------------iteration no 13------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4614
Epoch 02 — loss: 1.3397
Epoch 03 — loss: 1.2914
Stage 1: Error=0.5330, Alpha=1.6597
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7878
Epoch 02 — loss: 1.5952
Epoch 03 — loss: 1.5458
Stage 2: Error=0.7044, Alpha=0.9232
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7093
Epoch 02 — loss: 1.5876
Epoch 03 — loss: 1.5163
Stage 3: Error=0.6172, Alpha=1.3139
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8528
Epoch 02 — loss: 1.7733
Epoch 03 — loss: 1.6882
Stage 4: Error=0.7085, Alpha=0.9039
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8066
Epoch 02 — loss: 1.7367
Epoch 03 — loss: 1.6781
Stage 5: Error=0.6896, Alpha=0.9935
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4546
Epoch 02 — loss: 1.3494
Epoch 03 — loss: 1.2987
Stage 1: Error=0.5099, Alpha=1.7523
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6443
Epoch 02 — loss: 1.5359
Epoch 03 — loss: 1.5045
Stage 2: Error=0.6243, Alpha=1.2839
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7187
Epoch 02 — loss: 1.6524
Epoch 03 — loss: 1.6176
Stage 3: Error=0.6677, Alpha=1.0941
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7760
Epoch 02 — loss: 1.6729
Epoch 03 — loss: 1.6045
Stage 4: Error=0.6342, Alpha=1.2415
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7930
Epoch 02 — loss: 1.7158
Epoch 03 — loss: 1.6379
Stage 5: Error=0.6576, Alpha=1.1391
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6231
Epoch 02 — loss: 1.3867
Epoch 03 — loss: 1.2922
Stage 1: Error=0.5264, Alpha=1.6862
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7943
Epoch 02 — loss: 1.6001
Epoch 03 — loss: 1.5317
Stage 2: Error=0.6540, Alpha=1.1551
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7944
Epoch 02 — loss: 1.6571
Epoch 03 — loss: 1.6262
Stage 3: Error=0.6973, Alpha=0.9573
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8382
Epoch 02 — loss: 1.7393
Epoch 03 — loss: 1.6712
Stage 4: Error=0.7494, Alpha=0.6965
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8670
Epoch 02 — loss: 1.7840
Epoch 03 — loss: 1.7091
Stage 5: Error=0.7586, Alpha=0.6465
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      79.84
1  adaboost_set_transformer  ...                      81.52
2          adaboost_deepset  ...                      80.90

[3 rows x 5 columns]
------------------------------------iteration no 14------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4369
Epoch 02 — loss: 1.3356
Epoch 03 — loss: 1.2797
Stage 1: Error=0.5178, Alpha=1.7205
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8050
Epoch 02 — loss: 1.5932
Epoch 03 — loss: 1.5210
Stage 2: Error=0.6356, Alpha=1.2353
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7281
Epoch 02 — loss: 1.6145
Epoch 03 — loss: 1.5440
Stage 3: Error=0.6269, Alpha=1.2728
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8711
Epoch 02 — loss: 1.7640
Epoch 03 — loss: 1.7031
Stage 4: Error=0.7324, Alpha=0.7848
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8174
Epoch 02 — loss: 1.7645
Epoch 03 — loss: 1.7115
Stage 5: Error=0.7079, Alpha=0.9067
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4548
Epoch 02 — loss: 1.3390
Epoch 03 — loss: 1.3097
Stage 1: Error=0.5200, Alpha=1.7118
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6303
Epoch 02 — loss: 1.5450
Epoch 03 — loss: 1.4753
Stage 2: Error=0.6136, Alpha=1.3294
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7048
Epoch 02 — loss: 1.6447
Epoch 03 — loss: 1.5944
Stage 3: Error=0.6812, Alpha=1.0323
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7877
Epoch 02 — loss: 1.6808
Epoch 03 — loss: 1.6024
Stage 4: Error=0.6310, Alpha=1.2552
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8080
Epoch 02 — loss: 1.7061
Epoch 03 — loss: 1.6357
Stage 5: Error=0.6565, Alpha=1.1441
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6140
Epoch 02 — loss: 1.3563
Epoch 03 — loss: 1.2966
Stage 1: Error=0.5301, Alpha=1.6713
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8053
Epoch 02 — loss: 1.6205
Epoch 03 — loss: 1.5269
Stage 2: Error=0.6700, Alpha=1.0835
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7838
Epoch 02 — loss: 1.6441
Epoch 03 — loss: 1.5808
Stage 3: Error=0.6565, Alpha=1.1441
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8462
Epoch 02 — loss: 1.7385
Epoch 03 — loss: 1.6852
Stage 4: Error=0.7184, Alpha=0.8554
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8757
Epoch 02 — loss: 1.8200
Epoch 03 — loss: 1.7667
Stage 5: Error=0.7803, Alpha=0.5241
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.91
1  adaboost_set_transformer  ...                      81.95
2          adaboost_deepset  ...                      82.29

[3 rows x 5 columns]
------------------------------------iteration no 15------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4749
Epoch 02 — loss: 1.3496
Epoch 03 — loss: 1.3214
Stage 1: Error=0.5310, Alpha=1.6674
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7769
Epoch 02 — loss: 1.5915
Epoch 03 — loss: 1.4969
Stage 2: Error=0.6172, Alpha=1.3139
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7231
Epoch 02 — loss: 1.6301
Epoch 03 — loss: 1.5798
Stage 3: Error=0.6369, Alpha=1.2300
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8768
Epoch 02 — loss: 1.7675
Epoch 03 — loss: 1.6999
Stage 4: Error=0.7413, Alpha=0.7392
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8336
Epoch 02 — loss: 1.7717
Epoch 03 — loss: 1.7292
Stage 5: Error=0.6919, Alpha=0.9825
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4164
Epoch 02 — loss: 1.3157
Epoch 03 — loss: 1.2795
Stage 1: Error=0.5166, Alpha=1.7253
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6384
Epoch 02 — loss: 1.5342
Epoch 03 — loss: 1.4903
Stage 2: Error=0.6253, Alpha=1.2798
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6997
Epoch 02 — loss: 1.6431
Epoch 03 — loss: 1.6035
Stage 3: Error=0.6817, Alpha=1.0301
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7840
Epoch 02 — loss: 1.7026
Epoch 03 — loss: 1.6450
Stage 4: Error=0.6649, Alpha=1.1067
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8137
Epoch 02 — loss: 1.7261
Epoch 03 — loss: 1.6514
Stage 5: Error=0.6753, Alpha=1.0594
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6173
Epoch 02 — loss: 1.3407
Epoch 03 — loss: 1.3023
Stage 1: Error=0.5291, Alpha=1.6751
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7790
Epoch 02 — loss: 1.5612
Epoch 03 — loss: 1.4997
Stage 2: Error=0.6733, Alpha=1.0686
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7740
Epoch 02 — loss: 1.6276
Epoch 03 — loss: 1.5753
Stage 3: Error=0.6409, Alpha=1.2126
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8615
Epoch 02 — loss: 1.7472
Epoch 03 — loss: 1.7012
Stage 4: Error=0.7184, Alpha=0.8550
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8734
Epoch 02 — loss: 1.7930
Epoch 03 — loss: 1.7483
Stage 5: Error=0.7550, Alpha=0.6662
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.23
1  adaboost_set_transformer  ...                      81.57
2          adaboost_deepset  ...                      82.39

[3 rows x 5 columns]
------------------------------------iteration no 16------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4891
Epoch 02 — loss: 1.3571
Epoch 03 — loss: 1.3157
Stage 1: Error=0.5268, Alpha=1.6843
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7780
Epoch 02 — loss: 1.6233
Epoch 03 — loss: 1.5229
Stage 2: Error=0.6838, Alpha=1.0205
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6960
Epoch 02 — loss: 1.5854
Epoch 03 — loss: 1.5286
Stage 3: Error=0.6510, Alpha=1.1685
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8298
Epoch 02 — loss: 1.7149
Epoch 03 — loss: 1.6496
Stage 4: Error=0.6955, Alpha=0.9656
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7974
Epoch 02 — loss: 1.7470
Epoch 03 — loss: 1.7007
Stage 5: Error=0.7035, Alpha=0.9277
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4436
Epoch 02 — loss: 1.3320
Epoch 03 — loss: 1.3058
Stage 1: Error=0.5387, Alpha=1.6365
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6240
Epoch 02 — loss: 1.5060
Epoch 03 — loss: 1.4568
Stage 2: Error=0.6027, Alpha=1.3751
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7209
Epoch 02 — loss: 1.6196
Epoch 03 — loss: 1.5862
Stage 3: Error=0.6658, Alpha=1.1023
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7814
Epoch 02 — loss: 1.6719
Epoch 03 — loss: 1.6118
Stage 4: Error=0.6397, Alpha=1.2178
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8177
Epoch 02 — loss: 1.7046
Epoch 03 — loss: 1.6322
Stage 5: Error=0.6652, Alpha=1.1051
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6406
Epoch 02 — loss: 1.3717
Epoch 03 — loss: 1.3085
Stage 1: Error=0.5268, Alpha=1.6843
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7776
Epoch 02 — loss: 1.5656
Epoch 03 — loss: 1.5064
Stage 2: Error=0.6835, Alpha=1.0220
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7402
Epoch 02 — loss: 1.6037
Epoch 03 — loss: 1.5625
Stage 3: Error=0.6137, Alpha=1.3291
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8721
Epoch 02 — loss: 1.7845
Epoch 03 — loss: 1.7298
Stage 4: Error=0.7606, Alpha=0.6357
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8807
Epoch 02 — loss: 1.8093
Epoch 03 — loss: 1.7445
Stage 5: Error=0.7922, Alpha=0.4534
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.57
1  adaboost_set_transformer  ...                      82.58
2          adaboost_deepset  ...                      81.95

[3 rows x 5 columns]
------------------------------------iteration no 17------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4088
Epoch 02 — loss: 1.3321
Epoch 03 — loss: 1.2960
Stage 1: Error=0.5254, Alpha=1.6901
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7773
Epoch 02 — loss: 1.5870
Epoch 03 — loss: 1.5091
Stage 2: Error=0.6219, Alpha=1.2941
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7082
Epoch 02 — loss: 1.5973
Epoch 03 — loss: 1.5634
Stage 3: Error=0.6205, Alpha=1.3002
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8597
Epoch 02 — loss: 1.7621
Epoch 03 — loss: 1.7163
Stage 4: Error=0.7549, Alpha=0.6671
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8132
Epoch 02 — loss: 1.7384
Epoch 03 — loss: 1.6926
Stage 5: Error=0.7075, Alpha=0.9084
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4150
Epoch 02 — loss: 1.3279
Epoch 03 — loss: 1.2870
Stage 1: Error=0.5170, Alpha=1.7239
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6447
Epoch 02 — loss: 1.5400
Epoch 03 — loss: 1.4833
Stage 2: Error=0.6133, Alpha=1.3307
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7133
Epoch 02 — loss: 1.6412
Epoch 03 — loss: 1.5991
Stage 3: Error=0.6694, Alpha=1.0862
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7722
Epoch 02 — loss: 1.6820
Epoch 03 — loss: 1.6284
Stage 4: Error=0.6730, Alpha=1.0698
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7894
Epoch 02 — loss: 1.7132
Epoch 03 — loss: 1.6301
Stage 5: Error=0.6550, Alpha=1.1509
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6530
Epoch 02 — loss: 1.3908
Epoch 03 — loss: 1.3080
Stage 1: Error=0.5401, Alpha=1.6311
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7917
Epoch 02 — loss: 1.5695
Epoch 03 — loss: 1.5161
Stage 2: Error=0.6337, Alpha=1.2436
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7816
Epoch 02 — loss: 1.6629
Epoch 03 — loss: 1.6293
Stage 3: Error=0.6646, Alpha=1.1079
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8767
Epoch 02 — loss: 1.8016
Epoch 03 — loss: 1.7170
Stage 4: Error=0.7100, Alpha=0.8965
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.9047
Epoch 02 — loss: 1.8318
Epoch 03 — loss: 1.7432
Stage 5: Error=0.7608, Alpha=0.6346
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.28
1  adaboost_set_transformer  ...                      81.47
2          adaboost_deepset  ...                      81.33

[3 rows x 5 columns]
------------------------------------iteration no 18------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4702
Epoch 02 — loss: 1.3567
Epoch 03 — loss: 1.3057
Stage 1: Error=0.5176, Alpha=1.7215
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7845
Epoch 02 — loss: 1.6041
Epoch 03 — loss: 1.5399
Stage 2: Error=0.6746, Alpha=1.0627
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6906
Epoch 02 — loss: 1.5799
Epoch 03 — loss: 1.5211
Stage 3: Error=0.6328, Alpha=1.2476
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8340
Epoch 02 — loss: 1.7400
Epoch 03 — loss: 1.6846
Stage 4: Error=0.7410, Alpha=0.7405
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7825
Epoch 02 — loss: 1.7135
Epoch 03 — loss: 1.6743
Stage 5: Error=0.7105, Alpha=0.8938
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4215
Epoch 02 — loss: 1.3331
Epoch 03 — loss: 1.3001
Stage 1: Error=0.5207, Alpha=1.7089
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6658
Epoch 02 — loss: 1.5551
Epoch 03 — loss: 1.4863
Stage 2: Error=0.6157, Alpha=1.3205
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7224
Epoch 02 — loss: 1.6463
Epoch 03 — loss: 1.6165
Stage 3: Error=0.6741, Alpha=1.0649
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7832
Epoch 02 — loss: 1.6923
Epoch 03 — loss: 1.6708
Stage 4: Error=0.6709, Alpha=1.0795
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8200
Epoch 02 — loss: 1.7290
Epoch 03 — loss: 1.6308
Stage 5: Error=0.6611, Alpha=1.1235
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6475
Epoch 02 — loss: 1.4104
Epoch 03 — loss: 1.3180
Stage 1: Error=0.5271, Alpha=1.6834
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7872
Epoch 02 — loss: 1.5743
Epoch 03 — loss: 1.5130
Stage 2: Error=0.6331, Alpha=1.2463
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7728
Epoch 02 — loss: 1.6693
Epoch 03 — loss: 1.6117
Stage 3: Error=0.6668, Alpha=1.0982
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8728
Epoch 02 — loss: 1.7687
Epoch 03 — loss: 1.7110
Stage 4: Error=0.7320, Alpha=0.7870
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8901
Epoch 02 — loss: 1.8263
Epoch 03 — loss: 1.7535
Stage 5: Error=0.7466, Alpha=0.7110
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.76
1  adaboost_set_transformer  ...                      81.28
2          adaboost_deepset  ...                      83.30

[3 rows x 5 columns]
------------------------------------iteration no 19------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4527
Epoch 02 — loss: 1.3245
Epoch 03 — loss: 1.2769
Stage 1: Error=0.5236, Alpha=1.6973
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7997
Epoch 02 — loss: 1.6226
Epoch 03 — loss: 1.5338
Stage 2: Error=0.6459, Alpha=1.1909
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7192
Epoch 02 — loss: 1.6064
Epoch 03 — loss: 1.5261
Stage 3: Error=0.6349, Alpha=1.2386
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8697
Epoch 02 — loss: 1.7395
Epoch 03 — loss: 1.6684
Stage 4: Error=0.7180, Alpha=0.8570
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7840
Epoch 02 — loss: 1.7321
Epoch 03 — loss: 1.7021
Stage 5: Error=0.7553, Alpha=0.6645
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4665
Epoch 02 — loss: 1.3432
Epoch 03 — loss: 1.2927
Stage 1: Error=0.5158, Alpha=1.7287
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6227
Epoch 02 — loss: 1.5362
Epoch 03 — loss: 1.4733
Stage 2: Error=0.6146, Alpha=1.3252
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6980
Epoch 02 — loss: 1.6383
Epoch 03 — loss: 1.5867
Stage 3: Error=0.6886, Alpha=0.9982
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7695
Epoch 02 — loss: 1.6772
Epoch 03 — loss: 1.6112
Stage 4: Error=0.6473, Alpha=1.1846
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8061
Epoch 02 — loss: 1.6949
Epoch 03 — loss: 1.6400
Stage 5: Error=0.6687, Alpha=1.0897
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6283
Epoch 02 — loss: 1.3759
Epoch 03 — loss: 1.3300
Stage 1: Error=0.5282, Alpha=1.6790
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7961
Epoch 02 — loss: 1.5750
Epoch 03 — loss: 1.5089
Stage 2: Error=0.6442, Alpha=1.1980
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7783
Epoch 02 — loss: 1.6449
Epoch 03 — loss: 1.6104
Stage 3: Error=0.6575, Alpha=1.1397
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8760
Epoch 02 — loss: 1.7805
Epoch 03 — loss: 1.7144
Stage 4: Error=0.7537, Alpha=0.6731
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8697
Epoch 02 — loss: 1.8112
Epoch 03 — loss: 1.7363
Stage 5: Error=0.7629, Alpha=0.6229
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.86
1  adaboost_set_transformer  ...                      81.86
2          adaboost_deepset  ...                      82.39

[3 rows x 5 columns]
------------------------------------iteration no 20------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4533
Epoch 02 — loss: 1.3559
Epoch 03 — loss: 1.2899
Stage 1: Error=0.5304, Alpha=1.6698
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8196
Epoch 02 — loss: 1.5948
Epoch 03 — loss: 1.5126
Stage 2: Error=0.6609, Alpha=1.1245
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7033
Epoch 02 — loss: 1.5774
Epoch 03 — loss: 1.5124
Stage 3: Error=0.6089, Alpha=1.3489
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8803
Epoch 02 — loss: 1.7639
Epoch 03 — loss: 1.7054
Stage 4: Error=0.7654, Alpha=0.6091
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8101
Epoch 02 — loss: 1.7303
Epoch 03 — loss: 1.6861
Stage 5: Error=0.7005, Alpha=0.9423
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4554
Epoch 02 — loss: 1.3221
Epoch 03 — loss: 1.2939
Stage 1: Error=0.5286, Alpha=1.6771
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6309
Epoch 02 — loss: 1.5261
Epoch 03 — loss: 1.4747
Stage 2: Error=0.6225, Alpha=1.2915
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7407
Epoch 02 — loss: 1.6397
Epoch 03 — loss: 1.5976
Stage 3: Error=0.6513, Alpha=1.1669
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7730
Epoch 02 — loss: 1.6722
Epoch 03 — loss: 1.5953
Stage 4: Error=0.6371, Alpha=1.2291
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8069
Epoch 02 — loss: 1.7114
Epoch 03 — loss: 1.6349
Stage 5: Error=0.6534, Alpha=1.1577
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6153
Epoch 02 — loss: 1.3939
Epoch 03 — loss: 1.3161
Stage 1: Error=0.5262, Alpha=1.6867
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8003
Epoch 02 — loss: 1.5863
Epoch 03 — loss: 1.5349
Stage 2: Error=0.6445, Alpha=1.1969
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7813
Epoch 02 — loss: 1.6505
Epoch 03 — loss: 1.6073
Stage 3: Error=0.6577, Alpha=1.1385
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8542
Epoch 02 — loss: 1.7674
Epoch 03 — loss: 1.6962
Stage 4: Error=0.7474, Alpha=0.7070
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8597
Epoch 02 — loss: 1.7943
Epoch 03 — loss: 1.7281
Stage 5: Error=0.7592, Alpha=0.6434
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.52
1  adaboost_set_transformer  ...                      81.33
2          adaboost_deepset  ...                      80.85

[3 rows x 5 columns]
------------------------------------iteration no 21------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4386
Epoch 02 — loss: 1.3608
Epoch 03 — loss: 1.3126
Stage 1: Error=0.5197, Alpha=1.7128
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.8165
Epoch 02 — loss: 1.6186
Epoch 03 — loss: 1.5384
Stage 2: Error=0.6537, Alpha=1.1566
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6828
Epoch 02 — loss: 1.5559
Epoch 03 — loss: 1.5284
Stage 3: Error=0.6015, Alpha=1.3801
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8701
Epoch 02 — loss: 1.7613
Epoch 03 — loss: 1.7108
Stage 4: Error=0.7631, Alpha=0.6219
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8181
Epoch 02 — loss: 1.7633
Epoch 03 — loss: 1.7207
Stage 5: Error=0.7271, Alpha=0.8116
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4753
Epoch 02 — loss: 1.3359
Epoch 03 — loss: 1.3014
Stage 1: Error=0.5243, Alpha=1.6945
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6034
Epoch 02 — loss: 1.5168
Epoch 03 — loss: 1.4764
Stage 2: Error=0.6275, Alpha=1.2701
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6880
Epoch 02 — loss: 1.6369
Epoch 03 — loss: 1.6102
Stage 3: Error=0.6867, Alpha=1.0070
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7436
Epoch 02 — loss: 1.6574
Epoch 03 — loss: 1.5813
Stage 4: Error=0.6606, Alpha=1.1260
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7924
Epoch 02 — loss: 1.6894
Epoch 03 — loss: 1.6148
Stage 5: Error=0.6574, Alpha=1.1400
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6279
Epoch 02 — loss: 1.3648
Epoch 03 — loss: 1.3023
Stage 1: Error=0.5339, Alpha=1.6558
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7844
Epoch 02 — loss: 1.5934
Epoch 03 — loss: 1.5332
Stage 2: Error=0.6508, Alpha=1.1690
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7971
Epoch 02 — loss: 1.6595
Epoch 03 — loss: 1.6092
Stage 3: Error=0.6557, Alpha=1.1475
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8454
Epoch 02 — loss: 1.7573
Epoch 03 — loss: 1.7031
Stage 4: Error=0.7684, Alpha=0.5927
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8678
Epoch 02 — loss: 1.7890
Epoch 03 — loss: 1.7237
Stage 5: Error=0.7565, Alpha=0.6579
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.18
1  adaboost_set_transformer  ...                      81.33
2          adaboost_deepset  ...                      80.94

[3 rows x 5 columns]
------------------------------------iteration no 22------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4650
Epoch 02 — loss: 1.3258
Epoch 03 — loss: 1.2971
Stage 1: Error=0.5160, Alpha=1.7277
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7985
Epoch 02 — loss: 1.6291
Epoch 03 — loss: 1.5457
Stage 2: Error=0.6967, Alpha=0.9601
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7222
Epoch 02 — loss: 1.6054
Epoch 03 — loss: 1.5461
Stage 3: Error=0.6276, Alpha=1.2698
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8509
Epoch 02 — loss: 1.7338
Epoch 03 — loss: 1.6820
Stage 4: Error=0.7376, Alpha=0.7584
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7898
Epoch 02 — loss: 1.7403
Epoch 03 — loss: 1.7055
Stage 5: Error=0.7650, Alpha=0.6113
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4631
Epoch 02 — loss: 1.3384
Epoch 03 — loss: 1.2861
Stage 1: Error=0.5308, Alpha=1.6684
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6645
Epoch 02 — loss: 1.5469
Epoch 03 — loss: 1.4949
Stage 2: Error=0.6203, Alpha=1.3010
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6845
Epoch 02 — loss: 1.5993
Epoch 03 — loss: 1.5718
Stage 3: Error=0.6537, Alpha=1.1565
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7768
Epoch 02 — loss: 1.6605
Epoch 03 — loss: 1.6032
Stage 4: Error=0.6449, Alpha=1.1952
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8127
Epoch 02 — loss: 1.7127
Epoch 03 — loss: 1.6268
Stage 5: Error=0.6580, Alpha=1.1372
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6333
Epoch 02 — loss: 1.3938
Epoch 03 — loss: 1.3073
Stage 1: Error=0.5247, Alpha=1.6930
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7991
Epoch 02 — loss: 1.6031
Epoch 03 — loss: 1.5184
Stage 2: Error=0.6627, Alpha=1.1164
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7679
Epoch 02 — loss: 1.6498
Epoch 03 — loss: 1.5996
Stage 3: Error=0.6444, Alpha=1.1971
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8663
Epoch 02 — loss: 1.7842
Epoch 03 — loss: 1.7077
Stage 4: Error=0.7503, Alpha=0.6915
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8764
Epoch 02 — loss: 1.8037
Epoch 03 — loss: 1.7493
Stage 5: Error=0.7632, Alpha=0.6213
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.71
1  adaboost_set_transformer  ...                      80.08
2          adaboost_deepset  ...                      81.57

[3 rows x 5 columns]
------------------------------------iteration no 23------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4909
Epoch 02 — loss: 1.3522
Epoch 03 — loss: 1.3179
Stage 1: Error=0.5197, Alpha=1.7128
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7842
Epoch 02 — loss: 1.5806
Epoch 03 — loss: 1.5253
Stage 2: Error=0.6622, Alpha=1.1187
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6984
Epoch 02 — loss: 1.5807
Epoch 03 — loss: 1.5329
Stage 3: Error=0.6054, Alpha=1.3635
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8919
Epoch 02 — loss: 1.7567
Epoch 03 — loss: 1.6964
Stage 4: Error=0.7293, Alpha=0.8006
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7949
Epoch 02 — loss: 1.7406
Epoch 03 — loss: 1.6740
Stage 5: Error=0.7188, Alpha=0.8532
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4532
Epoch 02 — loss: 1.3662
Epoch 03 — loss: 1.3190
Stage 1: Error=0.5258, Alpha=1.6887
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6240
Epoch 02 — loss: 1.5181
Epoch 03 — loss: 1.4666
Stage 2: Error=0.6414, Alpha=1.2104
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6699
Epoch 02 — loss: 1.5994
Epoch 03 — loss: 1.5749
Stage 3: Error=0.6745, Alpha=1.0631
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7555
Epoch 02 — loss: 1.6581
Epoch 03 — loss: 1.5982
Stage 4: Error=0.6535, Alpha=1.1573
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8030
Epoch 02 — loss: 1.6881
Epoch 03 — loss: 1.5985
Stage 5: Error=0.6484, Alpha=1.1796
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6273
Epoch 02 — loss: 1.3885
Epoch 03 — loss: 1.3057
Stage 1: Error=0.5283, Alpha=1.6785
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7995
Epoch 02 — loss: 1.5987
Epoch 03 — loss: 1.5128
Stage 2: Error=0.6385, Alpha=1.2227
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7732
Epoch 02 — loss: 1.6613
Epoch 03 — loss: 1.6123
Stage 3: Error=0.6766, Alpha=1.0535
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8763
Epoch 02 — loss: 1.8054
Epoch 03 — loss: 1.7237
Stage 4: Error=0.7815, Alpha=0.5172
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8836
Epoch 02 — loss: 1.8031
Epoch 03 — loss: 1.7392
Stage 5: Error=0.7257, Alpha=0.8189
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      80.99
1  adaboost_set_transformer  ...                      81.62
2          adaboost_deepset  ...                      81.38

[3 rows x 5 columns]
------------------------------------iteration no 24------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4346
Epoch 02 — loss: 1.3562
Epoch 03 — loss: 1.3170
Stage 1: Error=0.5248, Alpha=1.6925
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7832
Epoch 02 — loss: 1.6120
Epoch 03 — loss: 1.5323
Stage 2: Error=0.6732, Alpha=1.0690
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.6871
Epoch 02 — loss: 1.5834
Epoch 03 — loss: 1.5251
Stage 3: Error=0.6354, Alpha=1.2364
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8413
Epoch 02 — loss: 1.7436
Epoch 03 — loss: 1.6791
Stage 4: Error=0.7252, Alpha=0.8211
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.7953
Epoch 02 — loss: 1.7298
Epoch 03 — loss: 1.6809
Stage 5: Error=0.7035, Alpha=0.9275
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4477
Epoch 02 — loss: 1.3226
Epoch 03 — loss: 1.2734
Stage 1: Error=0.5211, Alpha=1.7075
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6454
Epoch 02 — loss: 1.5255
Epoch 03 — loss: 1.4946
Stage 2: Error=0.6156, Alpha=1.3207
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7091
Epoch 02 — loss: 1.6233
Epoch 03 — loss: 1.6010
Stage 3: Error=0.6506, Alpha=1.1702
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8115
Epoch 02 — loss: 1.7182
Epoch 03 — loss: 1.6582
Stage 4: Error=0.6685, Alpha=1.0902
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8234
Epoch 02 — loss: 1.7290
Epoch 03 — loss: 1.6453
Stage 5: Error=0.6583, Alpha=1.1361
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.5976
Epoch 02 — loss: 1.3674
Epoch 03 — loss: 1.3168
Stage 1: Error=0.5385, Alpha=1.6374
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7715
Epoch 02 — loss: 1.5444
Epoch 03 — loss: 1.4726
Stage 2: Error=0.5924, Alpha=1.4178
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.8080
Epoch 02 — loss: 1.6608
Epoch 03 — loss: 1.6314
Stage 3: Error=0.6700, Alpha=1.0835
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8833
Epoch 02 — loss: 1.7836
Epoch 03 — loss: 1.6870
Stage 4: Error=0.7097, Alpha=0.8979
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8704
Epoch 02 — loss: 1.8237
Epoch 03 — loss: 1.7727
Stage 5: Error=0.7400, Alpha=0.7456
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      80.94
1  adaboost_set_transformer  ...                      83.11
2          adaboost_deepset  ...                      82.05

[3 rows x 5 columns]
------------------------------------iteration no 25------------------------------------
===== Processing adaboost_all =====
===== Training Boosting Ensemble (['set_transformer', 'deepset', 'set_transformer_xy', 'deepset_xy', 'set_transformer_additive', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4735
Epoch 02 — loss: 1.3512
Epoch 03 — loss: 1.3001
Stage 1: Error=0.5265, Alpha=1.6858
--- Boosting Stage 2/5 (deepset) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7908
Epoch 02 — loss: 1.6164
Epoch 03 — loss: 1.5348
Stage 2: Error=0.6571, Alpha=1.1414
--- Boosting Stage 3/5 (set_transformer_xy) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7436
Epoch 02 — loss: 1.6258
Epoch 03 — loss: 1.5638
Stage 3: Error=0.6153, Alpha=1.3220
--- Boosting Stage 4/5 (deepset_xy) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8762
Epoch 02 — loss: 1.8007
Epoch 03 — loss: 1.6988
Stage 4: Error=0.7450, Alpha=0.7196
--- Boosting Stage 5/5 (set_transformer_additive) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8089
Epoch 02 — loss: 1.7578
Epoch 03 — loss: 1.6910
Stage 5: Error=0.6779, Alpha=1.0476
Building final ensemble with 5 models.
Predictions for adaboost_all_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_set_transformer =====
===== Training Boosting Ensemble (['set_transformer', 'set_transformer_xy', 'set_transformer_additive'], 5 stages) =====
--- Boosting Stage 1/5 (set_transformer) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.4665
Epoch 02 — loss: 1.3471
Epoch 03 — loss: 1.3183
Stage 1: Error=0.5170, Alpha=1.7239
--- Boosting Stage 2/5 (set_transformer_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.6415
Epoch 02 — loss: 1.5413
Epoch 03 — loss: 1.4861
Stage 2: Error=0.6106, Alpha=1.3418
--- Boosting Stage 3/5 (set_transformer_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7063
Epoch 02 — loss: 1.6414
Epoch 03 — loss: 1.6087
Stage 3: Error=0.6524, Alpha=1.1623
--- Boosting Stage 4/5 (set_transformer) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.7734
Epoch 02 — loss: 1.6828
Epoch 03 — loss: 1.6150
Stage 4: Error=0.6463, Alpha=1.1890
--- Boosting Stage 5/5 (set_transformer_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8077
Epoch 02 — loss: 1.7199
Epoch 03 — loss: 1.6438
Stage 5: Error=0.6610, Alpha=1.1240
Building final ensemble with 5 models.
Predictions for adaboost_set_transformer_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
===== Processing adaboost_deepset =====
===== Training Boosting Ensemble (['deepset', 'deepset_xy', 'deepset_xy_additive'], 5 stages) =====
--- Boosting Stage 1/5 (deepset) ---
Training weak learner 1 for 3 epochs...
Epoch 01 — loss: 1.6203
Epoch 02 — loss: 1.3677
Epoch 03 — loss: 1.2997
Stage 1: Error=0.5309, Alpha=1.6679
--- Boosting Stage 2/5 (deepset_xy) ---
Training weak learner 2 for 3 epochs...
Epoch 01 — loss: 1.7837
Epoch 02 — loss: 1.5928
Epoch 03 — loss: 1.5223
Stage 2: Error=0.6292, Alpha=1.2631
--- Boosting Stage 3/5 (deepset_xy_additive) ---
Training weak learner 3 for 3 epochs...
Epoch 01 — loss: 1.7670
Epoch 02 — loss: 1.6546
Epoch 03 — loss: 1.6324
Stage 3: Error=0.6441, Alpha=1.1987
--- Boosting Stage 4/5 (deepset) ---
Training weak learner 4 for 3 epochs...
Epoch 01 — loss: 1.8696
Epoch 02 — loss: 1.7937
Epoch 03 — loss: 1.7186
Stage 4: Error=0.7396, Alpha=0.7481
--- Boosting Stage 5/5 (deepset_xy) ---
Training weak learner 5 for 3 epochs...
Epoch 01 — loss: 1.8850
Epoch 02 — loss: 1.8282
Epoch 03 — loss: 1.7497
Stage 5: Error=0.7523, Alpha=0.6806
Building final ensemble with 5 models.
Predictions for adaboost_deepset_preds exported to: result/model_comparison_results.xlsx
Outliers saved to: /home/patr/docker-dir/set-transformer-model/grade_predictor/result/outlier.xlsx
=== Model Comparison Summary ===
                 Model Type  ...  Val ±1 Grade Accuracy (%)
0              adaboost_all  ...                      81.28
1  adaboost_set_transformer  ...                      81.04
2          adaboost_deepset  ...                      80.85

[3 rows x 5 columns]
=== Accuracy Stability Summary ===
                 Model Type  ...  Val ±1 Grade Std (%)
0              adaboost_all  ...                 0.947
2          adaboost_deepset  ...                 0.818
1  adaboost_set_transformer  ...                 0.671

[3 rows x 9 columns]
----------------- Ordinal iteration 1/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3735
Epoch 02 — loss: 0.3171
Epoch 03 — loss: 0.2997
Epoch 04 — loss: 0.2903
Epoch 05 — loss: 0.2804
Epoch 06 — loss: 0.2713
Epoch 07 — loss: 0.2651
Epoch 08 — loss: 0.2569
Epoch 09 — loss: 0.2501
Epoch 10 — loss: 0.2437
Epoch 11 — loss: 0.2373
Epoch 12 — loss: 0.2284
Epoch 13 — loss: 0.2228
Epoch 14 — loss: 0.2175
Epoch 15 — loss: 0.2109
Epoch 16 — loss: 0.2032
Epoch 17 — loss: 0.1988
Epoch 18 — loss: 0.1954
Epoch 19 — loss: 0.1887
Epoch 20 — loss: 0.1807
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  81.230003
2    P(>V6)  84.120003
3    P(>V7)  87.050003
4    P(>V8)  92.879997
5    P(>V9)  96.290001
Overall accuracy: 48.12%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3736
Epoch 02 — loss: 0.3142
Epoch 03 — loss: 0.3029
Epoch 04 — loss: 0.2911
Epoch 05 — loss: 0.2804
Epoch 06 — loss: 0.2722
Epoch 07 — loss: 0.2614
Epoch 08 — loss: 0.2550
Epoch 09 — loss: 0.2488
Epoch 10 — loss: 0.2423
Epoch 11 — loss: 0.2362
Epoch 12 — loss: 0.2299
Epoch 13 — loss: 0.2243
Epoch 14 — loss: 0.2178
Epoch 15 — loss: 0.2132
Epoch 16 — loss: 0.2087
Epoch 17 — loss: 0.2018
Epoch 18 — loss: 0.1971
Epoch 19 — loss: 0.1924
Epoch 20 — loss: 0.1861
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  80.989998
2    P(>V6)  83.779999
3    P(>V7)  87.580002
4    P(>V8)  92.930000
5    P(>V9)  96.820000
Overall accuracy: 48.99%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3644
Epoch 02 — loss: 0.3144
Epoch 03 — loss: 0.3027
Epoch 04 — loss: 0.2928
Epoch 05 — loss: 0.2865
Epoch 06 — loss: 0.2793
Epoch 07 — loss: 0.2717
Epoch 08 — loss: 0.2667
Epoch 09 — loss: 0.2607
Epoch 10 — loss: 0.2556
Epoch 11 — loss: 0.2486
Epoch 12 — loss: 0.2422
Epoch 13 — loss: 0.2372
Epoch 14 — loss: 0.2309
Epoch 15 — loss: 0.2266
Epoch 16 — loss: 0.2217
Epoch 17 — loss: 0.2186
Epoch 18 — loss: 0.2112
Epoch 19 — loss: 0.2100
Epoch 20 — loss: 0.2026
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  83.059998
2    P(>V6)  85.080002
3    P(>V7)  88.309998
4    P(>V8)  93.169998
5    P(>V9)  96.919998
Overall accuracy: 50.05%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4647
Epoch 02 — loss: 0.3454
Epoch 03 — loss: 0.3057
Epoch 04 — loss: 0.2930
Epoch 05 — loss: 0.2865
Epoch 06 — loss: 0.2851
Epoch 07 — loss: 0.2788
Epoch 08 — loss: 0.2776
Epoch 09 — loss: 0.2750
Epoch 10 — loss: 0.2725
Epoch 11 — loss: 0.2736
Epoch 12 — loss: 0.2727
Epoch 13 — loss: 0.2705
Epoch 14 — loss: 0.2717
Epoch 15 — loss: 0.2725
Epoch 16 — loss: 0.2707
Epoch 17 — loss: 0.2694
Epoch 18 — loss: 0.2690
Epoch 19 — loss: 0.2685
Epoch 20 — loss: 0.2692
  threshold   accuracy
0    P(>V4)  81.139999
1    P(>V5)  80.510002
2    P(>V6)  83.779999
3    P(>V7)  87.389999
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 45.28%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4730
Epoch 02 — loss: 0.3379
Epoch 03 — loss: 0.3006
Epoch 04 — loss: 0.2894
Epoch 05 — loss: 0.2848
Epoch 06 — loss: 0.2814
Epoch 07 — loss: 0.2802
Epoch 08 — loss: 0.2778
Epoch 09 — loss: 0.2762
Epoch 10 — loss: 0.2747
Epoch 11 — loss: 0.2744
Epoch 12 — loss: 0.2731
Epoch 13 — loss: 0.2721
Epoch 14 — loss: 0.2727
Epoch 15 — loss: 0.2720
Epoch 16 — loss: 0.2721
Epoch 17 — loss: 0.2705
Epoch 18 — loss: 0.2699
Epoch 19 — loss: 0.2690
Epoch 20 — loss: 0.2682
  threshold   accuracy
0    P(>V4)  81.279999
1    P(>V5)  81.279999
2    P(>V6)  83.930000
3    P(>V7)  87.629997
4    P(>V8)  92.730003
5    P(>V9)  97.019997/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [17:47:26] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [17:48:47] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [17:49:47] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 46.25%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4341
Epoch 02 — loss: 0.3306
Epoch 03 — loss: 0.3161
Epoch 04 — loss: 0.3034
Epoch 05 — loss: 0.2966
Epoch 06 — loss: 0.2889
Epoch 07 — loss: 0.2864
Epoch 08 — loss: 0.2829
Epoch 09 — loss: 0.2803
Epoch 10 — loss: 0.2807
Epoch 11 — loss: 0.2786
Epoch 12 — loss: 0.2777
Epoch 13 — loss: 0.2754
Epoch 14 — loss: 0.2744
Epoch 15 — loss: 0.2733
Epoch 16 — loss: 0.2735
Epoch 17 — loss: 0.2715
Epoch 18 — loss: 0.2726
Epoch 19 — loss: 0.2734
Epoch 20 — loss: 0.2719
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.650002
2    P(>V6)  83.779999
3    P(>V7)  87.629997
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.20%
Ordinal stacking meta epoch 1: loss=0.2633
Ordinal stacking meta epoch 2: loss=0.1574
Ordinal stacking meta epoch 3: loss=0.1467
Ordinal stacking meta epoch 4: loss=0.1426
Ordinal stacking meta epoch 5: loss=0.1406
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.629997
2    P(>V6)  85.760002
3    P(>V7)  88.209999
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 49.95%
  threshold   accuracy
0    P(>V4)  84.790001
1    P(>V5)  82.440002
2    P(>V6)  84.309998
3    P(>V7)  88.309998
4    P(>V8)  93.209999
5    P(>V9)  96.820000
Overall accuracy: 48.80%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  82.720001
2    P(>V6)  84.599998
3    P(>V7)  88.110001
4    P(>V8)  93.019997
5    P(>V9)  96.489998
Overall accuracy: 49.66%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.239998
2    P(>V6)  84.309998
3    P(>V7)  87.680000
4    P(>V8)  92.830002
5    P(>V9)  96.629997
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.629997
2    P(>V6)  85.760002
3    P(>V7)  88.209999
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 49.95%
Ordinal stacking meta epoch 1: loss=0.3044
Ordinal stacking meta epoch 2: loss=0.1593
Ordinal stacking meta epoch 3: loss=0.1553
Ordinal stacking meta epoch 4: loss=0.1538
Ordinal stacking meta epoch 5: loss=0.1528
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  83.250000
2    P(>V6)  85.900002
3    P(>V7)  89.029999
4    P(>V8)  93.309998
5    P(>V9)  97.019997
Overall accuracy: 51.35%
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  83.300003
2    P(>V6)  85.900002
3    P(>V7)  88.790001
4    P(>V8)  92.879997
5    P(>V9)  96.489998
Overall accuracy: 50.00%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  82.870003
2    P(>V6)  85.269997
3    P(>V7)  88.070000
4    P(>V8)  92.730003
5    P(>V9)  96.199997
Overall accuracy: 49.71%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.150002
2    P(>V6)  84.940002
3    P(>V7)  87.870003
4    P(>V8)  92.589996
5    P(>V9)  96.199997
Overall accuracy: 48.89%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  83.250000
2    P(>V6)  85.900002
3    P(>V7)  89.029999
4    P(>V8)  93.309998
5    P(>V9)  97.019997
Overall accuracy: 51.35%
Ordinal stacking meta epoch 1: loss=0.3393
Ordinal stacking meta epoch 2: loss=0.2675
Ordinal stacking meta epoch 3: loss=0.2668
Ordinal stacking meta epoch 4: loss=0.2667
Ordinal stacking meta epoch 5: loss=0.2667
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  81.040001
2    P(>V6)  84.070000
3    P(>V7)  87.820000
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.34%
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  81.230003
2    P(>V6)  83.830002
3    P(>V7)  87.680000
4    P(>V8)  92.690002
5    P(>V9)  96.919998
Overall accuracy: 45.57%
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.370003
2    P(>V6)  83.349998
3    P(>V7)  87.540001
4    P(>V8)  92.440002
5    P(>V9)  96.580002
Overall accuracy: 45.24%
  threshold   accuracy
0    P(>V4)  81.040001
1    P(>V5)  80.559998
2    P(>V6)  82.959999
3    P(>V7)  86.809998
4    P(>V8)  91.820000
5    P(>V9)  96.540001
Overall accuracy: 45.28%
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  81.040001
2    P(>V6)  84.070000
3    P(>V7)  87.820000
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.34%
----------------- Ordinal iteration 2/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3814
Epoch 02 — loss: 0.3149
Epoch 03 — loss: 0.3034
Epoch 04 — loss: 0.2922
Epoch 05 — loss: 0.2821
Epoch 06 — loss: 0.2752
Epoch 07 — loss: 0.2657
Epoch 08 — loss: 0.2587
Epoch 09 — loss: 0.2515
Epoch 10 — loss: 0.2457
Epoch 11 — loss: 0.2395
Epoch 12 — loss: 0.2328
Epoch 13 — loss: 0.2272
Epoch 14 — loss: 0.2228
Epoch 15 — loss: 0.2169
Epoch 16 — loss: 0.2108
Epoch 17 — loss: 0.2074
Epoch 18 — loss: 0.2025
Epoch 19 — loss: 0.1957
Epoch 20 — loss: 0.1915
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  81.860001
2    P(>V6)  83.879997
3    P(>V7)  87.870003
4    P(>V8)  92.690002
5    P(>V9)  96.919998
Overall accuracy: 49.95%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3721
Epoch 02 — loss: 0.3110
Epoch 03 — loss: 0.2986
Epoch 04 — loss: 0.2914
Epoch 05 — loss: 0.2827
Epoch 06 — loss: 0.2755
Epoch 07 — loss: 0.2669
Epoch 08 — loss: 0.2621
Epoch 09 — loss: 0.2540
Epoch 10 — loss: 0.2480
Epoch 11 — loss: 0.2416
Epoch 12 — loss: 0.2364
Epoch 13 — loss: 0.2306
Epoch 14 — loss: 0.2239
Epoch 15 — loss: 0.2196
Epoch 16 — loss: 0.2151
Epoch 17 — loss: 0.2075
Epoch 18 — loss: 0.2015
Epoch 19 — loss: 0.1976
Epoch 20 — loss: 0.1919
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  81.709999
2    P(>V6)  84.500000
3    P(>V7)  88.070000
4    P(>V8)  92.199997
5    P(>V9)  96.629997
Overall accuracy: 48.75%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3608
Epoch 02 — loss: 0.3155
Epoch 03 — loss: 0.3016
Epoch 04 — loss: 0.2960
Epoch 05 — loss: 0.2880
Epoch 06 — loss: 0.2815
Epoch 07 — loss: 0.2764
Epoch 08 — loss: 0.2705
Epoch 09 — loss: 0.2660
Epoch 10 — loss: 0.2614
Epoch 11 — loss: 0.2551
Epoch 12 — loss: 0.2491
Epoch 13 — loss: 0.2447
Epoch 14 — loss: 0.2368
Epoch 15 — loss: 0.2349
Epoch 16 — loss: 0.2295
Epoch 17 — loss: 0.2248
Epoch 18 — loss: 0.2203
Epoch 19 — loss: 0.2160
Epoch 20 — loss: 0.2109
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.959999
2    P(>V6)  85.320000
3    P(>V7)  88.019997
4    P(>V8)  92.779999
5    P(>V9)  96.919998
Overall accuracy: 49.33%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4722
Epoch 02 — loss: 0.3345
Epoch 03 — loss: 0.2967
Epoch 04 — loss: 0.2879
Epoch 05 — loss: 0.2829
Epoch 06 — loss: 0.2796
Epoch 07 — loss: 0.2776
Epoch 08 — loss: 0.2756
Epoch 09 — loss: 0.2746
Epoch 10 — loss: 0.2757
Epoch 11 — loss: 0.2746
Epoch 12 — loss: 0.2722
Epoch 13 — loss: 0.2727
Epoch 14 — loss: 0.2710
Epoch 15 — loss: 0.2722
Epoch 16 — loss: 0.2708
Epoch 17 — loss: 0.2700
Epoch 18 — loss: 0.2694
Epoch 19 — loss: 0.2695
Epoch 20 — loss: 0.2682
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  81.040001
2    P(>V6)  83.930000
3    P(>V7)  87.629997
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 46.34%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4621
Epoch 02 — loss: 0.3269
Epoch 03 — loss: 0.2984
Epoch 04 — loss: 0.2879
Epoch 05 — loss: 0.2848
Epoch 06 — loss: 0.2803
Epoch 07 — loss: 0.2796
Epoch 08 — loss: 0.2769
Epoch 09 — loss: 0.2753
Epoch 10 — loss: 0.2743
Epoch 11 — loss: 0.2728
Epoch 12 — loss: 0.2706
Epoch 13 — loss: 0.2714
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2714
Epoch 16 — loss: 0.2713
Epoch 17 — loss: 0.2692
Epoch 18 — loss: 0.2692
Epoch 19 — loss: 0.2696
Epoch 20 — loss: 0.2684
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.989998
2    P(>V6)  83.970001
3    P(>V7)  87.820000
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 45.67%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4267
Epoch 02 — loss: 0.3238/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:12:15] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:13:34] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:14:32] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 03 — loss: 0.3077
Epoch 04 — loss: 0.2966
Epoch 05 — loss: 0.2905
Epoch 06 — loss: 0.2871
Epoch 07 — loss: 0.2840
Epoch 08 — loss: 0.2820
Epoch 09 — loss: 0.2791
Epoch 10 — loss: 0.2770
Epoch 11 — loss: 0.2758
Epoch 12 — loss: 0.2754
Epoch 13 — loss: 0.2737
Epoch 14 — loss: 0.2746
Epoch 15 — loss: 0.2744
Epoch 16 — loss: 0.2718
Epoch 17 — loss: 0.2747
Epoch 18 — loss: 0.2720
Epoch 19 — loss: 0.2720
Epoch 20 — loss: 0.2700
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.320000
2    P(>V6)  83.830002
3    P(>V7)  87.580002
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 44.85%
Ordinal stacking meta epoch 1: loss=0.2045
Ordinal stacking meta epoch 2: loss=0.1569
Ordinal stacking meta epoch 3: loss=0.1529
Ordinal stacking meta epoch 4: loss=0.1508
Ordinal stacking meta epoch 5: loss=0.1493
  threshold   accuracy
0    P(>V4)  84.650002
1    P(>V5)  82.580002
2    P(>V6)  85.370003
3    P(>V7)  88.500000
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  85.230003
1    P(>V5)  82.580002
2    P(>V6)  85.559998
3    P(>V7)  88.639999
4    P(>V8)  93.120003
5    P(>V9)  96.580002
Overall accuracy: 50.05%
  threshold   accuracy
0    P(>V4)  84.739998
1    P(>V5)  83.349998
2    P(>V6)  85.370003
3    P(>V7)  87.779999
4    P(>V8)  93.550003
5    P(>V9)  96.779999
Overall accuracy: 49.81%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  83.199997
2    P(>V6)  84.989998
3    P(>V7)  87.919998
4    P(>V8)  93.309998
5    P(>V9)  96.680000
Overall accuracy: 49.71%
  threshold   accuracy
0    P(>V4)  84.650002
1    P(>V5)  82.580002
2    P(>V6)  85.370003
3    P(>V7)  88.500000
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 49.76%
Ordinal stacking meta epoch 1: loss=0.2859
Ordinal stacking meta epoch 2: loss=0.1681
Ordinal stacking meta epoch 3: loss=0.1644
Ordinal stacking meta epoch 4: loss=0.1628
Ordinal stacking meta epoch 5: loss=0.1616
  threshold   accuracy
0    P(>V4)  85.320000
1    P(>V5)  83.059998
2    P(>V6)  86.529999
3    P(>V7)  89.169998
4    P(>V8)  93.699997
5    P(>V9)  97.110001
Overall accuracy: 52.12%
  threshold   accuracy
0    P(>V4)  85.709999
1    P(>V5)  83.250000
2    P(>V6)  86.139999
3    P(>V7)  88.930000
4    P(>V8)  93.120003
5    P(>V9)  96.820000
Overall accuracy: 52.07%
  threshold   accuracy
0    P(>V4)  84.650002
1    P(>V5)  83.449997
2    P(>V6)  85.760002
3    P(>V7)  88.349998
4    P(>V8)  92.879997
5    P(>V9)  96.730003
Overall accuracy: 50.38%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  83.160004
2    P(>V6)  85.849998
3    P(>V7)  87.779999
4    P(>V8)  93.209999
5    P(>V9)  96.779999
Overall accuracy: 50.24%
  threshold   accuracy
0    P(>V4)  85.320000
1    P(>V5)  83.059998
2    P(>V6)  86.529999
3    P(>V7)  89.169998
4    P(>V8)  93.699997
5    P(>V9)  97.110001
Overall accuracy: 52.12%
Ordinal stacking meta epoch 1: loss=0.3480
Ordinal stacking meta epoch 2: loss=0.2677
Ordinal stacking meta epoch 3: loss=0.2668
Ordinal stacking meta epoch 4: loss=0.2668
Ordinal stacking meta epoch 5: loss=0.2667
  threshold   accuracy
0    P(>V4)  81.809998
1    P(>V5)  80.989998
2    P(>V6)  83.970001
3    P(>V7)  87.580002
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.96%
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  80.750000
2    P(>V6)  84.019997
3    P(>V7)  87.680000
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.57%
  threshold   accuracy
0    P(>V4)  82.150002
1    P(>V5)  80.410004
2    P(>V6)  83.589996
3    P(>V7)  87.580002
4    P(>V8)  92.349998
5    P(>V9)  96.580002
Overall accuracy: 46.49%
  threshold   accuracy
0    P(>V4)  80.610001
1    P(>V5)  79.599998
2    P(>V6)  83.199997
3    P(>V7)  86.910004
4    P(>V8)  92.250000
5    P(>V9)  96.440002
Overall accuracy: 44.32%
  threshold   accuracy
0    P(>V4)  81.809998
1    P(>V5)  80.989998
2    P(>V6)  83.970001
3    P(>V7)  87.580002
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.96%
----------------- Ordinal iteration 3/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3715
Epoch 02 — loss: 0.3196
Epoch 03 — loss: 0.3052
Epoch 04 — loss: 0.2914
Epoch 05 — loss: 0.2846
Epoch 06 — loss: 0.2722
Epoch 07 — loss: 0.2659
Epoch 08 — loss: 0.2588
Epoch 09 — loss: 0.2498
Epoch 10 — loss: 0.2433
Epoch 11 — loss: 0.2351
Epoch 12 — loss: 0.2300
Epoch 13 — loss: 0.2228
Epoch 14 — loss: 0.2197
Epoch 15 — loss: 0.2129
Epoch 16 — loss: 0.2088
Epoch 17 — loss: 0.2051
Epoch 18 — loss: 0.1971
Epoch 19 — loss: 0.1933
Epoch 20 — loss: 0.1860
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  81.949997
2    P(>V6)  84.120003
3    P(>V7)  87.300003
4    P(>V8)  91.099998
5    P(>V9)  96.339996
Overall accuracy: 46.20%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3714
Epoch 02 — loss: 0.3126
Epoch 03 — loss: 0.3000
Epoch 04 — loss: 0.2935
Epoch 05 — loss: 0.2825
Epoch 06 — loss: 0.2749
Epoch 07 — loss: 0.2655
Epoch 08 — loss: 0.2587
Epoch 09 — loss: 0.2506
Epoch 10 — loss: 0.2452
Epoch 11 — loss: 0.2372
Epoch 12 — loss: 0.2329
Epoch 13 — loss: 0.2270
Epoch 14 — loss: 0.2216
Epoch 15 — loss: 0.2155
Epoch 16 — loss: 0.2093
Epoch 17 — loss: 0.2031
Epoch 18 — loss: 0.1996
Epoch 19 — loss: 0.1938
Epoch 20 — loss: 0.1863
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  82.480003
2    P(>V6)  84.650002
3    P(>V7)  87.300003
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 48.08%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3668
Epoch 02 — loss: 0.3132
Epoch 03 — loss: 0.3008
Epoch 04 — loss: 0.2948
Epoch 05 — loss: 0.2900
Epoch 06 — loss: 0.2843
Epoch 07 — loss: 0.2774
Epoch 08 — loss: 0.2722
Epoch 09 — loss: 0.2674
Epoch 10 — loss: 0.2608
Epoch 11 — loss: 0.2565
Epoch 12 — loss: 0.2497
Epoch 13 — loss: 0.2440
Epoch 14 — loss: 0.2400
Epoch 15 — loss: 0.2330
Epoch 16 — loss: 0.2303
Epoch 17 — loss: 0.2265
Epoch 18 — loss: 0.2206
Epoch 19 — loss: 0.2167
Epoch 20 — loss: 0.2099
  threshold   accuracy
0    P(>V4)  82.720001
1    P(>V5)  82.480003
2    P(>V6)  84.650002
3    P(>V7)  87.440002
4    P(>V8)  92.540001
5    P(>V9)  96.970001
Overall accuracy: 46.05%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4732
Epoch 02 — loss: 0.3516
Epoch 03 — loss: 0.3017
Epoch 04 — loss: 0.2903
Epoch 05 — loss: 0.2838
Epoch 06 — loss: 0.2793
Epoch 07 — loss: 0.2781
Epoch 08 — loss: 0.2762
Epoch 09 — loss: 0.2750
Epoch 10 — loss: 0.2731
Epoch 11 — loss: 0.2734
Epoch 12 — loss: 0.2723
Epoch 13 — loss: 0.2714
Epoch 14 — loss: 0.2718
Epoch 15 — loss: 0.2715
Epoch 16 — loss: 0.2689
Epoch 17 — loss: 0.2699
Epoch 18 — loss: 0.2710
Epoch 19 — loss: 0.2691
Epoch 20 — loss: 0.2681
  threshold   accuracy
0    P(>V4)  82.099998
1    P(>V5)  80.750000
2    P(>V6)  83.540001
3    P(>V7)  87.970001
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.25%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4617
Epoch 02 — loss: 0.3235
Epoch 03 — loss: 0.2947
Epoch 04 — loss: 0.2872
Epoch 05 — loss: 0.2828
Epoch 06 — loss: 0.2795
Epoch 07 — loss: 0.2776
Epoch 08 — loss: 0.2751
Epoch 09 — loss: 0.2742
Epoch 10 — loss: 0.2734
Epoch 11 — loss: 0.2733
Epoch 12 — loss: 0.2706
Epoch 13 — loss: 0.2703
Epoch 14 — loss: 0.2718
Epoch 15 — loss: 0.2707
Epoch 16 — loss: 0.2686
Epoch 17 — loss: 0.2692
Epoch 18 — loss: 0.2700
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2699
  threshold   accuracy
0    P(>V4)  81.040001
1    P(>V5)  80.510002
2    P(>V6)  83.589996
3    P(>V7)  87.540001
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 44.18%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4233
Epoch 02 — loss: 0.3263
Epoch 03 — loss: 0.3091
Epoch 04 — loss: 0.2993
Epoch 05 — loss: 0.2918
Epoch 06 — loss: 0.2872
Epoch 07 — loss: 0.2823/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:37:42] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:39:01] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [18:39:59] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 08 — loss: 0.2793
Epoch 09 — loss: 0.2799
Epoch 10 — loss: 0.2783
Epoch 11 — loss: 0.2759
Epoch 12 — loss: 0.2766
Epoch 13 — loss: 0.2741
Epoch 14 — loss: 0.2762
Epoch 15 — loss: 0.2741
Epoch 16 — loss: 0.2735
Epoch 17 — loss: 0.2722
Epoch 18 — loss: 0.2724
Epoch 19 — loss: 0.2720
Epoch 20 — loss: 0.2710
  threshold   accuracy
0    P(>V4)  80.750000
1    P(>V5)  80.650002
2    P(>V6)  83.930000
3    P(>V7)  87.680000
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 44.42%
Ordinal stacking meta epoch 1: loss=0.2965
Ordinal stacking meta epoch 2: loss=0.1576
Ordinal stacking meta epoch 3: loss=0.1512
Ordinal stacking meta epoch 4: loss=0.1484
Ordinal stacking meta epoch 5: loss=0.1468
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.769997
2    P(>V6)  85.709999
3    P(>V7)  88.639999
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  83.160004
2    P(>V6)  85.660004
3    P(>V7)  88.550003
4    P(>V8)  91.870003
5    P(>V9)  96.629997
Overall accuracy: 47.69%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.959999
2    P(>V6)  85.949997
3    P(>V7)  88.110001
4    P(>V8)  92.489998
5    P(>V9)  96.629997
Overall accuracy: 49.71%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.919998
2    P(>V6)  85.800003
3    P(>V7)  87.629997
4    P(>V8)  91.769997
5    P(>V9)  96.540001
Overall accuracy: 48.32%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.769997
2    P(>V6)  85.709999
3    P(>V7)  88.639999
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 48.65%
Ordinal stacking meta epoch 1: loss=0.2630
Ordinal stacking meta epoch 2: loss=0.1640
Ordinal stacking meta epoch 3: loss=0.1599
Ordinal stacking meta epoch 4: loss=0.1582
Ordinal stacking meta epoch 5: loss=0.1570
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  83.930000
2    P(>V6)  86.430000
3    P(>V7)  88.500000
4    P(>V8)  92.779999
5    P(>V9)  97.260002
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  83.639999
2    P(>V6)  86.239998
3    P(>V7)  89.080002
4    P(>V8)  92.540001
5    P(>V9)  96.779999
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  83.400002
2    P(>V6)  86.480003
3    P(>V7)  88.930000
4    P(>V8)  92.489998
5    P(>V9)  96.540001
Overall accuracy: 50.19%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.919998
2    P(>V6)  85.660004
3    P(>V7)  88.500000
4    P(>V8)  91.720001
5    P(>V9)  96.290001
Overall accuracy: 48.80%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  83.930000
2    P(>V6)  86.430000
3    P(>V7)  88.500000
4    P(>V8)  92.779999
5    P(>V9)  97.260002
Overall accuracy: 49.76%
Ordinal stacking meta epoch 1: loss=0.2923
Ordinal stacking meta epoch 2: loss=0.2676
Ordinal stacking meta epoch 3: loss=0.2673
Ordinal stacking meta epoch 4: loss=0.2672
Ordinal stacking meta epoch 5: loss=0.2672
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.699997
2    P(>V6)  83.779999
3    P(>V7)  87.970001
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 45.24%
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.699997
2    P(>V6)  83.970001
3    P(>V7)  87.779999
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 45.28%
  threshold   accuracy
0    P(>V4)  82.190002
1    P(>V5)  79.839996
2    P(>V6)  83.540001
3    P(>V7)  87.440002
4    P(>V8)  92.540001
5    P(>V9)  96.389999
Overall accuracy: 45.96%
  threshold   accuracy
0    P(>V4)  81.089996
1    P(>V5)  80.080002
2    P(>V6)  83.639999
3    P(>V7)  86.860001
4    P(>V8)  92.639999
5    P(>V9)  96.580002
Overall accuracy: 45.19%
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.699997
2    P(>V6)  83.779999
3    P(>V7)  87.970001
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 45.24%
----------------- Ordinal iteration 4/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3850
Epoch 02 — loss: 0.3184
Epoch 03 — loss: 0.2978
Epoch 04 — loss: 0.2894
Epoch 05 — loss: 0.2783
Epoch 06 — loss: 0.2683
Epoch 07 — loss: 0.2611
Epoch 08 — loss: 0.2519
Epoch 09 — loss: 0.2447
Epoch 10 — loss: 0.2383
Epoch 11 — loss: 0.2320
Epoch 12 — loss: 0.2249
Epoch 13 — loss: 0.2180
Epoch 14 — loss: 0.2126
Epoch 15 — loss: 0.2069
Epoch 16 — loss: 0.1996
Epoch 17 — loss: 0.1980
Epoch 18 — loss: 0.1923
Epoch 19 — loss: 0.1856
Epoch 20 — loss: 0.1798
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.440002
2    P(>V6)  84.839996
3    P(>V7)  88.209999
4    P(>V8)  92.489998
5    P(>V9)  96.870003
Overall accuracy: 48.32%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3714
Epoch 02 — loss: 0.3075
Epoch 03 — loss: 0.2933
Epoch 04 — loss: 0.2828
Epoch 05 — loss: 0.2730
Epoch 06 — loss: 0.2619
Epoch 07 — loss: 0.2538
Epoch 08 — loss: 0.2488
Epoch 09 — loss: 0.2411
Epoch 10 — loss: 0.2358
Epoch 11 — loss: 0.2281
Epoch 12 — loss: 0.2269
Epoch 13 — loss: 0.2190
Epoch 14 — loss: 0.2136
Epoch 15 — loss: 0.2080
Epoch 16 — loss: 0.2035
Epoch 17 — loss: 0.1981
Epoch 18 — loss: 0.1931
Epoch 19 — loss: 0.1887
Epoch 20 — loss: 0.1844
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  82.769997
2    P(>V6)  84.599998
3    P(>V7)  87.779999
4    P(>V8)  92.300003
5    P(>V9)  96.919998
Overall accuracy: 47.69%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3599
Epoch 02 — loss: 0.3101
Epoch 03 — loss: 0.2974
Epoch 04 — loss: 0.2888
Epoch 05 — loss: 0.2836
Epoch 06 — loss: 0.2770
Epoch 07 — loss: 0.2729
Epoch 08 — loss: 0.2680
Epoch 09 — loss: 0.2630
Epoch 10 — loss: 0.2563
Epoch 11 — loss: 0.2539
Epoch 12 — loss: 0.2486
Epoch 13 — loss: 0.2452
Epoch 14 — loss: 0.2389
Epoch 15 — loss: 0.2346
Epoch 16 — loss: 0.2307
Epoch 17 — loss: 0.2249
Epoch 18 — loss: 0.2202
Epoch 19 — loss: 0.2168
Epoch 20 — loss: 0.2116
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  81.949997
2    P(>V6)  85.660004
3    P(>V7)  88.690002
4    P(>V8)  93.169998
5    P(>V9)  97.059998
Overall accuracy: 50.48%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4690
Epoch 02 — loss: 0.3355
Epoch 03 — loss: 0.3030
Epoch 04 — loss: 0.2957
Epoch 05 — loss: 0.2871
Epoch 06 — loss: 0.2835
Epoch 07 — loss: 0.2780
Epoch 08 — loss: 0.2788
Epoch 09 — loss: 0.2755
Epoch 10 — loss: 0.2742
Epoch 11 — loss: 0.2744
Epoch 12 — loss: 0.2741
Epoch 13 — loss: 0.2705
Epoch 14 — loss: 0.2718
Epoch 15 — loss: 0.2713
Epoch 16 — loss: 0.2703
Epoch 17 — loss: 0.2716
Epoch 18 — loss: 0.2700
Epoch 19 — loss: 0.2705
Epoch 20 — loss: 0.2690
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.459999
2    P(>V6)  83.589996
3    P(>V7)  87.730003
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.15%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4676
Epoch 02 — loss: 0.3559
Epoch 03 — loss: 0.3060
Epoch 04 — loss: 0.2934
Epoch 05 — loss: 0.2844
Epoch 06 — loss: 0.2814
Epoch 07 — loss: 0.2779
Epoch 08 — loss: 0.2766
Epoch 09 — loss: 0.2738
Epoch 10 — loss: 0.2745
Epoch 11 — loss: 0.2718
Epoch 12 — loss: 0.2724
Epoch 13 — loss: 0.2731
Epoch 14 — loss: 0.2708
Epoch 15 — loss: 0.2715
Epoch 16 — loss: 0.2695
Epoch 17 — loss: 0.2698
Epoch 18 — loss: 0.2693
Epoch 19 — loss: 0.2698
Epoch 20 — loss: 0.2695
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.410004
2    P(>V6)  83.449997
3    P(>V7)  87.199997
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.96%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4323
Epoch 02 — loss: 0.3287
Epoch 03 — loss: 0.3124
Epoch 04 — loss: 0.3001
Epoch 05 — loss: 0.2912
Epoch 06 — loss: 0.2877
Epoch 07 — loss: 0.2851
Epoch 08 — loss: 0.2821
Epoch 09 — loss: 0.2790
Epoch 10 — loss: 0.2772
Epoch 11 — loss: 0.2777
Epoch 12 — loss: 0.2749/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:03:04] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:04:23] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:05:20] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 13 — loss: 0.2746
Epoch 14 — loss: 0.2744
Epoch 15 — loss: 0.2725
Epoch 16 — loss: 0.2746
Epoch 17 — loss: 0.2716
Epoch 18 — loss: 0.2722
Epoch 19 — loss: 0.2710
Epoch 20 — loss: 0.2716
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.800003
2    P(>V6)  84.260002
3    P(>V7)  87.820000
4    P(>V8)  92.400002
5    P(>V9)  97.019997
Overall accuracy: 45.91%
Ordinal stacking meta epoch 1: loss=0.2364
Ordinal stacking meta epoch 2: loss=0.1519
Ordinal stacking meta epoch 3: loss=0.1464
Ordinal stacking meta epoch 4: loss=0.1437
Ordinal stacking meta epoch 5: loss=0.1418
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.870003
2    P(>V6)  85.849998
3    P(>V7)  88.639999
4    P(>V8)  93.169998
5    P(>V9)  97.019997
Overall accuracy: 49.81%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  83.349998
2    P(>V6)  86.089996
3    P(>V7)  88.449997
4    P(>V8)  93.019997
5    P(>V9)  97.110001
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.769997
2    P(>V6)  85.709999
3    P(>V7)  88.349998
4    P(>V8)  92.300003
5    P(>V9)  96.820000
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  83.199997
2    P(>V6)  85.709999
3    P(>V7)  88.209999
4    P(>V8)  92.690002
5    P(>V9)  97.110001
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.870003
2    P(>V6)  85.849998
3    P(>V7)  88.639999
4    P(>V8)  93.169998
5    P(>V9)  97.019997
Overall accuracy: 49.81%
Ordinal stacking meta epoch 1: loss=0.2643
Ordinal stacking meta epoch 2: loss=0.1609
Ordinal stacking meta epoch 3: loss=0.1564
Ordinal stacking meta epoch 4: loss=0.1543
Ordinal stacking meta epoch 5: loss=0.1529
  threshold   accuracy
0    P(>V4)  84.699997
1    P(>V5)  83.449997
2    P(>V6)  86.860001
3    P(>V7)  88.879997
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 50.34%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  83.639999
2    P(>V6)  86.000000
3    P(>V7)  87.870003
4    P(>V8)  92.830002
5    P(>V9)  96.919998
Overall accuracy: 48.99%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  83.059998
2    P(>V6)  85.760002
3    P(>V7)  87.779999
4    P(>V8)  92.540001
5    P(>V9)  96.580002
Overall accuracy: 49.37%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  83.779999
2    P(>V6)  85.269997
3    P(>V7)  87.250000
4    P(>V8)  92.489998
5    P(>V9)  96.730003
Overall accuracy: 48.99%
  threshold   accuracy
0    P(>V4)  84.699997
1    P(>V5)  83.449997
2    P(>V6)  86.860001
3    P(>V7)  88.879997
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 50.34%
Ordinal stacking meta epoch 1: loss=0.3434
Ordinal stacking meta epoch 2: loss=0.2669
Ordinal stacking meta epoch 3: loss=0.2667
Ordinal stacking meta epoch 4: loss=0.2666
Ordinal stacking meta epoch 5: loss=0.2666
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.610001
2    P(>V6)  83.639999
3    P(>V7)  87.970001
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.86%
  threshold   accuracy
0    P(>V4)  81.949997
1    P(>V5)  80.650002
2    P(>V6)  84.260002
3    P(>V7)  87.730003
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.81%
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.900002
2    P(>V6)  83.589996
3    P(>V7)  87.199997
4    P(>V8)  92.400002
5    P(>V9)  96.339996
Overall accuracy: 45.33%
  threshold   accuracy
0    P(>V4)  80.989998
1    P(>V5)  79.019997
2    P(>V6)  83.300003
3    P(>V7)  86.379997
4    P(>V8)  92.349998
5    P(>V9)  96.680000
Overall accuracy: 44.42%
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.610001
2    P(>V6)  83.639999
3    P(>V7)  87.970001
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.86%
----------------- Ordinal iteration 5/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3723
Epoch 02 — loss: 0.3186
Epoch 03 — loss: 0.3032
Epoch 04 — loss: 0.2926
Epoch 05 — loss: 0.2815
Epoch 06 — loss: 0.2736
Epoch 07 — loss: 0.2644
Epoch 08 — loss: 0.2573
Epoch 09 — loss: 0.2504
Epoch 10 — loss: 0.2442
Epoch 11 — loss: 0.2382
Epoch 12 — loss: 0.2332
Epoch 13 — loss: 0.2276
Epoch 14 — loss: 0.2202
Epoch 15 — loss: 0.2165
Epoch 16 — loss: 0.2096
Epoch 17 — loss: 0.2054
Epoch 18 — loss: 0.1991
Epoch 19 — loss: 0.1939
Epoch 20 — loss: 0.1877
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  81.230003
2    P(>V6)  84.410004
3    P(>V7)  86.910004
4    P(>V8)  91.870003
5    P(>V9)  95.860001
Overall accuracy: 45.52%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3714
Epoch 02 — loss: 0.3087
Epoch 03 — loss: 0.2984
Epoch 04 — loss: 0.2903
Epoch 05 — loss: 0.2815
Epoch 06 — loss: 0.2738
Epoch 07 — loss: 0.2681
Epoch 08 — loss: 0.2586
Epoch 09 — loss: 0.2526
Epoch 10 — loss: 0.2435
Epoch 11 — loss: 0.2399
Epoch 12 — loss: 0.2348
Epoch 13 — loss: 0.2276
Epoch 14 — loss: 0.2230
Epoch 15 — loss: 0.2163
Epoch 16 — loss: 0.2094
Epoch 17 — loss: 0.2059
Epoch 18 — loss: 0.1987
Epoch 19 — loss: 0.1941
Epoch 20 — loss: 0.1883
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  81.570000
2    P(>V6)  84.599998
3    P(>V7)  87.440002
4    P(>V8)  92.059998
5    P(>V9)  96.629997
Overall accuracy: 47.69%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3580
Epoch 02 — loss: 0.3114
Epoch 03 — loss: 0.3001
Epoch 04 — loss: 0.2967
Epoch 05 — loss: 0.2915
Epoch 06 — loss: 0.2799
Epoch 07 — loss: 0.2758
Epoch 08 — loss: 0.2700
Epoch 09 — loss: 0.2647
Epoch 10 — loss: 0.2597
Epoch 11 — loss: 0.2539
Epoch 12 — loss: 0.2484
Epoch 13 — loss: 0.2429
Epoch 14 — loss: 0.2379
Epoch 15 — loss: 0.2324
Epoch 16 — loss: 0.2270
Epoch 17 — loss: 0.2204
Epoch 18 — loss: 0.2183
Epoch 19 — loss: 0.2150
Epoch 20 — loss: 0.2088
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.290001
2    P(>V6)  84.500000
3    P(>V7)  87.870003
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 49.13%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4624
Epoch 02 — loss: 0.3404
Epoch 03 — loss: 0.3037
Epoch 04 — loss: 0.2921
Epoch 05 — loss: 0.2864
Epoch 06 — loss: 0.2839
Epoch 07 — loss: 0.2797
Epoch 08 — loss: 0.2774
Epoch 09 — loss: 0.2777
Epoch 10 — loss: 0.2742
Epoch 11 — loss: 0.2749
Epoch 12 — loss: 0.2739
Epoch 13 — loss: 0.2722
Epoch 14 — loss: 0.2727
Epoch 15 — loss: 0.2709
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2715
Epoch 18 — loss: 0.2706
Epoch 19 — loss: 0.2707
Epoch 20 — loss: 0.2684
  threshold   accuracy
0    P(>V4)  81.809998
1    P(>V5)  80.459999
2    P(>V6)  83.730003
3    P(>V7)  87.580002
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 46.44%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4637
Epoch 02 — loss: 0.3302
Epoch 03 — loss: 0.2958
Epoch 04 — loss: 0.2881
Epoch 05 — loss: 0.2844
Epoch 06 — loss: 0.2802
Epoch 07 — loss: 0.2777
Epoch 08 — loss: 0.2784
Epoch 09 — loss: 0.2768
Epoch 10 — loss: 0.2750
Epoch 11 — loss: 0.2742
Epoch 12 — loss: 0.2734
Epoch 13 — loss: 0.2725
Epoch 14 — loss: 0.2733
Epoch 15 — loss: 0.2724
Epoch 16 — loss: 0.2705
Epoch 17 — loss: 0.2719
Epoch 18 — loss: 0.2699
Epoch 19 — loss: 0.2698
Epoch 20 — loss: 0.2698
  threshold   accuracy
0    P(>V4)  80.559998
1    P(>V5)  80.610001
2    P(>V6)  84.260002
3    P(>V7)  87.779999
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 46.44%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4310
Epoch 02 — loss: 0.3271
Epoch 03 — loss: 0.3076
Epoch 04 — loss: 0.2996
Epoch 05 — loss: 0.2905
Epoch 06 — loss: 0.2869
Epoch 07 — loss: 0.2832
Epoch 08 — loss: 0.2821
Epoch 09 — loss: 0.2806
Epoch 10 — loss: 0.2777
Epoch 11 — loss: 0.2766
Epoch 12 — loss: 0.2762
Epoch 13 — loss: 0.2748
Epoch 14 — loss: 0.2747
Epoch 15 — loss: 0.2736
Epoch 16 — loss: 0.2739
Epoch 17 — loss: 0.2725/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:28:34] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:29:53] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:30:51] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Epoch 18 — loss: 0.2738
Epoch 19 — loss: 0.2706
Epoch 20 — loss: 0.2713
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  81.139999
2    P(>V6)  83.690002
3    P(>V7)  87.250000
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 45.96%
Ordinal stacking meta epoch 1: loss=0.2431
Ordinal stacking meta epoch 2: loss=0.1632
Ordinal stacking meta epoch 3: loss=0.1550
Ordinal stacking meta epoch 4: loss=0.1514
Ordinal stacking meta epoch 5: loss=0.1493
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  83.059998
2    P(>V6)  85.849998
3    P(>V7)  88.400002
4    P(>V8)  92.970001
5    P(>V9)  97.059998
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  84.500000
1    P(>V5)  82.529999
2    P(>V6)  85.949997
3    P(>V7)  88.589996
4    P(>V8)  92.589996
5    P(>V9)  96.820000
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.870003
2    P(>V6)  85.660004
3    P(>V7)  88.449997
4    P(>V8)  92.250000
5    P(>V9)  96.050003
Overall accuracy: 48.32%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.150002
2    P(>V6)  85.849998
3    P(>V7)  88.980003
4    P(>V8)  92.730003
5    P(>V9)  96.389999
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  83.059998
2    P(>V6)  85.849998
3    P(>V7)  88.400002
4    P(>V8)  92.970001
5    P(>V9)  97.059998
Overall accuracy: 49.47%
Ordinal stacking meta epoch 1: loss=0.2671
Ordinal stacking meta epoch 2: loss=0.1673
Ordinal stacking meta epoch 3: loss=0.1627
Ordinal stacking meta epoch 4: loss=0.1608
Ordinal stacking meta epoch 5: loss=0.1598
  threshold   accuracy
0    P(>V4)  84.739998
1    P(>V5)  83.489998
2    P(>V6)  86.669998
3    P(>V7)  89.320000
4    P(>V8)  93.120003
5    P(>V9)  97.059998
Overall accuracy: 50.53%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  83.059998
2    P(>V6)  86.720001
3    P(>V7)  89.080002
4    P(>V8)  92.250000
5    P(>V9)  96.580002
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.629997
2    P(>V6)  86.570000
3    P(>V7)  88.839996
4    P(>V8)  92.690002
5    P(>V9)  96.150002
Overall accuracy: 49.81%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.820000
2    P(>V6)  86.330002
3    P(>V7)  89.559998
4    P(>V8)  92.730003
5    P(>V9)  96.580002
Overall accuracy: 50.05%
  threshold   accuracy
0    P(>V4)  84.739998
1    P(>V5)  83.489998
2    P(>V6)  86.669998
3    P(>V7)  89.320000
4    P(>V8)  93.120003
5    P(>V9)  97.059998
Overall accuracy: 50.53%
Ordinal stacking meta epoch 1: loss=0.3276
Ordinal stacking meta epoch 2: loss=0.2673
Ordinal stacking meta epoch 3: loss=0.2671
Ordinal stacking meta epoch 4: loss=0.2671
Ordinal stacking meta epoch 5: loss=0.2670
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.650002
2    P(>V6)  83.930000
3    P(>V7)  87.730003
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 46.20%
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.989998
2    P(>V6)  84.120003
3    P(>V7)  87.680000
4    P(>V8)  92.779999
5    P(>V9)  96.970001
Overall accuracy: 46.05%
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.459999
2    P(>V6)  84.019997
3    P(>V7)  87.099998
4    P(>V8)  92.300003
5    P(>V9)  96.440002
Overall accuracy: 45.52%
  threshold   accuracy
0    P(>V4)  80.610001
1    P(>V5)  79.739998
2    P(>V6)  83.400002
3    P(>V7)  86.379997
4    P(>V8)  92.589996
5    P(>V9)  96.389999
Overall accuracy: 44.80%
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.650002
2    P(>V6)  83.930000
3    P(>V7)  87.730003
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 46.20%
----------------- Ordinal iteration 6/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3758
Epoch 02 — loss: 0.3228
Epoch 03 — loss: 0.3076
Epoch 04 — loss: 0.2965
Epoch 05 — loss: 0.2892
Epoch 06 — loss: 0.2792
Epoch 07 — loss: 0.2723
Epoch 08 — loss: 0.2618
Epoch 09 — loss: 0.2549
Epoch 10 — loss: 0.2472
Epoch 11 — loss: 0.2398
Epoch 12 — loss: 0.2334
Epoch 13 — loss: 0.2256
Epoch 14 — loss: 0.2206
Epoch 15 — loss: 0.2136
Epoch 16 — loss: 0.2067
Epoch 17 — loss: 0.2015
Epoch 18 — loss: 0.1949
Epoch 19 — loss: 0.1888
Epoch 20 — loss: 0.1835
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  81.669998
2    P(>V6)  83.779999
3    P(>V7)  86.809998
4    P(>V8)  91.870003
5    P(>V9)  96.970001
Overall accuracy: 48.60%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3658
Epoch 02 — loss: 0.3103
Epoch 03 — loss: 0.3007
Epoch 04 — loss: 0.2906
Epoch 05 — loss: 0.2822
Epoch 06 — loss: 0.2735
Epoch 07 — loss: 0.2631
Epoch 08 — loss: 0.2556
Epoch 09 — loss: 0.2459
Epoch 10 — loss: 0.2373
Epoch 11 — loss: 0.2315
Epoch 12 — loss: 0.2246
Epoch 13 — loss: 0.2207
Epoch 14 — loss: 0.2140
Epoch 15 — loss: 0.2074
Epoch 16 — loss: 0.2009
Epoch 17 — loss: 0.1956
Epoch 18 — loss: 0.1912
Epoch 19 — loss: 0.1870
Epoch 20 — loss: 0.1795
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  81.230003
2    P(>V6)  84.070000
3    P(>V7)  87.680000
4    P(>V8)  91.430000
5    P(>V9)  96.580002
Overall accuracy: 48.65%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3600
Epoch 02 — loss: 0.3143
Epoch 03 — loss: 0.2997
Epoch 04 — loss: 0.2947
Epoch 05 — loss: 0.2872
Epoch 06 — loss: 0.2822
Epoch 07 — loss: 0.2744
Epoch 08 — loss: 0.2709
Epoch 09 — loss: 0.2658
Epoch 10 — loss: 0.2583
Epoch 11 — loss: 0.2562
Epoch 12 — loss: 0.2500
Epoch 13 — loss: 0.2445
Epoch 14 — loss: 0.2388
Epoch 15 — loss: 0.2327
Epoch 16 — loss: 0.2292
Epoch 17 — loss: 0.2248
Epoch 18 — loss: 0.2198
Epoch 19 — loss: 0.2155
Epoch 20 — loss: 0.2131
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  81.949997
2    P(>V6)  85.419998
3    P(>V7)  87.580002
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 48.94%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4641
Epoch 02 — loss: 0.3326
Epoch 03 — loss: 0.3010
Epoch 04 — loss: 0.2900
Epoch 05 — loss: 0.2850
Epoch 06 — loss: 0.2800
Epoch 07 — loss: 0.2782
Epoch 08 — loss: 0.2777
Epoch 09 — loss: 0.2769
Epoch 10 — loss: 0.2755
Epoch 11 — loss: 0.2749
Epoch 12 — loss: 0.2738
Epoch 13 — loss: 0.2714
Epoch 14 — loss: 0.2723
Epoch 15 — loss: 0.2709
Epoch 16 — loss: 0.2715
Epoch 17 — loss: 0.2703
Epoch 18 — loss: 0.2713
Epoch 19 — loss: 0.2709
Epoch 20 — loss: 0.2693
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.900002
2    P(>V6)  83.970001
3    P(>V7)  87.580002
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.43%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4653
Epoch 02 — loss: 0.3248
Epoch 03 — loss: 0.2965
Epoch 04 — loss: 0.2891
Epoch 05 — loss: 0.2845
Epoch 06 — loss: 0.2800
Epoch 07 — loss: 0.2803
Epoch 08 — loss: 0.2774
Epoch 09 — loss: 0.2755
Epoch 10 — loss: 0.2728
Epoch 11 — loss: 0.2742
Epoch 12 — loss: 0.2710
Epoch 13 — loss: 0.2712
Epoch 14 — loss: 0.2710
Epoch 15 — loss: 0.2700
Epoch 16 — loss: 0.2718
Epoch 17 — loss: 0.2693
Epoch 18 — loss: 0.2691
Epoch 19 — loss: 0.2688
Epoch 20 — loss: 0.2697
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  81.279999
2    P(>V6)  83.930000
3    P(>V7)  87.820000
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.39%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4257
Epoch 02 — loss: 0.3281
Epoch 03 — loss: 0.3115
Epoch 04 — loss: 0.2997
Epoch 05 — loss: 0.2920
Epoch 06 — loss: 0.2879
Epoch 07 — loss: 0.2844
Epoch 08 — loss: 0.2819
Epoch 09 — loss: 0.2799
Epoch 10 — loss: 0.2785
Epoch 11 — loss: 0.2782
Epoch 12 — loss: 0.2780
Epoch 13 — loss: 0.2744
Epoch 14 — loss: 0.2742
Epoch 15 — loss: 0.2756
Epoch 16 — loss: 0.2735
Epoch 17 — loss: 0.2735
Epoch 18 — loss: 0.2726
Epoch 19 — loss: 0.2714
Epoch 20 — loss: 0.2720
  threshold   accuracy
0    P(>V4)  80.900002
1    P(>V5)  80.699997
2    P(>V6)  83.730003
3    P(>V7)  87.779999
4    P(>V8)  92.639999
5    P(>V9)  96.970001/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:53:55] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:55:14] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [19:56:12] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 44.56%
Ordinal stacking meta epoch 1: loss=0.1864
Ordinal stacking meta epoch 2: loss=0.1502
Ordinal stacking meta epoch 3: loss=0.1462
Ordinal stacking meta epoch 4: loss=0.1438
Ordinal stacking meta epoch 5: loss=0.1423
  threshold   accuracy
0    P(>V4)  84.790001
1    P(>V5)  82.150002
2    P(>V6)  85.900002
3    P(>V7)  88.690002
4    P(>V8)  92.970001
5    P(>V9)  97.110001
Overall accuracy: 50.24%
  threshold   accuracy
0    P(>V4)  84.839996
1    P(>V5)  82.959999
2    P(>V6)  84.650002
3    P(>V7)  87.919998
4    P(>V8)  92.300003
5    P(>V9)  96.970001
Overall accuracy: 49.90%
  threshold   accuracy
0    P(>V4)  84.599998
1    P(>V5)  82.680000
2    P(>V6)  84.989998
3    P(>V7)  87.300003
4    P(>V8)  91.919998
5    P(>V9)  96.730003
Overall accuracy: 49.33%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.099998
2    P(>V6)  84.260002
3    P(>V7)  87.150002
4    P(>V8)  92.349998
5    P(>V9)  96.680000
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  84.790001
1    P(>V5)  82.150002
2    P(>V6)  85.900002
3    P(>V7)  88.690002
4    P(>V8)  92.970001
5    P(>V9)  97.110001
Overall accuracy: 50.24%
Ordinal stacking meta epoch 1: loss=0.2214
Ordinal stacking meta epoch 2: loss=0.1590
Ordinal stacking meta epoch 3: loss=0.1560
Ordinal stacking meta epoch 4: loss=0.1547
Ordinal stacking meta epoch 5: loss=0.1536
  threshold   accuracy
0    P(>V4)  84.839996
1    P(>V5)  83.199997
2    P(>V6)  85.610001
3    P(>V7)  88.639999
4    P(>V8)  92.879997
5    P(>V9)  97.110001
Overall accuracy: 50.96%
  threshold   accuracy
0    P(>V4)  84.739998
1    P(>V5)  83.250000
2    P(>V6)  85.370003
3    P(>V7)  88.110001
4    P(>V8)  92.690002
5    P(>V9)  96.730003
Overall accuracy: 50.53%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  83.349998
2    P(>V6)  85.029999
3    P(>V7)  87.629997
4    P(>V8)  92.010002
5    P(>V9)  96.919998
Overall accuracy: 49.52%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.000000
2    P(>V6)  84.500000
3    P(>V7)  87.389999
4    P(>V8)  92.540001
5    P(>V9)  96.730003
Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  84.839996
1    P(>V5)  83.199997
2    P(>V6)  85.610001
3    P(>V7)  88.639999
4    P(>V8)  92.879997
5    P(>V9)  97.110001
Overall accuracy: 50.96%
Ordinal stacking meta epoch 1: loss=0.3645
Ordinal stacking meta epoch 2: loss=0.2678
Ordinal stacking meta epoch 3: loss=0.2673
Ordinal stacking meta epoch 4: loss=0.2672
Ordinal stacking meta epoch 5: loss=0.2671
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  81.040001
2    P(>V6)  83.779999
3    P(>V7)  87.820000
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.14%
  threshold   accuracy
0    P(>V4)  81.949997
1    P(>V5)  81.419998
2    P(>V6)  83.930000
3    P(>V7)  87.779999
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.49%
  threshold   accuracy
0    P(>V4)  81.040001
1    P(>V5)  80.849998
2    P(>V6)  83.690002
3    P(>V7)  87.389999
4    P(>V8)  92.830002
5    P(>V9)  96.629997
Overall accuracy: 45.96%
  threshold   accuracy
0    P(>V4)  81.040001
1    P(>V5)  80.220001
2    P(>V6)  83.059998
3    P(>V7)  87.099998
4    P(>V8)  92.349998
5    P(>V9)  96.629997
Overall accuracy: 46.34%
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  81.040001
2    P(>V6)  83.779999
3    P(>V7)  87.820000
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.14%
----------------- Ordinal iteration 7/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3897
Epoch 02 — loss: 0.3270
Epoch 03 — loss: 0.3099
Epoch 04 — loss: 0.2967
Epoch 05 — loss: 0.2858
Epoch 06 — loss: 0.2747
Epoch 07 — loss: 0.2652
Epoch 08 — loss: 0.2584
Epoch 09 — loss: 0.2513
Epoch 10 — loss: 0.2449
Epoch 11 — loss: 0.2386
Epoch 12 — loss: 0.2313
Epoch 13 — loss: 0.2246
Epoch 14 — loss: 0.2203
Epoch 15 — loss: 0.2127
Epoch 16 — loss: 0.2087
Epoch 17 — loss: 0.2019
Epoch 18 — loss: 0.1986
Epoch 19 — loss: 0.1911
Epoch 20 — loss: 0.1855
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  81.040001
2    P(>V6)  83.970001
3    P(>V7)  87.050003
4    P(>V8)  92.059998
5    P(>V9)  96.339996
Overall accuracy: 46.87%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3574
Epoch 02 — loss: 0.3013
Epoch 03 — loss: 0.2909
Epoch 04 — loss: 0.2826
Epoch 05 — loss: 0.2745
Epoch 06 — loss: 0.2678
Epoch 07 — loss: 0.2627
Epoch 08 — loss: 0.2552
Epoch 09 — loss: 0.2484
Epoch 10 — loss: 0.2420
Epoch 11 — loss: 0.2382
Epoch 12 — loss: 0.2315
Epoch 13 — loss: 0.2255
Epoch 14 — loss: 0.2216
Epoch 15 — loss: 0.2152
Epoch 16 — loss: 0.2121
Epoch 17 — loss: 0.2052
Epoch 18 — loss: 0.2021
Epoch 19 — loss: 0.1971
Epoch 20 — loss: 0.1925
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  81.620003
2    P(>V6)  86.000000
3    P(>V7)  88.349998
4    P(>V8)  91.870003
5    P(>V9)  96.050003
Overall accuracy: 48.75%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3634
Epoch 02 — loss: 0.3147
Epoch 03 — loss: 0.3030
Epoch 04 — loss: 0.2947
Epoch 05 — loss: 0.2847
Epoch 06 — loss: 0.2759
Epoch 07 — loss: 0.2670
Epoch 08 — loss: 0.2625
Epoch 09 — loss: 0.2571
Epoch 10 — loss: 0.2494
Epoch 11 — loss: 0.2458
Epoch 12 — loss: 0.2409
Epoch 13 — loss: 0.2392
Epoch 14 — loss: 0.2321
Epoch 15 — loss: 0.2292
Epoch 16 — loss: 0.2251
Epoch 17 — loss: 0.2186
Epoch 18 — loss: 0.2169
Epoch 19 — loss: 0.2124
Epoch 20 — loss: 0.2085
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  81.470001
2    P(>V6)  85.419998
3    P(>V7)  88.070000
4    P(>V8)  92.300003
5    P(>V9)  96.870003
Overall accuracy: 46.82%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4734
Epoch 02 — loss: 0.3536
Epoch 03 — loss: 0.3006
Epoch 04 — loss: 0.2900
Epoch 05 — loss: 0.2845
Epoch 06 — loss: 0.2799
Epoch 07 — loss: 0.2777
Epoch 08 — loss: 0.2779
Epoch 09 — loss: 0.2756
Epoch 10 — loss: 0.2744
Epoch 11 — loss: 0.2721
Epoch 12 — loss: 0.2720
Epoch 13 — loss: 0.2728
Epoch 14 — loss: 0.2712
Epoch 15 — loss: 0.2711
Epoch 16 — loss: 0.2703
Epoch 17 — loss: 0.2689
Epoch 18 — loss: 0.2688
Epoch 19 — loss: 0.2698
Epoch 20 — loss: 0.2698
  threshold   accuracy
0    P(>V4)  80.849998
1    P(>V5)  80.849998
2    P(>V6)  83.830002
3    P(>V7)  87.629997
4    P(>V8)  92.639999
5    P(>V9)  97.059998
Overall accuracy: 46.20%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4720
Epoch 02 — loss: 0.3473
Epoch 03 — loss: 0.3031
Epoch 04 — loss: 0.2924
Epoch 05 — loss: 0.2868
Epoch 06 — loss: 0.2837
Epoch 07 — loss: 0.2801
Epoch 08 — loss: 0.2776
Epoch 09 — loss: 0.2772
Epoch 10 — loss: 0.2760
Epoch 11 — loss: 0.2748
Epoch 12 — loss: 0.2746
Epoch 13 — loss: 0.2727
Epoch 14 — loss: 0.2730
Epoch 15 — loss: 0.2720
Epoch 16 — loss: 0.2713
Epoch 17 — loss: 0.2711
Epoch 18 — loss: 0.2720
Epoch 19 — loss: 0.2709
Epoch 20 — loss: 0.2710
  threshold   accuracy
0    P(>V4)  81.279999
1    P(>V5)  80.849998
2    P(>V6)  83.779999
3    P(>V7)  87.680000
4    P(>V8)  92.830002
5    P(>V9)  97.059998
Overall accuracy: 45.48%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4198
Epoch 02 — loss: 0.3262
Epoch 03 — loss: 0.3111
Epoch 04 — loss: 0.3026
Epoch 05 — loss: 0.2977
Epoch 06 — loss: 0.2923
Epoch 07 — loss: 0.2886
Epoch 08 — loss: 0.2854
Epoch 09 — loss: 0.2842
Epoch 10 — loss: 0.2808
Epoch 11 — loss: 0.2830
Epoch 12 — loss: 0.2794
Epoch 13 — loss: 0.2804
Epoch 14 — loss: 0.2761
Epoch 15 — loss: 0.2769
Epoch 16 — loss: 0.2744
Epoch 17 — loss: 0.2746
Epoch 18 — loss: 0.2736
Epoch 19 — loss: 0.2742
Epoch 20 — loss: 0.2728
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.510002
2    P(>V6)  83.879997
3    P(>V7)  87.580002
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.15%
Ordinal stacking meta epoch 1: loss=0.2160
Ordinal stacking meta epoch 2: loss=0.1587
Ordinal stacking meta epoch 3: loss=0.1526/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:18:37] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:19:56] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:20:54] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:44:01] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Ordinal stacking meta epoch 4: loss=0.1505
Ordinal stacking meta epoch 5: loss=0.1491
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  82.440002
2    P(>V6)  85.800003
3    P(>V7)  89.029999
4    P(>V8)  93.260002
5    P(>V9)  97.059998
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  85.080002
1    P(>V5)  82.480003
2    P(>V6)  85.800003
3    P(>V7)  88.639999
4    P(>V8)  92.589996
5    P(>V9)  96.629997
Overall accuracy: 49.62%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  83.160004
2    P(>V6)  86.279999
3    P(>V7)  87.970001
4    P(>V8)  92.250000
5    P(>V9)  96.339996
Overall accuracy: 49.62%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  83.010002
2    P(>V6)  85.660004
3    P(>V7)  87.199997
4    P(>V8)  92.110001
5    P(>V9)  96.250000
Overall accuracy: 48.36%
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  82.440002
2    P(>V6)  85.800003
3    P(>V7)  89.029999
4    P(>V8)  93.260002
5    P(>V9)  97.059998
Overall accuracy: 49.76%
Ordinal stacking meta epoch 1: loss=0.1990
Ordinal stacking meta epoch 2: loss=0.1639
Ordinal stacking meta epoch 3: loss=0.1609
Ordinal stacking meta epoch 4: loss=0.1594
Ordinal stacking meta epoch 5: loss=0.1585
  threshold   accuracy
0    P(>V4)  84.989998
1    P(>V5)  82.959999
2    P(>V6)  86.910004
3    P(>V7)  88.739998
4    P(>V8)  93.019997
5    P(>V9)  96.820000
Overall accuracy: 50.48%
  threshold   accuracy
0    P(>V4)  84.940002
1    P(>V5)  83.059998
2    P(>V6)  86.379997
3    P(>V7)  89.169998
4    P(>V8)  92.730003
5    P(>V9)  96.339996
Overall accuracy: 50.19%
  threshold   accuracy
0    P(>V4)  84.889999
1    P(>V5)  82.529999
2    P(>V6)  86.430000
3    P(>V7)  88.589996
4    P(>V8)  92.639999
5    P(>V9)  96.050003
Overall accuracy: 50.67%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.820000
2    P(>V6)  85.800003
3    P(>V7)  88.260002
4    P(>V8)  92.300003
5    P(>V9)  96.199997
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  84.989998
1    P(>V5)  82.959999
2    P(>V6)  86.910004
3    P(>V7)  88.739998
4    P(>V8)  93.019997
5    P(>V9)  96.820000
Overall accuracy: 50.48%
Ordinal stacking meta epoch 1: loss=0.4727
Ordinal stacking meta epoch 2: loss=0.2684
Ordinal stacking meta epoch 3: loss=0.2678
Ordinal stacking meta epoch 4: loss=0.2677
Ordinal stacking meta epoch 5: loss=0.2677
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  80.900002
2    P(>V6)  83.830002
3    P(>V7)  87.820000
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.54%
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.849998
2    P(>V6)  83.779999
3    P(>V7)  88.019997
4    P(>V8)  92.779999
5    P(>V9)  96.970001
Overall accuracy: 46.10%
  threshold   accuracy
0    P(>V4)  81.139999
1    P(>V5)  81.139999
2    P(>V6)  83.449997
3    P(>V7)  86.860001
4    P(>V8)  92.690002
5    P(>V9)  96.489998
Overall accuracy: 45.24%
  threshold   accuracy
0    P(>V4)  80.610001
1    P(>V5)  80.269997
2    P(>V6)  82.919998
3    P(>V7)  86.910004
4    P(>V8)  92.400002
5    P(>V9)  96.970001
Overall accuracy: 44.71%
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  80.900002
2    P(>V6)  83.830002
3    P(>V7)  87.820000
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.54%
----------------- Ordinal iteration 8/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3789
Epoch 02 — loss: 0.3180
Epoch 03 — loss: 0.3046
Epoch 04 — loss: 0.2913
Epoch 05 — loss: 0.2829
Epoch 06 — loss: 0.2710
Epoch 07 — loss: 0.2625
Epoch 08 — loss: 0.2557
Epoch 09 — loss: 0.2474
Epoch 10 — loss: 0.2417
Epoch 11 — loss: 0.2348
Epoch 12 — loss: 0.2287
Epoch 13 — loss: 0.2220
Epoch 14 — loss: 0.2179
Epoch 15 — loss: 0.2125
Epoch 16 — loss: 0.2070
Epoch 17 — loss: 0.2012
Epoch 18 — loss: 0.1957
Epoch 19 — loss: 0.1904
Epoch 20 — loss: 0.1863
  threshold   accuracy
0    P(>V4)  83.160004
1    P(>V5)  82.099998
2    P(>V6)  84.220001
3    P(>V7)  87.389999
4    P(>V8)  92.440002
5    P(>V9)  97.019997
Overall accuracy: 48.22%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3743
Epoch 02 — loss: 0.3135
Epoch 03 — loss: 0.2970
Epoch 04 — loss: 0.2887
Epoch 05 — loss: 0.2778
Epoch 06 — loss: 0.2691
Epoch 07 — loss: 0.2622
Epoch 08 — loss: 0.2563
Epoch 09 — loss: 0.2517
Epoch 10 — loss: 0.2429
Epoch 11 — loss: 0.2386
Epoch 12 — loss: 0.2316
Epoch 13 — loss: 0.2282
Epoch 14 — loss: 0.2225
Epoch 15 — loss: 0.2157
Epoch 16 — loss: 0.2125
Epoch 17 — loss: 0.2035
Epoch 18 — loss: 0.2010
Epoch 19 — loss: 0.1964
Epoch 20 — loss: 0.1897
  threshold   accuracy
0    P(>V4)  82.629997
1    P(>V5)  82.480003
2    P(>V6)  84.650002
3    P(>V7)  87.339996
4    P(>V8)  92.540001
5    P(>V9)  96.970001
Overall accuracy: 47.93%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3593
Epoch 02 — loss: 0.3135
Epoch 03 — loss: 0.3037
Epoch 04 — loss: 0.2981
Epoch 05 — loss: 0.2903
Epoch 06 — loss: 0.2834
Epoch 07 — loss: 0.2748
Epoch 08 — loss: 0.2697
Epoch 09 — loss: 0.2662
Epoch 10 — loss: 0.2608
Epoch 11 — loss: 0.2578
Epoch 12 — loss: 0.2507
Epoch 13 — loss: 0.2456
Epoch 14 — loss: 0.2408
Epoch 15 — loss: 0.2358
Epoch 16 — loss: 0.2316
Epoch 17 — loss: 0.2272
Epoch 18 — loss: 0.2219
Epoch 19 — loss: 0.2164
Epoch 20 — loss: 0.2140
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  80.269997
2    P(>V6)  84.169998
3    P(>V7)  88.070000
4    P(>V8)  92.730003
5    P(>V9)  96.779999
Overall accuracy: 47.79%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4692
Epoch 02 — loss: 0.3419
Epoch 03 — loss: 0.3010
Epoch 04 — loss: 0.2904
Epoch 05 — loss: 0.2861
Epoch 06 — loss: 0.2830
Epoch 07 — loss: 0.2793
Epoch 08 — loss: 0.2783
Epoch 09 — loss: 0.2771
Epoch 10 — loss: 0.2742
Epoch 11 — loss: 0.2742
Epoch 12 — loss: 0.2731
Epoch 13 — loss: 0.2728
Epoch 14 — loss: 0.2734
Epoch 15 — loss: 0.2716
Epoch 16 — loss: 0.2697
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2705
Epoch 19 — loss: 0.2698
Epoch 20 — loss: 0.2691
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.650002
2    P(>V6)  84.070000
3    P(>V7)  87.779999
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.91%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4591
Epoch 02 — loss: 0.3288
Epoch 03 — loss: 0.3000
Epoch 04 — loss: 0.2924
Epoch 05 — loss: 0.2847
Epoch 06 — loss: 0.2818
Epoch 07 — loss: 0.2801
Epoch 08 — loss: 0.2770
Epoch 09 — loss: 0.2771
Epoch 10 — loss: 0.2741
Epoch 11 — loss: 0.2751
Epoch 12 — loss: 0.2757
Epoch 13 — loss: 0.2744
Epoch 14 — loss: 0.2729
Epoch 15 — loss: 0.2721
Epoch 16 — loss: 0.2720
Epoch 17 — loss: 0.2707
Epoch 18 — loss: 0.2713
Epoch 19 — loss: 0.2715
Epoch 20 — loss: 0.2708
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.750000
2    P(>V6)  83.779999
3    P(>V7)  87.540001
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.96%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4335
Epoch 02 — loss: 0.3283
Epoch 03 — loss: 0.3119
Epoch 04 — loss: 0.2990
Epoch 05 — loss: 0.2927
Epoch 06 — loss: 0.2877
Epoch 07 — loss: 0.2845
Epoch 08 — loss: 0.2830
Epoch 09 — loss: 0.2789
Epoch 10 — loss: 0.2782
Epoch 11 — loss: 0.2785
Epoch 12 — loss: 0.2767
Epoch 13 — loss: 0.2772
Epoch 14 — loss: 0.2740
Epoch 15 — loss: 0.2735
Epoch 16 — loss: 0.2736
Epoch 17 — loss: 0.2726
Epoch 18 — loss: 0.2741
Epoch 19 — loss: 0.2721
Epoch 20 — loss: 0.2721
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.800003
2    P(>V6)  83.779999
3    P(>V7)  87.629997
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 46.01%
Ordinal stacking meta epoch 1: loss=0.2395
Ordinal stacking meta epoch 2: loss=0.1569
Ordinal stacking meta epoch 3: loss=0.1519
Ordinal stacking meta epoch 4: loss=0.1494
Ordinal stacking meta epoch 5: loss=0.1475
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.580002
2    P(>V6)  85.709999
3    P(>V7)  88.589996
4    P(>V8)  92.930000
5    P(>V9)  97.019997/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:45:20] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:46:17] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [21:08:55] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 49.33%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.769997
2    P(>V6)  85.230003
3    P(>V7)  88.449997
4    P(>V8)  92.639999
5    P(>V9)  96.730003
Overall accuracy: 47.31%
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.720001
2    P(>V6)  85.370003
3    P(>V7)  88.260002
4    P(>V8)  92.400002
5    P(>V9)  96.779999
Overall accuracy: 47.69%
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  82.529999
2    P(>V6)  85.320000
3    P(>V7)  88.160004
4    P(>V8)  92.489998
5    P(>V9)  96.779999
Overall accuracy: 47.88%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.580002
2    P(>V6)  85.709999
3    P(>V7)  88.589996
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 49.33%
Ordinal stacking meta epoch 1: loss=0.1732
Ordinal stacking meta epoch 2: loss=0.1576
Ordinal stacking meta epoch 3: loss=0.1559
Ordinal stacking meta epoch 4: loss=0.1549
Ordinal stacking meta epoch 5: loss=0.1542
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  83.449997
2    P(>V6)  85.709999
3    P(>V7)  89.120003
4    P(>V8)  93.169998
5    P(>V9)  96.970001
Overall accuracy: 50.05%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  83.970001
2    P(>V6)  86.330002
3    P(>V7)  88.400002
4    P(>V8)  92.160004
5    P(>V9)  96.629997
Overall accuracy: 49.37%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  83.059998
2    P(>V6)  85.610001
3    P(>V7)  88.019997
4    P(>V8)  92.589996
5    P(>V9)  96.820000
Overall accuracy: 48.36%
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  83.110001
2    P(>V6)  85.269997
3    P(>V7)  87.629997
4    P(>V8)  92.489998
5    P(>V9)  96.970001
Overall accuracy: 48.80%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  83.449997
2    P(>V6)  85.709999
3    P(>V7)  89.120003
4    P(>V8)  93.169998
5    P(>V9)  96.970001
Overall accuracy: 50.05%
Ordinal stacking meta epoch 1: loss=0.3158
Ordinal stacking meta epoch 2: loss=0.2677
Ordinal stacking meta epoch 3: loss=0.2675
Ordinal stacking meta epoch 4: loss=0.2675
Ordinal stacking meta epoch 5: loss=0.2675
  threshold   accuracy
0    P(>V4)  81.910004
1    P(>V5)  80.900002
2    P(>V6)  83.830002
3    P(>V7)  87.970001
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.73%
  threshold   accuracy
0    P(>V4)  81.949997
1    P(>V5)  81.040001
2    P(>V6)  84.260002
3    P(>V7)  87.440002
4    P(>V8)  92.730003
5    P(>V9)  96.970001
Overall accuracy: 46.49%
  threshold   accuracy
0    P(>V4)  81.279999
1    P(>V5)  80.220001
2    P(>V6)  83.690002
3    P(>V7)  87.010002
4    P(>V8)  92.199997
5    P(>V9)  96.199997
Overall accuracy: 45.24%
  threshold   accuracy
0    P(>V4)  80.129997
1    P(>V5)  79.400002
2    P(>V6)  83.250000
3    P(>V7)  87.580002
4    P(>V8)  92.059998
5    P(>V9)  96.339996
Overall accuracy: 44.66%
  threshold   accuracy
0    P(>V4)  81.910004
1    P(>V5)  80.900002
2    P(>V6)  83.830002
3    P(>V7)  87.970001
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.73%
----------------- Ordinal iteration 9/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3805
Epoch 02 — loss: 0.3199
Epoch 03 — loss: 0.3075
Epoch 04 — loss: 0.2986
Epoch 05 — loss: 0.2901
Epoch 06 — loss: 0.2827
Epoch 07 — loss: 0.2711
Epoch 08 — loss: 0.2649
Epoch 09 — loss: 0.2568
Epoch 10 — loss: 0.2511
Epoch 11 — loss: 0.2428
Epoch 12 — loss: 0.2371
Epoch 13 — loss: 0.2314
Epoch 14 — loss: 0.2255
Epoch 15 — loss: 0.2194
Epoch 16 — loss: 0.2118
Epoch 17 — loss: 0.2066
Epoch 18 — loss: 0.2006
Epoch 19 — loss: 0.1944
Epoch 20 — loss: 0.1910
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  80.459999
2    P(>V6)  84.260002
3    P(>V7)  86.860001
4    P(>V8)  91.720001
5    P(>V9)  96.680000
Overall accuracy: 46.97%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3712
Epoch 02 — loss: 0.3154
Epoch 03 — loss: 0.2997
Epoch 04 — loss: 0.2882
Epoch 05 — loss: 0.2806
Epoch 06 — loss: 0.2725
Epoch 07 — loss: 0.2640
Epoch 08 — loss: 0.2551
Epoch 09 — loss: 0.2499
Epoch 10 — loss: 0.2402
Epoch 11 — loss: 0.2348
Epoch 12 — loss: 0.2294
Epoch 13 — loss: 0.2229
Epoch 14 — loss: 0.2177
Epoch 15 — loss: 0.2127
Epoch 16 — loss: 0.2054
Epoch 17 — loss: 0.2008
Epoch 18 — loss: 0.1940
Epoch 19 — loss: 0.1890
Epoch 20 — loss: 0.1828
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  81.809998
2    P(>V6)  85.129997
3    P(>V7)  87.099998
4    P(>V8)  92.199997
5    P(>V9)  96.730003
Overall accuracy: 46.97%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3680
Epoch 02 — loss: 0.3134
Epoch 03 — loss: 0.3017
Epoch 04 — loss: 0.2931
Epoch 05 — loss: 0.2908
Epoch 06 — loss: 0.2851
Epoch 07 — loss: 0.2762
Epoch 08 — loss: 0.2693
Epoch 09 — loss: 0.2649
Epoch 10 — loss: 0.2563
Epoch 11 — loss: 0.2503
Epoch 12 — loss: 0.2463
Epoch 13 — loss: 0.2421
Epoch 14 — loss: 0.2386
Epoch 15 — loss: 0.2319
Epoch 16 — loss: 0.2295
Epoch 17 — loss: 0.2261
Epoch 18 — loss: 0.2214
Epoch 19 — loss: 0.2155
Epoch 20 — loss: 0.2132
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  80.940002
2    P(>V6)  84.790001
3    P(>V7)  88.110001
4    P(>V8)  92.879997
5    P(>V9)  96.680000
Overall accuracy: 46.97%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4696
Epoch 02 — loss: 0.3460
Epoch 03 — loss: 0.3064
Epoch 04 — loss: 0.2955
Epoch 05 — loss: 0.2871
Epoch 06 — loss: 0.2835
Epoch 07 — loss: 0.2808
Epoch 08 — loss: 0.2789
Epoch 09 — loss: 0.2761
Epoch 10 — loss: 0.2751
Epoch 11 — loss: 0.2750
Epoch 12 — loss: 0.2716
Epoch 13 — loss: 0.2725
Epoch 14 — loss: 0.2720
Epoch 15 — loss: 0.2712
Epoch 16 — loss: 0.2695
Epoch 17 — loss: 0.2723
Epoch 18 — loss: 0.2707
Epoch 19 — loss: 0.2690
Epoch 20 — loss: 0.2694
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.800003
2    P(>V6)  83.879997
3    P(>V7)  87.970001
4    P(>V8)  92.489998
5    P(>V9)  97.019997
Overall accuracy: 45.38%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4672
Epoch 02 — loss: 0.3414
Epoch 03 — loss: 0.3016
Epoch 04 — loss: 0.2925
Epoch 05 — loss: 0.2863
Epoch 06 — loss: 0.2821
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2770
Epoch 09 — loss: 0.2761
Epoch 10 — loss: 0.2750
Epoch 11 — loss: 0.2730
Epoch 12 — loss: 0.2737
Epoch 13 — loss: 0.2709
Epoch 14 — loss: 0.2727
Epoch 15 — loss: 0.2724
Epoch 16 — loss: 0.2698
Epoch 17 — loss: 0.2711
Epoch 18 — loss: 0.2686
Epoch 19 — loss: 0.2690
Epoch 20 — loss: 0.2704
  threshold   accuracy
0    P(>V4)  80.849998
1    P(>V5)  79.639999
2    P(>V6)  83.589996
3    P(>V7)  86.860001
4    P(>V8)  92.690002
5    P(>V9)  97.059998
Overall accuracy: 42.97%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4401
Epoch 02 — loss: 0.3326
Epoch 03 — loss: 0.3125
Epoch 04 — loss: 0.3011
Epoch 05 — loss: 0.2934
Epoch 06 — loss: 0.2888
Epoch 07 — loss: 0.2870
Epoch 08 — loss: 0.2841
Epoch 09 — loss: 0.2817
Epoch 10 — loss: 0.2796
Epoch 11 — loss: 0.2774
Epoch 12 — loss: 0.2769
Epoch 13 — loss: 0.2759
Epoch 14 — loss: 0.2742
Epoch 15 — loss: 0.2742
Epoch 16 — loss: 0.2723
Epoch 17 — loss: 0.2739
Epoch 18 — loss: 0.2706
Epoch 19 — loss: 0.2710
Epoch 20 — loss: 0.2698
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.800003
2    P(>V6)  84.169998
3    P(>V7)  87.580002
4    P(>V8)  92.589996
5    P(>V9)  96.970001
Overall accuracy: 45.28%
Ordinal stacking meta epoch 1: loss=0.2527
Ordinal stacking meta epoch 2: loss=0.1603
Ordinal stacking meta epoch 3: loss=0.1535
Ordinal stacking meta epoch 4: loss=0.1502
Ordinal stacking meta epoch 5: loss=0.1481
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.150002
2    P(>V6)  86.000000
3    P(>V7)  88.930000
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 48.60%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.680000
2    P(>V6)  85.029999
3    P(>V7)  88.839996
4    P(>V8)  92.300003
5    P(>V9)  96.629997/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [21:10:14] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [21:11:11] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [21:34:18] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.190002
2    P(>V6)  85.470001
3    P(>V7)  89.120003
4    P(>V8)  92.300003
5    P(>V9)  96.820000
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  82.339996
2    P(>V6)  85.080002
3    P(>V7)  88.500000
4    P(>V8)  92.300003
5    P(>V9)  96.580002
Overall accuracy: 48.60%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.150002
2    P(>V6)  86.000000
3    P(>V7)  88.930000
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 48.60%
Ordinal stacking meta epoch 1: loss=0.2388
Ordinal stacking meta epoch 2: loss=0.1644
Ordinal stacking meta epoch 3: loss=0.1601
Ordinal stacking meta epoch 4: loss=0.1580
Ordinal stacking meta epoch 5: loss=0.1567
  threshold   accuracy
0    P(>V4)  84.839996
1    P(>V5)  82.629997
2    P(>V6)  86.190002
3    P(>V7)  89.410004
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 49.71%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.480003
2    P(>V6)  85.559998
3    P(>V7)  89.269997
4    P(>V8)  92.300003
5    P(>V9)  96.629997
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.820000
2    P(>V6)  85.470001
3    P(>V7)  88.589996
4    P(>V8)  91.820000
5    P(>V9)  96.730003
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  81.809998
2    P(>V6)  85.419998
3    P(>V7)  88.070000
4    P(>V8)  92.199997
5    P(>V9)  96.680000
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  84.839996
1    P(>V5)  82.629997
2    P(>V6)  86.190002
3    P(>V7)  89.410004
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 49.71%
Ordinal stacking meta epoch 1: loss=0.3282
Ordinal stacking meta epoch 2: loss=0.2677
Ordinal stacking meta epoch 3: loss=0.2670
Ordinal stacking meta epoch 4: loss=0.2669
Ordinal stacking meta epoch 5: loss=0.2669
  threshold   accuracy
0    P(>V4)  81.089996
1    P(>V5)  80.750000
2    P(>V6)  83.730003
3    P(>V7)  87.919998
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 44.37%
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  80.900002
2    P(>V6)  83.930000
3    P(>V7)  87.540001
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.91%
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.510002
2    P(>V6)  83.300003
3    P(>V7)  87.540001
4    P(>V8)  92.540001
5    P(>V9)  96.540001
Overall accuracy: 45.77%
  threshold   accuracy
0    P(>V4)  80.410004
1    P(>V5)  79.550003
2    P(>V6)  82.680000
3    P(>V7)  86.769997
4    P(>V8)  92.589996
5    P(>V9)  96.779999
Overall accuracy: 45.09%
  threshold   accuracy
0    P(>V4)  81.089996
1    P(>V5)  80.750000
2    P(>V6)  83.730003
3    P(>V7)  87.919998
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 44.37%
----------------- Ordinal iteration 10/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3796
Epoch 02 — loss: 0.3244
Epoch 03 — loss: 0.3074
Epoch 04 — loss: 0.2956
Epoch 05 — loss: 0.2849
Epoch 06 — loss: 0.2770
Epoch 07 — loss: 0.2678
Epoch 08 — loss: 0.2598
Epoch 09 — loss: 0.2492
Epoch 10 — loss: 0.2442
Epoch 11 — loss: 0.2356
Epoch 12 — loss: 0.2316
Epoch 13 — loss: 0.2234
Epoch 14 — loss: 0.2176
Epoch 15 — loss: 0.2128
Epoch 16 — loss: 0.2074
Epoch 17 — loss: 0.2009
Epoch 18 — loss: 0.1963
Epoch 19 — loss: 0.1898
Epoch 20 — loss: 0.1840
  threshold   accuracy
0    P(>V4)  82.580002
1    P(>V5)  80.370003
2    P(>V6)  84.019997
3    P(>V7)  87.629997
4    P(>V8)  91.190002
5    P(>V9)  96.440002
Overall accuracy: 45.48%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3678
Epoch 02 — loss: 0.3110
Epoch 03 — loss: 0.2972
Epoch 04 — loss: 0.2892
Epoch 05 — loss: 0.2794
Epoch 06 — loss: 0.2711
Epoch 07 — loss: 0.2644
Epoch 08 — loss: 0.2572
Epoch 09 — loss: 0.2509
Epoch 10 — loss: 0.2420
Epoch 11 — loss: 0.2346
Epoch 12 — loss: 0.2316
Epoch 13 — loss: 0.2273
Epoch 14 — loss: 0.2190
Epoch 15 — loss: 0.2134
Epoch 16 — loss: 0.2102
Epoch 17 — loss: 0.2024
Epoch 18 — loss: 0.1976
Epoch 19 — loss: 0.1936
Epoch 20 — loss: 0.1852
  threshold   accuracy
0    P(>V4)  83.160004
1    P(>V5)  81.910004
2    P(>V6)  86.330002
3    P(>V7)  88.449997
4    P(>V8)  93.070000
5    P(>V9)  96.580002
Overall accuracy: 48.46%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3649
Epoch 02 — loss: 0.3144
Epoch 03 — loss: 0.3040
Epoch 04 — loss: 0.2944
Epoch 05 — loss: 0.2907
Epoch 06 — loss: 0.2800
Epoch 07 — loss: 0.2747
Epoch 08 — loss: 0.2687
Epoch 09 — loss: 0.2638
Epoch 10 — loss: 0.2579
Epoch 11 — loss: 0.2503
Epoch 12 — loss: 0.2461
Epoch 13 — loss: 0.2408
Epoch 14 — loss: 0.2346
Epoch 15 — loss: 0.2300
Epoch 16 — loss: 0.2260
Epoch 17 — loss: 0.2210
Epoch 18 — loss: 0.2173
Epoch 19 — loss: 0.2115
Epoch 20 — loss: 0.2107
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.339996
2    P(>V6)  86.529999
3    P(>V7)  88.309998
4    P(>V8)  92.400002
5    P(>V9)  96.730003
Overall accuracy: 49.13%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4679
Epoch 02 — loss: 0.3485
Epoch 03 — loss: 0.3034
Epoch 04 — loss: 0.2910
Epoch 05 — loss: 0.2837
Epoch 06 — loss: 0.2793
Epoch 07 — loss: 0.2766
Epoch 08 — loss: 0.2758
Epoch 09 — loss: 0.2748
Epoch 10 — loss: 0.2726
Epoch 11 — loss: 0.2724
Epoch 12 — loss: 0.2728
Epoch 13 — loss: 0.2719
Epoch 14 — loss: 0.2721
Epoch 15 — loss: 0.2716
Epoch 16 — loss: 0.2691
Epoch 17 — loss: 0.2706
Epoch 18 — loss: 0.2701
Epoch 19 — loss: 0.2696
Epoch 20 — loss: 0.2683
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.940002
2    P(>V6)  83.639999
3    P(>V7)  87.629997
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 46.05%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4772
Epoch 02 — loss: 0.3428
Epoch 03 — loss: 0.2972
Epoch 04 — loss: 0.2879
Epoch 05 — loss: 0.2832
Epoch 06 — loss: 0.2804
Epoch 07 — loss: 0.2785
Epoch 08 — loss: 0.2787
Epoch 09 — loss: 0.2759
Epoch 10 — loss: 0.2741
Epoch 11 — loss: 0.2735
Epoch 12 — loss: 0.2732
Epoch 13 — loss: 0.2717
Epoch 14 — loss: 0.2717
Epoch 15 — loss: 0.2724
Epoch 16 — loss: 0.2720
Epoch 17 — loss: 0.2719
Epoch 18 — loss: 0.2700
Epoch 19 — loss: 0.2697
Epoch 20 — loss: 0.2701
  threshold   accuracy
0    P(>V4)  82.190002
1    P(>V5)  80.559998
2    P(>V6)  83.930000
3    P(>V7)  87.730003
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.28%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4405
Epoch 02 — loss: 0.3282
Epoch 03 — loss: 0.3098
Epoch 04 — loss: 0.2993
Epoch 05 — loss: 0.2933
Epoch 06 — loss: 0.2864
Epoch 07 — loss: 0.2846
Epoch 08 — loss: 0.2825
Epoch 09 — loss: 0.2813
Epoch 10 — loss: 0.2803
Epoch 11 — loss: 0.2778
Epoch 12 — loss: 0.2767
Epoch 13 — loss: 0.2755
Epoch 14 — loss: 0.2741
Epoch 15 — loss: 0.2742
Epoch 16 — loss: 0.2731
Epoch 17 — loss: 0.2737
Epoch 18 — loss: 0.2730
Epoch 19 — loss: 0.2721
Epoch 20 — loss: 0.2713
  threshold   accuracy
0    P(>V4)  80.940002
1    P(>V5)  80.320000
2    P(>V6)  83.690002
3    P(>V7)  87.389999
4    P(>V8)  92.440002
5    P(>V9)  97.019997
Overall accuracy: 44.61%
Ordinal stacking meta epoch 1: loss=0.2143
Ordinal stacking meta epoch 2: loss=0.1547
Ordinal stacking meta epoch 3: loss=0.1495
Ordinal stacking meta epoch 4: loss=0.1469
Ordinal stacking meta epoch 5: loss=0.1451
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  83.059998
2    P(>V6)  86.279999
3    P(>V7)  89.029999
4    P(>V8)  93.260002
5    P(>V9)  97.019997
Overall accuracy: 49.86%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.720001
2    P(>V6)  85.949997
3    P(>V7)  88.839996
4    P(>V8)  92.589996
5    P(>V9)  96.779999
Overall accuracy: 48.80%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.239998
2    P(>V6)  86.040001
3    P(>V7)  88.879997
4    P(>V8)  92.199997
5    P(>V9)  96.580002/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [21:35:37] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [21:36:35] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [21:59:42] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 48.27%
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  82.339996
2    P(>V6)  86.000000
3    P(>V7)  89.029999
4    P(>V8)  92.250000
5    P(>V9)  96.389999
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  83.059998
2    P(>V6)  86.279999
3    P(>V7)  89.029999
4    P(>V8)  93.260002
5    P(>V9)  97.019997
Overall accuracy: 49.86%
Ordinal stacking meta epoch 1: loss=0.2581
Ordinal stacking meta epoch 2: loss=0.1606
Ordinal stacking meta epoch 3: loss=0.1574
Ordinal stacking meta epoch 4: loss=0.1560
Ordinal stacking meta epoch 5: loss=0.1551
  threshold   accuracy
0    P(>V4)  85.269997
1    P(>V5)  82.480003
2    P(>V6)  86.910004
3    P(>V7)  89.650002
4    P(>V8)  92.970001
5    P(>V9)  96.820000
Overall accuracy: 50.38%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.389999
2    P(>V6)  86.529999
3    P(>V7)  89.269997
4    P(>V8)  92.690002
5    P(>V9)  96.629997
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.050003
2    P(>V6)  85.800003
3    P(>V7)  89.120003
4    P(>V8)  92.440002
5    P(>V9)  96.540001
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.099998
2    P(>V6)  85.800003
3    P(>V7)  88.739998
4    P(>V8)  92.300003
5    P(>V9)  96.339996
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  85.269997
1    P(>V5)  82.480003
2    P(>V6)  86.910004
3    P(>V7)  89.650002
4    P(>V8)  92.970001
5    P(>V9)  96.820000
Overall accuracy: 50.38%
Ordinal stacking meta epoch 1: loss=0.3554
Ordinal stacking meta epoch 2: loss=0.2682
Ordinal stacking meta epoch 3: loss=0.2674
Ordinal stacking meta epoch 4: loss=0.2673
Ordinal stacking meta epoch 5: loss=0.2673
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  81.040001
2    P(>V6)  83.779999
3    P(>V7)  87.680000
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 45.48%
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.750000
2    P(>V6)  83.730003
3    P(>V7)  87.580002
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 44.95%
  threshold   accuracy
0    P(>V4)  82.290001
1    P(>V5)  80.750000
2    P(>V6)  83.779999
3    P(>V7)  87.339996
4    P(>V8)  92.440002
5    P(>V9)  96.540001
Overall accuracy: 46.49%
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  79.739998
2    P(>V6)  83.349998
3    P(>V7)  86.529999
4    P(>V8)  92.489998
5    P(>V9)  96.680000
Overall accuracy: 45.67%
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  81.040001
2    P(>V6)  83.779999
3    P(>V7)  87.680000
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 45.48%
----------------- Ordinal iteration 11/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3905
Epoch 02 — loss: 0.3274
Epoch 03 — loss: 0.3051
Epoch 04 — loss: 0.2877
Epoch 05 — loss: 0.2744
Epoch 06 — loss: 0.2649
Epoch 07 — loss: 0.2575
Epoch 08 — loss: 0.2472
Epoch 09 — loss: 0.2423
Epoch 10 — loss: 0.2376
Epoch 11 — loss: 0.2291
Epoch 12 — loss: 0.2221
Epoch 13 — loss: 0.2156
Epoch 14 — loss: 0.2101
Epoch 15 — loss: 0.2049
Epoch 16 — loss: 0.2005
Epoch 17 — loss: 0.1954
Epoch 18 — loss: 0.1880
Epoch 19 — loss: 0.1834
Epoch 20 — loss: 0.1802
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  82.150002
2    P(>V6)  85.709999
3    P(>V7)  87.250000
4    P(>V8)  92.589996
5    P(>V9)  96.440002
Overall accuracy: 47.93%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3654
Epoch 02 — loss: 0.3137
Epoch 03 — loss: 0.3007
Epoch 04 — loss: 0.2925
Epoch 05 — loss: 0.2853
Epoch 06 — loss: 0.2758
Epoch 07 — loss: 0.2670
Epoch 08 — loss: 0.2635
Epoch 09 — loss: 0.2550
Epoch 10 — loss: 0.2462
Epoch 11 — loss: 0.2394
Epoch 12 — loss: 0.2339
Epoch 13 — loss: 0.2267
Epoch 14 — loss: 0.2209
Epoch 15 — loss: 0.2152
Epoch 16 — loss: 0.2109
Epoch 17 — loss: 0.2015
Epoch 18 — loss: 0.1971
Epoch 19 — loss: 0.1928
Epoch 20 — loss: 0.1877
  threshold   accuracy
0    P(>V4)  82.959999
1    P(>V5)  81.180000
2    P(>V6)  84.360001
3    P(>V7)  87.250000
4    P(>V8)  92.349998
5    P(>V9)  96.629997
Overall accuracy: 46.10%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3681
Epoch 02 — loss: 0.3180
Epoch 03 — loss: 0.3072
Epoch 04 — loss: 0.2946
Epoch 05 — loss: 0.2876
Epoch 06 — loss: 0.2806
Epoch 07 — loss: 0.2751
Epoch 08 — loss: 0.2666
Epoch 09 — loss: 0.2600
Epoch 10 — loss: 0.2542
Epoch 11 — loss: 0.2485
Epoch 12 — loss: 0.2452
Epoch 13 — loss: 0.2403
Epoch 14 — loss: 0.2353
Epoch 15 — loss: 0.2305
Epoch 16 — loss: 0.2264
Epoch 17 — loss: 0.2227
Epoch 18 — loss: 0.2181
Epoch 19 — loss: 0.2145
Epoch 20 — loss: 0.2089
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  82.190002
2    P(>V6)  84.790001
3    P(>V7)  88.019997
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 47.64%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4665
Epoch 02 — loss: 0.3429
Epoch 03 — loss: 0.3042
Epoch 04 — loss: 0.2912
Epoch 05 — loss: 0.2826
Epoch 06 — loss: 0.2793
Epoch 07 — loss: 0.2777
Epoch 08 — loss: 0.2759
Epoch 09 — loss: 0.2738
Epoch 10 — loss: 0.2737
Epoch 11 — loss: 0.2723
Epoch 12 — loss: 0.2720
Epoch 13 — loss: 0.2711
Epoch 14 — loss: 0.2711
Epoch 15 — loss: 0.2696
Epoch 16 — loss: 0.2705
Epoch 17 — loss: 0.2708
Epoch 18 — loss: 0.2689
Epoch 19 — loss: 0.2686
Epoch 20 — loss: 0.2695
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.849998
2    P(>V6)  83.779999
3    P(>V7)  87.970001
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 45.24%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4719
Epoch 02 — loss: 0.3417
Epoch 03 — loss: 0.3018
Epoch 04 — loss: 0.2923
Epoch 05 — loss: 0.2856
Epoch 06 — loss: 0.2820
Epoch 07 — loss: 0.2799
Epoch 08 — loss: 0.2760
Epoch 09 — loss: 0.2756
Epoch 10 — loss: 0.2756
Epoch 11 — loss: 0.2740
Epoch 12 — loss: 0.2729
Epoch 13 — loss: 0.2725
Epoch 14 — loss: 0.2706
Epoch 15 — loss: 0.2707
Epoch 16 — loss: 0.2707
Epoch 17 — loss: 0.2708
Epoch 18 — loss: 0.2702
Epoch 19 — loss: 0.2698
Epoch 20 — loss: 0.2699
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.800003
2    P(>V6)  83.879997
3    P(>V7)  88.019997
4    P(>V8)  92.730003
5    P(>V9)  97.059998
Overall accuracy: 45.14%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4259
Epoch 02 — loss: 0.3223
Epoch 03 — loss: 0.3092
Epoch 04 — loss: 0.3001
Epoch 05 — loss: 0.2959
Epoch 06 — loss: 0.2876
Epoch 07 — loss: 0.2865
Epoch 08 — loss: 0.2828
Epoch 09 — loss: 0.2805
Epoch 10 — loss: 0.2784
Epoch 11 — loss: 0.2772
Epoch 12 — loss: 0.2769
Epoch 13 — loss: 0.2761
Epoch 14 — loss: 0.2747
Epoch 15 — loss: 0.2741
Epoch 16 — loss: 0.2755
Epoch 17 — loss: 0.2723
Epoch 18 — loss: 0.2720
Epoch 19 — loss: 0.2711
Epoch 20 — loss: 0.2688
  threshold   accuracy
0    P(>V4)  81.089996
1    P(>V5)  81.139999
2    P(>V6)  84.019997
3    P(>V7)  87.440002
4    P(>V8)  92.489998
5    P(>V9)  97.019997
Overall accuracy: 45.24%
Ordinal stacking meta epoch 1: loss=0.2075
Ordinal stacking meta epoch 2: loss=0.1542
Ordinal stacking meta epoch 3: loss=0.1485
Ordinal stacking meta epoch 4: loss=0.1460
Ordinal stacking meta epoch 5: loss=0.1445
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.629997
2    P(>V6)  86.379997
3    P(>V7)  88.739998
4    P(>V8)  93.070000
5    P(>V9)  97.059998
Overall accuracy: 49.52%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.480003
2    P(>V6)  86.040001
3    P(>V7)  88.309998
4    P(>V8)  92.730003
5    P(>V9)  96.919998
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.720001
2    P(>V6)  86.379997
3    P(>V7)  88.639999
4    P(>V8)  92.199997
5    P(>V9)  96.389999
Overall accuracy: 48.85%
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  82.870003
2    P(>V6)  86.040001
3    P(>V7)  88.110001
4    P(>V8)  92.160004
5    P(>V9)  96.489998/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [22:01:02] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [22:01:59] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [22:25:08] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 48.32%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.629997
2    P(>V6)  86.379997
3    P(>V7)  88.739998
4    P(>V8)  93.070000
5    P(>V9)  97.059998
Overall accuracy: 49.52%
Ordinal stacking meta epoch 1: loss=0.2858
Ordinal stacking meta epoch 2: loss=0.1619
Ordinal stacking meta epoch 3: loss=0.1577
Ordinal stacking meta epoch 4: loss=0.1559
Ordinal stacking meta epoch 5: loss=0.1547
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  83.489998
2    P(>V6)  86.620003
3    P(>V7)  88.930000
4    P(>V8)  93.260002
5    P(>V9)  96.919998
Overall accuracy: 49.95%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  83.349998
2    P(>V6)  86.809998
3    P(>V7)  88.589996
4    P(>V8)  93.120003
5    P(>V9)  96.730003
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.529999
2    P(>V6)  87.010002
3    P(>V7)  88.309998
4    P(>V8)  92.540001
5    P(>V9)  96.290001
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  83.199997
2    P(>V6)  86.430000
3    P(>V7)  87.820000
4    P(>V8)  92.540001
5    P(>V9)  96.389999
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  83.489998
2    P(>V6)  86.620003
3    P(>V7)  88.930000
4    P(>V8)  93.260002
5    P(>V9)  96.919998
Overall accuracy: 49.95%
Ordinal stacking meta epoch 1: loss=0.3810
Ordinal stacking meta epoch 2: loss=0.2682
Ordinal stacking meta epoch 3: loss=0.2669
Ordinal stacking meta epoch 4: loss=0.2667
Ordinal stacking meta epoch 5: loss=0.2667
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.849998
2    P(>V6)  84.169998
3    P(>V7)  87.779999
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 45.43%
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  80.750000
2    P(>V6)  83.930000
3    P(>V7)  87.629997
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.38%
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  80.650002
2    P(>V6)  83.879997
3    P(>V7)  87.629997
4    P(>V8)  92.639999
5    P(>V9)  96.779999
Overall accuracy: 45.96%
  threshold   accuracy
0    P(>V4)  81.040001
1    P(>V5)  79.500000
2    P(>V6)  83.730003
3    P(>V7)  86.480003
4    P(>V8)  92.589996
5    P(>V9)  96.540001
Overall accuracy: 45.00%
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.849998
2    P(>V6)  84.169998
3    P(>V7)  87.779999
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 45.43%
----------------- Ordinal iteration 12/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3764
Epoch 02 — loss: 0.3200
Epoch 03 — loss: 0.3045
Epoch 04 — loss: 0.2927
Epoch 05 — loss: 0.2857
Epoch 06 — loss: 0.2775
Epoch 07 — loss: 0.2710
Epoch 08 — loss: 0.2626
Epoch 09 — loss: 0.2551
Epoch 10 — loss: 0.2492
Epoch 11 — loss: 0.2408
Epoch 12 — loss: 0.2381
Epoch 13 — loss: 0.2294
Epoch 14 — loss: 0.2246
Epoch 15 — loss: 0.2194
Epoch 16 — loss: 0.2136
Epoch 17 — loss: 0.2076
Epoch 18 — loss: 0.2039
Epoch 19 — loss: 0.1976
Epoch 20 — loss: 0.1922
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  79.260002
2    P(>V6)  81.809998
3    P(>V7)  86.089996
4    P(>V8)  92.010002
5    P(>V9)  96.580002
Overall accuracy: 44.85%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3785
Epoch 02 — loss: 0.3158
Epoch 03 — loss: 0.3030
Epoch 04 — loss: 0.2934
Epoch 05 — loss: 0.2840
Epoch 06 — loss: 0.2718
Epoch 07 — loss: 0.2652
Epoch 08 — loss: 0.2565
Epoch 09 — loss: 0.2504
Epoch 10 — loss: 0.2430
Epoch 11 — loss: 0.2364
Epoch 12 — loss: 0.2315
Epoch 13 — loss: 0.2255
Epoch 14 — loss: 0.2193
Epoch 15 — loss: 0.2122
Epoch 16 — loss: 0.2082
Epoch 17 — loss: 0.2018
Epoch 18 — loss: 0.1974
Epoch 19 — loss: 0.1911
Epoch 20 — loss: 0.1875
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  81.379997
2    P(>V6)  83.970001
3    P(>V7)  86.570000
4    P(>V8)  91.239998
5    P(>V9)  95.669998
Overall accuracy: 46.92%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3611
Epoch 02 — loss: 0.3115
Epoch 03 — loss: 0.3011
Epoch 04 — loss: 0.2925
Epoch 05 — loss: 0.2895
Epoch 06 — loss: 0.2807
Epoch 07 — loss: 0.2739
Epoch 08 — loss: 0.2689
Epoch 09 — loss: 0.2635
Epoch 10 — loss: 0.2592
Epoch 11 — loss: 0.2520
Epoch 12 — loss: 0.2443
Epoch 13 — loss: 0.2396
Epoch 14 — loss: 0.2349
Epoch 15 — loss: 0.2292
Epoch 16 — loss: 0.2238
Epoch 17 — loss: 0.2190
Epoch 18 — loss: 0.2137
Epoch 19 — loss: 0.2079
Epoch 20 — loss: 0.2031
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  81.330002
2    P(>V6)  85.760002
3    P(>V7)  88.260002
4    P(>V8)  92.489998
5    P(>V9)  96.919998
Overall accuracy: 48.22%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4648
Epoch 02 — loss: 0.3371
Epoch 03 — loss: 0.3042
Epoch 04 — loss: 0.2914
Epoch 05 — loss: 0.2865
Epoch 06 — loss: 0.2812
Epoch 07 — loss: 0.2801
Epoch 08 — loss: 0.2786
Epoch 09 — loss: 0.2759
Epoch 10 — loss: 0.2751
Epoch 11 — loss: 0.2731
Epoch 12 — loss: 0.2731
Epoch 13 — loss: 0.2734
Epoch 14 — loss: 0.2730
Epoch 15 — loss: 0.2719
Epoch 16 — loss: 0.2710
Epoch 17 — loss: 0.2713
Epoch 18 — loss: 0.2715
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2685
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  81.620003
2    P(>V6)  83.639999
3    P(>V7)  87.540001
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 45.38%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4683
Epoch 02 — loss: 0.3310
Epoch 03 — loss: 0.2985
Epoch 04 — loss: 0.2867
Epoch 05 — loss: 0.2832
Epoch 06 — loss: 0.2796
Epoch 07 — loss: 0.2777
Epoch 08 — loss: 0.2757
Epoch 09 — loss: 0.2736
Epoch 10 — loss: 0.2719
Epoch 11 — loss: 0.2722
Epoch 12 — loss: 0.2719
Epoch 13 — loss: 0.2730
Epoch 14 — loss: 0.2711
Epoch 15 — loss: 0.2698
Epoch 16 — loss: 0.2703
Epoch 17 — loss: 0.2702
Epoch 18 — loss: 0.2680
Epoch 19 — loss: 0.2685
Epoch 20 — loss: 0.2692
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  80.800003
2    P(>V6)  83.779999
3    P(>V7)  87.099998
4    P(>V8)  92.589996
5    P(>V9)  96.970001
Overall accuracy: 45.00%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4306
Epoch 02 — loss: 0.3253
Epoch 03 — loss: 0.3103
Epoch 04 — loss: 0.2995
Epoch 05 — loss: 0.2927
Epoch 06 — loss: 0.2876
Epoch 07 — loss: 0.2849
Epoch 08 — loss: 0.2816
Epoch 09 — loss: 0.2802
Epoch 10 — loss: 0.2783
Epoch 11 — loss: 0.2771
Epoch 12 — loss: 0.2760
Epoch 13 — loss: 0.2739
Epoch 14 — loss: 0.2749
Epoch 15 — loss: 0.2737
Epoch 16 — loss: 0.2729
Epoch 17 — loss: 0.2718
Epoch 18 — loss: 0.2719
Epoch 19 — loss: 0.2704
Epoch 20 — loss: 0.2715
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  81.230003
2    P(>V6)  83.930000
3    P(>V7)  87.779999
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 46.49%
Ordinal stacking meta epoch 1: loss=0.1973
Ordinal stacking meta epoch 2: loss=0.1598
Ordinal stacking meta epoch 3: loss=0.1554
Ordinal stacking meta epoch 4: loss=0.1531
Ordinal stacking meta epoch 5: loss=0.1514
  threshold   accuracy
0    P(>V4)  84.599998
1    P(>V5)  82.339996
2    P(>V6)  84.989998
3    P(>V7)  88.639999
4    P(>V8)  93.019997
5    P(>V9)  97.059998
Overall accuracy: 49.42%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  81.180000
2    P(>V6)  84.070000
3    P(>V7)  88.639999
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 47.45%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  81.519997
2    P(>V6)  85.129997
3    P(>V7)  88.209999
4    P(>V8)  92.589996
5    P(>V9)  96.870003
Overall accuracy: 48.60%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  81.139999
2    P(>V6)  84.360001
3    P(>V7)  87.389999
4    P(>V8)  92.690002
5    P(>V9)  96.870003
Overall accuracy: 47.16%
  threshold   accuracy
0    P(>V4)  84.599998
1    P(>V5)  82.339996
2    P(>V6)  84.989998
3    P(>V7)  88.639999
4    P(>V8)  93.019997
5    P(>V9)  97.059998/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [22:26:26] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [22:27:24] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [22:50:29] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 49.42%
Ordinal stacking meta epoch 1: loss=0.3000
Ordinal stacking meta epoch 2: loss=0.1687
Ordinal stacking meta epoch 3: loss=0.1643
Ordinal stacking meta epoch 4: loss=0.1623
Ordinal stacking meta epoch 5: loss=0.1610
  threshold   accuracy
0    P(>V4)  84.790001
1    P(>V5)  82.440002
2    P(>V6)  85.660004
3    P(>V7)  88.930000
4    P(>V8)  92.930000
5    P(>V9)  97.059998
Overall accuracy: 50.48%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.680000
2    P(>V6)  85.129997
3    P(>V7)  89.510002
4    P(>V8)  93.019997
5    P(>V9)  96.820000
Overall accuracy: 50.34%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  81.379997
2    P(>V6)  85.180000
3    P(>V7)  88.019997
4    P(>V8)  92.830002
5    P(>V9)  96.629997
Overall accuracy: 48.51%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  81.139999
2    P(>V6)  84.120003
3    P(>V7)  88.019997
4    P(>V8)  92.540001
5    P(>V9)  96.779999
Overall accuracy: 48.41%
  threshold   accuracy
0    P(>V4)  84.790001
1    P(>V5)  82.440002
2    P(>V6)  85.660004
3    P(>V7)  88.930000
4    P(>V8)  92.930000
5    P(>V9)  97.059998
Overall accuracy: 50.48%
Ordinal stacking meta epoch 1: loss=0.3102
Ordinal stacking meta epoch 2: loss=0.2675
Ordinal stacking meta epoch 3: loss=0.2671
Ordinal stacking meta epoch 4: loss=0.2670
Ordinal stacking meta epoch 5: loss=0.2669
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  81.089996
2    P(>V6)  84.070000
3    P(>V7)  87.540001
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.77%
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  81.180000
2    P(>V6)  84.360001
3    P(>V7)  87.440002
4    P(>V8)  92.639999
5    P(>V9)  96.970001
Overall accuracy: 46.01%
  threshold   accuracy
0    P(>V4)  80.940002
1    P(>V5)  80.459999
2    P(>V6)  83.449997
3    P(>V7)  87.730003
4    P(>V8)  92.160004
5    P(>V9)  96.489998
Overall accuracy: 44.85%
  threshold   accuracy
0    P(>V4)  80.989998
1    P(>V5)  79.260002
2    P(>V6)  84.019997
3    P(>V7)  86.910004
4    P(>V8)  92.400002
5    P(>V9)  96.820000
Overall accuracy: 44.80%
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  81.089996
2    P(>V6)  84.070000
3    P(>V7)  87.540001
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.77%
----------------- Ordinal iteration 13/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3822
Epoch 02 — loss: 0.3196
Epoch 03 — loss: 0.3026
Epoch 04 — loss: 0.2892
Epoch 05 — loss: 0.2811
Epoch 06 — loss: 0.2713
Epoch 07 — loss: 0.2630
Epoch 08 — loss: 0.2538
Epoch 09 — loss: 0.2483
Epoch 10 — loss: 0.2420
Epoch 11 — loss: 0.2352
Epoch 12 — loss: 0.2299
Epoch 13 — loss: 0.2231
Epoch 14 — loss: 0.2181
Epoch 15 — loss: 0.2135
Epoch 16 — loss: 0.2062
Epoch 17 — loss: 0.2007
Epoch 18 — loss: 0.1976
Epoch 19 — loss: 0.1908
Epoch 20 — loss: 0.1860
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  80.800003
2    P(>V6)  84.169998
3    P(>V7)  87.250000
4    P(>V8)  92.349998
5    P(>V9)  96.290001
Overall accuracy: 48.46%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3735
Epoch 02 — loss: 0.3069
Epoch 03 — loss: 0.2950
Epoch 04 — loss: 0.2873
Epoch 05 — loss: 0.2809
Epoch 06 — loss: 0.2720
Epoch 07 — loss: 0.2635
Epoch 08 — loss: 0.2591
Epoch 09 — loss: 0.2500
Epoch 10 — loss: 0.2444
Epoch 11 — loss: 0.2369
Epoch 12 — loss: 0.2311
Epoch 13 — loss: 0.2267
Epoch 14 — loss: 0.2185
Epoch 15 — loss: 0.2150
Epoch 16 — loss: 0.2089
Epoch 17 — loss: 0.2045
Epoch 18 — loss: 0.1994
Epoch 19 — loss: 0.1950
Epoch 20 — loss: 0.1903
  threshold   accuracy
0    P(>V4)  82.629997
1    P(>V5)  82.290001
2    P(>V6)  85.370003
3    P(>V7)  87.540001
4    P(>V8)  91.959999
5    P(>V9)  96.250000
Overall accuracy: 48.27%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3639
Epoch 02 — loss: 0.3125
Epoch 03 — loss: 0.3047
Epoch 04 — loss: 0.2944
Epoch 05 — loss: 0.2886
Epoch 06 — loss: 0.2826
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2723
Epoch 09 — loss: 0.2673
Epoch 10 — loss: 0.2606
Epoch 11 — loss: 0.2582
Epoch 12 — loss: 0.2515
Epoch 13 — loss: 0.2479
Epoch 14 — loss: 0.2404
Epoch 15 — loss: 0.2348
Epoch 16 — loss: 0.2300
Epoch 17 — loss: 0.2255
Epoch 18 — loss: 0.2213
Epoch 19 — loss: 0.2151
Epoch 20 — loss: 0.2096
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  81.379997
2    P(>V6)  84.889999
3    P(>V7)  87.779999
4    P(>V8)  92.690002
5    P(>V9)  96.870003
Overall accuracy: 48.12%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4664
Epoch 02 — loss: 0.3421
Epoch 03 — loss: 0.2989
Epoch 04 — loss: 0.2885
Epoch 05 — loss: 0.2851
Epoch 06 — loss: 0.2802
Epoch 07 — loss: 0.2789
Epoch 08 — loss: 0.2767
Epoch 09 — loss: 0.2748
Epoch 10 — loss: 0.2741
Epoch 11 — loss: 0.2744
Epoch 12 — loss: 0.2726
Epoch 13 — loss: 0.2714
Epoch 14 — loss: 0.2720
Epoch 15 — loss: 0.2720
Epoch 16 — loss: 0.2697
Epoch 17 — loss: 0.2711
Epoch 18 — loss: 0.2710
Epoch 19 — loss: 0.2690
Epoch 20 — loss: 0.2701
  threshold   accuracy
0    P(>V4)  81.379997
1    P(>V5)  80.800003
2    P(>V6)  83.970001
3    P(>V7)  87.730003
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.00%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4616
Epoch 02 — loss: 0.3317
Epoch 03 — loss: 0.2991
Epoch 04 — loss: 0.2907
Epoch 05 — loss: 0.2843
Epoch 06 — loss: 0.2841
Epoch 07 — loss: 0.2793
Epoch 08 — loss: 0.2775
Epoch 09 — loss: 0.2762
Epoch 10 — loss: 0.2756
Epoch 11 — loss: 0.2727
Epoch 12 — loss: 0.2724
Epoch 13 — loss: 0.2724
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2726
Epoch 16 — loss: 0.2688
Epoch 17 — loss: 0.2701
Epoch 18 — loss: 0.2696
Epoch 19 — loss: 0.2692
Epoch 20 — loss: 0.2685
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  80.989998
2    P(>V6)  83.779999
3    P(>V7)  87.730003
4    P(>V8)  92.540001
5    P(>V9)  97.019997
Overall accuracy: 45.28%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4256
Epoch 02 — loss: 0.3259
Epoch 03 — loss: 0.3086
Epoch 04 — loss: 0.2996
Epoch 05 — loss: 0.2933
Epoch 06 — loss: 0.2898
Epoch 07 — loss: 0.2853
Epoch 08 — loss: 0.2851
Epoch 09 — loss: 0.2814
Epoch 10 — loss: 0.2798
Epoch 11 — loss: 0.2788
Epoch 12 — loss: 0.2761
Epoch 13 — loss: 0.2772
Epoch 14 — loss: 0.2759
Epoch 15 — loss: 0.2745
Epoch 16 — loss: 0.2745
Epoch 17 — loss: 0.2745
Epoch 18 — loss: 0.2751
Epoch 19 — loss: 0.2743
Epoch 20 — loss: 0.2717
  threshold   accuracy
0    P(>V4)  80.900002
1    P(>V5)  79.449997
2    P(>V6)  83.830002
3    P(>V7)  87.150002
4    P(>V8)  92.349998
5    P(>V9)  96.970001
Overall accuracy: 42.59%
Ordinal stacking meta epoch 1: loss=0.2300
Ordinal stacking meta epoch 2: loss=0.1597
Ordinal stacking meta epoch 3: loss=0.1527
Ordinal stacking meta epoch 4: loss=0.1498
Ordinal stacking meta epoch 5: loss=0.1481
  threshold   accuracy
0    P(>V4)  84.940002
1    P(>V5)  82.150002
2    P(>V6)  86.330002
3    P(>V7)  88.839996
4    P(>V8)  93.120003
5    P(>V9)  97.059998
Overall accuracy: 50.34%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.440002
2    P(>V6)  85.849998
3    P(>V7)  88.589996
4    P(>V8)  92.489998
5    P(>V9)  95.860001
Overall accuracy: 48.46%
  threshold   accuracy
0    P(>V4)  84.699997
1    P(>V5)  82.769997
2    P(>V6)  85.419998
3    P(>V7)  88.449997
4    P(>V8)  92.199997
5    P(>V9)  96.099998
Overall accuracy: 50.10%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.529999
2    P(>V6)  85.709999
3    P(>V7)  88.160004
4    P(>V8)  92.059998
5    P(>V9)  96.199997
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  84.940002
1    P(>V5)  82.150002
2    P(>V6)  86.330002
3    P(>V7)  88.839996
4    P(>V8)  93.120003
5    P(>V9)  97.059998
Overall accuracy: 50.34%
Ordinal stacking meta epoch 1: loss=0.2366
Ordinal stacking meta epoch 2: loss=0.1646
Ordinal stacking meta epoch 3: loss=0.1624/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [22:51:48] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [22:52:46] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [23:15:54] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [23:17:13] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Ordinal stacking meta epoch 4: loss=0.1613
Ordinal stacking meta epoch 5: loss=0.1606
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.720001
2    P(>V6)  86.570000
3    P(>V7)  88.639999
4    P(>V8)  92.830002
5    P(>V9)  97.059998
Overall accuracy: 50.29%
  threshold   accuracy
0    P(>V4)  84.699997
1    P(>V5)  82.580002
2    P(>V6)  86.480003
3    P(>V7)  88.879997
4    P(>V8)  93.070000
5    P(>V9)  95.860001
Overall accuracy: 50.19%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.820000
2    P(>V6)  86.089996
3    P(>V7)  88.879997
4    P(>V8)  92.639999
5    P(>V9)  96.150002
Overall accuracy: 50.24%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  83.250000
2    P(>V6)  85.800003
3    P(>V7)  87.820000
4    P(>V8)  92.589996
5    P(>V9)  96.540001
Overall accuracy: 49.71%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.720001
2    P(>V6)  86.570000
3    P(>V7)  88.639999
4    P(>V8)  92.830002
5    P(>V9)  97.059998
Overall accuracy: 50.29%
Ordinal stacking meta epoch 1: loss=0.2958
Ordinal stacking meta epoch 2: loss=0.2672
Ordinal stacking meta epoch 3: loss=0.2672
Ordinal stacking meta epoch 4: loss=0.2672
Ordinal stacking meta epoch 5: loss=0.2671
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.559998
2    P(>V6)  83.879997
3    P(>V7)  87.629997
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 44.90%
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  81.040001
2    P(>V6)  83.730003
3    P(>V7)  87.730003
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.77%
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  79.980003
2    P(>V6)  83.489998
3    P(>V7)  87.389999
4    P(>V8)  92.199997
5    P(>V9)  96.680000
Overall accuracy: 45.24%
  threshold   accuracy
0    P(>V4)  80.510002
1    P(>V5)  79.309998
2    P(>V6)  83.199997
3    P(>V7)  87.010002
4    P(>V8)  92.059998
5    P(>V9)  96.680000
Overall accuracy: 44.51%
  threshold   accuracy
0    P(>V4)  81.330002
1    P(>V5)  80.559998
2    P(>V6)  83.879997
3    P(>V7)  87.629997
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 44.90%
----------------- Ordinal iteration 14/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3820
Epoch 02 — loss: 0.3216
Epoch 03 — loss: 0.3040
Epoch 04 — loss: 0.2922
Epoch 05 — loss: 0.2831
Epoch 06 — loss: 0.2733
Epoch 07 — loss: 0.2659
Epoch 08 — loss: 0.2572
Epoch 09 — loss: 0.2531
Epoch 10 — loss: 0.2456
Epoch 11 — loss: 0.2401
Epoch 12 — loss: 0.2338
Epoch 13 — loss: 0.2269
Epoch 14 — loss: 0.2225
Epoch 15 — loss: 0.2155
Epoch 16 — loss: 0.2074
Epoch 17 — loss: 0.2033
Epoch 18 — loss: 0.2002
Epoch 19 — loss: 0.1929
Epoch 20 — loss: 0.1894
  threshold   accuracy
0    P(>V4)  82.919998
1    P(>V5)  80.610001
2    P(>V6)  83.589996
3    P(>V7)  86.430000
4    P(>V8)  91.190002
5    P(>V9)  96.730003
Overall accuracy: 44.85%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3636
Epoch 02 — loss: 0.3113
Epoch 03 — loss: 0.2987
Epoch 04 — loss: 0.2890
Epoch 05 — loss: 0.2846
Epoch 06 — loss: 0.2739
Epoch 07 — loss: 0.2701
Epoch 08 — loss: 0.2606
Epoch 09 — loss: 0.2533
Epoch 10 — loss: 0.2481
Epoch 11 — loss: 0.2408
Epoch 12 — loss: 0.2346
Epoch 13 — loss: 0.2274
Epoch 14 — loss: 0.2233
Epoch 15 — loss: 0.2162
Epoch 16 — loss: 0.2120
Epoch 17 — loss: 0.2047
Epoch 18 — loss: 0.1986
Epoch 19 — loss: 0.1960
Epoch 20 — loss: 0.1879
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  81.620003
2    P(>V6)  85.559998
3    P(>V7)  88.400002
4    P(>V8)  92.199997
5    P(>V9)  96.870003
Overall accuracy: 50.10%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3624
Epoch 02 — loss: 0.3141
Epoch 03 — loss: 0.3004
Epoch 04 — loss: 0.2966
Epoch 05 — loss: 0.2893
Epoch 06 — loss: 0.2834
Epoch 07 — loss: 0.2771
Epoch 08 — loss: 0.2725
Epoch 09 — loss: 0.2668
Epoch 10 — loss: 0.2581
Epoch 11 — loss: 0.2529
Epoch 12 — loss: 0.2478
Epoch 13 — loss: 0.2426
Epoch 14 — loss: 0.2387
Epoch 15 — loss: 0.2328
Epoch 16 — loss: 0.2283
Epoch 17 — loss: 0.2236
Epoch 18 — loss: 0.2191
Epoch 19 — loss: 0.2131
Epoch 20 — loss: 0.2095
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  83.110001
2    P(>V6)  86.190002
3    P(>V7)  87.010002
4    P(>V8)  93.410004
5    P(>V9)  97.110001
Overall accuracy: 50.05%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4630
Epoch 02 — loss: 0.3334
Epoch 03 — loss: 0.3024
Epoch 04 — loss: 0.2922
Epoch 05 — loss: 0.2850
Epoch 06 — loss: 0.2817
Epoch 07 — loss: 0.2805
Epoch 08 — loss: 0.2787
Epoch 09 — loss: 0.2773
Epoch 10 — loss: 0.2769
Epoch 11 — loss: 0.2749
Epoch 12 — loss: 0.2722
Epoch 13 — loss: 0.2746
Epoch 14 — loss: 0.2710
Epoch 15 — loss: 0.2718
Epoch 16 — loss: 0.2705
Epoch 17 — loss: 0.2703
Epoch 18 — loss: 0.2702
Epoch 19 — loss: 0.2690
Epoch 20 — loss: 0.2699
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.900002
2    P(>V6)  83.970001
3    P(>V7)  87.489998
4    P(>V8)  92.589996
5    P(>V9)  97.059998
Overall accuracy: 45.24%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4702
Epoch 02 — loss: 0.3337
Epoch 03 — loss: 0.3000
Epoch 04 — loss: 0.2921
Epoch 05 — loss: 0.2850
Epoch 06 — loss: 0.2820
Epoch 07 — loss: 0.2783
Epoch 08 — loss: 0.2779
Epoch 09 — loss: 0.2749
Epoch 10 — loss: 0.2736
Epoch 11 — loss: 0.2721
Epoch 12 — loss: 0.2723
Epoch 13 — loss: 0.2730
Epoch 14 — loss: 0.2716
Epoch 15 — loss: 0.2707
Epoch 16 — loss: 0.2714
Epoch 17 — loss: 0.2698
Epoch 18 — loss: 0.2691
Epoch 19 — loss: 0.2709
Epoch 20 — loss: 0.2692
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.900002
2    P(>V6)  83.730003
3    P(>V7)  87.389999
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 45.67%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4221
Epoch 02 — loss: 0.3292
Epoch 03 — loss: 0.3102
Epoch 04 — loss: 0.2993
Epoch 05 — loss: 0.2910
Epoch 06 — loss: 0.2875
Epoch 07 — loss: 0.2832
Epoch 08 — loss: 0.2808
Epoch 09 — loss: 0.2792
Epoch 10 — loss: 0.2777
Epoch 11 — loss: 0.2765
Epoch 12 — loss: 0.2765
Epoch 13 — loss: 0.2747
Epoch 14 — loss: 0.2753
Epoch 15 — loss: 0.2736
Epoch 16 — loss: 0.2737
Epoch 17 — loss: 0.2729
Epoch 18 — loss: 0.2730
Epoch 19 — loss: 0.2707
Epoch 20 — loss: 0.2712
  threshold   accuracy
0    P(>V4)  80.900002
1    P(>V5)  80.800003
2    P(>V6)  83.589996
3    P(>V7)  87.580002
4    P(>V8)  92.639999
5    P(>V9)  96.970001
Overall accuracy: 44.08%
Ordinal stacking meta epoch 1: loss=0.2004
Ordinal stacking meta epoch 2: loss=0.1539
Ordinal stacking meta epoch 3: loss=0.1485
Ordinal stacking meta epoch 4: loss=0.1468
Ordinal stacking meta epoch 5: loss=0.1458
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.290001
2    P(>V6)  85.610001
3    P(>V7)  88.790001
4    P(>V8)  92.970001
5    P(>V9)  97.059998
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  82.580002
2    P(>V6)  86.040001
3    P(>V7)  88.790001
4    P(>V8)  93.169998
5    P(>V9)  96.919998
Overall accuracy: 48.60%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.529999
2    P(>V6)  85.709999
3    P(>V7)  88.500000
4    P(>V8)  92.440002
5    P(>V9)  96.440002
Overall accuracy: 48.36%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.680000
2    P(>V6)  85.180000
3    P(>V7)  87.820000
4    P(>V8)  92.440002
5    P(>V9)  96.440002
Overall accuracy: 47.83%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.290001
2    P(>V6)  85.610001
3    P(>V7)  88.790001
4    P(>V8)  92.970001
5    P(>V9)  97.059998
Overall accuracy: 49.09%
Ordinal stacking meta epoch 1: loss=0.2345
Ordinal stacking meta epoch 2: loss=0.1619
Ordinal stacking meta epoch 3: loss=0.1591
Ordinal stacking meta epoch 4: loss=0.1577
Ordinal stacking meta epoch 5: loss=0.1568
  threshold   accuracy
0    P(>V4)  84.839996
1    P(>V5)  83.589996
2    P(>V6)  86.959999
3    P(>V7)  88.980003
4    P(>V8)  93.260002
5    P(>V9)  97.160004/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [23:18:10] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [23:41:13] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [23:42:32] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 51.20%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  83.300003
2    P(>V6)  85.709999
3    P(>V7)  89.080002
4    P(>V8)  93.019997
5    P(>V9)  96.919998
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.629997
2    P(>V6)  85.419998
3    P(>V7)  88.879997
4    P(>V8)  92.489998
5    P(>V9)  96.489998
Overall accuracy: 48.80%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  83.199997
2    P(>V6)  85.949997
3    P(>V7)  88.349998
4    P(>V8)  92.589996
5    P(>V9)  96.250000
Overall accuracy: 50.10%
  threshold   accuracy
0    P(>V4)  84.839996
1    P(>V5)  83.589996
2    P(>V6)  86.959999
3    P(>V7)  88.980003
4    P(>V8)  93.260002
5    P(>V9)  97.160004
Overall accuracy: 51.20%
Ordinal stacking meta epoch 1: loss=0.3033
Ordinal stacking meta epoch 2: loss=0.2674
Ordinal stacking meta epoch 3: loss=0.2674
Ordinal stacking meta epoch 4: loss=0.2673
Ordinal stacking meta epoch 5: loss=0.2673
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.849998
2    P(>V6)  83.830002
3    P(>V7)  87.629997
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.33%
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  81.040001
2    P(>V6)  83.690002
3    P(>V7)  87.730003
4    P(>V8)  92.879997
5    P(>V9)  96.970001
Overall accuracy: 45.91%
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.940002
2    P(>V6)  84.070000
3    P(>V7)  86.529999
4    P(>V8)  92.349998
5    P(>V9)  96.680000
Overall accuracy: 46.10%
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.080002
2    P(>V6)  83.349998
3    P(>V7)  86.620003
4    P(>V8)  92.440002
5    P(>V9)  96.730003
Overall accuracy: 45.96%
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.849998
2    P(>V6)  83.830002
3    P(>V7)  87.629997
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.33%
----------------- Ordinal iteration 15/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3771
Epoch 02 — loss: 0.3233
Epoch 03 — loss: 0.3102
Epoch 04 — loss: 0.2973
Epoch 05 — loss: 0.2868
Epoch 06 — loss: 0.2753
Epoch 07 — loss: 0.2633
Epoch 08 — loss: 0.2561
Epoch 09 — loss: 0.2480
Epoch 10 — loss: 0.2399
Epoch 11 — loss: 0.2344
Epoch 12 — loss: 0.2283
Epoch 13 — loss: 0.2211
Epoch 14 — loss: 0.2156
Epoch 15 — loss: 0.2107
Epoch 16 — loss: 0.2040
Epoch 17 — loss: 0.1986
Epoch 18 — loss: 0.1934
Epoch 19 — loss: 0.1874
Epoch 20 — loss: 0.1806
  threshold   accuracy
0    P(>V4)  82.680000
1    P(>V5)  81.040001
2    P(>V6)  84.360001
3    P(>V7)  87.919998
4    P(>V8)  92.489998
5    P(>V9)  97.019997
Overall accuracy: 47.31%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3877
Epoch 02 — loss: 0.3137
Epoch 03 — loss: 0.2999
Epoch 04 — loss: 0.2872
Epoch 05 — loss: 0.2814
Epoch 06 — loss: 0.2711
Epoch 07 — loss: 0.2648
Epoch 08 — loss: 0.2577
Epoch 09 — loss: 0.2513
Epoch 10 — loss: 0.2441
Epoch 11 — loss: 0.2388
Epoch 12 — loss: 0.2302
Epoch 13 — loss: 0.2281
Epoch 14 — loss: 0.2216
Epoch 15 — loss: 0.2148
Epoch 16 — loss: 0.2078
Epoch 17 — loss: 0.2052
Epoch 18 — loss: 0.1993
Epoch 19 — loss: 0.1941
Epoch 20 — loss: 0.1884
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  81.230003
2    P(>V6)  84.309998
3    P(>V7)  88.449997
4    P(>V8)  92.059998
5    P(>V9)  96.199997
Overall accuracy: 46.73%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3574
Epoch 02 — loss: 0.3113
Epoch 03 — loss: 0.3013
Epoch 04 — loss: 0.2943
Epoch 05 — loss: 0.2867
Epoch 06 — loss: 0.2816
Epoch 07 — loss: 0.2758
Epoch 08 — loss: 0.2718
Epoch 09 — loss: 0.2645
Epoch 10 — loss: 0.2590
Epoch 11 — loss: 0.2522
Epoch 12 — loss: 0.2478
Epoch 13 — loss: 0.2425
Epoch 14 — loss: 0.2369
Epoch 15 — loss: 0.2332
Epoch 16 — loss: 0.2271
Epoch 17 — loss: 0.2226
Epoch 18 — loss: 0.2169
Epoch 19 — loss: 0.2120
Epoch 20 — loss: 0.2068
  threshold   accuracy
0    P(>V4)  82.629997
1    P(>V5)  82.190002
2    P(>V6)  85.510002
3    P(>V7)  88.839996
4    P(>V8)  92.779999
5    P(>V9)  96.629997
Overall accuracy: 48.99%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4554
Epoch 02 — loss: 0.3296
Epoch 03 — loss: 0.2986
Epoch 04 — loss: 0.2891
Epoch 05 — loss: 0.2854
Epoch 06 — loss: 0.2815
Epoch 07 — loss: 0.2795
Epoch 08 — loss: 0.2784
Epoch 09 — loss: 0.2767
Epoch 10 — loss: 0.2751
Epoch 11 — loss: 0.2749
Epoch 12 — loss: 0.2727
Epoch 13 — loss: 0.2710
Epoch 14 — loss: 0.2723
Epoch 15 — loss: 0.2714
Epoch 16 — loss: 0.2696
Epoch 17 — loss: 0.2691
Epoch 18 — loss: 0.2699
Epoch 19 — loss: 0.2693
Epoch 20 — loss: 0.2704
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.750000
2    P(>V6)  83.879997
3    P(>V7)  87.580002
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.57%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4651
Epoch 02 — loss: 0.3305
Epoch 03 — loss: 0.2952
Epoch 04 — loss: 0.2876
Epoch 05 — loss: 0.2827
Epoch 06 — loss: 0.2812
Epoch 07 — loss: 0.2788
Epoch 08 — loss: 0.2753
Epoch 09 — loss: 0.2748
Epoch 10 — loss: 0.2750
Epoch 11 — loss: 0.2729
Epoch 12 — loss: 0.2732
Epoch 13 — loss: 0.2730
Epoch 14 — loss: 0.2709
Epoch 15 — loss: 0.2712
Epoch 16 — loss: 0.2707
Epoch 17 — loss: 0.2702
Epoch 18 — loss: 0.2696
Epoch 19 — loss: 0.2697
Epoch 20 — loss: 0.2704
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  81.089996
2    P(>V6)  83.879997
3    P(>V7)  87.820000
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 45.67%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4326
Epoch 02 — loss: 0.3270
Epoch 03 — loss: 0.3080
Epoch 04 — loss: 0.2966
Epoch 05 — loss: 0.2934
Epoch 06 — loss: 0.2868
Epoch 07 — loss: 0.2849
Epoch 08 — loss: 0.2813
Epoch 09 — loss: 0.2815
Epoch 10 — loss: 0.2792
Epoch 11 — loss: 0.2785
Epoch 12 — loss: 0.2754
Epoch 13 — loss: 0.2763
Epoch 14 — loss: 0.2755
Epoch 15 — loss: 0.2720
Epoch 16 — loss: 0.2746
Epoch 17 — loss: 0.2714
Epoch 18 — loss: 0.2715
Epoch 19 — loss: 0.2722
Epoch 20 — loss: 0.2712
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.559998
2    P(>V6)  83.779999
3    P(>V7)  87.540001
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 46.20%
Ordinal stacking meta epoch 1: loss=0.2105
Ordinal stacking meta epoch 2: loss=0.1539
Ordinal stacking meta epoch 3: loss=0.1472
Ordinal stacking meta epoch 4: loss=0.1441
Ordinal stacking meta epoch 5: loss=0.1422
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.919998
2    P(>V6)  85.849998
3    P(>V7)  89.410004
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 50.24%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.629997
2    P(>V6)  84.459999
3    P(>V7)  88.879997
4    P(>V8)  92.489998
5    P(>V9)  96.489998
Overall accuracy: 47.79%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.239998
2    P(>V6)  84.120003
3    P(>V7)  89.120003
4    P(>V8)  92.440002
5    P(>V9)  96.199997
Overall accuracy: 48.17%
  threshold   accuracy
0    P(>V4)  83.059998
1    P(>V5)  81.709999
2    P(>V6)  83.970001
3    P(>V7)  88.690002
4    P(>V8)  92.349998
5    P(>V9)  96.290001
Overall accuracy: 46.97%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.919998
2    P(>V6)  85.849998
3    P(>V7)  89.410004
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 50.24%
Ordinal stacking meta epoch 1: loss=0.2374
Ordinal stacking meta epoch 2: loss=0.1568
Ordinal stacking meta epoch 3: loss=0.1539
Ordinal stacking meta epoch 4: loss=0.1524
Ordinal stacking meta epoch 5: loss=0.1515
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.919998
2    P(>V6)  85.760002
3    P(>V7)  90.040001
4    P(>V8)  92.830002
5    P(>V9)  96.820000
Overall accuracy: 50.48%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.870003
2    P(>V6)  85.370003
3    P(>V7)  89.610001
4    P(>V8)  92.349998
5    P(>V9)  96.290001/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [23:43:29] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:06:18] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:07:37] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.959999
2    P(>V6)  84.889999
3    P(>V7)  88.790001
4    P(>V8)  92.400002
5    P(>V9)  96.099998
Overall accuracy: 48.60%
  threshold   accuracy
0    P(>V4)  82.919998
1    P(>V5)  82.769997
2    P(>V6)  84.889999
3    P(>V7)  88.930000
4    P(>V8)  92.489998
5    P(>V9)  96.339996
Overall accuracy: 48.75%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.919998
2    P(>V6)  85.760002
3    P(>V7)  90.040001
4    P(>V8)  92.830002
5    P(>V9)  96.820000
Overall accuracy: 50.48%
Ordinal stacking meta epoch 1: loss=0.3311
Ordinal stacking meta epoch 2: loss=0.2680
Ordinal stacking meta epoch 3: loss=0.2672
Ordinal stacking meta epoch 4: loss=0.2671
Ordinal stacking meta epoch 5: loss=0.2671
  threshold   accuracy
0    P(>V4)  81.860001
1    P(>V5)  80.989998
2    P(>V6)  83.930000
3    P(>V7)  87.779999
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.34%
  threshold   accuracy
0    P(>V4)  82.000000
1    P(>V5)  80.800003
2    P(>V6)  84.260002
3    P(>V7)  87.919998
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 46.39%
  threshold   accuracy
0    P(>V4)  80.320000
1    P(>V5)  80.320000
2    P(>V6)  83.690002
3    P(>V7)  87.820000
4    P(>V8)  92.300003
5    P(>V9)  96.680000
Overall accuracy: 44.61%
  threshold   accuracy
0    P(>V4)  80.459999
1    P(>V5)  78.680000
2    P(>V6)  83.349998
3    P(>V7)  87.010002
4    P(>V8)  92.160004
5    P(>V9)  96.580002
Overall accuracy: 44.80%
  threshold   accuracy
0    P(>V4)  81.860001
1    P(>V5)  80.989998
2    P(>V6)  83.930000
3    P(>V7)  87.779999
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.34%
----------------- Ordinal iteration 16/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3773
Epoch 02 — loss: 0.3160
Epoch 03 — loss: 0.3037
Epoch 04 — loss: 0.2919
Epoch 05 — loss: 0.2833
Epoch 06 — loss: 0.2760
Epoch 07 — loss: 0.2699
Epoch 08 — loss: 0.2644
Epoch 09 — loss: 0.2547
Epoch 10 — loss: 0.2502
Epoch 11 — loss: 0.2457
Epoch 12 — loss: 0.2397
Epoch 13 — loss: 0.2336
Epoch 14 — loss: 0.2271
Epoch 15 — loss: 0.2210
Epoch 16 — loss: 0.2181
Epoch 17 — loss: 0.2096
Epoch 18 — loss: 0.2051
Epoch 19 — loss: 0.2011
Epoch 20 — loss: 0.1960
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  81.089996
2    P(>V6)  85.029999
3    P(>V7)  87.680000
4    P(>V8)  92.540001
5    P(>V9)  96.919998
Overall accuracy: 49.04%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3736
Epoch 02 — loss: 0.3133
Epoch 03 — loss: 0.2989
Epoch 04 — loss: 0.2878
Epoch 05 — loss: 0.2828
Epoch 06 — loss: 0.2725
Epoch 07 — loss: 0.2653
Epoch 08 — loss: 0.2599
Epoch 09 — loss: 0.2533
Epoch 10 — loss: 0.2463
Epoch 11 — loss: 0.2391
Epoch 12 — loss: 0.2324
Epoch 13 — loss: 0.2251
Epoch 14 — loss: 0.2199
Epoch 15 — loss: 0.2140
Epoch 16 — loss: 0.2081
Epoch 17 — loss: 0.2024
Epoch 18 — loss: 0.1979
Epoch 19 — loss: 0.1922
Epoch 20 — loss: 0.1872
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  81.330002
2    P(>V6)  85.129997
3    P(>V7)  88.449997
4    P(>V8)  92.349998
5    P(>V9)  96.680000
Overall accuracy: 48.94%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3632
Epoch 02 — loss: 0.3115
Epoch 03 — loss: 0.3022
Epoch 04 — loss: 0.2941
Epoch 05 — loss: 0.2851
Epoch 06 — loss: 0.2781
Epoch 07 — loss: 0.2721
Epoch 08 — loss: 0.2665
Epoch 09 — loss: 0.2612
Epoch 10 — loss: 0.2531
Epoch 11 — loss: 0.2495
Epoch 12 — loss: 0.2427
Epoch 13 — loss: 0.2369
Epoch 14 — loss: 0.2322
Epoch 15 — loss: 0.2287
Epoch 16 — loss: 0.2224
Epoch 17 — loss: 0.2178
Epoch 18 — loss: 0.2119
Epoch 19 — loss: 0.2085
Epoch 20 — loss: 0.2050
  threshold   accuracy
0    P(>V4)  84.650002
1    P(>V5)  82.580002
2    P(>V6)  86.040001
3    P(>V7)  88.260002
4    P(>V8)  92.199997
5    P(>V9)  96.389999
Overall accuracy: 49.42%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4655
Epoch 02 — loss: 0.3395
Epoch 03 — loss: 0.3056
Epoch 04 — loss: 0.2931
Epoch 05 — loss: 0.2863
Epoch 06 — loss: 0.2823
Epoch 07 — loss: 0.2794
Epoch 08 — loss: 0.2771
Epoch 09 — loss: 0.2752
Epoch 10 — loss: 0.2747
Epoch 11 — loss: 0.2735
Epoch 12 — loss: 0.2728
Epoch 13 — loss: 0.2728
Epoch 14 — loss: 0.2722
Epoch 15 — loss: 0.2721
Epoch 16 — loss: 0.2706
Epoch 17 — loss: 0.2690
Epoch 18 — loss: 0.2696
Epoch 19 — loss: 0.2704
Epoch 20 — loss: 0.2707
  threshold   accuracy
0    P(>V4)  80.989998
1    P(>V5)  80.410004
2    P(>V6)  83.730003
3    P(>V7)  87.629997
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.67%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4689
Epoch 02 — loss: 0.3522
Epoch 03 — loss: 0.2995
Epoch 04 — loss: 0.2891
Epoch 05 — loss: 0.2821
Epoch 06 — loss: 0.2796
Epoch 07 — loss: 0.2778
Epoch 08 — loss: 0.2750
Epoch 09 — loss: 0.2734
Epoch 10 — loss: 0.2722
Epoch 11 — loss: 0.2715
Epoch 12 — loss: 0.2718
Epoch 13 — loss: 0.2712
Epoch 14 — loss: 0.2711
Epoch 15 — loss: 0.2706
Epoch 16 — loss: 0.2703
Epoch 17 — loss: 0.2691
Epoch 18 — loss: 0.2684
Epoch 19 — loss: 0.2702
Epoch 20 — loss: 0.2685
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.750000
2    P(>V6)  84.169998
3    P(>V7)  87.870003
4    P(>V8)  92.440002
5    P(>V9)  97.019997
Overall accuracy: 45.38%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4183
Epoch 02 — loss: 0.3189
Epoch 03 — loss: 0.3050
Epoch 04 — loss: 0.2955
Epoch 05 — loss: 0.2904
Epoch 06 — loss: 0.2853
Epoch 07 — loss: 0.2829
Epoch 08 — loss: 0.2806
Epoch 09 — loss: 0.2811
Epoch 10 — loss: 0.2780
Epoch 11 — loss: 0.2792
Epoch 12 — loss: 0.2761
Epoch 13 — loss: 0.2756
Epoch 14 — loss: 0.2757
Epoch 15 — loss: 0.2749
Epoch 16 — loss: 0.2730
Epoch 17 — loss: 0.2728
Epoch 18 — loss: 0.2726
Epoch 19 — loss: 0.2719
Epoch 20 — loss: 0.2727
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.989998
2    P(>V6)  84.070000
3    P(>V7)  87.300003
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.81%
Ordinal stacking meta epoch 1: loss=0.2718
Ordinal stacking meta epoch 2: loss=0.1597
Ordinal stacking meta epoch 3: loss=0.1527
Ordinal stacking meta epoch 4: loss=0.1502
Ordinal stacking meta epoch 5: loss=0.1487
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  81.519997
2    P(>V6)  85.709999
3    P(>V7)  88.639999
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  84.790001
1    P(>V5)  82.680000
2    P(>V6)  86.089996
3    P(>V7)  88.639999
4    P(>V8)  92.730003
5    P(>V9)  96.779999
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.720001
2    P(>V6)  86.239998
3    P(>V7)  88.400002
4    P(>V8)  92.879997
5    P(>V9)  96.779999
Overall accuracy: 49.62%
  threshold   accuracy
0    P(>V4)  83.830002
1    P(>V5)  82.239998
2    P(>V6)  86.480003
3    P(>V7)  88.400002
4    P(>V8)  92.589996
5    P(>V9)  96.820000
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  81.519997
2    P(>V6)  85.709999
3    P(>V7)  88.639999
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 48.70%
Ordinal stacking meta epoch 1: loss=0.3052
Ordinal stacking meta epoch 2: loss=0.1693
Ordinal stacking meta epoch 3: loss=0.1661
Ordinal stacking meta epoch 4: loss=0.1646
Ordinal stacking meta epoch 5: loss=0.1636
  threshold   accuracy
0    P(>V4)  85.129997
1    P(>V5)  82.480003
2    P(>V6)  86.480003
3    P(>V7)  88.690002
4    P(>V8)  93.459999
5    P(>V9)  97.019997
Overall accuracy: 50.58%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  82.720001
2    P(>V6)  86.669998
3    P(>V7)  88.589996
4    P(>V8)  92.730003
5    P(>V9)  96.730003
Overall accuracy: 49.37%
  threshold   accuracy
0    P(>V4)  84.599998
1    P(>V5)  82.239998
2    P(>V6)  86.279999
3    P(>V7)  88.639999
4    P(>V8)  92.730003
5    P(>V9)  96.580002/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:08:35] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:31:38] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:32:59] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 49.66%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.099998
2    P(>V6)  85.949997
3    P(>V7)  88.309998
4    P(>V8)  92.440002
5    P(>V9)  96.779999
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  85.129997
1    P(>V5)  82.480003
2    P(>V6)  86.480003
3    P(>V7)  88.690002
4    P(>V8)  93.459999
5    P(>V9)  97.019997
Overall accuracy: 50.58%
Ordinal stacking meta epoch 1: loss=0.3703
Ordinal stacking meta epoch 2: loss=0.2677
Ordinal stacking meta epoch 3: loss=0.2671
Ordinal stacking meta epoch 4: loss=0.2671
Ordinal stacking meta epoch 5: loss=0.2671
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.849998
2    P(>V6)  83.830002
3    P(>V7)  87.540001
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 45.67%
  threshold   accuracy
0    P(>V4)  81.910004
1    P(>V5)  80.750000
2    P(>V6)  84.169998
3    P(>V7)  87.730003
4    P(>V8)  92.489998
5    P(>V9)  96.970001
Overall accuracy: 46.15%
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.459999
2    P(>V6)  83.970001
3    P(>V7)  87.389999
4    P(>V8)  92.639999
5    P(>V9)  96.580002
Overall accuracy: 46.01%
  threshold   accuracy
0    P(>V4)  81.139999
1    P(>V5)  79.599998
2    P(>V6)  83.449997
3    P(>V7)  86.570000
4    P(>V8)  92.349998
5    P(>V9)  96.730003
Overall accuracy: 45.14%
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.849998
2    P(>V6)  83.830002
3    P(>V7)  87.540001
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 45.67%
----------------- Ordinal iteration 17/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3882
Epoch 02 — loss: 0.3273
Epoch 03 — loss: 0.3050
Epoch 04 — loss: 0.2912
Epoch 05 — loss: 0.2784
Epoch 06 — loss: 0.2678
Epoch 07 — loss: 0.2561
Epoch 08 — loss: 0.2487
Epoch 09 — loss: 0.2435
Epoch 10 — loss: 0.2349
Epoch 11 — loss: 0.2292
Epoch 12 — loss: 0.2240
Epoch 13 — loss: 0.2159
Epoch 14 — loss: 0.2115
Epoch 15 — loss: 0.2049
Epoch 16 — loss: 0.1996
Epoch 17 — loss: 0.1923
Epoch 18 — loss: 0.1888
Epoch 19 — loss: 0.1833
Epoch 20 — loss: 0.1780
  threshold   accuracy
0    P(>V4)  82.629997
1    P(>V5)  82.190002
2    P(>V6)  85.559998
3    P(>V7)  88.449997
4    P(>V8)  92.779999
5    P(>V9)  96.919998
Overall accuracy: 49.13%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3711
Epoch 02 — loss: 0.3131
Epoch 03 — loss: 0.2990
Epoch 04 — loss: 0.2914
Epoch 05 — loss: 0.2823
Epoch 06 — loss: 0.2720
Epoch 07 — loss: 0.2650
Epoch 08 — loss: 0.2570
Epoch 09 — loss: 0.2497
Epoch 10 — loss: 0.2432
Epoch 11 — loss: 0.2368
Epoch 12 — loss: 0.2315
Epoch 13 — loss: 0.2251
Epoch 14 — loss: 0.2180
Epoch 15 — loss: 0.2130
Epoch 16 — loss: 0.2074
Epoch 17 — loss: 0.2022
Epoch 18 — loss: 0.1950
Epoch 19 — loss: 0.1913
Epoch 20 — loss: 0.1874
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  80.220001
2    P(>V6)  83.930000
3    P(>V7)  86.769997
4    P(>V8)  92.250000
5    P(>V9)  96.730003
Overall accuracy: 47.40%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3677
Epoch 02 — loss: 0.3162
Epoch 03 — loss: 0.3023
Epoch 04 — loss: 0.2937
Epoch 05 — loss: 0.2874
Epoch 06 — loss: 0.2769
Epoch 07 — loss: 0.2679
Epoch 08 — loss: 0.2640
Epoch 09 — loss: 0.2559
Epoch 10 — loss: 0.2516
Epoch 11 — loss: 0.2452
Epoch 12 — loss: 0.2409
Epoch 13 — loss: 0.2363
Epoch 14 — loss: 0.2303
Epoch 15 — loss: 0.2258
Epoch 16 — loss: 0.2218
Epoch 17 — loss: 0.2187
Epoch 18 — loss: 0.2131
Epoch 19 — loss: 0.2101
Epoch 20 — loss: 0.2031
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  83.010002
2    P(>V6)  85.370003
3    P(>V7)  88.160004
4    P(>V8)  92.690002
5    P(>V9)  96.779999
Overall accuracy: 48.99%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4722
Epoch 02 — loss: 0.3448
Epoch 03 — loss: 0.3053
Epoch 04 — loss: 0.2944
Epoch 05 — loss: 0.2872
Epoch 06 — loss: 0.2821
Epoch 07 — loss: 0.2806
Epoch 08 — loss: 0.2777
Epoch 09 — loss: 0.2765
Epoch 10 — loss: 0.2765
Epoch 11 — loss: 0.2726
Epoch 12 — loss: 0.2738
Epoch 13 — loss: 0.2730
Epoch 14 — loss: 0.2719
Epoch 15 — loss: 0.2720
Epoch 16 — loss: 0.2698
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2706
Epoch 19 — loss: 0.2701
Epoch 20 — loss: 0.2707
  threshold   accuracy
0    P(>V4)  80.650002
1    P(>V5)  80.849998
2    P(>V6)  83.779999
3    P(>V7)  87.730003
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 45.67%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4680
Epoch 02 — loss: 0.3385
Epoch 03 — loss: 0.2981
Epoch 04 — loss: 0.2913
Epoch 05 — loss: 0.2853
Epoch 06 — loss: 0.2816
Epoch 07 — loss: 0.2782
Epoch 08 — loss: 0.2767
Epoch 09 — loss: 0.2752
Epoch 10 — loss: 0.2744
Epoch 11 — loss: 0.2743
Epoch 12 — loss: 0.2716
Epoch 13 — loss: 0.2717
Epoch 14 — loss: 0.2711
Epoch 15 — loss: 0.2699
Epoch 16 — loss: 0.2701
Epoch 17 — loss: 0.2696
Epoch 18 — loss: 0.2701
Epoch 19 — loss: 0.2712
Epoch 20 — loss: 0.2691
  threshold   accuracy
0    P(>V4)  81.949997
1    P(>V5)  80.940002
2    P(>V6)  84.120003
3    P(>V7)  87.820000
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.73%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4288
Epoch 02 — loss: 0.3278
Epoch 03 — loss: 0.3125
Epoch 04 — loss: 0.3020
Epoch 05 — loss: 0.2944
Epoch 06 — loss: 0.2893
Epoch 07 — loss: 0.2872
Epoch 08 — loss: 0.2847
Epoch 09 — loss: 0.2801
Epoch 10 — loss: 0.2798
Epoch 11 — loss: 0.2790
Epoch 12 — loss: 0.2767
Epoch 13 — loss: 0.2750
Epoch 14 — loss: 0.2754
Epoch 15 — loss: 0.2758
Epoch 16 — loss: 0.2747
Epoch 17 — loss: 0.2737
Epoch 18 — loss: 0.2736
Epoch 19 — loss: 0.2721
Epoch 20 — loss: 0.2723
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.989998
2    P(>V6)  84.220001
3    P(>V7)  87.680000
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 46.34%
Ordinal stacking meta epoch 1: loss=0.2564
Ordinal stacking meta epoch 2: loss=0.1537
Ordinal stacking meta epoch 3: loss=0.1448
Ordinal stacking meta epoch 4: loss=0.1413
Ordinal stacking meta epoch 5: loss=0.1395
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.190002
2    P(>V6)  86.000000
3    P(>V7)  88.790001
4    P(>V8)  93.120003
5    P(>V9)  97.059998
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  82.870003
2    P(>V6)  86.480003
3    P(>V7)  88.790001
4    P(>V8)  93.070000
5    P(>V9)  96.870003
Overall accuracy: 49.66%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  82.680000
2    P(>V6)  85.370003
3    P(>V7)  88.879997
4    P(>V8)  93.070000
5    P(>V9)  96.730003
Overall accuracy: 50.05%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.480003
2    P(>V6)  85.610001
3    P(>V7)  88.260002
4    P(>V8)  92.830002
5    P(>V9)  96.730003
Overall accuracy: 50.00%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.190002
2    P(>V6)  86.000000
3    P(>V7)  88.790001
4    P(>V8)  93.120003
5    P(>V9)  97.059998
Overall accuracy: 49.57%
Ordinal stacking meta epoch 1: loss=0.2163
Ordinal stacking meta epoch 2: loss=0.1525
Ordinal stacking meta epoch 3: loss=0.1498
Ordinal stacking meta epoch 4: loss=0.1486
Ordinal stacking meta epoch 5: loss=0.1477
  threshold   accuracy
0    P(>V4)  84.739998
1    P(>V5)  83.250000
2    P(>V6)  85.949997
3    P(>V7)  88.930000
4    P(>V8)  93.019997
5    P(>V9)  97.059998
Overall accuracy: 50.05%
  threshold   accuracy
0    P(>V4)  84.019997
1    P(>V5)  83.449997
2    P(>V6)  86.279999
3    P(>V7)  89.120003
4    P(>V8)  92.970001
5    P(>V9)  96.779999
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.680000
2    P(>V6)  85.660004
3    P(>V7)  89.029999
4    P(>V8)  93.019997
5    P(>V9)  96.919998
Overall accuracy: 50.19%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.290001
2    P(>V6)  86.089996
3    P(>V7)  88.839996
4    P(>V8)  92.930000
5    P(>V9)  96.919998/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:33:56] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:56:45] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:58:04] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 50.43%
  threshold   accuracy
0    P(>V4)  84.739998
1    P(>V5)  83.250000
2    P(>V6)  85.949997
3    P(>V7)  88.930000
4    P(>V8)  93.019997
5    P(>V9)  97.059998
Overall accuracy: 50.05%
Ordinal stacking meta epoch 1: loss=0.2955
Ordinal stacking meta epoch 2: loss=0.2671
Ordinal stacking meta epoch 3: loss=0.2670
Ordinal stacking meta epoch 4: loss=0.2670
Ordinal stacking meta epoch 5: loss=0.2670
  threshold   accuracy
0    P(>V4)  81.910004
1    P(>V5)  80.940002
2    P(>V6)  84.169998
3    P(>V7)  87.629997
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 46.54%
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  81.139999
2    P(>V6)  84.260002
3    P(>V7)  87.680000
4    P(>V8)  92.879997
5    P(>V9)  96.970001
Overall accuracy: 46.39%
  threshold   accuracy
0    P(>V4)  80.989998
1    P(>V5)  80.559998
2    P(>V6)  83.489998
3    P(>V7)  87.339996
4    P(>V8)  92.930000
5    P(>V9)  96.540001
Overall accuracy: 45.09%
  threshold   accuracy
0    P(>V4)  80.989998
1    P(>V5)  80.269997
2    P(>V6)  83.830002
3    P(>V7)  86.809998
4    P(>V8)  92.879997
5    P(>V9)  96.339996
Overall accuracy: 45.96%
  threshold   accuracy
0    P(>V4)  81.910004
1    P(>V5)  80.940002
2    P(>V6)  84.169998
3    P(>V7)  87.629997
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 46.54%
----------------- Ordinal iteration 18/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3709
Epoch 02 — loss: 0.3161
Epoch 03 — loss: 0.3007
Epoch 04 — loss: 0.2895
Epoch 05 — loss: 0.2817
Epoch 06 — loss: 0.2702
Epoch 07 — loss: 0.2649
Epoch 08 — loss: 0.2580
Epoch 09 — loss: 0.2513
Epoch 10 — loss: 0.2461
Epoch 11 — loss: 0.2392
Epoch 12 — loss: 0.2322
Epoch 13 — loss: 0.2275
Epoch 14 — loss: 0.2219
Epoch 15 — loss: 0.2141
Epoch 16 — loss: 0.2105
Epoch 17 — loss: 0.2038
Epoch 18 — loss: 0.1992
Epoch 19 — loss: 0.1937
Epoch 20 — loss: 0.1861
  threshold   accuracy
0    P(>V4)  82.769997
1    P(>V5)  81.669998
2    P(>V6)  84.120003
3    P(>V7)  87.199997
4    P(>V8)  91.870003
5    P(>V9)  96.730003
Overall accuracy: 47.50%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3677
Epoch 02 — loss: 0.3079
Epoch 03 — loss: 0.2954
Epoch 04 — loss: 0.2895
Epoch 05 — loss: 0.2794
Epoch 06 — loss: 0.2708
Epoch 07 — loss: 0.2633
Epoch 08 — loss: 0.2571
Epoch 09 — loss: 0.2460
Epoch 10 — loss: 0.2419
Epoch 11 — loss: 0.2341
Epoch 12 — loss: 0.2286
Epoch 13 — loss: 0.2229
Epoch 14 — loss: 0.2157
Epoch 15 — loss: 0.2084
Epoch 16 — loss: 0.2022
Epoch 17 — loss: 0.1970
Epoch 18 — loss: 0.1916
Epoch 19 — loss: 0.1860
Epoch 20 — loss: 0.1807
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.389999
2    P(>V6)  84.500000
3    P(>V7)  87.970001
4    P(>V8)  92.540001
5    P(>V9)  96.489998
Overall accuracy: 49.42%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3667
Epoch 02 — loss: 0.3142
Epoch 03 — loss: 0.2987
Epoch 04 — loss: 0.2916
Epoch 05 — loss: 0.2859
Epoch 06 — loss: 0.2793
Epoch 07 — loss: 0.2728
Epoch 08 — loss: 0.2693
Epoch 09 — loss: 0.2643
Epoch 10 — loss: 0.2568
Epoch 11 — loss: 0.2538
Epoch 12 — loss: 0.2514
Epoch 13 — loss: 0.2433
Epoch 14 — loss: 0.2395
Epoch 15 — loss: 0.2348
Epoch 16 — loss: 0.2302
Epoch 17 — loss: 0.2250
Epoch 18 — loss: 0.2220
Epoch 19 — loss: 0.2167
Epoch 20 — loss: 0.2117
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  82.050003
2    P(>V6)  85.610001
3    P(>V7)  88.209999
4    P(>V8)  92.589996
5    P(>V9)  96.820000
Overall accuracy: 48.12%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4706
Epoch 02 — loss: 0.3425
Epoch 03 — loss: 0.3015
Epoch 04 — loss: 0.2930
Epoch 05 — loss: 0.2875
Epoch 06 — loss: 0.2840
Epoch 07 — loss: 0.2800
Epoch 08 — loss: 0.2784
Epoch 09 — loss: 0.2761
Epoch 10 — loss: 0.2743
Epoch 11 — loss: 0.2739
Epoch 12 — loss: 0.2726
Epoch 13 — loss: 0.2727
Epoch 14 — loss: 0.2718
Epoch 15 — loss: 0.2711
Epoch 16 — loss: 0.2716
Epoch 17 — loss: 0.2688
Epoch 18 — loss: 0.2710
Epoch 19 — loss: 0.2703
Epoch 20 — loss: 0.2705
  threshold   accuracy
0    P(>V4)  81.809998
1    P(>V5)  80.610001
2    P(>V6)  84.220001
3    P(>V7)  87.540001
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.01%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4634
Epoch 02 — loss: 0.3265
Epoch 03 — loss: 0.2930
Epoch 04 — loss: 0.2869
Epoch 05 — loss: 0.2827
Epoch 06 — loss: 0.2797
Epoch 07 — loss: 0.2781
Epoch 08 — loss: 0.2780
Epoch 09 — loss: 0.2752
Epoch 10 — loss: 0.2742
Epoch 11 — loss: 0.2728
Epoch 12 — loss: 0.2727
Epoch 13 — loss: 0.2715
Epoch 14 — loss: 0.2717
Epoch 15 — loss: 0.2712
Epoch 16 — loss: 0.2717
Epoch 17 — loss: 0.2700
Epoch 18 — loss: 0.2692
Epoch 19 — loss: 0.2713
Epoch 20 — loss: 0.2687
  threshold   accuracy
0    P(>V4)  81.949997
1    P(>V5)  80.940002
2    P(>V6)  84.019997
3    P(>V7)  87.489998
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 46.34%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4265
Epoch 02 — loss: 0.3304
Epoch 03 — loss: 0.3177
Epoch 04 — loss: 0.3025
Epoch 05 — loss: 0.2944
Epoch 06 — loss: 0.2866
Epoch 07 — loss: 0.2823
Epoch 08 — loss: 0.2809
Epoch 09 — loss: 0.2801
Epoch 10 — loss: 0.2795
Epoch 11 — loss: 0.2767
Epoch 12 — loss: 0.2751
Epoch 13 — loss: 0.2735
Epoch 14 — loss: 0.2726
Epoch 15 — loss: 0.2732
Epoch 16 — loss: 0.2727
Epoch 17 — loss: 0.2713
Epoch 18 — loss: 0.2714
Epoch 19 — loss: 0.2705
Epoch 20 — loss: 0.2700
  threshold   accuracy
0    P(>V4)  81.139999
1    P(>V5)  80.269997
2    P(>V6)  83.690002
3    P(>V7)  87.820000
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 44.51%
Ordinal stacking meta epoch 1: loss=0.2004
Ordinal stacking meta epoch 2: loss=0.1495
Ordinal stacking meta epoch 3: loss=0.1439
Ordinal stacking meta epoch 4: loss=0.1416
Ordinal stacking meta epoch 5: loss=0.1401
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.720001
2    P(>V6)  85.949997
3    P(>V7)  88.739998
4    P(>V8)  93.120003
5    P(>V9)  97.160004
Overall accuracy: 50.38%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  83.160004
2    P(>V6)  85.760002
3    P(>V7)  88.349998
4    P(>V8)  92.489998
5    P(>V9)  96.870003
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  84.410004
1    P(>V5)  83.199997
2    P(>V6)  85.080002
3    P(>V7)  87.870003
4    P(>V8)  92.639999
5    P(>V9)  96.250000
Overall accuracy: 49.33%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.529999
2    P(>V6)  85.510002
3    P(>V7)  87.580002
4    P(>V8)  92.160004
5    P(>V9)  96.199997
Overall accuracy: 49.13%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.720001
2    P(>V6)  85.949997
3    P(>V7)  88.739998
4    P(>V8)  93.120003
5    P(>V9)  97.160004
Overall accuracy: 50.38%
Ordinal stacking meta epoch 1: loss=0.2134
Ordinal stacking meta epoch 2: loss=0.1593
Ordinal stacking meta epoch 3: loss=0.1549
Ordinal stacking meta epoch 4: loss=0.1528
Ordinal stacking meta epoch 5: loss=0.1517
  threshold   accuracy
0    P(>V4)  84.889999
1    P(>V5)  83.300003
2    P(>V6)  86.720001
3    P(>V7)  88.589996
4    P(>V8)  93.120003
5    P(>V9)  97.019997
Overall accuracy: 51.06%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  83.830002
2    P(>V6)  85.760002
3    P(>V7)  88.160004
4    P(>V8)  92.300003
5    P(>V9)  96.580002
Overall accuracy: 50.00%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  83.639999
2    P(>V6)  85.320000
3    P(>V7)  87.629997
4    P(>V8)  92.400002
5    P(>V9)  96.290001
Overall accuracy: 49.66%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.339996
2    P(>V6)  85.709999
3    P(>V7)  87.389999
4    P(>V8)  91.870003
5    P(>V9)  96.440002
Overall accuracy: 48.60%
  threshold   accuracy
0    P(>V4)  84.889999
1    P(>V5)  83.300003
2    P(>V6)  86.720001
3    P(>V7)  88.589996
4    P(>V8)  93.120003
5    P(>V9)  97.019997/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [00:59:02] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:22:03] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:23:23] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 51.06%
Ordinal stacking meta epoch 1: loss=0.3840
Ordinal stacking meta epoch 2: loss=0.2675
Ordinal stacking meta epoch 3: loss=0.2669
Ordinal stacking meta epoch 4: loss=0.2669
Ordinal stacking meta epoch 5: loss=0.2669
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  80.900002
2    P(>V6)  83.879997
3    P(>V7)  87.730003
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 45.62%
  threshold   accuracy
0    P(>V4)  81.809998
1    P(>V5)  81.330002
2    P(>V6)  84.220001
3    P(>V7)  87.820000
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 46.58%
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.800003
2    P(>V6)  84.019997
3    P(>V7)  87.540001
4    P(>V8)  92.540001
5    P(>V9)  96.440002
Overall accuracy: 46.49%
  threshold   accuracy
0    P(>V4)  80.370003
1    P(>V5)  79.690002
2    P(>V6)  83.250000
3    P(>V7)  87.489998
4    P(>V8)  92.199997
5    P(>V9)  96.540001
Overall accuracy: 44.75%
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  80.900002
2    P(>V6)  83.879997
3    P(>V7)  87.730003
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 45.62%
----------------- Ordinal iteration 19/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3799
Epoch 02 — loss: 0.3174
Epoch 03 — loss: 0.3074
Epoch 04 — loss: 0.2937
Epoch 05 — loss: 0.2865
Epoch 06 — loss: 0.2746
Epoch 07 — loss: 0.2662
Epoch 08 — loss: 0.2596
Epoch 09 — loss: 0.2513
Epoch 10 — loss: 0.2442
Epoch 11 — loss: 0.2377
Epoch 12 — loss: 0.2304
Epoch 13 — loss: 0.2244
Epoch 14 — loss: 0.2207
Epoch 15 — loss: 0.2109
Epoch 16 — loss: 0.2091
Epoch 17 — loss: 0.2021
Epoch 18 — loss: 0.1973
Epoch 19 — loss: 0.1895
Epoch 20 — loss: 0.1860
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  81.180000
2    P(>V6)  84.019997
3    P(>V7)  88.110001
4    P(>V8)  92.110001
5    P(>V9)  96.730003
Overall accuracy: 46.49%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3606
Epoch 02 — loss: 0.3081
Epoch 03 — loss: 0.2953
Epoch 04 — loss: 0.2897
Epoch 05 — loss: 0.2809
Epoch 06 — loss: 0.2725
Epoch 07 — loss: 0.2625
Epoch 08 — loss: 0.2535
Epoch 09 — loss: 0.2516
Epoch 10 — loss: 0.2409
Epoch 11 — loss: 0.2369
Epoch 12 — loss: 0.2330
Epoch 13 — loss: 0.2250
Epoch 14 — loss: 0.2200
Epoch 15 — loss: 0.2149
Epoch 16 — loss: 0.2096
Epoch 17 — loss: 0.2046
Epoch 18 — loss: 0.1986
Epoch 19 — loss: 0.1930
Epoch 20 — loss: 0.1897
  threshold   accuracy
0    P(>V4)  82.629997
1    P(>V5)  83.010002
2    P(>V6)  84.989998
3    P(>V7)  88.839996
4    P(>V8)  92.489998
5    P(>V9)  96.820000
Overall accuracy: 47.26%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3546
Epoch 02 — loss: 0.3110
Epoch 03 — loss: 0.3027
Epoch 04 — loss: 0.2916
Epoch 05 — loss: 0.2820
Epoch 06 — loss: 0.2744
Epoch 07 — loss: 0.2673
Epoch 08 — loss: 0.2614
Epoch 09 — loss: 0.2551
Epoch 10 — loss: 0.2509
Epoch 11 — loss: 0.2468
Epoch 12 — loss: 0.2398
Epoch 13 — loss: 0.2341
Epoch 14 — loss: 0.2310
Epoch 15 — loss: 0.2258
Epoch 16 — loss: 0.2197
Epoch 17 — loss: 0.2172
Epoch 18 — loss: 0.2104
Epoch 19 — loss: 0.2065
Epoch 20 — loss: 0.2022
  threshold   accuracy
0    P(>V4)  82.629997
1    P(>V5)  81.949997
2    P(>V6)  86.330002
3    P(>V7)  87.970001
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 50.19%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4646
Epoch 02 — loss: 0.3444
Epoch 03 — loss: 0.3103
Epoch 04 — loss: 0.2965
Epoch 05 — loss: 0.2895
Epoch 06 — loss: 0.2840
Epoch 07 — loss: 0.2801
Epoch 08 — loss: 0.2772
Epoch 09 — loss: 0.2732
Epoch 10 — loss: 0.2752
Epoch 11 — loss: 0.2751
Epoch 12 — loss: 0.2722
Epoch 13 — loss: 0.2715
Epoch 14 — loss: 0.2713
Epoch 15 — loss: 0.2720
Epoch 16 — loss: 0.2702
Epoch 17 — loss: 0.2706
Epoch 18 — loss: 0.2695
Epoch 19 — loss: 0.2687
Epoch 20 — loss: 0.2685
  threshold   accuracy
0    P(>V4)  81.809998
1    P(>V5)  80.320000
2    P(>V6)  83.730003
3    P(>V7)  87.540001
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 44.95%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4517
Epoch 02 — loss: 0.3274
Epoch 03 — loss: 0.2968
Epoch 04 — loss: 0.2886
Epoch 05 — loss: 0.2823
Epoch 06 — loss: 0.2801
Epoch 07 — loss: 0.2790
Epoch 08 — loss: 0.2765
Epoch 09 — loss: 0.2756
Epoch 10 — loss: 0.2747
Epoch 11 — loss: 0.2719
Epoch 12 — loss: 0.2728
Epoch 13 — loss: 0.2719
Epoch 14 — loss: 0.2713
Epoch 15 — loss: 0.2705
Epoch 16 — loss: 0.2694
Epoch 17 — loss: 0.2702
Epoch 18 — loss: 0.2696
Epoch 19 — loss: 0.2691
Epoch 20 — loss: 0.2679
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  80.370003
2    P(>V6)  83.830002
3    P(>V7)  87.389999
4    P(>V8)  92.349998
5    P(>V9)  96.970001
Overall accuracy: 43.74%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4255
Epoch 02 — loss: 0.3292
Epoch 03 — loss: 0.3135
Epoch 04 — loss: 0.3018
Epoch 05 — loss: 0.2960
Epoch 06 — loss: 0.2894
Epoch 07 — loss: 0.2842
Epoch 08 — loss: 0.2813
Epoch 09 — loss: 0.2815
Epoch 10 — loss: 0.2786
Epoch 11 — loss: 0.2763
Epoch 12 — loss: 0.2747
Epoch 13 — loss: 0.2761
Epoch 14 — loss: 0.2766
Epoch 15 — loss: 0.2741
Epoch 16 — loss: 0.2747
Epoch 17 — loss: 0.2725
Epoch 18 — loss: 0.2736
Epoch 19 — loss: 0.2702
Epoch 20 — loss: 0.2718
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.610001
2    P(>V6)  83.779999
3    P(>V7)  87.099998
4    P(>V8)  92.730003
5    P(>V9)  96.970001
Overall accuracy: 46.20%
Ordinal stacking meta epoch 1: loss=0.2329
Ordinal stacking meta epoch 2: loss=0.1574
Ordinal stacking meta epoch 3: loss=0.1493
Ordinal stacking meta epoch 4: loss=0.1458
Ordinal stacking meta epoch 5: loss=0.1437
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  83.059998
2    P(>V6)  85.900002
3    P(>V7)  88.879997
4    P(>V8)  93.360001
5    P(>V9)  97.059998
Overall accuracy: 49.71%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  83.779999
2    P(>V6)  86.279999
3    P(>V7)  89.650002
4    P(>V8)  92.879997
5    P(>V9)  96.970001
Overall accuracy: 50.14%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  83.730003
2    P(>V6)  86.279999
3    P(>V7)  89.269997
4    P(>V8)  92.879997
5    P(>V9)  96.580002
Overall accuracy: 50.43%
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  83.970001
2    P(>V6)  85.900002
3    P(>V7)  88.980003
4    P(>V8)  92.730003
5    P(>V9)  96.870003
Overall accuracy: 50.38%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  83.059998
2    P(>V6)  85.900002
3    P(>V7)  88.879997
4    P(>V8)  93.360001
5    P(>V9)  97.059998
Overall accuracy: 49.71%
Ordinal stacking meta epoch 1: loss=0.2644
Ordinal stacking meta epoch 2: loss=0.1627
Ordinal stacking meta epoch 3: loss=0.1576
Ordinal stacking meta epoch 4: loss=0.1555
Ordinal stacking meta epoch 5: loss=0.1543
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  84.070000
2    P(>V6)  86.959999
3    P(>V7)  90.089996
4    P(>V8)  93.169998
5    P(>V9)  97.110001
Overall accuracy: 51.68%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  83.489998
2    P(>V6)  86.480003
3    P(>V7)  89.750000
4    P(>V8)  93.169998
5    P(>V9)  96.629997
Overall accuracy: 50.48%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  83.930000
2    P(>V6)  86.000000
3    P(>V7)  89.940002
4    P(>V8)  92.730003
5    P(>V9)  96.540001
Overall accuracy: 50.63%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  83.300003
2    P(>V6)  85.849998
3    P(>V7)  88.790001
4    P(>V8)  93.070000
5    P(>V9)  96.540001
Overall accuracy: 50.38%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  84.070000
2    P(>V6)  86.959999
3    P(>V7)  90.089996
4    P(>V8)  93.169998
5    P(>V9)  97.110001
Overall accuracy: 51.68%
Ordinal stacking meta epoch 1: loss=0.3104
Ordinal stacking meta epoch 2: loss=0.2679
Ordinal stacking meta epoch 3: loss=0.2672/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:24:20] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:47:28] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:48:48] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [01:49:47] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Ordinal stacking meta epoch 4: loss=0.2670
Ordinal stacking meta epoch 5: loss=0.2669
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.559998
2    P(>V6)  83.830002
3    P(>V7)  87.680000
4    P(>V8)  92.970001
5    P(>V9)  96.970001
Overall accuracy: 45.24%
  threshold   accuracy
0    P(>V4)  81.709999
1    P(>V5)  80.699997
2    P(>V6)  84.169998
3    P(>V7)  87.779999
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 46.01%
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.459999
2    P(>V6)  83.349998
3    P(>V7)  87.440002
4    P(>V8)  92.199997
5    P(>V9)  96.489998
Overall accuracy: 46.05%
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  79.639999
2    P(>V6)  83.400002
3    P(>V7)  86.959999
4    P(>V8)  92.300003
5    P(>V9)  96.489998
Overall accuracy: 46.39%
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.559998
2    P(>V6)  83.830002
3    P(>V7)  87.680000
4    P(>V8)  92.970001
5    P(>V9)  96.970001
Overall accuracy: 45.24%
----------------- Ordinal iteration 20/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3763
Epoch 02 — loss: 0.3202
Epoch 03 — loss: 0.3037
Epoch 04 — loss: 0.2925
Epoch 05 — loss: 0.2830
Epoch 06 — loss: 0.2778
Epoch 07 — loss: 0.2679
Epoch 08 — loss: 0.2580
Epoch 09 — loss: 0.2499
Epoch 10 — loss: 0.2446
Epoch 11 — loss: 0.2376
Epoch 12 — loss: 0.2298
Epoch 13 — loss: 0.2228
Epoch 14 — loss: 0.2179
Epoch 15 — loss: 0.2127
Epoch 16 — loss: 0.2062
Epoch 17 — loss: 0.2010
Epoch 18 — loss: 0.1948
Epoch 19 — loss: 0.1893
Epoch 20 — loss: 0.1832
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  82.190002
2    P(>V6)  84.220001
3    P(>V7)  87.150002
4    P(>V8)  91.339996
5    P(>V9)  95.519997
Overall accuracy: 47.79%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3711
Epoch 02 — loss: 0.3105
Epoch 03 — loss: 0.3007
Epoch 04 — loss: 0.2916
Epoch 05 — loss: 0.2820
Epoch 06 — loss: 0.2747
Epoch 07 — loss: 0.2672
Epoch 08 — loss: 0.2593
Epoch 09 — loss: 0.2539
Epoch 10 — loss: 0.2463
Epoch 11 — loss: 0.2421
Epoch 12 — loss: 0.2353
Epoch 13 — loss: 0.2290
Epoch 14 — loss: 0.2253
Epoch 15 — loss: 0.2186
Epoch 16 — loss: 0.2140
Epoch 17 — loss: 0.2076
Epoch 18 — loss: 0.2006
Epoch 19 — loss: 0.1979
Epoch 20 — loss: 0.1936
  threshold   accuracy
0    P(>V4)  82.870003
1    P(>V5)  82.529999
2    P(>V6)  85.849998
3    P(>V7)  88.349998
4    P(>V8)  93.360001
5    P(>V9)  96.919998
Overall accuracy: 48.75%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3643
Epoch 02 — loss: 0.3165
Epoch 03 — loss: 0.3022
Epoch 04 — loss: 0.2965
Epoch 05 — loss: 0.2883
Epoch 06 — loss: 0.2806
Epoch 07 — loss: 0.2741
Epoch 08 — loss: 0.2680
Epoch 09 — loss: 0.2602
Epoch 10 — loss: 0.2566
Epoch 11 — loss: 0.2501
Epoch 12 — loss: 0.2449
Epoch 13 — loss: 0.2403
Epoch 14 — loss: 0.2356
Epoch 15 — loss: 0.2286
Epoch 16 — loss: 0.2242
Epoch 17 — loss: 0.2189
Epoch 18 — loss: 0.2142
Epoch 19 — loss: 0.2090
Epoch 20 — loss: 0.2032
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  81.419998
2    P(>V6)  84.500000
3    P(>V7)  88.019997
4    P(>V8)  93.019997
5    P(>V9)  96.629997
Overall accuracy: 48.65%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4736
Epoch 02 — loss: 0.3533
Epoch 03 — loss: 0.3041
Epoch 04 — loss: 0.2918
Epoch 05 — loss: 0.2864
Epoch 06 — loss: 0.2834
Epoch 07 — loss: 0.2806
Epoch 08 — loss: 0.2785
Epoch 09 — loss: 0.2769
Epoch 10 — loss: 0.2748
Epoch 11 — loss: 0.2744
Epoch 12 — loss: 0.2738
Epoch 13 — loss: 0.2723
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2707
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2699
Epoch 18 — loss: 0.2688
Epoch 19 — loss: 0.2712
Epoch 20 — loss: 0.2704
  threshold   accuracy
0    P(>V4)  81.949997
1    P(>V5)  80.559998
2    P(>V6)  83.779999
3    P(>V7)  87.540001
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 46.15%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4606
Epoch 02 — loss: 0.3308
Epoch 03 — loss: 0.2983
Epoch 04 — loss: 0.2908
Epoch 05 — loss: 0.2861
Epoch 06 — loss: 0.2823
Epoch 07 — loss: 0.2798
Epoch 08 — loss: 0.2776
Epoch 09 — loss: 0.2758
Epoch 10 — loss: 0.2756
Epoch 11 — loss: 0.2755
Epoch 12 — loss: 0.2722
Epoch 13 — loss: 0.2741
Epoch 14 — loss: 0.2726
Epoch 15 — loss: 0.2706
Epoch 16 — loss: 0.2712
Epoch 17 — loss: 0.2700
Epoch 18 — loss: 0.2697
Epoch 19 — loss: 0.2687
Epoch 20 — loss: 0.2679
  threshold   accuracy
0    P(>V4)  81.139999
1    P(>V5)  80.510002
2    P(>V6)  84.019997
3    P(>V7)  87.680000
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 46.44%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4281
Epoch 02 — loss: 0.3281
Epoch 03 — loss: 0.3134
Epoch 04 — loss: 0.3023
Epoch 05 — loss: 0.2928
Epoch 06 — loss: 0.2884
Epoch 07 — loss: 0.2842
Epoch 08 — loss: 0.2826
Epoch 09 — loss: 0.2814
Epoch 10 — loss: 0.2792
Epoch 11 — loss: 0.2760
Epoch 12 — loss: 0.2744
Epoch 13 — loss: 0.2738
Epoch 14 — loss: 0.2730
Epoch 15 — loss: 0.2733
Epoch 16 — loss: 0.2721
Epoch 17 — loss: 0.2727
Epoch 18 — loss: 0.2718
Epoch 19 — loss: 0.2715
Epoch 20 — loss: 0.2708
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  80.900002
2    P(>V6)  83.830002
3    P(>V7)  87.580002
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 45.43%
Ordinal stacking meta epoch 1: loss=0.2230
Ordinal stacking meta epoch 2: loss=0.1582
Ordinal stacking meta epoch 3: loss=0.1517
Ordinal stacking meta epoch 4: loss=0.1488
Ordinal stacking meta epoch 5: loss=0.1471
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.720001
2    P(>V6)  85.510002
3    P(>V7)  88.639999
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 49.13%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  83.489998
2    P(>V6)  85.610001
3    P(>V7)  88.790001
4    P(>V8)  92.930000
5    P(>V9)  96.919998
Overall accuracy: 49.90%
  threshold   accuracy
0    P(>V4)  84.220001
1    P(>V5)  83.449997
2    P(>V6)  85.610001
3    P(>V7)  87.919998
4    P(>V8)  92.779999
5    P(>V9)  96.680000
Overall accuracy: 50.14%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  83.400002
2    P(>V6)  85.370003
3    P(>V7)  88.449997
4    P(>V8)  92.690002
5    P(>V9)  96.540001
Overall accuracy: 49.81%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.720001
2    P(>V6)  85.510002
3    P(>V7)  88.639999
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 49.13%
Ordinal stacking meta epoch 1: loss=0.2610
Ordinal stacking meta epoch 2: loss=0.1650
Ordinal stacking meta epoch 3: loss=0.1613
Ordinal stacking meta epoch 4: loss=0.1596
Ordinal stacking meta epoch 5: loss=0.1587
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  83.639999
2    P(>V6)  86.279999
3    P(>V7)  88.589996
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 50.10%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  83.400002
2    P(>V6)  85.849998
3    P(>V7)  88.639999
4    P(>V8)  93.309998
5    P(>V9)  96.580002
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  84.699997
1    P(>V5)  83.250000
2    P(>V6)  85.419998
3    P(>V7)  88.500000
4    P(>V8)  92.970001
5    P(>V9)  96.919998
Overall accuracy: 51.20%
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  83.059998
2    P(>V6)  84.940002
3    P(>V7)  88.500000
4    P(>V8)  92.879997
5    P(>V9)  96.389999
Overall accuracy: 49.18%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  83.639999
2    P(>V6)  86.279999
3    P(>V7)  88.589996
4    P(>V8)  93.209999
5    P(>V9)  97.019997
Overall accuracy: 50.10%
Ordinal stacking meta epoch 1: loss=0.3196
Ordinal stacking meta epoch 2: loss=0.2678
Ordinal stacking meta epoch 3: loss=0.2671
Ordinal stacking meta epoch 4: loss=0.2669
Ordinal stacking meta epoch 5: loss=0.2669
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.849998
2    P(>V6)  83.930000
3    P(>V7)  87.970001
4    P(>V8)  92.639999
5    P(>V9)  97.019997/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:12:56] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:14:15] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:15:12] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 46.15%
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.800003
2    P(>V6)  83.830002
3    P(>V7)  87.779999
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.67%
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.220001
2    P(>V6)  84.019997
3    P(>V7)  87.489998
4    P(>V8)  92.199997
5    P(>V9)  96.440002
Overall accuracy: 45.33%
  threshold   accuracy
0    P(>V4)  81.139999
1    P(>V5)  79.790001
2    P(>V6)  83.400002
3    P(>V7)  87.540001
4    P(>V8)  92.250000
5    P(>V9)  96.820000
Overall accuracy: 45.24%
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.849998
2    P(>V6)  83.930000
3    P(>V7)  87.970001
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 46.15%
----------------- Ordinal iteration 21/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3887
Epoch 02 — loss: 0.3235
Epoch 03 — loss: 0.3102
Epoch 04 — loss: 0.2952
Epoch 05 — loss: 0.2861
Epoch 06 — loss: 0.2755
Epoch 07 — loss: 0.2691
Epoch 08 — loss: 0.2560
Epoch 09 — loss: 0.2516
Epoch 10 — loss: 0.2429
Epoch 11 — loss: 0.2374
Epoch 12 — loss: 0.2306
Epoch 13 — loss: 0.2255
Epoch 14 — loss: 0.2185
Epoch 15 — loss: 0.2129
Epoch 16 — loss: 0.2090
Epoch 17 — loss: 0.2021
Epoch 18 — loss: 0.1982
Epoch 19 — loss: 0.1936
Epoch 20 — loss: 0.1868
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.870003
2    P(>V6)  84.739998
3    P(>V7)  87.339996
4    P(>V8)  92.730003
5    P(>V9)  96.629997
Overall accuracy: 48.41%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3670
Epoch 02 — loss: 0.3100
Epoch 03 — loss: 0.2971
Epoch 04 — loss: 0.2841
Epoch 05 — loss: 0.2746
Epoch 06 — loss: 0.2644
Epoch 07 — loss: 0.2593
Epoch 08 — loss: 0.2502
Epoch 09 — loss: 0.2423
Epoch 10 — loss: 0.2377
Epoch 11 — loss: 0.2317
Epoch 12 — loss: 0.2251
Epoch 13 — loss: 0.2214
Epoch 14 — loss: 0.2156
Epoch 15 — loss: 0.2112
Epoch 16 — loss: 0.2070
Epoch 17 — loss: 0.2003
Epoch 18 — loss: 0.1936
Epoch 19 — loss: 0.1915
Epoch 20 — loss: 0.1857
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  82.190002
2    P(>V6)  85.269997
3    P(>V7)  87.680000
4    P(>V8)  92.059998
5    P(>V9)  96.150002
Overall accuracy: 47.55%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3583
Epoch 02 — loss: 0.3106
Epoch 03 — loss: 0.2968
Epoch 04 — loss: 0.2921
Epoch 05 — loss: 0.2857
Epoch 06 — loss: 0.2803
Epoch 07 — loss: 0.2735
Epoch 08 — loss: 0.2699
Epoch 09 — loss: 0.2648
Epoch 10 — loss: 0.2573
Epoch 11 — loss: 0.2532
Epoch 12 — loss: 0.2480
Epoch 13 — loss: 0.2419
Epoch 14 — loss: 0.2357
Epoch 15 — loss: 0.2308
Epoch 16 — loss: 0.2278
Epoch 17 — loss: 0.2222
Epoch 18 — loss: 0.2180
Epoch 19 — loss: 0.2123
Epoch 20 — loss: 0.2087
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.000000
2    P(>V6)  84.650002
3    P(>V7)  87.250000
4    P(>V8)  91.959999
5    P(>V9)  96.680000
Overall accuracy: 46.34%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4696
Epoch 02 — loss: 0.3400
Epoch 03 — loss: 0.3044
Epoch 04 — loss: 0.2919
Epoch 05 — loss: 0.2883
Epoch 06 — loss: 0.2836
Epoch 07 — loss: 0.2807
Epoch 08 — loss: 0.2791
Epoch 09 — loss: 0.2769
Epoch 10 — loss: 0.2755
Epoch 11 — loss: 0.2757
Epoch 12 — loss: 0.2734
Epoch 13 — loss: 0.2752
Epoch 14 — loss: 0.2715
Epoch 15 — loss: 0.2711
Epoch 16 — loss: 0.2713
Epoch 17 — loss: 0.2713
Epoch 18 — loss: 0.2703
Epoch 19 — loss: 0.2694
Epoch 20 — loss: 0.2691
  threshold   accuracy
0    P(>V4)  81.279999
1    P(>V5)  80.320000
2    P(>V6)  83.970001
3    P(>V7)  87.489998
4    P(>V8)  92.400002
5    P(>V9)  97.019997
Overall accuracy: 45.86%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4620
Epoch 02 — loss: 0.3318
Epoch 03 — loss: 0.2987
Epoch 04 — loss: 0.2889
Epoch 05 — loss: 0.2832
Epoch 06 — loss: 0.2819
Epoch 07 — loss: 0.2791
Epoch 08 — loss: 0.2763
Epoch 09 — loss: 0.2767
Epoch 10 — loss: 0.2765
Epoch 11 — loss: 0.2734
Epoch 12 — loss: 0.2733
Epoch 13 — loss: 0.2727
Epoch 14 — loss: 0.2717
Epoch 15 — loss: 0.2717
Epoch 16 — loss: 0.2711
Epoch 17 — loss: 0.2712
Epoch 18 — loss: 0.2692
Epoch 19 — loss: 0.2714
Epoch 20 — loss: 0.2686
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.750000
2    P(>V6)  84.070000
3    P(>V7)  87.680000
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 45.86%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4184
Epoch 02 — loss: 0.3278
Epoch 03 — loss: 0.3121
Epoch 04 — loss: 0.2984
Epoch 05 — loss: 0.2906
Epoch 06 — loss: 0.2880
Epoch 07 — loss: 0.2834
Epoch 08 — loss: 0.2800
Epoch 09 — loss: 0.2785
Epoch 10 — loss: 0.2790
Epoch 11 — loss: 0.2768
Epoch 12 — loss: 0.2739
Epoch 13 — loss: 0.2752
Epoch 14 — loss: 0.2759
Epoch 15 — loss: 0.2731
Epoch 16 — loss: 0.2727
Epoch 17 — loss: 0.2731
Epoch 18 — loss: 0.2735
Epoch 19 — loss: 0.2731
Epoch 20 — loss: 0.2720
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.800003
2    P(>V6)  83.830002
3    P(>V7)  87.629997
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 46.15%
Ordinal stacking meta epoch 1: loss=0.2045
Ordinal stacking meta epoch 2: loss=0.1556
Ordinal stacking meta epoch 3: loss=0.1489
Ordinal stacking meta epoch 4: loss=0.1461
Ordinal stacking meta epoch 5: loss=0.1445
  threshold   accuracy
0    P(>V4)  84.699997
1    P(>V5)  82.339996
2    P(>V6)  85.760002
3    P(>V7)  88.790001
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 49.95%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  83.250000
2    P(>V6)  85.800003
3    P(>V7)  88.400002
4    P(>V8)  93.169998
5    P(>V9)  96.389999
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  83.059998
2    P(>V6)  86.139999
3    P(>V7)  88.500000
4    P(>V8)  92.830002
5    P(>V9)  96.680000
Overall accuracy: 49.95%
  threshold   accuracy
0    P(>V4)  83.879997
1    P(>V5)  82.529999
2    P(>V6)  85.900002
3    P(>V7)  87.779999
4    P(>V8)  92.589996
5    P(>V9)  96.440002
Overall accuracy: 48.94%
  threshold   accuracy
0    P(>V4)  84.699997
1    P(>V5)  82.339996
2    P(>V6)  85.760002
3    P(>V7)  88.790001
4    P(>V8)  93.019997
5    P(>V9)  97.019997
Overall accuracy: 49.95%
Ordinal stacking meta epoch 1: loss=0.2186
Ordinal stacking meta epoch 2: loss=0.1601
Ordinal stacking meta epoch 3: loss=0.1578
Ordinal stacking meta epoch 4: loss=0.1564
Ordinal stacking meta epoch 5: loss=0.1554
  threshold   accuracy
0    P(>V4)  84.650002
1    P(>V5)  83.349998
2    P(>V6)  86.720001
3    P(>V7)  88.839996
4    P(>V8)  93.070000
5    P(>V9)  96.870003
Overall accuracy: 50.67%
  threshold   accuracy
0    P(>V4)  84.459999
1    P(>V5)  83.449997
2    P(>V6)  86.620003
3    P(>V7)  88.879997
4    P(>V8)  93.070000
5    P(>V9)  96.290001
Overall accuracy: 50.53%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  82.440002
2    P(>V6)  86.570000
3    P(>V7)  88.639999
4    P(>V8)  92.730003
5    P(>V9)  96.580002
Overall accuracy: 49.90%
  threshold   accuracy
0    P(>V4)  83.779999
1    P(>V5)  82.720001
2    P(>V6)  86.809998
3    P(>V7)  88.449997
4    P(>V8)  92.349998
5    P(>V9)  96.440002
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  84.650002
1    P(>V5)  83.349998
2    P(>V6)  86.720001
3    P(>V7)  88.839996
4    P(>V8)  93.070000
5    P(>V9)  96.870003
Overall accuracy: 50.67%
Ordinal stacking meta epoch 1: loss=0.3536
Ordinal stacking meta epoch 2: loss=0.2686
Ordinal stacking meta epoch 3: loss=0.2675
Ordinal stacking meta epoch 4: loss=0.2673
Ordinal stacking meta epoch 5: loss=0.2672
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.900002
2    P(>V6)  84.120003
3    P(>V7)  87.970001
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.25%
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  80.559998
2    P(>V6)  84.309998
3    P(>V7)  87.779999
4    P(>V8)  92.639999
5    P(>V9)  97.019997/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:38:15] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:39:34] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [02:40:31] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 45.28%
  threshold   accuracy
0    P(>V4)  81.230003
1    P(>V5)  80.989998
2    P(>V6)  84.169998
3    P(>V7)  87.919998
4    P(>V8)  92.489998
5    P(>V9)  96.730003
Overall accuracy: 46.82%
  threshold   accuracy
0    P(>V4)  79.360001
1    P(>V5)  79.500000
2    P(>V6)  83.250000
3    P(>V7)  87.150002
4    P(>V8)  92.440002
5    P(>V9)  96.730003
Overall accuracy: 43.79%
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.900002
2    P(>V6)  84.120003
3    P(>V7)  87.970001
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.25%
----------------- Ordinal iteration 22/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3903
Epoch 02 — loss: 0.3250
Epoch 03 — loss: 0.3089
Epoch 04 — loss: 0.2956
Epoch 05 — loss: 0.2852
Epoch 06 — loss: 0.2764
Epoch 07 — loss: 0.2653
Epoch 08 — loss: 0.2584
Epoch 09 — loss: 0.2522
Epoch 10 — loss: 0.2439
Epoch 11 — loss: 0.2381
Epoch 12 — loss: 0.2307
Epoch 13 — loss: 0.2235
Epoch 14 — loss: 0.2188
Epoch 15 — loss: 0.2133
Epoch 16 — loss: 0.2064
Epoch 17 — loss: 0.2026
Epoch 18 — loss: 0.1944
Epoch 19 — loss: 0.1886
Epoch 20 — loss: 0.1842
  threshold   accuracy
0    P(>V4)  82.440002
1    P(>V5)  81.709999
2    P(>V6)  83.639999
3    P(>V7)  88.349998
4    P(>V8)  92.199997
5    P(>V9)  96.629997
Overall accuracy: 47.26%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3669
Epoch 02 — loss: 0.3068
Epoch 03 — loss: 0.2951
Epoch 04 — loss: 0.2861
Epoch 05 — loss: 0.2784
Epoch 06 — loss: 0.2680
Epoch 07 — loss: 0.2603
Epoch 08 — loss: 0.2549
Epoch 09 — loss: 0.2473
Epoch 10 — loss: 0.2408
Epoch 11 — loss: 0.2340
Epoch 12 — loss: 0.2281
Epoch 13 — loss: 0.2216
Epoch 14 — loss: 0.2158
Epoch 15 — loss: 0.2089
Epoch 16 — loss: 0.2051
Epoch 17 — loss: 0.1978
Epoch 18 — loss: 0.1940
Epoch 19 — loss: 0.1869
Epoch 20 — loss: 0.1832
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  82.290001
2    P(>V6)  84.309998
3    P(>V7)  87.779999
4    P(>V8)  92.160004
5    P(>V9)  96.489998
Overall accuracy: 47.83%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3574
Epoch 02 — loss: 0.3090
Epoch 03 — loss: 0.2978
Epoch 04 — loss: 0.2915
Epoch 05 — loss: 0.2863
Epoch 06 — loss: 0.2796
Epoch 07 — loss: 0.2758
Epoch 08 — loss: 0.2691
Epoch 09 — loss: 0.2656
Epoch 10 — loss: 0.2580
Epoch 11 — loss: 0.2548
Epoch 12 — loss: 0.2480
Epoch 13 — loss: 0.2442
Epoch 14 — loss: 0.2399
Epoch 15 — loss: 0.2347
Epoch 16 — loss: 0.2296
Epoch 17 — loss: 0.2262
Epoch 18 — loss: 0.2202
Epoch 19 — loss: 0.2157
Epoch 20 — loss: 0.2110
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  81.230003
2    P(>V6)  84.699997
3    P(>V7)  87.540001
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 47.21%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4755
Epoch 02 — loss: 0.3639
Epoch 03 — loss: 0.3086
Epoch 04 — loss: 0.2960
Epoch 05 — loss: 0.2883
Epoch 06 — loss: 0.2841
Epoch 07 — loss: 0.2798
Epoch 08 — loss: 0.2768
Epoch 09 — loss: 0.2753
Epoch 10 — loss: 0.2748
Epoch 11 — loss: 0.2729
Epoch 12 — loss: 0.2710
Epoch 13 — loss: 0.2715
Epoch 14 — loss: 0.2700
Epoch 15 — loss: 0.2701
Epoch 16 — loss: 0.2703
Epoch 17 — loss: 0.2690
Epoch 18 — loss: 0.2691
Epoch 19 — loss: 0.2704
Epoch 20 — loss: 0.2687
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.800003
2    P(>V6)  83.879997
3    P(>V7)  87.540001
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.19%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4688
Epoch 02 — loss: 0.3290
Epoch 03 — loss: 0.2960
Epoch 04 — loss: 0.2881
Epoch 05 — loss: 0.2806
Epoch 06 — loss: 0.2807
Epoch 07 — loss: 0.2765
Epoch 08 — loss: 0.2758
Epoch 09 — loss: 0.2744
Epoch 10 — loss: 0.2732
Epoch 11 — loss: 0.2715
Epoch 12 — loss: 0.2718
Epoch 13 — loss: 0.2712
Epoch 14 — loss: 0.2717
Epoch 15 — loss: 0.2696
Epoch 16 — loss: 0.2693
Epoch 17 — loss: 0.2683
Epoch 18 — loss: 0.2688
Epoch 19 — loss: 0.2694
Epoch 20 — loss: 0.2699
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  80.320000
2    P(>V6)  83.830002
3    P(>V7)  87.629997
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 46.01%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4132
Epoch 02 — loss: 0.3167
Epoch 03 — loss: 0.3045
Epoch 04 — loss: 0.2950
Epoch 05 — loss: 0.2924
Epoch 06 — loss: 0.2870
Epoch 07 — loss: 0.2831
Epoch 08 — loss: 0.2808
Epoch 09 — loss: 0.2801
Epoch 10 — loss: 0.2809
Epoch 11 — loss: 0.2781
Epoch 12 — loss: 0.2760
Epoch 13 — loss: 0.2760
Epoch 14 — loss: 0.2752
Epoch 15 — loss: 0.2734
Epoch 16 — loss: 0.2735
Epoch 17 — loss: 0.2721
Epoch 18 — loss: 0.2723
Epoch 19 — loss: 0.2739
Epoch 20 — loss: 0.2710
  threshold   accuracy
0    P(>V4)  80.940002
1    P(>V5)  79.879997
2    P(>V6)  83.400002
3    P(>V7)  86.860001
4    P(>V8)  92.589996
5    P(>V9)  97.019997
Overall accuracy: 44.71%
Ordinal stacking meta epoch 1: loss=0.1843
Ordinal stacking meta epoch 2: loss=0.1480
Ordinal stacking meta epoch 3: loss=0.1432
Ordinal stacking meta epoch 4: loss=0.1409
Ordinal stacking meta epoch 5: loss=0.1393
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.580002
2    P(>V6)  85.849998
3    P(>V7)  89.080002
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 49.28%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.720001
2    P(>V6)  84.989998
3    P(>V7)  88.309998
4    P(>V8)  93.360001
5    P(>V9)  96.489998
Overall accuracy: 48.03%
  threshold   accuracy
0    P(>V4)  82.099998
1    P(>V5)  82.580002
2    P(>V6)  85.419998
3    P(>V7)  88.550003
4    P(>V8)  92.540001
5    P(>V9)  96.440002
Overall accuracy: 47.40%
  threshold   accuracy
0    P(>V4)  82.529999
1    P(>V5)  83.010002
2    P(>V6)  85.370003
3    P(>V7)  87.870003
4    P(>V8)  92.589996
5    P(>V9)  96.389999
Overall accuracy: 48.08%
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.580002
2    P(>V6)  85.849998
3    P(>V7)  89.080002
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 49.28%
Ordinal stacking meta epoch 1: loss=0.3335
Ordinal stacking meta epoch 2: loss=0.1620
Ordinal stacking meta epoch 3: loss=0.1575
Ordinal stacking meta epoch 4: loss=0.1555
Ordinal stacking meta epoch 5: loss=0.1542
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  83.540001
2    P(>V6)  86.040001
3    P(>V7)  89.029999
4    P(>V8)  93.120003
5    P(>V9)  96.970001
Overall accuracy: 49.66%
  threshold   accuracy
0    P(>V4)  84.070000
1    P(>V5)  82.769997
2    P(>V6)  86.000000
3    P(>V7)  88.739998
4    P(>V8)  93.019997
5    P(>V9)  96.540001
Overall accuracy: 48.56%
  threshold   accuracy
0    P(>V4)  83.400002
1    P(>V5)  82.680000
2    P(>V6)  85.760002
3    P(>V7)  88.449997
4    P(>V8)  93.070000
5    P(>V9)  96.440002
Overall accuracy: 49.04%
  threshold   accuracy
0    P(>V4)  82.959999
1    P(>V5)  82.769997
2    P(>V6)  85.470001
3    P(>V7)  88.019997
4    P(>V8)  92.489998
5    P(>V9)  96.779999
Overall accuracy: 48.89%
  threshold   accuracy
0    P(>V4)  84.120003
1    P(>V5)  83.540001
2    P(>V6)  86.040001
3    P(>V7)  89.029999
4    P(>V8)  93.120003
5    P(>V9)  96.970001
Overall accuracy: 49.66%
Ordinal stacking meta epoch 1: loss=0.3714
Ordinal stacking meta epoch 2: loss=0.2683
Ordinal stacking meta epoch 3: loss=0.2673
Ordinal stacking meta epoch 4: loss=0.2671
Ordinal stacking meta epoch 5: loss=0.2670
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.610001
2    P(>V6)  83.970001
3    P(>V7)  87.730003
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.67%
  threshold   accuracy
0    P(>V4)  81.470001
1    P(>V5)  80.510002
2    P(>V6)  83.730003
3    P(>V7)  87.680000
4    P(>V8)  92.730003
5    P(>V9)  97.019997
Overall accuracy: 45.24%
  threshold   accuracy
0    P(>V4)  80.989998
1    P(>V5)  80.940002
2    P(>V6)  83.879997
3    P(>V7)  87.440002
4    P(>V8)  93.019997
5    P(>V9)  96.580002/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:03:07] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:04:26] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:05:23] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 45.91%
  threshold   accuracy
0    P(>V4)  80.559998
1    P(>V5)  80.559998
2    P(>V6)  83.930000
3    P(>V7)  87.540001
4    P(>V8)  92.489998
5    P(>V9)  96.290001
Overall accuracy: 46.20%
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  80.610001
2    P(>V6)  83.970001
3    P(>V7)  87.730003
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.67%
----------------- Ordinal iteration 23/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3887
Epoch 02 — loss: 0.3278
Epoch 03 — loss: 0.3099
Epoch 04 — loss: 0.2974
Epoch 05 — loss: 0.2869
Epoch 06 — loss: 0.2737
Epoch 07 — loss: 0.2641
Epoch 08 — loss: 0.2564
Epoch 09 — loss: 0.2476
Epoch 10 — loss: 0.2406
Epoch 11 — loss: 0.2353
Epoch 12 — loss: 0.2296
Epoch 13 — loss: 0.2225
Epoch 14 — loss: 0.2173
Epoch 15 — loss: 0.2130
Epoch 16 — loss: 0.2073
Epoch 17 — loss: 0.2005
Epoch 18 — loss: 0.1954
Epoch 19 — loss: 0.1894
Epoch 20 — loss: 0.1846
  threshold   accuracy
0    P(>V4)  83.199997
1    P(>V5)  81.519997
2    P(>V6)  84.309998
3    P(>V7)  87.680000
4    P(>V8)  92.059998
5    P(>V9)  96.730003
Overall accuracy: 47.21%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3665
Epoch 02 — loss: 0.3018
Epoch 03 — loss: 0.2908
Epoch 04 — loss: 0.2829
Epoch 05 — loss: 0.2750
Epoch 06 — loss: 0.2689
Epoch 07 — loss: 0.2611
Epoch 08 — loss: 0.2532
Epoch 09 — loss: 0.2469
Epoch 10 — loss: 0.2394
Epoch 11 — loss: 0.2354
Epoch 12 — loss: 0.2278
Epoch 13 — loss: 0.2213
Epoch 14 — loss: 0.2173
Epoch 15 — loss: 0.2113
Epoch 16 — loss: 0.2038
Epoch 17 — loss: 0.1989
Epoch 18 — loss: 0.1958
Epoch 19 — loss: 0.1870
Epoch 20 — loss: 0.1828
  threshold   accuracy
0    P(>V4)  82.680000
1    P(>V5)  81.620003
2    P(>V6)  85.180000
3    P(>V7)  87.339996
4    P(>V8)  92.730003
5    P(>V9)  96.680000
Overall accuracy: 48.32%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3674
Epoch 02 — loss: 0.3188
Epoch 03 — loss: 0.3064
Epoch 04 — loss: 0.2940
Epoch 05 — loss: 0.2898
Epoch 06 — loss: 0.2831
Epoch 07 — loss: 0.2754
Epoch 08 — loss: 0.2679
Epoch 09 — loss: 0.2643
Epoch 10 — loss: 0.2567
Epoch 11 — loss: 0.2550
Epoch 12 — loss: 0.2469
Epoch 13 — loss: 0.2423
Epoch 14 — loss: 0.2398
Epoch 15 — loss: 0.2333
Epoch 16 — loss: 0.2301
Epoch 17 — loss: 0.2252
Epoch 18 — loss: 0.2220
Epoch 19 — loss: 0.2158
Epoch 20 — loss: 0.2130
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  82.190002
2    P(>V6)  85.800003
3    P(>V7)  88.400002
4    P(>V8)  93.120003
5    P(>V9)  96.820000
Overall accuracy: 47.88%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4689
Epoch 02 — loss: 0.3487
Epoch 03 — loss: 0.3011
Epoch 04 — loss: 0.2900
Epoch 05 — loss: 0.2840
Epoch 06 — loss: 0.2809
Epoch 07 — loss: 0.2772
Epoch 08 — loss: 0.2755
Epoch 09 — loss: 0.2747
Epoch 10 — loss: 0.2740
Epoch 11 — loss: 0.2747
Epoch 12 — loss: 0.2727
Epoch 13 — loss: 0.2724
Epoch 14 — loss: 0.2712
Epoch 15 — loss: 0.2707
Epoch 16 — loss: 0.2710
Epoch 17 — loss: 0.2690
Epoch 18 — loss: 0.2701
Epoch 19 — loss: 0.2698
Epoch 20 — loss: 0.2700
  threshold   accuracy
0    P(>V4)  82.239998
1    P(>V5)  81.230003
2    P(>V6)  84.120003
3    P(>V7)  87.540001
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 47.35%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4630
Epoch 02 — loss: 0.3236
Epoch 03 — loss: 0.2948
Epoch 04 — loss: 0.2872
Epoch 05 — loss: 0.2834
Epoch 06 — loss: 0.2806
Epoch 07 — loss: 0.2780
Epoch 08 — loss: 0.2759
Epoch 09 — loss: 0.2754
Epoch 10 — loss: 0.2731
Epoch 11 — loss: 0.2742
Epoch 12 — loss: 0.2721
Epoch 13 — loss: 0.2731
Epoch 14 — loss: 0.2721
Epoch 15 — loss: 0.2712
Epoch 16 — loss: 0.2715
Epoch 17 — loss: 0.2702
Epoch 18 — loss: 0.2703
Epoch 19 — loss: 0.2706
Epoch 20 — loss: 0.2695
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.900002
2    P(>V6)  83.779999
3    P(>V7)  87.489998
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.72%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4292
Epoch 02 — loss: 0.3244
Epoch 03 — loss: 0.3093
Epoch 04 — loss: 0.3000
Epoch 05 — loss: 0.2946
Epoch 06 — loss: 0.2871
Epoch 07 — loss: 0.2855
Epoch 08 — loss: 0.2825
Epoch 09 — loss: 0.2794
Epoch 10 — loss: 0.2793
Epoch 11 — loss: 0.2776
Epoch 12 — loss: 0.2766
Epoch 13 — loss: 0.2761
Epoch 14 — loss: 0.2761
Epoch 15 — loss: 0.2736
Epoch 16 — loss: 0.2731
Epoch 17 — loss: 0.2720
Epoch 18 — loss: 0.2732
Epoch 19 — loss: 0.2731
Epoch 20 — loss: 0.2718
  threshold   accuracy
0    P(>V4)  80.459999
1    P(>V5)  80.029999
2    P(>V6)  83.639999
3    P(>V7)  87.339996
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 43.74%
Ordinal stacking meta epoch 1: loss=0.1881
Ordinal stacking meta epoch 2: loss=0.1514
Ordinal stacking meta epoch 3: loss=0.1475
Ordinal stacking meta epoch 4: loss=0.1458
Ordinal stacking meta epoch 5: loss=0.1447
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.050003
2    P(>V6)  85.849998
3    P(>V7)  88.400002
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  83.449997
1    P(>V5)  82.389999
2    P(>V6)  86.480003
3    P(>V7)  88.449997
4    P(>V8)  92.730003
5    P(>V9)  96.820000
Overall accuracy: 47.83%
  threshold   accuracy
0    P(>V4)  83.489998
1    P(>V5)  83.250000
2    P(>V6)  86.139999
3    P(>V7)  88.209999
4    P(>V8)  92.589996
5    P(>V9)  96.919998
Overall accuracy: 49.09%
  threshold   accuracy
0    P(>V4)  82.389999
1    P(>V5)  83.110001
2    P(>V6)  86.000000
3    P(>V7)  87.919998
4    P(>V8)  92.349998
5    P(>V9)  97.059998
Overall accuracy: 48.32%
  threshold   accuracy
0    P(>V4)  83.690002
1    P(>V5)  82.050003
2    P(>V6)  85.849998
3    P(>V7)  88.400002
4    P(>V8)  92.970001
5    P(>V9)  97.019997
Overall accuracy: 48.70%
Ordinal stacking meta epoch 1: loss=0.2135
Ordinal stacking meta epoch 2: loss=0.1611
Ordinal stacking meta epoch 3: loss=0.1581
Ordinal stacking meta epoch 4: loss=0.1566
Ordinal stacking meta epoch 5: loss=0.1556
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.680000
2    P(>V6)  86.620003
3    P(>V7)  89.080002
4    P(>V8)  92.970001
5    P(>V9)  96.970001
Overall accuracy: 49.81%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.629997
2    P(>V6)  86.809998
3    P(>V7)  88.980003
4    P(>V8)  93.070000
5    P(>V9)  96.489998
Overall accuracy: 48.70%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.480003
2    P(>V6)  86.190002
3    P(>V7)  87.680000
4    P(>V8)  92.489998
5    P(>V9)  96.779999
Overall accuracy: 49.76%
  threshold   accuracy
0    P(>V4)  83.110001
1    P(>V5)  82.099998
2    P(>V6)  85.470001
3    P(>V7)  87.919998
4    P(>V8)  92.160004
5    P(>V9)  96.680000
Overall accuracy: 48.17%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.680000
2    P(>V6)  86.620003
3    P(>V7)  89.080002
4    P(>V8)  92.970001
5    P(>V9)  96.970001
Overall accuracy: 49.81%
Ordinal stacking meta epoch 1: loss=0.3281
Ordinal stacking meta epoch 2: loss=0.2671
Ordinal stacking meta epoch 3: loss=0.2670
Ordinal stacking meta epoch 4: loss=0.2670
Ordinal stacking meta epoch 5: loss=0.2670
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.989998
2    P(>V6)  84.070000
3    P(>V7)  87.629997
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 45.77%
  threshold   accuracy
0    P(>V4)  81.760002
1    P(>V5)  81.139999
2    P(>V6)  83.879997
3    P(>V7)  87.540001
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 46.25%
  threshold   accuracy
0    P(>V4)  82.150002
1    P(>V5)  80.900002
2    P(>V6)  83.400002
3    P(>V7)  87.489998
4    P(>V8)  92.879997
5    P(>V9)  96.870003
Overall accuracy: 47.31%
  threshold   accuracy
0    P(>V4)  81.040001
1    P(>V5)  79.500000
2    P(>V6)  83.059998
3    P(>V7)  86.570000
4    P(>V8)  92.300003
5    P(>V9)  97.059998/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:28:30] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:29:49] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:30:47] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 45.38%
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.989998
2    P(>V6)  84.070000
3    P(>V7)  87.629997
4    P(>V8)  92.930000
5    P(>V9)  97.019997
Overall accuracy: 45.77%
----------------- Ordinal iteration 24/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3807
Epoch 02 — loss: 0.3252
Epoch 03 — loss: 0.3094
Epoch 04 — loss: 0.2962
Epoch 05 — loss: 0.2853
Epoch 06 — loss: 0.2766
Epoch 07 — loss: 0.2652
Epoch 08 — loss: 0.2574
Epoch 09 — loss: 0.2489
Epoch 10 — loss: 0.2440
Epoch 11 — loss: 0.2381
Epoch 12 — loss: 0.2307
Epoch 13 — loss: 0.2253
Epoch 14 — loss: 0.2204
Epoch 15 — loss: 0.2116
Epoch 16 — loss: 0.2083
Epoch 17 — loss: 0.2019
Epoch 18 — loss: 0.1976
Epoch 19 — loss: 0.1922
Epoch 20 — loss: 0.1866
  threshold   accuracy
0    P(>V4)  81.949997
1    P(>V5)  80.940002
2    P(>V6)  84.459999
3    P(>V7)  88.019997
4    P(>V8)  93.790001
5    P(>V9)  96.870003
Overall accuracy: 47.31%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3774
Epoch 02 — loss: 0.3141
Epoch 03 — loss: 0.2986
Epoch 04 — loss: 0.2900
Epoch 05 — loss: 0.2804
Epoch 06 — loss: 0.2704
Epoch 07 — loss: 0.2655
Epoch 08 — loss: 0.2550
Epoch 09 — loss: 0.2476
Epoch 10 — loss: 0.2403
Epoch 11 — loss: 0.2354
Epoch 12 — loss: 0.2307
Epoch 13 — loss: 0.2220
Epoch 14 — loss: 0.2184
Epoch 15 — loss: 0.2138
Epoch 16 — loss: 0.2068
Epoch 17 — loss: 0.1993
Epoch 18 — loss: 0.1965
Epoch 19 — loss: 0.1908
Epoch 20 — loss: 0.1855
  threshold   accuracy
0    P(>V4)  83.010002
1    P(>V5)  81.330002
2    P(>V6)  84.550003
3    P(>V7)  86.959999
4    P(>V8)  92.540001
5    P(>V9)  96.730003
Overall accuracy: 46.63%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3678
Epoch 02 — loss: 0.3104
Epoch 03 — loss: 0.2949
Epoch 04 — loss: 0.2912
Epoch 05 — loss: 0.2846
Epoch 06 — loss: 0.2789
Epoch 07 — loss: 0.2709
Epoch 08 — loss: 0.2670
Epoch 09 — loss: 0.2585
Epoch 10 — loss: 0.2552
Epoch 11 — loss: 0.2492
Epoch 12 — loss: 0.2461
Epoch 13 — loss: 0.2416
Epoch 14 — loss: 0.2387
Epoch 15 — loss: 0.2335
Epoch 16 — loss: 0.2279
Epoch 17 — loss: 0.2234
Epoch 18 — loss: 0.2191
Epoch 19 — loss: 0.2143
Epoch 20 — loss: 0.2103
  threshold   accuracy
0    P(>V4)  83.730003
1    P(>V5)  82.580002
2    P(>V6)  85.230003
3    P(>V7)  87.919998
4    P(>V8)  92.440002
5    P(>V9)  97.019997
Overall accuracy: 49.13%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4737
Epoch 02 — loss: 0.3637
Epoch 03 — loss: 0.3105
Epoch 04 — loss: 0.2953
Epoch 05 — loss: 0.2880
Epoch 06 — loss: 0.2827
Epoch 07 — loss: 0.2811
Epoch 08 — loss: 0.2779
Epoch 09 — loss: 0.2760
Epoch 10 — loss: 0.2760
Epoch 11 — loss: 0.2758
Epoch 12 — loss: 0.2738
Epoch 13 — loss: 0.2723
Epoch 14 — loss: 0.2714
Epoch 15 — loss: 0.2718
Epoch 16 — loss: 0.2708
Epoch 17 — loss: 0.2722
Epoch 18 — loss: 0.2699
Epoch 19 — loss: 0.2712
Epoch 20 — loss: 0.2691
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.849998
2    P(>V6)  83.730003
3    P(>V7)  87.870003
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 44.66%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4626
Epoch 02 — loss: 0.3328
Epoch 03 — loss: 0.2986
Epoch 04 — loss: 0.2890
Epoch 05 — loss: 0.2841
Epoch 06 — loss: 0.2799
Epoch 07 — loss: 0.2775
Epoch 08 — loss: 0.2770
Epoch 09 — loss: 0.2756
Epoch 10 — loss: 0.2742
Epoch 11 — loss: 0.2723
Epoch 12 — loss: 0.2710
Epoch 13 — loss: 0.2708
Epoch 14 — loss: 0.2708
Epoch 15 — loss: 0.2702
Epoch 16 — loss: 0.2692
Epoch 17 — loss: 0.2705
Epoch 18 — loss: 0.2691
Epoch 19 — loss: 0.2705
Epoch 20 — loss: 0.2679
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  81.040001
2    P(>V6)  83.730003
3    P(>V7)  87.730003
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.86%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4208
Epoch 02 — loss: 0.3228
Epoch 03 — loss: 0.3082
Epoch 04 — loss: 0.3011
Epoch 05 — loss: 0.2936
Epoch 06 — loss: 0.2876
Epoch 07 — loss: 0.2845
Epoch 08 — loss: 0.2822
Epoch 09 — loss: 0.2796
Epoch 10 — loss: 0.2782
Epoch 11 — loss: 0.2768
Epoch 12 — loss: 0.2757
Epoch 13 — loss: 0.2745
Epoch 14 — loss: 0.2762
Epoch 15 — loss: 0.2733
Epoch 16 — loss: 0.2729
Epoch 17 — loss: 0.2738
Epoch 18 — loss: 0.2734
Epoch 19 — loss: 0.2726
Epoch 20 — loss: 0.2705
  threshold   accuracy
0    P(>V4)  81.180000
1    P(>V5)  80.750000
2    P(>V6)  83.449997
3    P(>V7)  87.389999
4    P(>V8)  92.779999
5    P(>V9)  97.019997
Overall accuracy: 45.77%
Ordinal stacking meta epoch 1: loss=0.1883
Ordinal stacking meta epoch 2: loss=0.1517
Ordinal stacking meta epoch 3: loss=0.1475
Ordinal stacking meta epoch 4: loss=0.1453
Ordinal stacking meta epoch 5: loss=0.1436
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.190002
2    P(>V6)  85.419998
3    P(>V7)  88.589996
4    P(>V8)  93.260002
5    P(>V9)  97.019997
Overall accuracy: 48.99%
  threshold   accuracy
0    P(>V4)  84.309998
1    P(>V5)  82.680000
2    P(>V6)  84.739998
3    P(>V7)  88.690002
4    P(>V8)  93.260002
5    P(>V9)  96.870003
Overall accuracy: 48.89%
  threshold   accuracy
0    P(>V4)  83.250000
1    P(>V5)  81.760002
2    P(>V6)  84.940002
3    P(>V7)  88.309998
4    P(>V8)  92.830002
5    P(>V9)  96.680000
Overall accuracy: 48.17%
  threshold   accuracy
0    P(>V4)  82.580002
1    P(>V5)  82.529999
2    P(>V6)  84.459999
3    P(>V7)  88.449997
4    P(>V8)  92.349998
5    P(>V9)  96.730003
Overall accuracy: 47.35%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.190002
2    P(>V6)  85.419998
3    P(>V7)  88.589996
4    P(>V8)  93.260002
5    P(>V9)  97.019997
Overall accuracy: 48.99%
Ordinal stacking meta epoch 1: loss=0.2499
Ordinal stacking meta epoch 2: loss=0.1602
Ordinal stacking meta epoch 3: loss=0.1571
Ordinal stacking meta epoch 4: loss=0.1558
Ordinal stacking meta epoch 5: loss=0.1549
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  83.489998
2    P(>V6)  86.000000
3    P(>V7)  89.120003
4    P(>V8)  93.410004
5    P(>V9)  97.059998
Overall accuracy: 51.25%
  threshold   accuracy
0    P(>V4)  84.169998
1    P(>V5)  82.769997
2    P(>V6)  85.800003
3    P(>V7)  88.500000
4    P(>V8)  93.169998
5    P(>V9)  96.779999
Overall accuracy: 49.47%
  threshold   accuracy
0    P(>V4)  83.970001
1    P(>V5)  82.820000
2    P(>V6)  85.029999
3    P(>V7)  88.449997
4    P(>V8)  93.309998
5    P(>V9)  96.629997
Overall accuracy: 49.23%
  threshold   accuracy
0    P(>V4)  82.959999
1    P(>V5)  82.339996
2    P(>V6)  84.790001
3    P(>V7)  88.160004
4    P(>V8)  92.970001
5    P(>V9)  96.580002
Overall accuracy: 48.17%
  threshold   accuracy
0    P(>V4)  84.550003
1    P(>V5)  83.489998
2    P(>V6)  86.000000
3    P(>V7)  89.120003
4    P(>V8)  93.410004
5    P(>V9)  97.059998
Overall accuracy: 51.25%
Ordinal stacking meta epoch 1: loss=0.4883
Ordinal stacking meta epoch 2: loss=0.2688
Ordinal stacking meta epoch 3: loss=0.2677
Ordinal stacking meta epoch 4: loss=0.2674
Ordinal stacking meta epoch 5: loss=0.2673
  threshold   accuracy
0    P(>V4)  81.860001
1    P(>V5)  80.750000
2    P(>V6)  83.779999
3    P(>V7)  87.540001
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 45.81%
  threshold   accuracy
0    P(>V4)  81.419998
1    P(>V5)  80.559998
2    P(>V6)  83.730003
3    P(>V7)  87.970001
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.14%
  threshold   accuracy
0    P(>V4)  81.279999
1    P(>V5)  80.699997
2    P(>V6)  83.250000
3    P(>V7)  87.150002
4    P(>V8)  92.690002
5    P(>V9)  96.389999
Overall accuracy: 45.57%
  threshold   accuracy
0    P(>V4)  80.989998
1    P(>V5)  79.209999
2    P(>V6)  82.820000
3    P(>V7)  86.910004
4    P(>V8)  92.349998
5    P(>V9)  96.440002
Overall accuracy: 44.61%
  threshold   accuracy
0    P(>V4)  81.860001
1    P(>V5)  80.750000
2    P(>V6)  83.779999
3    P(>V7)  87.540001
4    P(>V8)  92.879997
5    P(>V9)  97.019997/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:53:47] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:55:05] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
/home/patr/anaconda3/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [03:56:02] WARNING: /workspace/src/learner.cc:790: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)

Overall accuracy: 45.81%
----------------- Ordinal iteration 25/25 -----------------
=== Training ordinal model: set_transformer_ordinal ===
Epoch 01 — loss: 0.3880
Epoch 02 — loss: 0.3181
Epoch 03 — loss: 0.3021
Epoch 04 — loss: 0.2891
Epoch 05 — loss: 0.2783
Epoch 06 — loss: 0.2697
Epoch 07 — loss: 0.2610
Epoch 08 — loss: 0.2533
Epoch 09 — loss: 0.2463
Epoch 10 — loss: 0.2383
Epoch 11 — loss: 0.2335
Epoch 12 — loss: 0.2256
Epoch 13 — loss: 0.2203
Epoch 14 — loss: 0.2152
Epoch 15 — loss: 0.2101
Epoch 16 — loss: 0.2036
Epoch 17 — loss: 0.1965
Epoch 18 — loss: 0.1913
Epoch 19 — loss: 0.1875
Epoch 20 — loss: 0.1799
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  80.559998
2    P(>V6)  84.650002
3    P(>V7)  87.779999
4    P(>V8)  92.879997
5    P(>V9)  96.680000
Overall accuracy: 47.74%
=== Training ordinal model: set_transformer_ordinal_xy ===
Epoch 01 — loss: 0.3695
Epoch 02 — loss: 0.3108
Epoch 03 — loss: 0.2953
Epoch 04 — loss: 0.2857
Epoch 05 — loss: 0.2775
Epoch 06 — loss: 0.2684
Epoch 07 — loss: 0.2609
Epoch 08 — loss: 0.2513
Epoch 09 — loss: 0.2463
Epoch 10 — loss: 0.2404
Epoch 11 — loss: 0.2328
Epoch 12 — loss: 0.2291
Epoch 13 — loss: 0.2244
Epoch 14 — loss: 0.2186
Epoch 15 — loss: 0.2138
Epoch 16 — loss: 0.2076
Epoch 17 — loss: 0.2038
Epoch 18 — loss: 0.1981
Epoch 19 — loss: 0.1940
Epoch 20 — loss: 0.1874
  threshold   accuracy
0    P(>V4)  81.519997
1    P(>V5)  81.860001
2    P(>V6)  84.650002
3    P(>V7)  87.389999
4    P(>V8)  93.360001
5    P(>V9)  97.059998
Overall accuracy: 48.22%
=== Training ordinal model: set_transformer_ordinal_xy_additive ===
Epoch 01 — loss: 0.3527
Epoch 02 — loss: 0.3119
Epoch 03 — loss: 0.3030
Epoch 04 — loss: 0.2937
Epoch 05 — loss: 0.2874
Epoch 06 — loss: 0.2819
Epoch 07 — loss: 0.2759
Epoch 08 — loss: 0.2698
Epoch 09 — loss: 0.2665
Epoch 10 — loss: 0.2606
Epoch 11 — loss: 0.2546
Epoch 12 — loss: 0.2518
Epoch 13 — loss: 0.2447
Epoch 14 — loss: 0.2400
Epoch 15 — loss: 0.2341
Epoch 16 — loss: 0.2317
Epoch 17 — loss: 0.2267
Epoch 18 — loss: 0.2234
Epoch 19 — loss: 0.2171
Epoch 20 — loss: 0.2131
  threshold   accuracy
0    P(>V4)  83.349998
1    P(>V5)  80.410004
2    P(>V6)  83.250000
3    P(>V7)  86.379997
4    P(>V8)  91.820000
5    P(>V9)  96.870003
Overall accuracy: 45.19%
=== Training ordinal model: deepset_ordinal ===
Epoch 01 — loss: 0.4643
Epoch 02 — loss: 0.3316
Epoch 03 — loss: 0.2989
Epoch 04 — loss: 0.2884
Epoch 05 — loss: 0.2825
Epoch 06 — loss: 0.2793
Epoch 07 — loss: 0.2779
Epoch 08 — loss: 0.2755
Epoch 09 — loss: 0.2734
Epoch 10 — loss: 0.2738
Epoch 11 — loss: 0.2731
Epoch 12 — loss: 0.2721
Epoch 13 — loss: 0.2717
Epoch 14 — loss: 0.2710
Epoch 15 — loss: 0.2708
Epoch 16 — loss: 0.2709
Epoch 17 — loss: 0.2716
Epoch 18 — loss: 0.2689
Epoch 19 — loss: 0.2695
Epoch 20 — loss: 0.2688
  threshold   accuracy
0    P(>V4)  81.570000
1    P(>V5)  80.370003
2    P(>V6)  83.300003
3    P(>V7)  87.440002
4    P(>V8)  92.879997
5    P(>V9)  97.059998
Overall accuracy: 46.54%
=== Training ordinal model: deepset_ordinal_xy ===
Epoch 01 — loss: 0.4677
Epoch 02 — loss: 0.3334
Epoch 03 — loss: 0.2977
Epoch 04 — loss: 0.2911
Epoch 05 — loss: 0.2871
Epoch 06 — loss: 0.2836
Epoch 07 — loss: 0.2789
Epoch 08 — loss: 0.2760
Epoch 09 — loss: 0.2763
Epoch 10 — loss: 0.2760
Epoch 11 — loss: 0.2756
Epoch 12 — loss: 0.2722
Epoch 13 — loss: 0.2735
Epoch 14 — loss: 0.2723
Epoch 15 — loss: 0.2718
Epoch 16 — loss: 0.2701
Epoch 17 — loss: 0.2704
Epoch 18 — loss: 0.2716
Epoch 19 — loss: 0.2705
Epoch 20 — loss: 0.2703
  threshold   accuracy
0    P(>V4)  81.620003
1    P(>V5)  80.940002
2    P(>V6)  83.730003
3    P(>V7)  88.019997
4    P(>V8)  92.639999
5    P(>V9)  97.019997
Overall accuracy: 45.67%
=== Training ordinal model: deepset_ordinal_xy_additive ===
Epoch 01 — loss: 0.4282
Epoch 02 — loss: 0.3287
Epoch 03 — loss: 0.3085
Epoch 04 — loss: 0.2973
Epoch 05 — loss: 0.2918
Epoch 06 — loss: 0.2887
Epoch 07 — loss: 0.2840
Epoch 08 — loss: 0.2817
Epoch 09 — loss: 0.2793
Epoch 10 — loss: 0.2776
Epoch 11 — loss: 0.2757
Epoch 12 — loss: 0.2767
Epoch 13 — loss: 0.2736
Epoch 14 — loss: 0.2737
Epoch 15 — loss: 0.2732
Epoch 16 — loss: 0.2730
Epoch 17 — loss: 0.2715
Epoch 18 — loss: 0.2705
Epoch 19 — loss: 0.2711
Epoch 20 — loss: 0.2715
  threshold   accuracy
0    P(>V4)  81.910004
1    P(>V5)  80.650002
2    P(>V6)  84.599998
3    P(>V7)  87.680000
4    P(>V8)  92.879997
5    P(>V9)  97.019997
Overall accuracy: 46.82%
Ordinal stacking meta epoch 1: loss=0.2047
Ordinal stacking meta epoch 2: loss=0.1550
Ordinal stacking meta epoch 3: loss=0.1490
Ordinal stacking meta epoch 4: loss=0.1467
Ordinal stacking meta epoch 5: loss=0.1454
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.290001
2    P(>V6)  85.510002
3    P(>V7)  88.879997
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 49.57%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.389999
2    P(>V6)  85.470001
3    P(>V7)  88.739998
4    P(>V8)  93.410004
5    P(>V9)  96.290001
Overall accuracy: 47.69%
  threshold   accuracy
0    P(>V4)  83.540001
1    P(>V5)  82.529999
2    P(>V6)  85.709999
3    P(>V7)  88.400002
4    P(>V8)  92.879997
5    P(>V9)  96.250000
Overall accuracy: 48.08%
  threshold   accuracy
0    P(>V4)  83.300003
1    P(>V5)  82.680000
2    P(>V6)  85.029999
3    P(>V7)  88.110001
4    P(>V8)  93.309998
5    P(>V9)  96.540001
Overall accuracy: 48.65%
  threshold   accuracy
0    P(>V4)  84.260002
1    P(>V5)  82.290001
2    P(>V6)  85.510002
3    P(>V7)  88.879997
4    P(>V8)  93.070000
5    P(>V9)  97.019997
Overall accuracy: 49.57%
Ordinal stacking meta epoch 1: loss=0.3210
Ordinal stacking meta epoch 2: loss=0.1648
Ordinal stacking meta epoch 3: loss=0.1602
Ordinal stacking meta epoch 4: loss=0.1581
Ordinal stacking meta epoch 5: loss=0.1569
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.959999
2    P(>V6)  86.330002
3    P(>V7)  89.220001
4    P(>V8)  93.550003
5    P(>V9)  97.110001
Overall accuracy: 50.29%
  threshold   accuracy
0    P(>V4)  83.639999
1    P(>V5)  82.529999
2    P(>V6)  85.760002
3    P(>V7)  89.510002
4    P(>V8)  93.309998
5    P(>V9)  96.440002
Overall accuracy: 48.22%
  threshold   accuracy
0    P(>V4)  84.360001
1    P(>V5)  82.919998
2    P(>V6)  85.230003
3    P(>V7)  88.639999
4    P(>V8)  92.879997
5    P(>V9)  96.050003
Overall accuracy: 49.52%
  threshold   accuracy
0    P(>V4)  83.589996
1    P(>V5)  82.290001
2    P(>V6)  85.180000
3    P(>V7)  88.449997
4    P(>V8)  93.120003
5    P(>V9)  96.389999
Overall accuracy: 48.41%
  threshold   accuracy
0    P(>V4)  83.930000
1    P(>V5)  82.959999
2    P(>V6)  86.330002
3    P(>V7)  89.220001
4    P(>V8)  93.550003
5    P(>V9)  97.110001
Overall accuracy: 50.29%
Ordinal stacking meta epoch 1: loss=0.3450
Ordinal stacking meta epoch 2: loss=0.2677
Ordinal stacking meta epoch 3: loss=0.2671
Ordinal stacking meta epoch 4: loss=0.2671
Ordinal stacking meta epoch 5: loss=0.2671
  threshold   accuracy
0    P(>V4)  82.050003
1    P(>V5)  80.940002
2    P(>V6)  84.120003
3    P(>V7)  87.919998
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 46.87%
  threshold   accuracy
0    P(>V4)  81.669998
1    P(>V5)  80.559998
2    P(>V6)  84.260002
3    P(>V7)  87.870003
4    P(>V8)  92.690002
5    P(>V9)  97.019997
Overall accuracy: 45.81%
  threshold   accuracy
0    P(>V4)  81.809998
1    P(>V5)  80.900002
2    P(>V6)  84.120003
3    P(>V7)  87.300003
4    P(>V8)  92.010002
5    P(>V9)  96.339996
Overall accuracy: 46.15%
  threshold   accuracy
0    P(>V4)  80.699997
1    P(>V5)  79.400002
2    P(>V6)  82.919998
3    P(>V7)  86.860001
4    P(>V8)  92.400002
5    P(>V9)  96.440002
Overall accuracy: 45.24%
  threshold   accuracy
0    P(>V4)  82.050003
1    P(>V5)  80.940002
2    P(>V6)  84.120003
3    P(>V7)  87.919998
4    P(>V8)  92.830002
5    P(>V9)  97.019997
Overall accuracy: 46.87%
Saved aggregated ordinal results to ./result/ordinal_result.xlsx
                                           model  ...  overall_accuracy_std
0                                deepset_ordinal  ...              0.607764
1                             deepset_ordinal_xy  ...              0.866886
2                    deepset_ordinal_xy_additive  ...              1.012876
3                  ordinal_adaboost_ensemble_all  ...              0.533799
4              ordinal_adaboost_ensemble_deepset  ...              0.600307
5      ordinal_adaboost_ensemble_set_transformer  ...              0.635396
6                       ordinal_gbm_ensemble_all  ...              0.845828
7                   ordinal_gbm_ensemble_deepset  ...              0.638747
8           ordinal_gbm_ensemble_set_transformer  ...              0.756121
9               ordinal_soft_voting_ensemble_all  ...              0.533799
10          ordinal_soft_voting_ensemble_deepset  ...              0.600307
11  ordinal_soft_voting_ensemble_set_transformer  ...              0.635396
12                 ordinal_stacking_ensemble_all  ...              0.872730
13             ordinal_stacking_ensemble_deepset  ...              0.460854
14     ordinal_stacking_ensemble_set_transformer  ...              0.829796
15                  ordinal_xgboost_ensemble_all  ...              0.857813
16              ordinal_xgboost_ensemble_deepset  ...              0.660124
17      ordinal_xgboost_ensemble_set_transformer  ...              0.671820
18                       set_transformer_ordinal  ...              1.313508
19                    set_transformer_ordinal_xy  ...              0.946213
20           set_transformer_ordinal_xy_additive  ...              1.367918

[21 rows x 3 columns]
