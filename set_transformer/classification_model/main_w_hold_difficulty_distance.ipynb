{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe7aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import torch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7d8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "from modules_modified import ISAB, SAB, PMA\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e8bb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A1': 0, 'B1': 1, 'C1': 2, 'D1': 3, 'E1': 4, 'F1': 5, 'G1': 6, 'H1': 7, 'I1': 8, 'J1': 9, 'K1': 10, 'A2': 11, 'B2': 12, 'C2': 13, 'D2': 14, 'E2': 15, 'F2': 16, 'G2': 17, 'H2': 18, 'I2': 19, 'J2': 20, 'K2': 21, 'A3': 22, 'B3': 23, 'C3': 24, 'D3': 25, 'E3': 26, 'F3': 27, 'G3': 28, 'H3': 29, 'I3': 30, 'J3': 31, 'K3': 32, 'A4': 33, 'B4': 34, 'C4': 35, 'D4': 36, 'E4': 37, 'F4': 38, 'G4': 39, 'H4': 40, 'I4': 41, 'J4': 42, 'K4': 43, 'A5': 44, 'B5': 45, 'C5': 46, 'D5': 47, 'E5': 48, 'F5': 49, 'G5': 50, 'H5': 51, 'I5': 52, 'J5': 53, 'K5': 54, 'A6': 55, 'B6': 56, 'C6': 57, 'D6': 58, 'E6': 59, 'F6': 60, 'G6': 61, 'H6': 62, 'I6': 63, 'J6': 64, 'K6': 65, 'A7': 66, 'B7': 67, 'C7': 68, 'D7': 69, 'E7': 70, 'F7': 71, 'G7': 72, 'H7': 73, 'I7': 74, 'J7': 75, 'K7': 76, 'A8': 77, 'B8': 78, 'C8': 79, 'D8': 80, 'E8': 81, 'F8': 82, 'G8': 83, 'H8': 84, 'I8': 85, 'J8': 86, 'K8': 87, 'A9': 88, 'B9': 89, 'C9': 90, 'D9': 91, 'E9': 92, 'F9': 93, 'G9': 94, 'H9': 95, 'I9': 96, 'J9': 97, 'K9': 98, 'A10': 99, 'B10': 100, 'C10': 101, 'D10': 102, 'E10': 103, 'F10': 104, 'G10': 105, 'H10': 106, 'I10': 107, 'J10': 108, 'K10': 109, 'A11': 110, 'B11': 111, 'C11': 112, 'D11': 113, 'E11': 114, 'F11': 115, 'G11': 116, 'H11': 117, 'I11': 118, 'J11': 119, 'K11': 120, 'A12': 121, 'B12': 122, 'C12': 123, 'D12': 124, 'E12': 125, 'F12': 126, 'G12': 127, 'H12': 128, 'I12': 129, 'J12': 130, 'K12': 131, 'A13': 132, 'B13': 133, 'C13': 134, 'D13': 135, 'E13': 136, 'F13': 137, 'G13': 138, 'H13': 139, 'I13': 140, 'J13': 141, 'K13': 142, 'A14': 143, 'B14': 144, 'C14': 145, 'D14': 146, 'E14': 147, 'F14': 148, 'G14': 149, 'H14': 150, 'I14': 151, 'J14': 152, 'K14': 153, 'A15': 154, 'B15': 155, 'C15': 156, 'D15': 157, 'E15': 158, 'F15': 159, 'G15': 160, 'H15': 161, 'I15': 162, 'J15': 163, 'K15': 164, 'A16': 165, 'B16': 166, 'C16': 167, 'D16': 168, 'E16': 169, 'F16': 170, 'G16': 171, 'H16': 172, 'I16': 173, 'J16': 174, 'K16': 175, 'A17': 176, 'B17': 177, 'C17': 178, 'D17': 179, 'E17': 180, 'F17': 181, 'G17': 182, 'H17': 183, 'I17': 184, 'J17': 185, 'K17': 186, 'A18': 187, 'B18': 188, 'C18': 189, 'D18': 190, 'E18': 191, 'F18': 192, 'G18': 193, 'H18': 194, 'I18': 195, 'J18': 196, 'K18': 197}\n"
     ]
    }
   ],
   "source": [
    "# Mappings --------------------------------------------------------\n",
    "# Map each hold like \"A1\"…\"K18\" to an integer 0…(11*18−1)=197\n",
    "cols = [chr(c) for c in range(ord('A'), ord('K')+1)]\n",
    "rows = list(range(1, 19))\n",
    "hold_to_idx = {f\"{c}{r}\": i for i, (c, r) in enumerate((c, r) for r in rows for c in cols)}\n",
    "\n",
    "\n",
    "# Map grades \"V4\"…\"V11\" \n",
    "grade_to_label = {f\"V{i}\": i - 4 for i in range(4, 12)}  \n",
    "label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "print(hold_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a6dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully parsed hold difficulty file\n",
      "successfully prepare type vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Holds difficulty data --------------------------------------------------------\n",
    "hold_difficulty = {}\n",
    "with open(\"hold_difficulty.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if \":\" not in line:\n",
    "            continue  # skip malformed line\n",
    "        hold, rest = line.strip().split(\":\", 1)\n",
    "        parts = rest.strip().split(\",\")\n",
    "        difficulty = int(parts[0].strip())\n",
    "        types = [t.strip() for t in parts[1:]]\n",
    "        hold_difficulty[hold.strip()] = (difficulty, types)\n",
    "    print(\"successfully parsed hold difficulty file\")\n",
    "\n",
    "# prepare type vocabulary\n",
    "unique_types = set()\n",
    "for _, (_, types) in hold_difficulty.items():\n",
    "    unique_types.update(types)\n",
    "\n",
    "type_to_idx = {t: i for i, t in enumerate(sorted(unique_types))}\n",
    "print(f\"successfully prepare type vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5b93f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created (x,y) position to each hold:\n",
      "{'A1': (0, 0), 'A2': (0, 1), 'A3': (0, 2), 'A4': (0, 3), 'A5': (0, 4), 'A6': (0, 5), 'A7': (0, 6), 'A8': (0, 7), 'A9': (0, 8), 'A10': (0, 9), 'A11': (0, 10), 'A12': (0, 11), 'A13': (0, 12), 'A14': (0, 13), 'A15': (0, 14), 'A16': (0, 15), 'A17': (0, 16), 'A18': (0, 17), 'B1': (1, 0), 'B2': (1, 1), 'B3': (1, 2), 'B4': (1, 3), 'B5': (1, 4), 'B6': (1, 5), 'B7': (1, 6), 'B8': (1, 7), 'B9': (1, 8), 'B10': (1, 9), 'B11': (1, 10), 'B12': (1, 11), 'B13': (1, 12), 'B14': (1, 13), 'B15': (1, 14), 'B16': (1, 15), 'B17': (1, 16), 'B18': (1, 17), 'C1': (2, 0), 'C2': (2, 1), 'C3': (2, 2), 'C4': (2, 3), 'C5': (2, 4), 'C6': (2, 5), 'C7': (2, 6), 'C8': (2, 7), 'C9': (2, 8), 'C10': (2, 9), 'C11': (2, 10), 'C12': (2, 11), 'C13': (2, 12), 'C14': (2, 13), 'C15': (2, 14), 'C16': (2, 15), 'C17': (2, 16), 'C18': (2, 17), 'D1': (3, 0), 'D2': (3, 1), 'D3': (3, 2), 'D4': (3, 3), 'D5': (3, 4), 'D6': (3, 5), 'D7': (3, 6), 'D8': (3, 7), 'D9': (3, 8), 'D10': (3, 9), 'D11': (3, 10), 'D12': (3, 11), 'D13': (3, 12), 'D14': (3, 13), 'D15': (3, 14), 'D16': (3, 15), 'D17': (3, 16), 'D18': (3, 17), 'E1': (4, 0), 'E2': (4, 1), 'E3': (4, 2), 'E4': (4, 3), 'E5': (4, 4), 'E6': (4, 5), 'E7': (4, 6), 'E8': (4, 7), 'E9': (4, 8), 'E10': (4, 9), 'E11': (4, 10), 'E12': (4, 11), 'E13': (4, 12), 'E14': (4, 13), 'E15': (4, 14), 'E16': (4, 15), 'E17': (4, 16), 'E18': (4, 17), 'F1': (5, 0), 'F2': (5, 1), 'F3': (5, 2), 'F4': (5, 3), 'F5': (5, 4), 'F6': (5, 5), 'F7': (5, 6), 'F8': (5, 7), 'F9': (5, 8), 'F10': (5, 9), 'F11': (5, 10), 'F12': (5, 11), 'F13': (5, 12), 'F14': (5, 13), 'F15': (5, 14), 'F16': (5, 15), 'F17': (5, 16), 'F18': (5, 17), 'G1': (6, 0), 'G2': (6, 1), 'G3': (6, 2), 'G4': (6, 3), 'G5': (6, 4), 'G6': (6, 5), 'G7': (6, 6), 'G8': (6, 7), 'G9': (6, 8), 'G10': (6, 9), 'G11': (6, 10), 'G12': (6, 11), 'G13': (6, 12), 'G14': (6, 13), 'G15': (6, 14), 'G16': (6, 15), 'G17': (6, 16), 'G18': (6, 17), 'H1': (7, 0), 'H2': (7, 1), 'H3': (7, 2), 'H4': (7, 3), 'H5': (7, 4), 'H6': (7, 5), 'H7': (7, 6), 'H8': (7, 7), 'H9': (7, 8), 'H10': (7, 9), 'H11': (7, 10), 'H12': (7, 11), 'H13': (7, 12), 'H14': (7, 13), 'H15': (7, 14), 'H16': (7, 15), 'H17': (7, 16), 'H18': (7, 17), 'I1': (8, 0), 'I2': (8, 1), 'I3': (8, 2), 'I4': (8, 3), 'I5': (8, 4), 'I6': (8, 5), 'I7': (8, 6), 'I8': (8, 7), 'I9': (8, 8), 'I10': (8, 9), 'I11': (8, 10), 'I12': (8, 11), 'I13': (8, 12), 'I14': (8, 13), 'I15': (8, 14), 'I16': (8, 15), 'I17': (8, 16), 'I18': (8, 17), 'J1': (9, 0), 'J2': (9, 1), 'J3': (9, 2), 'J4': (9, 3), 'J5': (9, 4), 'J6': (9, 5), 'J7': (9, 6), 'J8': (9, 7), 'J9': (9, 8), 'J10': (9, 9), 'J11': (9, 10), 'J12': (9, 11), 'J13': (9, 12), 'J14': (9, 13), 'J15': (9, 14), 'J16': (9, 15), 'J17': (9, 16), 'J18': (9, 17), 'K1': (10, 0), 'K2': (10, 1), 'K3': (10, 2), 'K4': (10, 3), 'K5': (10, 4), 'K6': (10, 5), 'K7': (10, 6), 'K8': (10, 7), 'K9': (10, 8), 'K10': (10, 9), 'K11': (10, 10), 'K12': (10, 11), 'K13': (10, 12), 'K14': (10, 13), 'K15': (10, 14), 'K16': (10, 15), 'K17': (10, 16), 'K18': (10, 17)}\n"
     ]
    }
   ],
   "source": [
    "# assign x,y position to each holds -------------------------------\n",
    "import string\n",
    "\n",
    "# Board columns A–K → indices 0–10\n",
    "cols = list(string.ascii_uppercase[:11])  # A–K\n",
    "# Rows 1–18 → indices 0–17\n",
    "rows = list(range(1, 19))  # 1–18\n",
    "\n",
    "# Generate hold_to_coord dictionary\n",
    "hold_to_coord = {}\n",
    "\n",
    "for x, col in enumerate(cols):\n",
    "    for y, row in enumerate(rows):\n",
    "        hold_name = f\"{col}{row}\"\n",
    "        hold_to_coord[hold_name] = (x, y)\n",
    "\n",
    "print(\"successfully created (x,y) position to each hold:\")\n",
    "print(hold_to_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonBoardDataset(Dataset):\n",
    "    def __init__(self, json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord, max_difficulty=10):\n",
    "        self.hold_to_idx = hold_to_idx\n",
    "        self.grade_to_label = grade_to_label\n",
    "        self.hold_difficulty = hold_difficulty\n",
    "        self.type_to_idx = type_to_idx\n",
    "        self.hold_to_coord = hold_to_coord\n",
    "        self.max_difficulty = max_difficulty\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.raw = [json.loads(line) for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.raw[idx]\n",
    "        holds = item['holds']\n",
    "\n",
    "        hold_idxs = []\n",
    "        diff_values = []\n",
    "        type_vecs = []\n",
    "        xy_coords = []\n",
    "\n",
    "        for h in holds:\n",
    "            hold_idxs.append(self.hold_to_idx[h])\n",
    "\n",
    "            difficulty, types = self.hold_difficulty[h]\n",
    "            diff_values.append(difficulty / self.max_difficulty)\n",
    "\n",
    "            # multi-hot vector\n",
    "            type_vec = torch.zeros(len(self.type_to_idx), dtype=torch.float)\n",
    "            for t in types:\n",
    "                if t in self.type_to_idx:\n",
    "                    type_vec[self.type_to_idx[t]] = 1.0\n",
    "            type_vecs.append(type_vec)\n",
    "\n",
    "            # normalized (x, y)\n",
    "            x, y = self.hold_to_coord[h]\n",
    "            xy_coords.append(torch.tensor([x / 10.0, y / 17.0], dtype=torch.float))\n",
    "\n",
    "        return {\n",
    "            \"indices\": torch.tensor(hold_idxs, dtype=torch.long),\n",
    "            \"difficulty\": torch.tensor(diff_values, dtype=torch.float),\n",
    "            \"type\": torch.stack(type_vecs),       # (N, T)\n",
    "            \"xy\": torch.stack(xy_coords)          # (N, 2)\n",
    "        }, torch.tensor(self.grade_to_label[item['grade']], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f372766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Model with XY -----------------------------------------------\n",
    "\n",
    "# --- set transformer model ---\n",
    "class SetTransformerClassifierXY(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_in=64, dim_hidden=128, num_heads=4, num_inds=16, num_classes=8, type_vec_dim=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_in)  # hold embedding\n",
    "        input_dim = dim_in + 1 + type_vec_dim + 2  # +1 for difficulty, +2 for (x, y)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            ISAB(input_dim, dim_hidden, num_heads, num_inds, ln=True),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=True),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            PMA(dim_hidden, num_heads, 1, ln=True),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(dim_hidden, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hold_idx, difficulty, type_tensor, xy_tensor = inputs  # shapes: (B, N), (B, N), (B, N, T), (B, N, 2)\n",
    "        x_embed = self.embedding(hold_idx)              # (B, N, dim_in)\n",
    "        difficulty = difficulty.unsqueeze(-1)           # (B, N, 1)\n",
    "        x = torch.cat([x_embed, difficulty, type_tensor, xy_tensor], dim=-1)  # (B, N, D+1+T+2)\n",
    "        x_enc = self.encoder(x)\n",
    "        return self.decoder(x_enc)\n",
    "\n",
    "# revised set transformer, embedding for type + XY\n",
    "class SetTransformerClassifierXYAdditive(nn.Module):\n",
    "    def __init__(self, vocab_size, feat_dim=64, dim_hidden=128, num_heads=4, num_inds=16, num_classes=8, type_vec_dim=10):\n",
    "        super().__init__()\n",
    "        # (1) hold ID → embedding\n",
    "        self.hold_emb   = nn.Embedding(vocab_size, feat_dim)          # (B,N,feat_dim)\n",
    "        # (2) difficulty (scalar) → linear projection to feat_dim\n",
    "        self.diff_proj  = nn.Linear(1, feat_dim)                      # (B,N,1) → (B,N,feat_dim)\n",
    "        # (3) type (multi-hot length T) → embedding matrix, then matmul\n",
    "        self.type_emb   = nn.Parameter(torch.randn(type_vec_dim, feat_dim))  # (T,feat_dim)\n",
    "        # (4) XY (2-dim) → MLP to feat_dim\n",
    "        self.xy_mlp     = nn.Sequential(\n",
    "                nn.Linear(2, feat_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(feat_dim, feat_dim)\n",
    "            )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            ISAB(feat_dim, dim_hidden, num_heads, num_inds, ln=True),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=True),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            PMA(dim_hidden, num_heads, 1, ln=True),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(dim_hidden, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hold_idx, difficulty, type_tensor, xy_tensor = inputs\n",
    "        # shapes: (B,N)  (B,N)  (B,N,T)  (B,N,2)\n",
    "        # hold ID\n",
    "        h  = self.hold_emb(hold_idx)                       # (B,N,feat_dim)\n",
    "        # hold difficulty\n",
    "        d  = difficulty.unsqueeze(-1)                     # (B,N,1)\n",
    "        d  = self.diff_proj(d)                            # (B,N,feat_dim)\n",
    "        # hold types\n",
    "        t  = type_tensor @ self.type_emb                  # (B,N,feat_dim)\n",
    "        # XY position\n",
    "        xy = self.xy_mlp(xy_tensor)                       # (B,N,feat_dim)\n",
    "        # element-wise addition\n",
    "        # TODO: 重みを入れてみる 学習可能な重みを掛け算して\n",
    "        x = h + d + t + xy                                # (B,N,feat_dim)\n",
    "        x_enc = self.encoder(x)\n",
    "        return self.decoder(x_enc)\n",
    "    \n",
    "# --- deepset model ---\n",
    "class DeepSetClassifierXY(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_in=64, dim_hidden=128, num_classes=8, type_vec_dim=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_in)\n",
    "        input_dim = dim_in + 1 + type_vec_dim + 2  # hold_emb + difficulty + type_vec + (x, y)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hold_idx, difficulty, type_tensor, xy_tensor = inputs  # (B, N), (B, N), (B, N, T), (B, N, 2)\n",
    "\n",
    "        x_embed = self.embedding(hold_idx)             # (B, N, dim_in)\n",
    "        difficulty = difficulty.unsqueeze(-1)          # (B, N, 1)\n",
    "\n",
    "        x = torch.cat([x_embed, difficulty, type_tensor, xy_tensor], dim=-1)  # (B, N, total_input_dim)\n",
    "        x = self.encoder(x)                            # (B, N, hidden_dim)\n",
    "        x = x.mean(dim=1)                              # (B, hidden_dim)\n",
    "        return self.decoder(x)                         # (B, num_classes)\n",
    "\n",
    "\n",
    "# Classifier Model no XY-----------------------------------------------\n",
    "\n",
    "# --- set transformer model ---\n",
    "class SetTransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_in=64, dim_hidden=128, num_heads=4, num_inds=16, num_classes=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, dim_in)\n",
    "        self.encoder = nn.Sequential(\n",
    "            ISAB(dim_in, dim_hidden, num_heads, num_inds, ln=True),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=True),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            PMA(dim_hidden, num_heads, 1, ln=True),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(dim_hidden, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, dim_in)\n",
    "        x = self.embedding(x)\n",
    "        x_enc = self.encoder(x)\n",
    "        return self.decoder(x_enc)\n",
    "    \n",
    "# --- deepset model ---\n",
    "class DeepSetClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_in=64, dim_hidden=128, num_classes=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, dim_in)  # Embed hold indices\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(dim_in, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim_hidden, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, hold_idx):  # hold_idx: (B, N)\n",
    "        x = self.embedding(hold_idx)   # (B, N, dim_in)\n",
    "        x = self.encoder(x)            # (B, N, hidden)\n",
    "        x = x.mean(dim=1)              # (B, hidden)\n",
    "        out = self.decoder(x)          # (B, num_classes)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b750533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print the model structure -----------------------------------------------\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = SetTransformerClassifier().to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66965b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop ------------------------------------------------\n",
    "\n",
    "# --- Set Hyperparameters ---\n",
    "json_path = './cleaned_moonboard2024_grouped.json'\n",
    "embed_dim = 64\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- Collate Function Factory ---\n",
    "def make_collate_fn(model_type):\n",
    "    def collate_fn(batch):\n",
    "        X_indices = [x[\"indices\"] for x, _ in batch]\n",
    "        X_difficulty = [x[\"difficulty\"] for x, _ in batch]\n",
    "        X_type = [x[\"type\"] for x, _ in batch]\n",
    "        y_batch = [y for _, y in batch]\n",
    "\n",
    "        X_indices = pad_sequence(X_indices, batch_first=True)\n",
    "        X_difficulty = pad_sequence(X_difficulty, batch_first=True)\n",
    "        X_type = pad_sequence(X_type, batch_first=True)\n",
    "        y_tensor = torch.stack(y_batch)\n",
    "\n",
    "        if model_type in [\"set_transformer_xy\", \"deepset_xy\", \"set_transformer_additive\"]:\n",
    "            X_xy = [x[\"xy\"] for x, _ in batch]\n",
    "            X_xy = pad_sequence(X_xy, batch_first=True)\n",
    "            return (X_indices, X_difficulty, X_type, X_xy), y_tensor\n",
    "        else:\n",
    "            return (X_indices,), y_tensor\n",
    "    return collate_fn\n",
    "\n",
    "# --- Dataset Loader ---\n",
    "def load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord):\n",
    "    return MoonBoardDataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "\n",
    "# --- DataLoader Preparation ---\n",
    "def prepare_dataloaders(dataset, grade_to_label, batch_size, collate_fn):\n",
    "    targets = [grade_to_label[item['grade']] for item in dataset.raw]\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(targets), y=targets)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(dataset))), test_size=0.2, stratify=targets, random_state=42\n",
    "    )\n",
    "\n",
    "    train_data = Subset(dataset, train_idx)\n",
    "    val_data = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, class_weights, train_idx, val_idx\n",
    "\n",
    "# --- Training ---\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            X = tuple(x.to(device) for x in X)\n",
    "            y = y.to(device)\n",
    "            if len(X) == 1:\n",
    "                logits = model(X[0])\n",
    "            else:\n",
    "                logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch:02d} — loss: {total_loss / len(train_loader):.4f}\")\n",
    "    return model\n",
    "\n",
    "# --- Main Per Model ---\n",
    "def main(model_type):\n",
    "    vocab_size = len(hold_to_idx)\n",
    "    num_classes = len(grade_to_label) - 1\n",
    "    type_vec_dim = len(type_to_idx)\n",
    "\n",
    "    if model_type == \"set_transformer_xy\":\n",
    "        ModelClass = SetTransformerClassifierXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == \"set_transformer_additive\":\n",
    "        ModelClass = SetTransformerClassifierXYAdditive\n",
    "        kwargs = dict(vocab_size=vocab_size, feat_dim=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == \"set_transformer\":\n",
    "        ModelClass = SetTransformerClassifier\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    elif model_type == \"deepset_xy\":\n",
    "        ModelClass = DeepSetClassifierXY\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes, type_vec_dim=type_vec_dim)\n",
    "    elif model_type == \"deepset\":\n",
    "        ModelClass = DeepSetClassifier\n",
    "        kwargs = dict(vocab_size=vocab_size, dim_in=embed_dim, num_classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "    collate_fn = make_collate_fn(model_type)    \n",
    "    dataset = load_dataset(json_path, hold_to_idx, grade_to_label, hold_difficulty, type_to_idx, hold_to_coord)\n",
    "    train_loader, val_loader, class_weights, train_idx, val_idx = prepare_dataloaders(dataset, grade_to_label, batch_size, collate_fn)\n",
    "\n",
    "    model = ModelClass(**kwargs).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, epochs)\n",
    "    return train_loader, val_loader, model, dataset, train_idx, val_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f5db06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training set_transformer =====\n",
      "Epoch 01 — loss: 1.7684\n",
      "Epoch 02 — loss: 1.6267\n",
      "Epoch 03 — loss: 1.5707\n",
      "Epoch 04 — loss: 1.5465\n",
      "Epoch 05 — loss: 1.4985\n",
      "Epoch 06 — loss: 1.4795\n",
      "Epoch 07 — loss: 1.4495\n",
      "Epoch 08 — loss: 1.4123\n",
      "Epoch 09 — loss: 1.3931\n",
      "Epoch 10 — loss: 1.3523\n",
      "Epoch 11 — loss: 1.3159\n",
      "Epoch 12 — loss: 1.2860\n",
      "Epoch 13 — loss: 1.2599\n",
      "Epoch 14 — loss: 1.2258\n",
      "Epoch 15 — loss: 1.1805\n",
      "Epoch 16 — loss: 1.1443\n",
      "Epoch 17 — loss: 1.1080\n",
      "Epoch 18 — loss: 1.0785\n",
      "Epoch 19 — loss: 1.0402\n",
      "Epoch 20 — loss: 0.9982\n",
      "Confusion matrix for set_transformer saved and inserted into model_comparison_results.xlsx (sheet: set_transformer)\n",
      "Predictions for set_transformer_preds exported to: model_comparison_results.xlsx\n",
      "\n",
      "===== Training deepset =====\n",
      "Epoch 01 — loss: 1.8989\n",
      "Epoch 02 — loss: 1.6403\n",
      "Epoch 03 — loss: 1.5553\n",
      "Epoch 04 — loss: 1.5261\n",
      "Epoch 05 — loss: 1.5077\n",
      "Epoch 06 — loss: 1.4910\n",
      "Epoch 07 — loss: 1.4939\n",
      "Epoch 08 — loss: 1.4763\n",
      "Epoch 09 — loss: 1.4769\n",
      "Epoch 10 — loss: 1.4725\n",
      "Epoch 11 — loss: 1.4723\n",
      "Epoch 12 — loss: 1.4679\n",
      "Epoch 13 — loss: 1.4584\n",
      "Epoch 14 — loss: 1.4610\n",
      "Epoch 15 — loss: 1.4561\n",
      "Epoch 16 — loss: 1.4534\n",
      "Epoch 17 — loss: 1.4439\n",
      "Epoch 18 — loss: 1.4491\n",
      "Epoch 19 — loss: 1.4393\n",
      "Epoch 20 — loss: 1.4426\n",
      "Confusion matrix for deepset saved and inserted into model_comparison_results.xlsx (sheet: deepset)\n",
      "Predictions for deepset_preds exported to: model_comparison_results.xlsx\n",
      "\n",
      "===== Training set_transformer_xy =====\n",
      "Epoch 01 — loss: 1.7321\n",
      "Epoch 02 — loss: 1.5990\n",
      "Epoch 03 — loss: 1.5511\n",
      "Epoch 04 — loss: 1.5236\n",
      "Epoch 05 — loss: 1.4891\n",
      "Epoch 06 — loss: 1.4666\n",
      "Epoch 07 — loss: 1.4320\n",
      "Epoch 08 — loss: 1.4062\n",
      "Epoch 09 — loss: 1.3707\n",
      "Epoch 10 — loss: 1.3567\n",
      "Epoch 11 — loss: 1.3135\n",
      "Epoch 12 — loss: 1.2757\n",
      "Epoch 13 — loss: 1.2493\n",
      "Epoch 14 — loss: 1.2100\n",
      "Epoch 15 — loss: 1.1798\n",
      "Epoch 16 — loss: 1.1452\n",
      "Epoch 17 — loss: 1.1010\n",
      "Epoch 18 — loss: 1.0677\n",
      "Epoch 19 — loss: 1.0333\n",
      "Epoch 20 — loss: 0.9989\n",
      "Confusion matrix for set_transformer_xy saved and inserted into model_comparison_results.xlsx (sheet: set_transformer_xy)\n",
      "Predictions for set_transformer_xy_preds exported to: model_comparison_results.xlsx\n",
      "\n",
      "===== Training deepset_xy =====\n",
      "Epoch 01 — loss: 1.9001\n",
      "Epoch 02 — loss: 1.6215\n",
      "Epoch 03 — loss: 1.5491\n",
      "Epoch 04 — loss: 1.5358\n",
      "Epoch 05 — loss: 1.5146\n",
      "Epoch 06 — loss: 1.5059\n",
      "Epoch 07 — loss: 1.4952\n",
      "Epoch 08 — loss: 1.4952\n",
      "Epoch 09 — loss: 1.4797\n",
      "Epoch 10 — loss: 1.4796\n",
      "Epoch 11 — loss: 1.4766\n",
      "Epoch 12 — loss: 1.4725\n",
      "Epoch 13 — loss: 1.4585\n",
      "Epoch 14 — loss: 1.4575\n",
      "Epoch 15 — loss: 1.4579\n",
      "Epoch 16 — loss: 1.4517\n",
      "Epoch 17 — loss: 1.4566\n",
      "Epoch 18 — loss: 1.4536\n",
      "Epoch 19 — loss: 1.4482\n",
      "Epoch 20 — loss: 1.4483\n",
      "Confusion matrix for deepset_xy saved and inserted into model_comparison_results.xlsx (sheet: deepset_xy)\n",
      "Predictions for deepset_xy_preds exported to: model_comparison_results.xlsx\n",
      "\n",
      "===== Training set_transformer_additive =====\n",
      "Epoch 01 — loss: 1.7455\n",
      "Epoch 02 — loss: 1.6398\n",
      "Epoch 03 — loss: 1.5957\n",
      "Epoch 04 — loss: 1.5680\n",
      "Epoch 05 — loss: 1.5432\n",
      "Epoch 06 — loss: 1.5269\n",
      "Epoch 07 — loss: 1.5049\n",
      "Epoch 08 — loss: 1.4828\n",
      "Epoch 09 — loss: 1.4644\n",
      "Epoch 10 — loss: 1.4460\n",
      "Epoch 11 — loss: 1.4177\n",
      "Epoch 12 — loss: 1.3929\n",
      "Epoch 13 — loss: 1.3822\n",
      "Epoch 14 — loss: 1.3412\n",
      "Epoch 15 — loss: 1.3212\n",
      "Epoch 16 — loss: 1.2950\n",
      "Epoch 17 — loss: 1.2731\n",
      "Epoch 18 — loss: 1.2488\n",
      "Epoch 19 — loss: 1.2149\n",
      "Epoch 20 — loss: 1.1952\n",
      "Confusion matrix for set_transformer_additive saved and inserted into model_comparison_results.xlsx (sheet: set_transformer_additive)\n",
      "Predictions for set_transformer_additive_preds exported to: model_comparison_results.xlsx\n",
      "\n",
      "=== Model Comparison Summary ===\n",
      "                 Model Type  Strict Accuracy (%)  ±1 Grade Accuracy (%)\n",
      "0           set_transformer                45.24                  81.33\n",
      "1                   deepset                43.70                  80.03\n",
      "2        set_transformer_xy                43.79                  81.67\n",
      "3                deepset_xy                44.66                  80.13\n",
      "4  set_transformer_additive                44.56                  81.09\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.drawing.image import Image as XLImage\n",
    "\n",
    "# --- plot confusion matrix and save to excel---\n",
    "def save_confusion_matrix_to_excel(y_true, y_pred, class_labels, model_name, excel_path):\n",
    "    # Plot confusion matrix and save as image\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(class_labels)), normalize='true')\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "    plt.xlabel(\"Predicted Grade\")\n",
    "    plt.ylabel(\"Actual Grade\")\n",
    "    plt.tight_layout()\n",
    "    img_path = f\"confusion_{model_name}.png\"\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Insert image into Excel (new sheet per model)\n",
    "    wb = load_workbook(excel_path)\n",
    "    if model_name in wb.sheetnames:\n",
    "        ws = wb[model_name]\n",
    "    else:\n",
    "        ws = wb.create_sheet(title=model_name)\n",
    "    img = XLImage(img_path)\n",
    "    ws.add_image(img, \"A1\")\n",
    "    wb.save(excel_path)\n",
    "    print(f\"Confusion matrix for {model_name} saved and inserted into {excel_path} (sheet: {model_name})\")\n",
    "\n",
    "\n",
    "# --- export the predictions to excel ---\n",
    "def export_predictions_to_excel(model, dataloader, device, grade_to_label, excel_path, sheet_name):\n",
    "    results = []\n",
    "    raw_dataset = dataloader.dataset.dataset  # MoonBoardDataset\n",
    "    indices = dataloader.dataset.indices      # Subset indices\n",
    "    label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "    current_index = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if isinstance(X, tuple):\n",
    "                X = tuple(x.to(device) for x in X)\n",
    "                if len(X) == 1:\n",
    "                    preds = model(X[0]).argmax(dim=1)\n",
    "                else:\n",
    "                    preds = model(X).argmax(dim=1)\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "                preds = model(X).argmax(dim=1)\n",
    "            y = y.to(device)\n",
    "            for i in range(y.size(0)):\n",
    "                real_label = y[i].item()\n",
    "                pred_label = preds[i].item()\n",
    "                dataset_index = indices[current_index]\n",
    "                current_index += 1\n",
    "                raw_item = raw_dataset.raw[dataset_index]\n",
    "                problem_name = raw_item.get('problem_name', f\"problem_{dataset_index}\")\n",
    "                results.append({\n",
    "                    \"problem_name\": problem_name,\n",
    "                    \"y_true\": label_to_grade.get(real_label, f\"Unknown({real_label})\"),\n",
    "                    \"y_pred\": label_to_grade.get(pred_label, f\"Unknown({pred_label})\"),\n",
    "                    \"diff\": real_label - pred_label\n",
    "                })\n",
    "    df = pd.DataFrame(results)\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Predictions for {sheet_name} exported to: {excel_path}\")\n",
    "\n",
    "# --- Compare Models ---\n",
    "def compare_models():\n",
    "    model_types = [\n",
    "        \"set_transformer\",\n",
    "        \"deepset\",\n",
    "        \"set_transformer_xy\",\n",
    "        \"deepset_xy\",\n",
    "        \"set_transformer_additive\"\n",
    "    ]\n",
    "    results = []\n",
    "    excel_path = \"model_comparison_results.xlsx\"\n",
    "    class_labels = [f\"V{i}\" for i in range(4, 12)]\n",
    "\n",
    "    for idx, mtype in enumerate(model_types):\n",
    "        print(f\"\\n===== Training {mtype} =====\")\n",
    "        train_loader, val_loader, model, dataset, train_idx, val_idx = main(mtype)\n",
    "\n",
    "        strict_correct, loose_correct, total = 0, 0, 0\n",
    "        y_true, y_pred = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X = tuple(x.to(device) for x in X)\n",
    "                y = y.to(device)\n",
    "                if len(X) == 1:\n",
    "                    preds = model(X[0]).argmax(dim=1)\n",
    "                else:\n",
    "                    preds = model(X).argmax(dim=1)\n",
    "                total += y.size(0)\n",
    "                strict_correct += (preds == y).sum().item()\n",
    "                loose_correct += ((preds - y).abs() <= 1).sum().item()\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "        results.append({\n",
    "            \"Model Type\": mtype,\n",
    "            \"Strict Accuracy (%)\": round(100.0 * strict_correct / total, 2),\n",
    "            \"±1 Grade Accuracy (%)\": round(100.0 * loose_correct / total, 2)\n",
    "        })\n",
    "\n",
    "        # Save summary table on first iteration (so Excel file exists)\n",
    "        if idx == 0:\n",
    "            df_results = pd.DataFrame(results)\n",
    "            df_results.to_excel(excel_path, index=False)\n",
    "\n",
    "        # Save confusion matrix to Excel\n",
    "        save_confusion_matrix_to_excel(y_true, y_pred, class_labels, mtype, excel_path)\n",
    "\n",
    "        # Export predictions to Excel (new sheet per model)\n",
    "        export_predictions_to_excel(model, val_loader, device, grade_to_label, excel_path, sheet_name=f\"{mtype}_preds\")\n",
    "\n",
    "    # Save summary table again at the end (with all models)\n",
    "    df_results = pd.DataFrame(results)\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "        df_results.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    print(\"\\n=== Model Comparison Summary ===\")\n",
    "    print(df_results)\n",
    "\n",
    "# usage\n",
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "092f8281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 — loss: 1.7371\n",
      "Epoch 02 — loss: 1.6441\n",
      "Epoch 03 — loss: 1.6020\n",
      "Epoch 04 — loss: 1.5706\n",
      "Epoch 05 — loss: 1.5514\n",
      "Epoch 06 — loss: 1.5267\n",
      "Epoch 07 — loss: 1.5120\n",
      "Epoch 08 — loss: 1.4864\n",
      "Epoch 09 — loss: 1.4807\n",
      "Epoch 10 — loss: 1.4560\n",
      "Epoch 11 — loss: 1.4317\n",
      "Epoch 12 — loss: 1.4027\n",
      "Epoch 13 — loss: 1.3757\n",
      "Epoch 14 — loss: 1.3672\n",
      "Epoch 15 — loss: 1.3323\n",
      "\n",
      "=== MoonBoard Problem Evaluation ===\n",
      "\n",
      "🔹 Problem Name   : PHYSICAL Q\n",
      "   Holds Used     : {'I18': 3, 'J12': 8, 'F13': 5, 'D10': 4, 'E6': 3, 'J2': 5}\n",
      "   Setter Grade   : V9\n",
      "   Predicted Grade: V7\n",
      "   Dataset Split  : Train\n",
      "\n",
      "🔹 Problem Name   : TRIANGULATION Q\n",
      "   Holds Used     : {'A18': 5, 'J13': 4, 'D16': 4, 'E9': 3, 'I4': 4}\n",
      "   Setter Grade   : V7\n",
      "   Predicted Grade: V5\n",
      "   Dataset Split  : Train\n",
      "\n",
      "🔹 Problem Name   : WARMUP CRIMPS Q\n",
      "   Holds Used     : {'I18': 3, 'I7': 6, 'I9': 7, 'I15': 8, 'G11': 7, 'J14': 6, 'J12': 8, 'H4': 4, 'K6': 6}\n",
      "   Setter Grade   : V4\n",
      "   Predicted Grade: V7\n",
      "   Dataset Split  : Train\n",
      "\n",
      "🔹 Problem Name   : RONANI QD\n",
      "   Holds Used     : {'F18': 4, 'I15': 8, 'I10': 8, 'K9': 6, 'K6': 6, 'G14': 6, 'D16': 4, 'E9': 3, 'E4': 4, 'H5': 3}\n",
      "   Setter Grade   : V5\n",
      "   Predicted Grade: V5\n",
      "   Dataset Split  : Train\n",
      "\n",
      "🔹 Problem Name   : Don't Fart Alan\n",
      "   Holds Used     : {'K18': 5, 'J15': 3, 'F14': 7, 'F13': 5, 'D10': 4, 'E6': 3, 'I7': 6, 'I5': 7, 'F1': 3}\n",
      "   Setter Grade   : Unknown\n",
      "   Predicted Grade: V8\n",
      "   Dataset Split  : Not Found\n",
      "\n",
      "🔹 Problem Name   : FINALE MAXI 2025 POCKET 2 V9\n",
      "   Holds Used     : {'G3': 5, 'F3': 7, 'F4': 8, 'A6': 3, 'A11': 4, 'B17': 10, 'C9': 6, 'D17': 9, 'H18': 4}\n",
      "   Setter Grade   : Unknown\n",
      "   Predicted Grade: V9\n",
      "   Dataset Split  : Not Found\n",
      "\n",
      "🔹 Problem Name   : Khai's V7\n",
      "   Holds Used     : {'D18': 9, 'A15': 4, 'A12': 3, 'C9': 6, 'E7': 9, 'H8': 8, 'I6': 4, 'E1': 3}\n",
      "   Setter Grade   : Unknown\n",
      "   Predicted Grade: V7\n",
      "   Dataset Split  : Not Found\n",
      "\n",
      "🔹 Problem Name   : YUMS IN MY TUMS 9\n",
      "   Holds Used     : {'F18': 4, 'G12': 3, 'E1': 3, 'D13': 9, 'I9': 7, 'F8': 7, 'I2': 4, 'F16': 4, 'E4': 4, 'E6': 3}\n",
      "   Setter Grade   : V5\n",
      "   Predicted Grade: V7\n",
      "   Dataset Split  : Train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate problems\n",
    "def evaluate_problems(\n",
    "    model, problem_dict, hold_to_idx, hold_difficulty, type_to_idx, device,\n",
    "    grade_to_label, hold_to_coord, dataset, train_idx, val_idx, model_type\n",
    "):\n",
    "    label_to_grade = {v: k for k, v in grade_to_label.items()}\n",
    "    print(\"\\n=== MoonBoard Problem Evaluation ===\\n\")\n",
    "\n",
    "    for fallback_name, holds in problem_dict.items():\n",
    "        try:\n",
    "            hold_idxs = []\n",
    "            diff_values = []\n",
    "            type_vecs = []\n",
    "            xy_coords = []\n",
    "\n",
    "            for h in holds:\n",
    "                if h not in hold_difficulty or h not in hold_to_idx or h not in hold_to_coord:\n",
    "                    raise ValueError(f\"[ERROR] Hold '{h}' missing from required dictionaries.\")\n",
    "\n",
    "                hold_idxs.append(hold_to_idx[h])\n",
    "                difficulty, types = hold_difficulty[h]\n",
    "                diff_values.append(difficulty / 10.0)\n",
    "\n",
    "                # Multi-hot vector\n",
    "                type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "                for t in types:\n",
    "                    if t in type_to_idx:\n",
    "                        type_vec[type_to_idx[t]] = 1.0\n",
    "                type_vecs.append(type_vec)\n",
    "\n",
    "                xy_coords.append(torch.tensor([hold_to_coord[h][0] / 10.0, hold_to_coord[h][1] / 17.0], dtype=torch.float))\n",
    "\n",
    "            # Convert to tensors\n",
    "            hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)\n",
    "            difficulty_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)\n",
    "            type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)\n",
    "            xy_tensor = torch.stack(xy_coords).unsqueeze(0).to(device)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # --- Select input format based on model_type ---\n",
    "                if model_type in [\"set_transformer_xy\", \"set_transformer_additive\", \"deepset_xy\"]:\n",
    "                    input_data = (hold_tensor, difficulty_tensor, type_tensor, xy_tensor)\n",
    "                elif model_type == \"set_transformer\":\n",
    "                    input_data = (hold_tensor,)\n",
    "                elif model_type == \"deepset\":\n",
    "                    input_data = (hold_tensor,)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "                logits = model(input_data)\n",
    "                pred_label = logits.argmax(dim=1).item()\n",
    "                pred_grade = label_to_grade.get(pred_label, f\"Unknown({pred_label})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{fallback_name}] Skipping due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Search in dataset for match\n",
    "        found_idx = None\n",
    "        split = \"Not Found\"\n",
    "        setter_grade = \"Unknown\"\n",
    "        problem_name = fallback_name\n",
    "\n",
    "        for idx, item in enumerate(dataset.raw):\n",
    "            if set(item['holds']) == set(holds):\n",
    "                found_idx = idx\n",
    "                setter_grade = item.get('grade', 'Unknown')\n",
    "                problem_name = item.get('problem_name', fallback_name)\n",
    "                if found_idx in train_idx:\n",
    "                    split = \"Train\"\n",
    "                elif found_idx in val_idx:\n",
    "                    split = \"Validation\"\n",
    "                else:\n",
    "                    split = \"Found (Unknown Split)\"\n",
    "                break\n",
    "\n",
    "        holds_with_difficulty = {h: hold_difficulty[h][0] if h in hold_difficulty else \"N/A\" for h in holds}\n",
    "        print(f\"🔹 Problem Name   : {problem_name}\")\n",
    "        print(f\"   Holds Used     : {holds_with_difficulty}\")\n",
    "        print(f\"   Setter Grade   : {setter_grade}\")\n",
    "        print(f\"   Predicted Grade: {pred_grade}\")\n",
    "        print(f\"   Dataset Split  : {split}\\n\")\n",
    "\n",
    "named_problems = {\n",
    "    \"Physical V9 Benchmark\": [\"I18\", \"J12\", \"F13\", \"D10\", \"E6\", \"J2\"],\n",
    "    \"Triangulation V7\": [\"A18\", \"J13\", \"D16\", \"E9\", \"E9\", \"I4\"],\n",
    "    \"warmup crimps\": [\"I18\", \"I7\", \"I9\", \"I15\", \"G11\", \"J14\", \"J12\", \"I15\", \"J14\", \"H4\", \"K6\"],\n",
    "    \"Ronani V5\": [\"F18\", \"I15\", \"I10\", \"K9\", \"K6\", \"G14\", \"D16\", \"E9\", \"K6\", \"I15\", \"E4\", \"H5\"],\n",
    "    \"Don't Fart Alan\": [\"K18\", \"J15\", \"F14\", \"F13\", \"D10\", \"E6\", \"I7\", \"I5\", \"F1\"],\n",
    "    \"FINALE MAXI 2025 POCKET 2 V9\": [\"G3\", \"F3\", \"F4\", \"A6\", \"A11\", \"B17\", \"C9\", \"D17\", \"H18\"],\n",
    "    \"Khai's V7\": [\"D18\", \"A15\", \"A12\", \"C9\", \"E7\", \"H8\", \"I6\", \"E1\"],\n",
    "    \"Yums In My Tums V5\": [\"F18\", \"G12\", \"E1\", \"D13\", \"I9\", \"F8\", \"I2\", \"F16\", \"E4\", \"E6\"],\n",
    "}\n",
    "\n",
    "group1 = {\n",
    "    \"simma mot strommen\": [\"A18\", \"C12\", \"A9\", \"B14\", \"B16\", \"D1\", \"F5\", \"F5\"],\n",
    "    \"MAXIMUS!\": [\"K18\", \"E3\", \"K14\", \"I13\", \"K7\", \"I2\", \"H16\", \"K11\", \"G7\", \"H4\"],\n",
    "    \"interstate\": [\"K18\", \"H17\", \"J11\", \"I9\", \"G13\", \"H15\", \"I5\", \"I6\"],\n",
    "    \"krakatoa pusher\": [\"H18\", \"H11\", \"J8\", \"F7\", \"K15\", \"F4\", \"J3\"],\n",
    "    \"doublement\": [\"A18\", \"E16\", \"F8\", \"B14\", \"G8\", \"E12\", \"F4\", \"F3\", \"F3\"],\n",
    "    \"animal instinct\": [\"F18\", \"J11\", \"F9\", \"H15\", \"E13\", \"J11\", \"I6\", \"F4\"],\n",
    "    \"blue bin day\": [\"B18\", \"C18\", \"A8\", \"C12\", \"B15\", \"A5\", \"C3\"]\n",
    "}\n",
    "\n",
    "train_loader, val_loader, model, dataset, train_idx, val_idx = main(\"set_transformer_additive\")\n",
    "\n",
    "evaluate_problems(\n",
    "    model=model,\n",
    "    problem_dict=named_problems,\n",
    "    hold_to_idx=hold_to_idx,\n",
    "    hold_difficulty=hold_difficulty,\n",
    "    type_to_idx=type_to_idx,\n",
    "    device=device,\n",
    "    grade_to_label=grade_to_label,\n",
    "    hold_to_coord=hold_to_coord,\n",
    "    dataset=dataset,\n",
    "    train_idx=train_idx,\n",
    "    val_idx=val_idx,\n",
    "    model_type=\"set_transformer_xy\"  # or your current model type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228393ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "def visualize_attention_for_problem(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device):\n",
    "    model.eval()\n",
    "\n",
    "    hold_idxs = []\n",
    "    diff_values = []\n",
    "    type_vecs = []\n",
    "    xy_coords = []\n",
    "\n",
    "    for h in holds:\n",
    "        hold_idxs.append(hold_to_idx[h])\n",
    "        difficulty, types = hold_difficulty[h]\n",
    "        diff_values.append(difficulty / 10.0)\n",
    "\n",
    "        type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "        for t in types:\n",
    "            if t in type_to_idx:\n",
    "                type_vec[type_to_idx[t]] = 1.0\n",
    "        type_vecs.append(type_vec)\n",
    "\n",
    "        x, y = hold_to_coord[h]\n",
    "        xy_coords.append([x / 10.0, y / 17.0])\n",
    "\n",
    "    hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    diff_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)\n",
    "    type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)\n",
    "    xy_tensor = torch.tensor(xy_coords, dtype=torch.float).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model((hold_tensor, diff_tensor, type_tensor, xy_tensor))\n",
    "\n",
    "    attn_isab1 = model.encoder[0].mab0.attn_weights.cpu().numpy()\n",
    "    attn_isab2 = model.encoder[1].mab0.attn_weights.cpu().numpy()\n",
    "\n",
    "    num_heads = attn_isab1.shape[0]\n",
    "    fig, axes = plt.subplots(2, num_heads, figsize=(4 * num_heads, 8))\n",
    "    if num_heads == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "\n",
    "    for h in range(num_heads):\n",
    "        sns.heatmap(attn_isab1[h], ax=axes[0, h], cmap=\"viridis\", xticklabels=holds)\n",
    "        axes[0, h].set_title(f\"ISAB1 – Head {h}\")\n",
    "        axes[0, h].set_xlabel(\"Key (Hold)\")\n",
    "        axes[0, h].set_ylabel(\"Seed\")\n",
    "\n",
    "        sns.heatmap(attn_isab2[h], ax=axes[1, h], cmap=\"viridis\", xticklabels=holds)\n",
    "        axes[1, h].set_title(f\"ISAB2 – Head {h}\")\n",
    "        axes[1, h].set_xlabel(\"Key (Hold)\")\n",
    "        axes[1, h].set_ylabel(\"Seed\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66931c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_attention_per_hold(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device):\n",
    "    model.eval()\n",
    "\n",
    "    hold_idxs = []\n",
    "    diff_values = []\n",
    "    type_vecs = []\n",
    "    xy_coords = []\n",
    "\n",
    "    for h in holds:\n",
    "        hold_idxs.append(hold_to_idx[h])\n",
    "        difficulty, types = hold_difficulty[h]\n",
    "        diff_values.append(difficulty / 10.0)\n",
    "\n",
    "        # Multi-hot type vector\n",
    "        type_vec = torch.zeros(len(type_to_idx), dtype=torch.float)\n",
    "        for t in types:\n",
    "            if t in type_to_idx:\n",
    "                type_vec[type_to_idx[t]] = 1.0\n",
    "        type_vecs.append(type_vec)\n",
    "\n",
    "        # XY coordinate\n",
    "        if h not in hold_to_coord:\n",
    "            raise ValueError(f\"[ERROR] Hold '{h}' has no coordinate in hold_to_coord.\")\n",
    "        x, y = hold_to_coord[h]\n",
    "        xy_coords.append([x / 10.0, y / 17.0])\n",
    "\n",
    "    # Build model input tensors\n",
    "    hold_tensor = torch.tensor(hold_idxs, dtype=torch.long).unsqueeze(0).to(device)       # (1, N)\n",
    "    diff_tensor = torch.tensor(diff_values, dtype=torch.float).unsqueeze(0).to(device)    # (1, N)\n",
    "    type_tensor = torch.stack(type_vecs).unsqueeze(0).to(device)                          # (1, N, T)\n",
    "    xy_tensor = torch.tensor(xy_coords, dtype=torch.float).unsqueeze(0).to(device)        # (1, N, 2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model((hold_tensor, diff_tensor, type_tensor, xy_tensor))\n",
    "\n",
    "    attn_weights = model.encoder[0].mab0.attn_weights  # shape: (heads, seeds, holds)\n",
    "    avg_attn = attn_weights.mean(dim=(0, 1)).cpu().numpy()  # average across heads & seeds → (num_holds,)\n",
    "\n",
    "    return list(zip(holds, avg_attn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce93aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention and scores (with XY support)\n",
    "\n",
    "holds = named_problems[\"warmup crimps\"]\n",
    "\n",
    "if not hasattr(model.encoder[0], 'mab0') or not hasattr(model.encoder[0].mab0, 'attn_weights'):\n",
    "    raise ValueError(\"The provided model does not support attention visualization.\")\n",
    "\n",
    "visualize_attention_for_problem(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device)\n",
    "attention_scores = get_avg_attention_per_hold(model, holds, hold_to_idx, hold_difficulty, type_to_idx, hold_to_coord, device)\n",
    "\n",
    "# Print sorted scores\n",
    "attention_scores_sorted = sorted(attention_scores, key=lambda x: x[1], reverse=True)\n",
    "print(\"Average Attention Per Hold (sorted):\")\n",
    "for h, score in attention_scores_sorted:\n",
    "    difficulty = hold_difficulty[h][0] if h in hold_difficulty else \"N/A\"\n",
    "    print(f\"{h}: {score:.4f} (difficulty: {difficulty})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb97c2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sousei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
